{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen-1_8B-chat Intel CPU deployment practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This article describes the process of deploying the Qwen 1.8B model on Intel devices. You need a machine with at least 16GB of memory to complete this task. We will use Intel's BigDL library for inference of large models to complete the entire process.\n",
    "\n",
    "Bigdl-llm is an acceleration library for running LLM (Large Language Model) on Intel devices. It uses INT4/FP4/INT8/FP8 precision quantization and architecture-specific optimization to achieve low resource usage and high-speed inference capabilities of large models on Intel CPUs and GPUs (applicable to any PyTorch model).\n",
    "\n",
    "For the sake of generality, this article only involves CPU-related code. If you want to learn how to deploy large models on Intel GPUs, you can refer to the official website documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment configuration\n",
    "\n",
    "Before we start, we need to prepare bigdl-llm and the related operating environment for subsequent deployment. We recommend that you perform subsequent operations in the python 3.9 environment.\n",
    "\n",
    "If you find that the download speed is too slow, you can try to change the default mirror source: `pip config set global.index-url https://pypi.doubanio.com/simple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --pre --upgrade bigdl-llm[all] \n",
    "%pip install gradio \n",
    "%pip install hf-transfer\n",
    "%pip install transformers_stream_generator einops\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model download\n",
    "\n",
    "First, we use huggingface-cli to obtain the qwen-1.8B model, which takes a long time and requires a while to wait; considering the domestic download restrictions, we add environment variables to speed up the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setting environment variables\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "# Download the model\n",
    "os.system('huggingface-cli download --resume-download qwen/Qwen-1_8B-Chat --local-dir qwen18chat_src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save quantized model\n",
    "\n",
    "In order to achieve low resource consumption inference of large language models, we first need to quantize the model to int4 precision, and then serialize and save it in the corresponding local folder for repeated loading and inference; using the `save_low_bit` api, we can easily achieve this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from transformers import  AutoTokenizer\n",
    "import os\n",
    "if __name__ == '__main__':\n",
    "    model_path = os.path.join(os.getcwd(),\"qwen18chat_src\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, load_in_low_bit='sym_int4', trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model.save_low_bit('qwen18chat_int4')\n",
    "    tokenizer.save_pretrained('qwen18chat_int4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading quantized model\n",
    "\n",
    "After saving the int4 model file, we can load it into memory for further reasoning; if you cannot export the quantized model on your local machine, you can also save the model on a machine with larger memory and then transfer it to a small memory end device for running. Most common home PCs can meet the resource requirements for the actual operation of the int4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "QWEN_PROMPT_FORMAT = \"<human>{prompt} <bot>\"\n",
    "load_path = \"qwen18chat_int4\"\n",
    "model = AutoModelForCausalLM.load_low_bit(load_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path, trust_remote_code=True)\n",
    "\n",
    "input_str = \"给我讲一个年轻人奋斗创业最终取得成功的故事\"\n",
    "with torch.inference_mode():\n",
    "    prompt = QWEN_PROMPT_FORMAT.format(prompt=input_str)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    st = time.time()\n",
    "    output = model.generate(input_ids,\n",
    "                            max_new_tokens=512)\n",
    "    end = time.time()\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f'Inference time: {end-st} s')\n",
    "    print('-'*20, 'Prompt', '-'*20)\n",
    "    print(prompt)\n",
    "    print('-'*20, 'Output', '-'*20)\n",
    "    print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradio-demo experience\n",
    "\n",
    "In order to get a better multi-round conversation experience, a simple `gradio` demo interface is provided here for easy debugging. You can modify the built-in `system` information and even fine-tune the model to make the local model closer to the large model requirements you envision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 18:55:07,928 - WARNING - Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "2023-12-19 18:55:07,928 - WARNING - Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "2023-12-19 18:55:07,929 - WARNING - Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "2023-12-19 18:55:08,065 - INFO - Converting the current model to sym_int4 format......\n",
      "2023-12-19 18:55:09,487 - INFO - HTTP Request: GET http://127.0.0.1:7862/startup-events \"HTTP/1.1 200 OK\"\n",
      "2023-12-19 18:55:09,496 - INFO - HTTP Request: HEAD http://127.0.0.1:7862/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 18:55:10,170 - INFO - HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 OK\"\n",
      "2023-12-19 18:55:10,207 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2023-12-19 18:55:10,296 - INFO - HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 OK\"\n",
      "2023-12-19 18:55:11,111 - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ \"HTTP/1.1 200 OK\"\n",
      "2023-12-19 18:55:11,246 - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "from bigdl.llm.transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "QWEN_PROMPT_FORMAT = \"<human>{prompt} <bot>\"\n",
    "\n",
    "load_path = \"qwen18chat_int4\"\n",
    "model = AutoModelForCausalLM.load_low_bit(load_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path,trust_remote_code=True)\n",
    "\n",
    "def add_text(history, text):\n",
    "    _, history = model.chat(tokenizer, text, history=history)\n",
    "    return history, gr.Textbox(value=\"\", interactive=False)\n",
    "\n",
    "def bot(history):\n",
    "    response =  history[-1][1]\n",
    "    history[-1][1] = \"\"\n",
    "    for character in response:\n",
    "        history[-1][1] += character\n",
    "        time.sleep(0.05)\n",
    "        yield history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(\n",
    "        [], \n",
    "        elem_id=\"chatbot\",\n",
    "        bubble_full_width=False,\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(\n",
    "            scale=4,\n",
    "            show_label=False,\n",
    "            placeholder=\"Enter text and press enter\",\n",
    "            container=False,\n",
    "        )\n",
    "\n",
    "    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(\n",
    "        bot, chatbot, chatbot, api_name=\"bot_response\"\n",
    "    )\n",
    "    txt_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Intel's large language model inference framework, we can achieve high-performance inference of large models on Intel devices. Only 2G memory is required to achieve smooth conversation with the local large model. Let's experience it together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdltest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
