{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# Chapter 4 Model Chain\n",
    "\n",
    "- [1. Set OpenAI API Key](#1. Set OpenAI-API-Key)\n",
    "- [2. Large Language Model Chain](#2. Large Language Model Chain)\n",
    "- [2.1 Import Data](#2.1-Import Data)\n",
    "- [2.2 Initialize Language Model](#2.2-Initialize Language Model)\n",
    "- [2.3 Initialize Prompt Template](#2.3-Initialize Prompt Template)\n",
    "- [2.4 Build Large Language Model Chain](#2.4-Build Large Language Model Chain)\n",
    "- [2.5 Run Large Language Model Chain](#2.5-Run Large Language Model Chain)\n",
    "- [2.6 Chinese Prompt](#2.6-Chinese Prompt)\n",
    "- [3. Sequential Chain](#3. Sequential Chain)\n",
    "- [3.1 Simple Sequential Chain](#3.1-Simple Sequential Chain)\n",
    "- [3.1.1 Create Two Subchains](#3.1.1-Create Two Subchains)\n",
    "- [3.1.2 Build a simple sequential chain](#3.1.2-Build a simple sequential chain)\n",
    "- [3.1.3 Run a simple sequential chain](#3.1.3-Run a simple sequential chain)\n",
    "- [3.1.4 Chinese prompts](#3.1.4-Chinese prompts)- [3.2 Sequence chain](#3.2-Sequence chain)\n",
    "- [3.2.1 Create four subchains](#3.2.1-Create four subchains)\n",
    "- [3.2.2 Combine four subchains](#3.2.2-Combining four subchains)\n",
    "- [3.2.3 Chinese prompts](#3.2.3-Chinese prompts)\n",
    "- [IV. Routing chain](#IV.-Routing chain)\n",
    "- [4.1 Define prompt template](#4.1-Define prompt template)\n",
    "- [4.2 Name and describe prompt template](#4.2-Name and describe prompt template)\n",
    "- [4.3 Create corresponding target chain based on prompt template information](#4.3-Create corresponding target chain based on prompt template information--)\n",
    "- [4.4 Create default target chain](#4.4-Create default target chain)\n",
    "- [4.5 Define routing templates between different chains](#4.5-Define routing templates between different chains)\n",
    "- [4.6 Build routing chains](#4.6-Build routing chains)\n",
    "- [4.7 Create overall links](#4.7-Create overall links)\n",
    "- [4.8 Ask questions](#4.8-Ask questions)\n",
    "- [4.9 Chinese prompts](#4.9-Chinese promptsText Tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54810ef7",
   "metadata": {},
   "source": [
    "Chains typically combine a large language model (LLM) with a prompt, based on which we can perform a series of operations on text or data.\n",
    "\n",
    "Chains can accept multiple inputs at once\n",
    "\n",
    "For example, we can create a chain that accepts user input, formats it using a prompt template, and then passes the formatted response to the LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21009bf6-49bd-466e-8177-74c23533d938",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Set up OpenAI API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5993b-f28d-4be4-984f-2d05be9f4579",
   "metadata": {},
   "source": [
    "Log in to your [OpenAI account](https://platform.openai.com/account/api-keys) to get your API Key, and then set it as an environment variable.\n",
    "\n",
    "- If you want to set it as a global environment variable, you can refer to [Zhihu article](https://zhuanlan.zhihu.com/p/627665725).\n",
    "- If you want to set it as a local/project environment variable, create a `.env` file in this file directory, open the file and enter the following content.\n",
    "\n",
    "<p style=\"font-family:verdana; font-size:12px;color:green\">\n",
    "OPENAI_API_KEY=\"your_api_key\" \n",
    "</p>\n",
    "\n",
    "Replace \"your_api_key\" with your own API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc3519c-4d12-4011-9223-2f3cb3c42b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the required packages python-dotenv and openai\n",
    "# If you need to view the installation process log, you can remove -q\n",
    "!pip install -q python-dotenv\n",
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ad53241-bef6-42b8-894b-bcbbc8c64df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Read local/project environment variables.\n",
    "\n",
    "# find_dotenv() finds and locates the path of the .env file\n",
    "# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment\n",
    "# If you set a global environment variable, this line of code will have no effect.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get the environment variable OPENAI_API_KEY\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940ce7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Large Language Model Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000bd16",
   "metadata": {},
   "source": [
    "The Large Language Model Chain (LLMChain) is a simple but very powerful chain, and is the basis for many of the chains we will introduce later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e93b9-52ba-4f86-a953-e4661b895a3d",
   "metadata": {},
   "source": [
    "### 2.1 Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84e441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a09c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                                             Review\n",
       "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
       "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
       "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
       "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
       "4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa83d10-04ee-4355-abe3-ab699c9eaca9",
   "metadata": {},
   "source": [
    "### 2.2 Initialize the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92dff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.prompts import ChatPromptTemplate  \n",
    "from langchain.chains import LLMChain   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "943237a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the parameter temperature to 0.0 to reduce the randomness of the generated answers.\n",
    "# If you want to get different and innovative answers every time, you can try adjusting this parameter.\n",
    "llm = ChatOpenAI(temperature=0.0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81887434",
   "metadata": {},
   "source": [
    "### 2.3 Initialize prompt template\n",
    "Initialize prompt, this prompt will accept a variable called product. This prompt will ask LLM to generate the best name to describe the company that makes the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdcdb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(   \n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22cb13",
   "metadata": {},
   "source": [
    "### 2.4 Building a Large Language Model Chain\n",
    "\n",
    "Combine the Large Language Model (LLM) and Prompt into a chain. This large language model chain is very simple and allows us to run the prompt in a sequential manner and combine it into the large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7abc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d5ff6",
   "metadata": {},
   "source": [
    "### 2.5 Running a Large Language Model Chain\n",
    "So if we have a product called \"Queen Size Sheet Set\", we can run it through this chain by using chain.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad44d1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Royal Linens.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ede1c",
   "metadata": {},
   "source": [
    "You can enter any product description and see what results the chain will output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab31583-5cdf-4885-94d7-59c0d3b90b2e",
   "metadata": {},
   "source": [
    "### 2.6 Chinese prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2181be10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"豪华床纺\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(   \n",
    "    \"描述制造{product}的一个公司的最佳名称是什么?\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "product = \"大号床单套装\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49158430",
   "metadata": {},
   "source": [
    "## 3. Sequence Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03469",
   "metadata": {},
   "source": [
    "### 3.1 Simple Sequential Chains\n",
    "\n",
    "Sequential Chains are chains that execute their links in a predefined order. Specifically, we will use SimpleSequentialChain, which is the simplest type of sequential chain, where each step has an input/output and the output of one step is the input of the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "febee243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d019d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e732589",
   "metadata": {},
   "source": [
    "#### 3.1.1 Create two subchains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f31aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template 1: This prompt will accept a product and return the best name to describe the company\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f5d5b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt Template 2: Accepts a company name and outputs a 20-word description of the company\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1991f4",
   "metadata": {},
   "source": [
    "#### 3.1.2 Building a Simple Sequential Chain\n",
    "Now we can combine the two LLMChains so that we can create the company name and description in one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c1eb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122f26a",
   "metadata": {},
   "source": [
    "Give an input and run the above chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80d102-da6b-49f4-8abd-7bff97211232",
   "metadata": {},
   "source": [
    "#### 3.1.3 Running a simple sequence chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78458efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mRoyal Rest Linens\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mRoyal Rest Linens provides high-quality and luxurious linens for a comfortable and restful night's sleep.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Royal Rest Linens provides high-quality and luxurious linens for a comfortable and restful night's sleep.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983eb68-adf0-4c18-beed-ff6fc09cd78b",
   "metadata": {},
   "source": [
    "#### 3.1.4 Chinese prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c32997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"尺寸王床品有限公司\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m尺寸王床品有限公司是一家专注于床上用品生产的公司。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'尺寸王床品有限公司是一家专注于床上用品生产的公司。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(   \n",
    "    \"描述制造{product}的一个公司的最好的名称是什么\"\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(   \n",
    "    \"写一个20字的描述对于下面这个\\\n",
    "    公司：{company_name}的\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],verbose=True)\n",
    "product = \"大号床单套装\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ce18c",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69f4c0",
   "metadata": {},
   "source": [
    "When there is only one input and one output, a simple sequential chain (SimpleSequentialChain) can be implemented. When there are multiple inputs or multiple outputs, we need to use a sequential chain (SequentialChain) to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c129ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI    #导入OpenAI模型\n",
    "from langchain.prompts import ChatPromptTemplate   #导入聊天提示模板\n",
    "from langchain.chains import LLMChain    #导入LLM链。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03a8e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811445c",
   "metadata": {},
   "source": [
    "Next we will create a series of chains and then use them one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e87ec-b99a-4130-a5b5-cdce064dc6ca",
   "metadata": {},
   "source": [
    "#### 3.2.1 Create four subchains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "016187ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subchain 1\n",
    "\n",
    "# prompt template 1: Translate into English (translate the following review into English)\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: Input: Review Output: English Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fb0730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subchain 2\n",
    "\n",
    "# prompt template 2: Summarize the following review in one sentence\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: Input: English Review Output: Summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6accf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subchain 3\n",
    "\n",
    "# prompt template 3: What language is used in the following review\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: Input: Review Output: Language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7a46121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subchain 4\n",
    "\n",
    "# prompt template 4: Write a follow-up response to the following summary using specific language\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: Input: summary, language Output: follow-up reply\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff435291-5d20-4c3e-9ed7-76a1140f96d2",
   "metadata": {},
   "source": [
    "#### 3.2.2 Combining four subchains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89603117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: review\n",
    "#Output: English review, summary, follow-up reply\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509de01",
   "metadata": {},
   "source": [
    "Let's select a review and pass it through the entire chain. We can see that the original review is in French, the English review can be seen as a translation, followed by a summary based on the English review, and the final output is a continuation of the original French information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51b04f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in the store and the taste is much better...\\nOld batch or counterfeit!?\",\n",
       " 'summary': 'The reviewer is disappointed with the taste and consistency of the product, suspecting that either an old batch or counterfeit version is the cause.',\n",
       " 'followup_message': \"Réponse de suivi :\\n\\nCher(e) critique,\\n\\nNous sommes désolés d'apprendre que notre produit ne répond pas à vos attentes. Nous prenons très au sérieux les commentaires de nos clients et nous tenons à vous rassurer que nous attachons une grande importance à la qualité de nos produits.\\n\\nNous vérifions constamment nos processus de production afin de nous assurer que nos produits sont toujours frais et authentiques. Si vous pensez avoir reçu un produit contrefait ou un lot périmé, nous vous prions de nous contacter directement afin de résoudre ce problème au plus vite. Nous avons un service client dédié qui sera ravi de vous aider avec votre préoccupation.\\n\\nNous tenons à nous excuser pour toute déception ou désagrément que cela aurait pu causer. Votre satisfaction est notre priorité absolue et nous ferons tout notre possible pour remédier à la situation et regagner votre confiance.\\n\\nNous vous remercions de votre compréhension et de votre patience. Votre avis est important pour nous et nous espérons avoir l'occasion de nous rattraper à l'avenir.\\n\\nCordialement,\\nL'équipe de [Nom de l'entreprise]\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d72f1d-8a1e-4eb6-b00f-1963b3cf3adb",
   "metadata": {},
   "source": [
    "#### 3.2.3 Chinese prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31624a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': \"I find the taste mediocre. The foam doesn't last, it's weird. I buy the same ones from the store and the taste is much better...\\nOld batch or counterfeit!?\",\n",
       " 'summary': \"The taste is mediocre, the foam doesn't last, and there is suspicion of old batch or counterfeit.\",\n",
       " 'followup_message': \"回复: Je suis désolé de vous entendre dire que le goût est moyen et que la mousse ne dure pas longtemps. Nous prenons ces problèmes très au sérieux. Nous allons enquêter pour vérifier s'il s'agit d'un ancien lot ou d'une contrefaçon. Votre satisfaction est notre priorité et nous ferons de notre mieux pour résoudre ce problème. Merci de nous avoir informés.\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "\n",
    "#Subchain 1\n",
    "# prompt template 1: Translate into English (translate the following review into English)\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"把下面的评论review翻译成英文:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: Input: Review Output: English Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n",
    "\n",
    "#Subchain 2\n",
    "# prompt template 2: Summarize the following review in one sentence\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"请你用一句话来总结下面的评论review:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: Input: English Review Output: Summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n",
    "\n",
    "\n",
    "#Subchain 3\n",
    "# prompt template 3: What language is used in the following review\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"下面的评论review使用的什么语言:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: Input: Review Output: Language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
    "\n",
    "\n",
    "#Subchain 4\n",
    "# prompt template 4: Write a follow-up response to the following summary using specific language\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"使用特定的语言对下面的总结写一个后续回复:\"\n",
    "    \"\\n\\n总结: {summary}\\n\\n语言: {language}\"\n",
    ")\n",
    "# chain 4: Input: summary, language Output: follow-up reply\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n",
    "\n",
    "\n",
    "# Combine the four subchains\n",
    "#Input: review Output: English review, summary, follow-up response\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041ea4c",
   "metadata": {},
   "source": [
    "## 4. Routing Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c32f97",
   "metadata": {},
   "source": [
    "So far, we've learned about large language model chains and sequential chains. But what if we want to do something more complex?\n",
    "\n",
    "A fairly common but basic operation is to take an input and route it to a chain, depending on what exactly that input is. If you have multiple child chains, each specialized for a specific type of input, you can make up a routing chain that first decides which child chain to pass it to, and then passes it to that chain.\n",
    "\n",
    "A router consists of two components:\n",
    "\n",
    "- Router Chain: The router chain itself, which is responsible for selecting the next chain to call\n",
    "\n",
    "- destination_chains: The chains that the router chain can route to\n",
    "\n",
    "For a concrete example, let's look at where we are routing between different types of chains, where we have different prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31b06fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain  #导入多提示链\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3f50bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b4708",
   "metadata": {},
   "source": [
    "### 4.1 Defining prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b85285-c736-4a5d-bd14-d9b5025ca29b",
   "metadata": {},
   "source": [
    "First, we define prompt templates that are suitable for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ade83f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first hint is suitable for answering physics questions\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The second prompt is suitable for answering math problems\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The third one is suitable for answering historical questions\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The fourth one is suitable for answering computer questions\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b749b-0d3c-46f2-afab-b1213f208b13",
   "metadata": {},
   "source": [
    "### 4.2 Naming and describing the prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922b35e",
   "metadata": {},
   "source": [
    "After defining these prompt templates, we can name each template and give a specific description. For example, the first physics description is suitable for answering questions about physics, and this information will be passed to the routing chain, which will then decide when to use this sub-chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "141a3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795cd42",
   "metadata": {},
   "source": [
    "LLMRouterChain (This chain uses the LLM to determine how to route things)\n",
    "\n",
    "Here we need a **multi-tip chain**. This is a specific type of chain that is used to route between multiple different tip templates. But this is just one type of routing, we can also route between any type of chain.\n",
    "\n",
    "A couple of the classes we are going to implement here is the large model router chain. This class itself uses the language model to route between different child chains. This is where the description and name provided above will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46633b43",
   "metadata": {},
   "source": [
    "### 4.3 Create the corresponding target chain based on the prompt template information \n",
    "The target chain is the chain called by the routing chain, and each target chain is a language model chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8eefec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba115de",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.4 Create a default target chain\n",
    "In addition to the target chain, we also need a default target chain. This is a chain that is called when the router cannot decide which child chain to use. In the example above, it might be called when the input question has nothing to do with physics, mathematics, history, or computer science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f98018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948700c4",
   "metadata": {},
   "source": [
    "### 4.5 Define routing templates between different chains\n",
    "\n",
    "This includes a description of the task to be completed and the specific format the output should be in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f30c2c",
   "metadata": {},
   "source": [
    "Note: An example is added here based on the original tutorial, mainly because the \"gpt-3.5-turbo\" model is not well adapted to understand the meaning of the template, and using \"text-davinci-003\" or \"gpt-4-0613\" can work well, so more example prompts are added here to make it better to learn.\n",
    "eg:\n",
    "<< INPUT >>\n",
    "\"What is black body radiation?\"\n",
    "<< OUTPUT >>\n",
    "```json\n",
    "{{{{\n",
    "\"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "\"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11b2e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\n",
    "\n",
    "eg:\n",
    "<< INPUT >>\n",
    "\"What is black body radiation?\"\n",
    "<< OUTPUT >>\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c46d0",
   "metadata": {},
   "source": [
    "### 4.6 Building the Routing Chain\n",
    "First, we create the complete router template by formatting the target defined above. This template can be used for many different types of targets.\n",
    "So here, instead of just physics, math, history, and computer science, you could add a different subject like English or Latin.\n",
    "\n",
    "Next, we create the prompt template from this template\n",
    "\n",
    "Finally, we create the routing chain by passing in the llm and the entire routing prompt. It is important to note that there is routing output parsing here, which is important because it will help this link decide which sub-links to route between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1387109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e92355c",
   "metadata": {},
   "source": [
    "### 4.7 Create the overall link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fb7d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple prompt chains\n",
    "chain = MultiPromptChain(router_chain=router_chain,    #l路由链路\n",
    "                         destination_chains=destination_chains,   #目标链路\n",
    "                         default_chain=default_chain,      #默认链路\n",
    "                         verbose=True   \n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086503f7",
   "metadata": {},
   "source": [
    "### 4.8 Asking questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969cd878",
   "metadata": {},
   "source": [
    "If we ask a physical question, we expect to see it routed to the physical link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Black body radiation is the electromagnetic radiation emitted by a perfect black body, which absorbs all incident radiation and reflects none. It is characterized by a continuous spectrum of radiated energy that is dependent on the temperature of the body, with higher temperatures leading to more intense and shorter wavelength radiation. This phenomenon is an important concept in thermal physics and has numerous applications, ranging from understanding stellar spectra to designing artificial light sources.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question: What is blackbody radiation?\n",
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c5ca9",
   "metadata": {},
   "source": [
    "If we ask a math question, we would like to see it routed to the math link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b717379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I can answer this question. The answer to 2 + 2 is 4.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question: What is 2+2?\n",
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186a2b9",
   "metadata": {},
   "source": [
    "What happens if we pass it a question that has nothing to do with any of the sub-links?\n",
    "\n",
    "Here, we asked a question about biology, and we can see that the link it chose is None. This means it will be passed to the default link, which itself is just a generic call to the language model. The language model luckily knows a lot about biology, so it can help us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29e5be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Every cell in our body contains DNA because DNA carries the genetic information that determines the characteristics and functions of each cell. DNA contains the instructions for the synthesis of proteins, which are essential for the structure and function of cells. Additionally, DNA is responsible for the transmission of genetic information from one generation to the next. Therefore, every cell in our body needs DNA to carry out its specific functions and to maintain the integrity of the organism as a whole.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question: Why does every cell in our body contain DNA?\n",
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b5f1d-b010-4f33-a928-d3b9e85bf289",
   "metadata": {},
   "source": [
    "### 4.9 Chinese prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7fade7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "#The first hint is suitable for answering physics questions\n",
    "physics_template = \"\"\"你是一个非常聪明的物理专家。 \\\n",
    "你擅长用一种简洁并且易于理解的方式去回答问题。\\\n",
    "当你不知道问题的答案时，你承认\\\n",
    "你不知道.\n",
    "\n",
    "这是一个问题:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The second hint is suitable for answering math questions\n",
    "math_template = \"\"\"你是一个非常优秀的数学家。 \\\n",
    "你擅长回答数学问题。 \\\n",
    "你之所以如此优秀， \\\n",
    "是因为你能够将棘手的问题分解为组成部分，\\\n",
    "回答组成部分，然后将它们组合在一起，回答更广泛的问题。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The third one is suitable for answering historical questions\n",
    "history_template = \"\"\"你是以为非常优秀的历史学家。 \\\n",
    "你对一系列历史时期的人物、事件和背景有着极好的学识和理解\\\n",
    "你有能力思考、反思、辩证、讨论和评估过去。\\\n",
    "你尊重历史证据，并有能力利用它来支持你的解释和判断。\n",
    "\n",
    "这是一个问题:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The fourth one is suitable for answering computer questions\n",
    "computerscience_template = \"\"\" 你是一个成功的计算机科学专家。\\\n",
    "你有创造力、协作精神、\\\n",
    "前瞻性思维、自信、解决问题的能力、\\\n",
    "对理论和算法的理解以及出色的沟通技巧。\\\n",
    "你非常擅长回答编程问题。\\\n",
    "你之所以如此优秀，是因为你知道  \\\n",
    "如何通过以机器可以轻松解释的命令式步骤描述解决方案来解决问题，\\\n",
    "并且你知道如何选择在时间复杂性和空间复杂性之间取得良好平衡的解决方案。\n",
    "\n",
    "这还是一个输入：\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "deb8aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"名字\": \"物理学\", \n",
    "        \"描述\": \"擅长回答关于物理学的问题\", \n",
    "        \"提示模板\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"名字\": \"数学\", \n",
    "        \"描述\": \"擅长回答数学问题\", \n",
    "        \"提示模板\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"名字\": \"历史\", \n",
    "        \"描述\": \"擅长回答历史问题\", \n",
    "        \"提示模板\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"名字\": \"计算机科学\", \n",
    "        \"描述\": \"擅长回答计算机科学问题\", \n",
    "        \"提示模板\": computerscience_template\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6eb641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"名字\"]\n",
    "    prompt_template = p_info[\"提示模板\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['名字']}: {p['描述']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af088d6b-a70b-4cd7-bc6e-4ad7b32e6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aae035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "\n",
    "# Multiple Prompt Routing Template\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"给语言模型一个原始文本输入，\\\n",
    "让其选择最适合输入的模型提示。\\\n",
    "系统将为您提供可用提示的名称以及最适合改提示的描述。\\\n",
    "如果你认为修改原始输入最终会导致语言模型做出更好的响应，\\\n",
    "你也可以修改原始输入。\n",
    "\n",
    "\n",
    "<< 格式 >>\n",
    "返回一个带有JSON对象的markdown代码片段，该JSON对象的格式如下：\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": 字符串 \\ 使用的提示名字或者使用 \"DEFAULT\"\n",
    "    \"next_inputs\": 字符串 \\ 原始输入的改进版本\n",
    "}}}}\n",
    "```\n",
    "\n",
    "\n",
    "记住：“destination”必须是下面指定的候选提示名称之一，\\\n",
    "或者如果输入不太适合任何候选提示，\\\n",
    "则可以是 “DEFAULT” 。\n",
    "记住：如果您认为不需要任何修改，\\\n",
    "则 “next_inputs” 可以只是原始输入。\n",
    "\n",
    "<< 候选提示 >>\n",
    "{destinations}\n",
    "\n",
    "<< 输入 >>\n",
    "{{input}}\n",
    "\n",
    "<< 输出 (记得要包含 ```json)>>\n",
    "\n",
    "样例:\n",
    "<< 输入 >>\n",
    "\"什么是黑体辐射?\"\n",
    "<< 输出 >>\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": 字符串 \\ 使用的提示名字或者使用 \"DEFAULT\"\n",
    "    \"next_inputs\": 字符串 \\ 原始输入的改进版本\n",
    "}}}}\n",
    "```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470b25c-ef82-496c-a6e0-e04b99b08e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple prompt chains\n",
    "chain = MultiPromptChain(router_chain=router_chain,    #l路由链路\n",
    "                         destination_chains=destination_chains,   #目标链路\n",
    "                         default_chain=default_chain,      #默认链路\n",
    "                         verbose=True   \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4446724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': '什么是黑体辐射？'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'黑体辐射是指一个理想化的物体，它能够完全吸收所有入射到它表面的辐射能量，并以热辐射的形式重新发射出来。黑体辐射的特点是其辐射能量的分布与温度有关，随着温度的升高，辐射能量的峰值会向更短的波长方向移动。这个现象被称为黑体辐射谱的位移定律，由普朗克在20世纪初提出。黑体辐射在研究热力学、量子力学和宇宙学等领域中具有重要的应用。'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chinese\n",
    "chain.run(\"什么是黑体辐射？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef81eda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "History: {'input': '你知道李白是谁嘛?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'李白是唐朝时期的一位著名诗人。他的诗歌以豪放、奔放、自由的风格著称，被誉为“诗仙”。他的作品涉及广泛，包括山水田园、历史传说、哲理思考等多个方面，对中国古典文学的发展产生了深远的影响。'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "chain.run(\"你知道李白是谁嘛?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "795bea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': '2 + 2 等于多少'}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2 + 2 等于 4。'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "chain.run(\"2 + 2 等于多少\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a64d0759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': '为什么我们身体里的每个细胞都包含DNA？'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我们身体里的每个细胞都包含DNA，是因为DNA是遗传信息的载体。DNA是由四种碱基（腺嘌呤、鸟嘌呤、胸腺嘧啶和鳞嘌呤）组成的长链状分子，它存储了生物体的遗传信息，包括个体的特征、生长发育、代谢功能等。每个细胞都需要这些遗传信息来执行其特定的功能和任务，因此每个细胞都需要包含DNA。此外，DNA还能通过复制和传递给下一代细胞和个体，以保证遗传信息的传承。'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "chain.run(\"为什么我们身体里的每个细胞都包含DNA？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
