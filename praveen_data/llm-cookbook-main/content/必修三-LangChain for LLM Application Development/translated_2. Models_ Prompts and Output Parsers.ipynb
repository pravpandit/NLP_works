{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Model, prompts and output interpreter\n",
    "\n",
    "- [1. Set OpenAI API Key](#1. Set OpenAI-API-Key)\n",
    "- [2. Use OpenAI directly](#2. Use OpenAI directly)\n",
    "- [2.1 Calculate 1+1](#2.1-Calculate 1+1)\n",
    "- [2.2 Express Pirate Mail in American English](#2.2-Express Pirate Mail in American English)\n",
    "- [2.3 Chinese version](#2.3-Chinese version)\n",
    "- [3. Use OpenAI through LangChain](#3. Use OpenAI through LangChain)\n",
    "- [3.1 Model](#3.1-Model)\n",
    "- [3.2 Prompt Template](#3.2-Prompt Template)\n",
    "- [3.2.1 Use LangChain Prompt Template](#3.2.1-Use LangChain Prompt Template)\n",
    "- [3.2.2 Chinese version](#3.2.2-Chinese version)\n",
    "- [3.2.2 Why do we need a prompt template?](#3.2.2-Why do we need a prompt template?)\n",
    "- [3.3 Output parser](#3.3-Output parser)\n",
    "- [3.3.1 If there is no output parserAnalyzer](#3.3.1-If there is no output analyzer)\n",
    "- [3.3.2 Chinese version](#3.3.2-Chinese version)\n",
    "- [3.3.3 LangChain output analyzer](#3.3.3-LangChain output analyzer)\n",
    "- [3.3.4 Chinese version](#3.3.4-Chinese version)\n",
    "- [IV. Supplementary materials](#IV. Supplementary materials)\n",
    "- [4.1 Chained thinking reasoning (ReAct)](#4.1-Chained thinking reasoning (ReAct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Set OpenAI API Key\n",
    "\n",
    "Log in to [OpenAI account](https://platform.openai.com/account/api-keys) to get API Key, and then set it as an environment variable.\n",
    "\n",
    "- If you want to set it as a global environment variable, you can refer to [Zhihu article](https://zhuanlan.zhihu.com/p/627665725).\n",
    "\n",
    "- If you want to set it as a local/project environment variable, create a `.env` file in this file directory, open the file and enter the following content.\n",
    "\n",
    "<p style=\"font-family:verdana; font-size:12px;color:green\">\n",
    "OPENAI_API_KEY=\"your_api_key\" \n",
    "</p>\n",
    "\n",
    "Replace \"your_api_key\" with your own API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the required packages python-dotenv and openai\n",
    "# If you need to view the installation process log, you can remove -q\n",
    "!pip install -q python-dotenv\n",
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Read local/project environment variables.\n",
    "\n",
    "# find_dotenv() finds and locates the path of the .env file\n",
    "# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment\n",
    "# If you set a global environment variable, this line of code will have no effect.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get the environment variable OPENAI_API_KEY\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Use OpenAI directly\n",
    "\n",
    "Let's start by calling OpenAI's API directly.\n",
    "\n",
    "The `get_completion` function is a wrapper function based on `openai`, which outputs the corresponding answer for a given prompt. It contains two parameters\n",
    "\n",
    "- `prompt` is a required input parameter. The **prompt you give to the model can be a question, or it can be something you need the model to help you do** (change the text writing style, translate, reply to messages, etc.).\n",
    "\n",
    "- `model` is a non-required input parameter. By default, gpt-3.5-turbo is used. You can also choose other models.\n",
    "\n",
    "The prompt here corresponds to the question we gave to chatgpt, and the output given by the function corresponds to the answer given to us by chatpgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Calculate 1+1\n",
    "\n",
    "Let's take a simple example - ask the model in Chinese and English\n",
    "\n",
    "- Prompt in Chinese: `What is 1+1?`\n",
    "- Prompt in English: `What is 1+1?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1ç­‰äº2ã€‚'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "get_completion(\"1+1æ˜¯ä»€ä¹ˆï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English\n",
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Pirate Email in American English\n",
    "\n",
    "In the simple example above, the model `gpt-3.5-turbo` answered our question about what 1+1 is.\n",
    "\n",
    "Now let's look at a more complex example:\n",
    "\n",
    "Suppose we are an employee of an e-commerce company, and our customer is a pirate A. He bought a juicer on our website to make milkshakes. During the milkshake making process, the lid of the milkshake flew out and covered the kitchen wall. So Pirate A wrote the following email to our customer service center: `customer_email`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our customer service staff finds the pirate's wording a bit difficult to understand. Now we want to achieve two small goals:\n",
    "\n",
    "- Let the model translate the pirate's email using American English, so that the customer service staff can understand it better. *Here, the pirate's English expression can be understood as an English dialect, and its relationship with American English is like the relationship between Sichuan dialect and Mandarin.\n",
    "- Let the model express in a calm and respectful tone when translating, and the customer service staff will feel better.\n",
    "\n",
    "Based on these two small goals, define the text expression style: `style`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# American English + calm, respectful tone\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we need to do is combine `customer_email` and `style` to construct our prompt: `prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informal\n",
    "customer_email = \"\"\"   \n",
    "é˜¿ï¼Œæˆ‘å¾ˆç”Ÿæ°”ï¼Œ\\\n",
    "å› ä¸ºæˆ‘çš„æ…æ‹Œæœºç›–æ‰äº†ï¼Œ\\\n",
    "æŠŠå¥¶æ˜”æº…åˆ°äº†å¨æˆ¿çš„å¢™ä¸Šï¼\\\n",
    "æ›´ç³Ÿç³•çš„æ˜¯ï¼Œä¿ä¿®ä¸åŒ…æ‹¬æ‰“æ‰«å¨æˆ¿çš„è´¹ç”¨ã€‚\\\n",
    "æˆ‘ç°åœ¨éœ€è¦ä½ çš„å¸®åŠ©ï¼Œä¼™è®¡ï¼\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```   \n",
      "é˜¿ï¼Œæˆ‘å¾ˆç”Ÿæ°”ï¼Œå› ä¸ºæˆ‘çš„æ…æ‹Œæœºç›–æ‰äº†ï¼ŒæŠŠå¥¶æ˜”æº…åˆ°äº†å¨æˆ¿çš„å¢™ä¸Šï¼æ›´ç³Ÿç³•çš„æ˜¯ï¼Œä¿ä¿®ä¸åŒ…æ‹¬æ‰“æ‰«å¨æˆ¿çš„è´¹ç”¨ã€‚æˆ‘ç°åœ¨éœ€è¦ä½ çš„å¸®åŠ©ï¼Œä¼™è®¡ï¼\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Require the model to convert according to the given tone\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `prompt` constructed, we can call `get_completion` to get the result we want - a pirate language email in a calm and respectful tone, in American English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, I'm really frustrated because the lid of my blender fell off and splattered the milkshake all over the kitchen wall! To make matters worse, the warranty doesn't cover the cost of cleaning the kitchen. I could really use your help right now, buddy!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the language style before and after the change, the words used are more formal, the expression of extreme emotions is replaced, and gratitude is expressed.\n",
    "\n",
    "âœ¨ You can try to modify the prompt to see what different results you can get ğŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 Chinese version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mandarin + calm, respectful tone\n",
    "style = \"\"\"æ­£å¼æ™®é€šè¯ \\\n",
    "ç”¨ä¸€ä¸ªå¹³é™ã€å°Šæ•¬çš„è¯­è°ƒ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŠŠç”±ä¸‰ä¸ªåå¼•å·åˆ†éš”çš„æ–‡æœ¬ç¿»è¯‘æˆä¸€ç§æ­£å¼æ™®é€šè¯ ç”¨ä¸€ä¸ªå¹³é™ã€å°Šæ•¬çš„è¯­è°ƒ\n",
      "é£æ ¼ã€‚\n",
      "æ–‡æœ¬: ```   \n",
      "é˜¿ï¼Œæˆ‘å¾ˆç”Ÿæ°”ï¼Œå› ä¸ºæˆ‘çš„æ…æ‹Œæœºç›–æ‰äº†ï¼ŒæŠŠå¥¶æ˜”æº…åˆ°äº†å¨æˆ¿çš„å¢™ä¸Šï¼æ›´ç³Ÿç³•çš„æ˜¯ï¼Œä¿ä¿®ä¸åŒ…æ‹¬æ‰“æ‰«å¨æˆ¿çš„è´¹ç”¨ã€‚æˆ‘ç°åœ¨éœ€è¦ä½ çš„å¸®åŠ©ï¼Œä¼™è®¡ï¼\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Require the model to convert according to the given tone\n",
    "prompt = f\"\"\"æŠŠç”±ä¸‰ä¸ªåå¼•å·åˆ†éš”çš„æ–‡æœ¬\\\n",
    "ç¿»è¯‘æˆä¸€ç§{style}é£æ ¼ã€‚\n",
    "æ–‡æœ¬: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'å°Šæ•¬çš„æœ‹å‹ä»¬ï¼Œæˆ‘æ„Ÿåˆ°éå¸¸ä¸å®‰ï¼Œå› ä¸ºæˆ‘çš„æ…æ‹Œæœºç›–å­ä¸æ…æ‰è½ï¼Œå¯¼è‡´å¥¶æ˜”æº…åˆ°äº†å¨æˆ¿çš„å¢™å£ä¸Šï¼æ›´åŠ ä»¤äººç³Ÿå¿ƒçš„æ˜¯ï¼Œä¿ä¿®æœåŠ¡å¹¶ä¸åŒ…å«å¨æˆ¿æ¸…æ´çš„è´¹ç”¨ã€‚æ­¤åˆ»ï¼Œæˆ‘çœŸè¯šåœ°è¯·æ±‚å„ä½çš„å¸®åŠ©ï¼Œæœ‹å‹ä»¬ï¼'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Using OpenAI with LangChain\n",
    "\n",
    "In the previous part, we used the wrapper function `get_completion` to directly call OpenAI to complete the translation of the dialect email, and obtained an email expressed in a calm and respectful tone and formal Mandarin.\n",
    "\n",
    "Let's try to use LangChain to achieve the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you need to view the installation process log, you can remove -q\n",
    "# --upgrade allows us to install the latest version of langchain\n",
    "!pip install -q --upgrade langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Model\n",
    "\n",
    "Import `OpenAI`'s dialogue model `ChatOpenAI` from `langchain.chat_models`. In addition to OpenAI, `langchain.chat_models` also integrates other dialogue models. For more details, please refer to [Langchain official documentation](https://python.langchain.com/en/latest/modules/models/chat/integrations.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we set the parameter temperature to 0.0 to reduce the randomness of the generated answers.\n",
    "# If you want to get different and innovative answers every time, you can try adjusting this parameter.\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that the default model of ChatOpenAI is `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Template\n",
    "\n",
    "In the previous example, we added the Python expression values â€‹â€‹`style` and `customer_email` to the `prompt` string through [f-strings](https://docs.python.org/zh-cn/3/tutorial/inputoutput.html#tut-f-strings).\n",
    "\n",
    "```python\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "```\n",
    "`langchain` provides an interface for quickly and easily constructing and using prompts. Now let's see how to use `langchain` to construct prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.2.1 Using LangChain prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 1ï¸âƒ£ Construct a prompt template string\n",
    "We construct a prompt template string: `template_string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 2ï¸âƒ£ Construct LangChain prompt template\n",
    "We call `ChatPromptTemplate.from_template()` function to convert the above prompt template string `template_string` into prompt template `prompt_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['style', 'text'] output_parser=None partial_variables={} template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.messages[0].prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output above, `prompt_template` has two input variables: `style` and `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['style', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3ï¸âƒ£ Use templates to get customer message prompts\n",
    "\n",
    "The langchain prompt template `prompt_template` requires two input variables: `style` and `text`. Here they correspond to \n",
    "- `customer_style`: the customer email style we want\n",
    "- `customer_email`: the original email text of the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given `customer_style` and `customer_email`, we can use the `format_messages` method of the prompt template `prompt_template` to generate the desired customer messages `customer_messages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain.schema.messages.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the variable type of `customer_messages` is a list (`list`), and the element variable type in the list is a langchain custom message (`langchain.schema.HumanMessage`).\n",
    "\n",
    "Printing the first element gives the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='æŠŠç”±ä¸‰ä¸ªåå¼•å·åˆ†éš”çš„æ–‡æœ¬ç¿»è¯‘æˆä¸€ç§æ­£å¼æ™®é€šè¯ ç”¨ä¸€ä¸ªå¹³é™ã€å°Šæ•¬çš„è¯­æ°”\\né£æ ¼ã€‚æ–‡æœ¬: ```\\né˜¿ï¼Œæˆ‘å¾ˆç”Ÿæ°”ï¼Œå› ä¸ºæˆ‘çš„æ…æ‹Œæœºç›–æ‰äº†ï¼ŒæŠŠå¥¶æ˜”æº…åˆ°äº†å¨æˆ¿çš„å¢™ä¸Šï¼æ›´ç³Ÿç³•çš„æ˜¯ï¼Œä¿ä¿®ä¸åŒ…æ‹¬æ‰“æ‰«å¨æˆ¿çš„è´¹ç”¨ã€‚æˆ‘ç°åœ¨éœ€è¦ä½ çš„å¸®åŠ©ï¼Œä¼™è®¡ï¼\\n```\\n' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "# Chinese prompts\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_string = \"\"\"æŠŠç”±ä¸‰ä¸ªåå¼•å·åˆ†éš”çš„æ–‡æœ¬\\\n",
    "ç¿»è¯‘æˆä¸€ç§{style}é£æ ¼ã€‚\\\n",
    "æ–‡æœ¬: ```{text}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "customer_style = \"\"\"æ­£å¼æ™®é€šè¯ \\\n",
    "ç”¨ä¸€ä¸ªå¹³é™ã€å°Šæ•¬çš„è¯­æ°”\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "é˜¿ï¼Œæˆ‘å¾ˆç”Ÿæ°”ï¼Œ\\\n",
    "å› ä¸ºæˆ‘çš„æ…æ‹Œæœºç›–æ‰äº†ï¼Œ\\\n",
    "æŠŠå¥¶æ˜”æº…åˆ°äº†å¨æˆ¿çš„å¢™ä¸Šï¼\\\n",
    "æ›´ç³Ÿç³•çš„æ˜¯ï¼Œä¿ä¿®ä¸åŒ…æ‹¬æ‰“æ‰«å¨æˆ¿çš„è´¹ç”¨ã€‚\\\n",
    "æˆ‘ç°åœ¨éœ€è¦ä½ çš„å¸®åŠ©ï¼Œä¼™è®¡ï¼\n",
    "\"\"\"\n",
    "\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)\n",
    "\n",
    "\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4ï¸âƒ£ Call the chat model to convert the customer message style\n",
    "\n",
    "Now we can call the chat model defined in the [model](#model) section to convert the customer message style. So far, we have implemented the tasks in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°Šæ•¬çš„ä¼™è®¡ï¼Œæˆ‘æ„Ÿåˆ°éå¸¸æ„¤æ€’ï¼Œå› ä¸ºæˆ‘çš„æ…æ‹Œæœºç›–å­ä¸æ…æ‰è½ï¼Œå¯¼è‡´å¥¶æ˜”æº…åˆ°äº†å¨æˆ¿çš„å¢™å£ä¸Šï¼æ›´åŠ ä»¤äººç³Ÿå¿ƒçš„æ˜¯ï¼Œä¿ä¿®å¹¶ä¸åŒ…æ‹¬å¨æˆ¿æ¸…æ´çš„è´¹ç”¨ã€‚ç°åœ¨ï¼Œæˆ‘éœ€è¦ä½ çš„å¸®åŠ©ï¼Œè¯·ä½ ç»™äºˆæ´æ‰‹ï¼\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°Šæ•¬çš„ä¼™è®¡ï¼Œæˆ‘æ„Ÿåˆ°éå¸¸æ„¤æ€’ï¼Œå› ä¸ºæˆ‘çš„æ…æ‹Œæœºç›–å­ä¸æ…æ‰è½ï¼Œå¯¼è‡´å¥¶æ˜”æº…åˆ°äº†å¨æˆ¿çš„å¢™å£ä¸Šï¼æ›´åŠ ä»¤äººç³Ÿå¿ƒçš„æ˜¯ï¼Œä¿ä¿®å¹¶ä¸åŒ…æ‹¬å¨æˆ¿æ¸…æ´çš„è´¹ç”¨ã€‚ç°åœ¨ï¼Œæˆ‘éœ€è¦ä½ çš„å¸®åŠ©ï¼Œè¯·ä½ ç»™äºˆæ´æ‰‹ï¼\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5ï¸âƒ£ Use the template to get the reply message prompt\n",
    "\n",
    "Next, we go a step further and convert the customer service staff's reply message into the pirate language style and make sure the message is polite. \n",
    "\n",
    "Here, we can continue to use the langchain prompt template constructed in step 2ï¸âƒ£ to get our reply message prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŠŠç”±ä¸‰ä¸ªåå¼•å·åˆ†éš”çš„æ–‡æœ¬ç¿»è¯‘æˆä¸€ç§a polite tone that speaks in English Pirateé£æ ¼ã€‚æ–‡æœ¬: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6ï¸âƒ£ Call the chat model to convert the reply message style\n",
    "\n",
    "Call the chat model defined in the [model](#model) section to convert the reply message style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, matey! I regret to inform ye, valued customer, that the warranty be not coverin' the expenses o' cleanin' yer galley due to yer own negligence. 'Tis yer own fault, ye see, fer ye be misusin' yer blender by forgettin' to secure the lid afore settin' it in motion. Aye, 'tis a tough break, indeed! Fare thee well, me heartie!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Chinese version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŠŠç”±ä¸‰ä¸ªåå¼•å·åˆ†éš”çš„æ–‡æœ¬ç¿»è¯‘æˆä¸€ç§ä¸€ä¸ªæœ‰ç¤¼è²Œçš„è¯­æ°” ä½¿ç”¨æ­£å¼çš„æ™®é€šè¯ é£æ ¼ã€‚æ–‡æœ¬: ```å˜¿ï¼Œé¡¾å®¢ï¼Œ ä¿ä¿®ä¸åŒ…æ‹¬å¨æˆ¿çš„æ¸…æ´è´¹ç”¨ï¼Œ å› ä¸ºæ‚¨åœ¨å¯åŠ¨æ…æ‹Œæœºä¹‹å‰ å¿˜è®°ç›–ä¸Šç›–å­è€Œè¯¯ç”¨æ…æ‹Œæœº, è¿™æ˜¯æ‚¨çš„é”™ã€‚ å€’éœ‰ï¼ å†è§ï¼\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_reply = \"\"\"å˜¿ï¼Œé¡¾å®¢ï¼Œ \\\n",
    "ä¿ä¿®ä¸åŒ…æ‹¬å¨æˆ¿çš„æ¸…æ´è´¹ç”¨ï¼Œ \\\n",
    "å› ä¸ºæ‚¨åœ¨å¯åŠ¨æ…æ‹Œæœºä¹‹å‰ \\\n",
    "å¿˜è®°ç›–ä¸Šç›–å­è€Œè¯¯ç”¨æ…æ‹Œæœº, \\\n",
    "è¿™æ˜¯æ‚¨çš„é”™ã€‚ \\\n",
    "å€’éœ‰ï¼ å†è§ï¼\n",
    "\"\"\"\n",
    "\n",
    "service_style_pirate = \"\"\"\\\n",
    "ä¸€ä¸ªæœ‰ç¤¼è²Œçš„è¯­æ°” \\\n",
    "ä½¿ç”¨æ­£å¼çš„æ™®é€šè¯ \\\n",
    "\"\"\"\n",
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°Šæ•¬çš„é¡¾å®¢ï¼Œå¾ˆæŠ±æ­‰å‘ŠçŸ¥æ‚¨ï¼Œä¿ä¿®æœåŠ¡ä¸åŒ…å«å¨æˆ¿æ¸…æ´è´¹ç”¨ã€‚è¿™æ˜¯å› ä¸ºåœ¨æ‚¨ä½¿ç”¨æ…æ‹Œæœºä¹‹å‰ï¼Œä¸æ…å¿˜è®°ç›–ä¸Šç›–å­è€Œå¯¼è‡´è¯¯ç”¨æ…æ‹Œæœºï¼Œè¿™å±äºæ‚¨çš„ç–å¿½ã€‚éå¸¸é—æ†¾ï¼ç¥æ‚¨ä¸€åˆ‡é¡ºåˆ©ï¼å†è§ï¼\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.2.2 Why do we need a prompt template?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied to more complex scenarios, prompts may be very long and contain many details. **Using prompt templates allows us to reuse designed prompts more easily**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a longer prompt template. Students study online and submit their assignments. The following prompts are used to grade the students' submitted assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English version\n",
    "prompt = \"\"\" Your task is to determine if the student's solution is correct or not\n",
    "\n",
    "    To solve the problem do the following:\n",
    "    - First, workout your own solution to the problem\n",
    "    - Then compare your solution to the student's solution \n",
    "    and evaluate if the sudtent's solution is correct or not.\n",
    "    ...\n",
    "    Use the following format:\n",
    "    Question:\n",
    "    ```\n",
    "    question here\n",
    "    ```\n",
    "    Student's solution:\n",
    "    ```\n",
    "    student's solution here\n",
    "    ```\n",
    "    Actual solution:\n",
    "    ```\n",
    "    ...\n",
    "    steps to work out the solution and your solution here\n",
    "    ```\n",
    "    Is the student's solution the same as acutal solution \\\n",
    "    just calculated:\n",
    "    ```\n",
    "    yes or no\n",
    "    ```\n",
    "    Student grade\n",
    "    ```\n",
    "    correct or incorrect\n",
    "    ```\n",
    "    \n",
    "    Question:\n",
    "    ```\n",
    "    {question}\n",
    "    ```\n",
    "    Student's solution:\n",
    "    ```\n",
    "    {student's solution}\n",
    "    ```\n",
    "    Actual solution:\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese Version\n",
    "prompt = \"\"\" ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆæ˜¯æ­£ç¡®çš„è¿˜æ˜¯ä¸æ­£ç¡®çš„\n",
    "\n",
    "è¦è§£å†³è¯¥é—®é¢˜ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n",
    " - é¦–å…ˆï¼Œåˆ¶å®šè‡ªå·±çš„é—®é¢˜è§£å†³æ–¹æ¡ˆ\n",
    " - ç„¶åå°†æ‚¨çš„è§£å†³æ–¹æ¡ˆä¸å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆè¿›è¡Œæ¯”è¾ƒ\n",
    " å¹¶è¯„ä¼°å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚\n",
    "...\n",
    "ä½¿ç”¨ä¸‹é¢çš„æ ¼å¼:\n",
    "\n",
    "é—®é¢˜:\n",
    "```\n",
    "é—®é¢˜æ–‡æœ¬\n",
    "```\n",
    "å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆ:\n",
    "```\n",
    "å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆæ–‡æœ¬\n",
    "```\n",
    "å®é™…è§£å†³æ–¹æ¡ˆ:\n",
    "```\n",
    "...\n",
    "åˆ¶å®šè§£å†³æ–¹æ¡ˆçš„æ­¥éª¤ä»¥åŠæ‚¨çš„è§£å†³æ–¹æ¡ˆè¯·å‚è§æ­¤å¤„\n",
    "```\n",
    "å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆå’Œå®é™…è§£å†³æ–¹æ¡ˆæ˜¯å¦ç›¸åŒ \\\n",
    "åªè®¡ç®—ï¼š\n",
    "```\n",
    "æ˜¯æˆ–è€…ä¸æ˜¯\n",
    "```\n",
    "å­¦ç”Ÿçš„æˆç»©\n",
    "```\n",
    "æ­£ç¡®æˆ–è€…ä¸æ­£ç¡®\n",
    "```\n",
    "\n",
    "é—®é¢˜:\n",
    "```\n",
    "{question}\n",
    "```\n",
    "å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆ:\n",
    "```\n",
    "{student's solution}\n",
    "```\n",
    "å®é™…è§£å†³æ–¹æ¡ˆ:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In addition, LangChain also provides prompt templates for some common scenarios. For example, automatic summarization, question and answer, connecting to SQL databases, and connecting to different APIs. By using LangChain's built-in prompt templates, you can quickly build your own large model application without spending time designing and constructing prompts.\n",
    "\n",
    "Finally, when we build large model applications, we usually want the output of the model to be in a given format, such as using specific keywords in the output to structure the output. The following is an example of chain thinking reasoning using a large model. For the question: *What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?*\n",
    "\n",
    "By using the LangChain library function, the output uses \"Thought\", \"Action\", and \"Observation\" as the keywords for chain thinking reasoning to structure the output.\n",
    "\n",
    "```\n",
    "Thought: I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into?ends into, then find the elevation range of the area.\n",
    "Action: Search[Colorado orogeny]\n",
    "Observation: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought: It does not mention the eastern sector. So I need to look up the eastern sector.\n",
    "Action: Lookup[eastern sector]\n",
    "Observation: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought: The eastern sector of Colorado orogeny extends into the High Plains. So I need to search the High Plains and find its elevation range.\n",
    "Action: Search[High Plains]\n",
    "Observation: High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought: I need to search High Plains (United States) instead.\n",
    "Action: Search[High Plains (United States)]\n",
    "Observation: The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\n",
    "\n",
    "Thought: High Plains rise in elevation fromaround 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "Action: Finish[1,800 to 7,000 ft]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the supplementary material, you can see another code example of chained reasoning using LangChain and OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 If there is no output parser\n",
    "\n",
    "For a given review `customer_review`, we want to extract the information and output it in the following format:\n",
    "\n",
    "```python\n",
    "{\n",
    "\"gift\": False,\n",
    "\"delivery_days\": 5,\n",
    "\"price_value\": \"pretty affordable!\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1ï¸âƒ£ Construct a prompt template string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2ï¸âƒ£ Construct langchain prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3ï¸âƒ£ Use template to get prompt message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4ï¸âƒ£ Call the chat model to extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": false,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0.0)\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ğŸ“ Analysis and summary\n",
    "`response.content` is a string (`str`), not a dictionary (`dict`). Using the `get` method directly will result in an error. Therefore, we need to output the interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgift\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "response.content.get('gift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Chinese version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='å¯¹äºä»¥ä¸‹æ–‡æœ¬ï¼Œè¯·ä»ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼š\\n\\nç¤¼ç‰©ï¼šè¯¥å•†å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿ å¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼›å¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\\n\\näº¤è´§å¤©æ•°ï¼šäº§å“éœ€è¦å¤šå°‘å¤©åˆ°è¾¾ï¼Ÿ å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\\n\\nä»·é’±ï¼šæå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œå¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨ã€‚\\n\\nä½¿ç”¨ä»¥ä¸‹é”®å°†è¾“å‡ºæ ¼å¼åŒ–ä¸º JSONï¼š\\nç¤¼ç‰©\\näº¤è´§å¤©æ•°\\nä»·é’±\\n\\næ–‡æœ¬: {text}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "customer_review = \"\"\"\\\n",
    "è¿™æ¬¾å¹å¶æœºéå¸¸ç¥å¥‡ã€‚ å®ƒæœ‰å››ä¸ªè®¾ç½®ï¼š\\\n",
    "å¹èœ¡çƒ›ã€å¾®é£ã€é£åŸã€é¾™å·é£ã€‚ \\\n",
    "ä¸¤å¤©åå°±åˆ°äº†ï¼Œæ­£å¥½èµ¶ä¸Šæˆ‘å¦»å­çš„\\\n",
    "å‘¨å¹´çºªå¿µç¤¼ç‰©ã€‚ \\\n",
    "æˆ‘æƒ³æˆ‘çš„å¦»å­ä¼šå–œæ¬¢å®ƒåˆ°è¯´ä¸å‡ºè¯æ¥ã€‚ \\\n",
    "åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘æ˜¯å”¯ä¸€ä¸€ä¸ªä½¿ç”¨å®ƒçš„äººï¼Œè€Œä¸”æˆ‘ä¸€ç›´\\\n",
    "æ¯éš”ä¸€å¤©æ—©ä¸Šç”¨å®ƒæ¥æ¸…ç†è‰åªä¸Šçš„å¶å­ã€‚ \\\n",
    "å®ƒæ¯”å…¶ä»–å¹å¶æœºç¨å¾®è´µä¸€ç‚¹ï¼Œ\\\n",
    "ä½†æˆ‘è®¤ä¸ºå®ƒçš„é¢å¤–åŠŸèƒ½æ˜¯å€¼å¾—çš„ã€‚\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "å¯¹äºä»¥ä¸‹æ–‡æœ¬ï¼Œè¯·ä»ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼š\n",
    "\n",
    "ç¤¼ç‰©ï¼šè¯¥å•†å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿ \\\n",
    "å¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼›å¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\n",
    "\n",
    "äº¤è´§å¤©æ•°ï¼šäº§å“éœ€è¦å¤šå°‘å¤©\\\n",
    "åˆ°è¾¾ï¼Ÿ å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\n",
    "\n",
    "ä»·é’±ï¼šæå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œ\\\n",
    "å¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨ã€‚\n",
    "\n",
    "ä½¿ç”¨ä»¥ä¸‹é”®å°†è¾“å‡ºæ ¼å¼åŒ–ä¸º JSONï¼š\n",
    "ç¤¼ç‰©\n",
    "äº¤è´§å¤©æ•°\n",
    "ä»·é’±\n",
    "\n",
    "æ–‡æœ¬: {text}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ç¤¼ç‰©\": \"æ˜¯çš„\",\n",
      "  \"äº¤è´§å¤©æ•°\": 2,\n",
      "  \"ä»·é’±\": [\"å®ƒæ¯”å…¶ä»–å¹å¶æœºç¨å¾®è´µä¸€ç‚¹\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 LangChain Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1ï¸âƒ£ Construct a prompt template string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2ï¸âƒ£ Construct langchain prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ğŸ”¥ Construct output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3ï¸âƒ£ Use template to get prompt message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt.format_messages(text=customer_review, format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: è¿™æ¬¾å¹å¶æœºéå¸¸ç¥å¥‡ã€‚ å®ƒæœ‰å››ä¸ªè®¾ç½®ï¼šå¹èœ¡çƒ›ã€å¾®é£ã€é£åŸã€é¾™å·é£ã€‚ ä¸¤å¤©åå°±åˆ°äº†ï¼Œæ­£å¥½èµ¶ä¸Šæˆ‘å¦»å­çš„å‘¨å¹´çºªå¿µç¤¼ç‰©ã€‚ æˆ‘æƒ³æˆ‘çš„å¦»å­ä¼šå–œæ¬¢å®ƒåˆ°è¯´ä¸å‡ºè¯æ¥ã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘æ˜¯å”¯ä¸€ä¸€ä¸ªä½¿ç”¨å®ƒçš„äººï¼Œè€Œä¸”æˆ‘ä¸€ç›´æ¯éš”ä¸€å¤©æ—©ä¸Šç”¨å®ƒæ¥æ¸…ç†è‰åªä¸Šçš„å¶å­ã€‚ å®ƒæ¯”å…¶ä»–å¹å¶æœºç¨å¾®è´µä¸€ç‚¹ï¼Œä½†æˆ‘è®¤ä¸ºå®ƒçš„é¢å¤–åŠŸèƒ½æ˜¯å€¼å¾—çš„ã€‚\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4ï¸âƒ£ Call the chat model to extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": false,\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": \"å®ƒæ¯”å…¶ä»–å¹å¶æœºç¨å¾®è´µä¸€ç‚¹\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5ï¸âƒ£ Use output parser to parse output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': '2', 'price_value': 'å®ƒæ¯”å…¶ä»–å¹å¶æœºç¨å¾®è´µä¸€ç‚¹'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ğŸ“ Analysis and summary\n",
    "`output_dict` is of dictionary type (`dict`), and the `get` method can be used directly. Such output is more convenient for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 Chinese version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"ç¤¼ç‰©\": string  // è¿™ä»¶ç‰©å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿ                            å¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼Œ                            å¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\n",
      "\t\"äº¤è´§å¤©æ•°\": string  // äº§å“éœ€è¦å¤šå°‘å¤©æ‰èƒ½åˆ°è¾¾ï¼Ÿ                                      å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\n",
      "\t\"ä»·é’±\": string  // æå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œ                                    å¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Chinese\n",
    "review_template_2 = \"\"\"\\\n",
    "å¯¹äºä»¥ä¸‹æ–‡æœ¬ï¼Œè¯·ä»ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼šï¼š\n",
    "\n",
    "ç¤¼ç‰©ï¼šè¯¥å•†å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿ\n",
    "å¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼›å¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\n",
    "\n",
    "äº¤è´§å¤©æ•°ï¼šäº§å“åˆ°è¾¾éœ€è¦å¤šå°‘å¤©ï¼Ÿ å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\n",
    "\n",
    "ä»·é’±ï¼šæå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œå¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨ã€‚\n",
    "\n",
    "æ–‡æœ¬: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "gift_schema = ResponseSchema(name=\"ç¤¼ç‰©\",\n",
    "                             description=\"è¿™ä»¶ç‰©å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿ\\\n",
    "                            å¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼Œ\\\n",
    "                            å¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\")\n",
    "\n",
    "delivery_days_schema = ResponseSchema(name=\"äº¤è´§å¤©æ•°\",\n",
    "                                      description=\"äº§å“éœ€è¦å¤šå°‘å¤©æ‰èƒ½åˆ°è¾¾ï¼Ÿ\\\n",
    "                                      å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\")\n",
    "\n",
    "price_value_schema = ResponseSchema(name=\"ä»·é’±\",\n",
    "                                    description=\"æå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œ\\\n",
    "                                    å¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨\")\n",
    "\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: è¿™æ¬¾å¹å¶æœºéå¸¸ç¥å¥‡ã€‚ å®ƒæœ‰å››ä¸ªè®¾ç½®ï¼šå¹èœ¡çƒ›ã€å¾®é£ã€é£åŸã€é¾™å·é£ã€‚ ä¸¤å¤©åå°±åˆ°äº†ï¼Œæ­£å¥½èµ¶ä¸Šæˆ‘å¦»å­çš„å‘¨å¹´çºªå¿µç¤¼ç‰©ã€‚ æˆ‘æƒ³æˆ‘çš„å¦»å­ä¼šå–œæ¬¢å®ƒåˆ°è¯´ä¸å‡ºè¯æ¥ã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘æ˜¯å”¯ä¸€ä¸€ä¸ªä½¿ç”¨å®ƒçš„äººï¼Œè€Œä¸”æˆ‘ä¸€ç›´æ¯éš”ä¸€å¤©æ—©ä¸Šç”¨å®ƒæ¥æ¸…ç†è‰åªä¸Šçš„å¶å­ã€‚ å®ƒæ¯”å…¶ä»–å¹å¶æœºç¨å¾®è´µä¸€ç‚¹ï¼Œä½†æˆ‘è®¤ä¸ºå®ƒçš„é¢å¤–åŠŸèƒ½æ˜¯å€¼å¾—çš„ã€‚\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"ç¤¼ç‰©\": string  // è¿™ä»¶ç‰©å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿ                            å¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼Œ                            å¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\n",
      "\t\"äº¤è´§å¤©æ•°\": string  // äº§å“éœ€è¦å¤šå°‘å¤©æ‰èƒ½åˆ°è¾¾ï¼Ÿ                                      å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\n",
      "\t\"ä»·é’±\": string  // æå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œ                                    å¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = prompt.format_messages(text=customer_review, format_instructions=format_instructions)\n",
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"ç¤¼ç‰©\": false,\n",
      "\t\"äº¤è´§å¤©æ•°\": \"ä¸¤å¤©å\",\n",
      "\t\"ä»·é’±\": \"å®ƒæ¯”å…¶ä»–å¹å¶æœºç¨å¾®è´µä¸€ç‚¹\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ç¤¼ç‰©': False, 'äº¤è´§å¤©æ•°': 'ä¸¤å¤©å', 'ä»·é’±': 'å®ƒæ¯”å…¶ä»–å¹å¶æœºç¨å¾®è´µä¸€ç‚¹'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Supplementary Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Chained Reasoning (ReAct)\n",
    "Reference: [ReAct (Reason+Act) prompting in OpenAI GPT and LangChain](https://tsmatz.wordpress.com/2023/03/07/react-with-openai-gpt-and-langchain/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to search David Chanoff and find the U.S. Navy admiral he collaborated with. Then I need to find the president under whom the admiral served as the ambassador to the United Kingdom.\n",
      "Action: Search[David Chanoff]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mDavid Chanoff is a noted author of non-fiction work. His work has typically involved collaborations with the principal protagonist of the work concerned. His collaborators have included; Augustus A. White, Joycelyn Elders, ÄoÃ n VÄƒn Toáº¡i, William J. Crowe, Ariel Sharon, Kenneth Good and Felix Zandman. He has also written about a wide range of subjects including literary history, education and foreign for The Washington Post, The New Republic and The New York Times Magazine. He has published more than twelve books.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDavid Chanoff has collaborated with several individuals, including Augustus A. White, Joycelyn Elders, ÄoÃ n VÄƒn Toáº¡i, William J. Crowe, Ariel Sharon, Kenneth Good, and Felix Zandman. I need to search each of these individuals to find the U.S. Navy admiral. \n",
      "Action: Search[Augustus A. White]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAugustus A. White III (born June 4, 1936) is the Ellen and Melvin Gordon Distinguished Professor of Medical Education and Professor of Orthopedic Surgery at Harvard Medical School and a former Orthopaedic Surgeon-in-Chief at Beth Israel Hospital, Boston, Massachusetts. White was the first African American medical student at Stanford, surgical resident at Yale University, professor of medicine at Yale, and department head at a Harvard-affiliated hospital (Beth Israel Hospital).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAugustus A. White III is not a U.S. Navy admiral. I need to search the next individual, Joycelyn Elders.\n",
      "Action: Search[Joycelyn Elders]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mMinnie Joycelyn Elders (born Minnie Lee Jones; August 13, 1933) is an American pediatrician and public health administrator who served as Surgeon General of the United States from 1993 to 1994. A vice admiral in the Public Health Service Commissioned Corps, she was the second woman, second person of color, and first African American to serve as Surgeon General. \n",
      "Elders is best known for her frank discussion of her views on controversial issues such as drug legalization, masturbation, and distributing contraception in schools. She was forced to resign in December 1994 amidst controversy as a result of her views. She is currently a professor emerita of pediatrics at the University of Arkansas for Medical Sciences.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mJoycelyn Elders is a pediatrician and public health administrator, not a U.S. Navy admiral. I need to search the next individual, ÄoÃ n VÄƒn Toáº¡i.\n",
      "Action: Search[ÄoÃ n VÄƒn Toáº¡i]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mÄoÃ n VÄƒn Toáº¡i (1945 in Vietnam â€“ November 2017 in California) was a Vietnamese-born naturalized American activist and the author of The Vietnamese Gulag (Simon & Schuster, 1986).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mÄoÃ n VÄƒn Toáº¡i is an activist and author, not a U.S. Navy admiral. I need to search the next individual, William J. Crowe.\n",
      "Action: Search[William J. Crowe]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWilliam James Crowe Jr. (January 2, 1925 â€“ October 18, 2007) was a United States Navy admiral and diplomat who served as the 11th chairman of the Joint Chiefs of Staff under Presidents Ronald Reagan and George H. W. Bush, and as the ambassador to the United Kingdom and Chair of the Intelligence Oversight Board under President Bill Clinton.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWilliam J. Crowe is a U.S. Navy admiral and diplomat. I need to find the president under whom he served as the ambassador to the United Kingdom.\n",
      "Action: Search[William J. Crowe ambassador to United Kingdom]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWilliam James Crowe Jr. (January 2, 1925 â€“ October 18, 2007) was a United States Navy admiral and diplomat who served as the 11th chairman of the Joint Chiefs of Staff under Presidents Ronald Reagan and George H. W. Bush, and as the ambassador to the United Kingdom and Chair of the Intelligence Oversight Board under President Bill Clinton.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWilliam J. Crowe served as the ambassador to the United Kingdom under President Bill Clinton. So the answer is Bill Clinton.\n",
      "Action: Finish[Bill Clinton]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bill Clinton'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.docstore.wikipedia import Wikipedia\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import initialize_agent, Tool, AgentExecutor\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "\n",
    "docstore=DocstoreExplorer(Wikipedia())\n",
    "tools = [\n",
    "  Tool(\n",
    "    name=\"Search\",\n",
    "    func=docstore.search,\n",
    "    description=\"Search for a term in the docstore.\",\n",
    "  ),\n",
    "  Tool(\n",
    "    name=\"Lookup\",\n",
    "    func=docstore.lookup,\n",
    "    description=\"Lookup a term in the docstore.\",\n",
    "  )\n",
    "]\n",
    "\n",
    "# Using a large language model\n",
    "llm = OpenAI(\n",
    "  model_name=\"gpt-3.5-turbo\",\n",
    "  temperature=0,\n",
    ")\n",
    "\n",
    "# Initialize ReAct agent\n",
    "react = initialize_agent(tools, llm, agent=\"react-docstore\", verbose=True)\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "  agent=react.agent,\n",
    "  tools=tools,\n",
    "  verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "question = \"Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?\"\n",
    "agent_executor.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
