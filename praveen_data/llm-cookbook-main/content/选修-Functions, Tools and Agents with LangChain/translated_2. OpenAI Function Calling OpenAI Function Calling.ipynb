{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7134fad1",
   "metadata": {
    "id": "7134fad1"
   },
   "source": [
    "# Chapter 2 OpenAI Function Calling OpenAI Function Calling\n",
    "\n",
    "This chapter mainly introduces the new features provided by OpenAI's API. In recent months, OpenAI has fine-tuned some new models, namely `gpt-3.5-turbo-0613` and `gpt-4-0613`.\n",
    "\n",
    "If we have a tool function that needs to be called in some specific situations, after fine-tuning, these models can pass in new parameters, and these new parameters can be used to automatically determine whether to call the tool function. If it is determined that the tool function needs to be called, the tool function and the corresponding input parameters will be returned.\n",
    "\n",
    "- [1. New parameters of OpenAI function](#1 new parameters of openai function)\n",
    "- [1.1 Simple example: get the current weather](#11-Simple example to get the current weather)\n",
    "- [1.2 New parameters: functions](#12-New parameters functions)\n",
    "- [1.3 Related prompt call results](#13-Relevant prompt call results)\n",
    "- [1.4 Unrelated prompt call results](#14-Unrelated prompt call results)\n",
    "- [2. Function Call parameter mode](#2 function-call parameter mode)\n",
    "- [2.1 Automatically determine whether to call](#21-Automatically determine whether to call) \n",
    "- [2.2 Forced not to call](#22-Forced not to call) \n",
    "- [2.3 Forced to call](#23-Forced to call) \n",
    "- [III. Function call and function execution](#III Function call and function execution) \n",
    "- [IV. English version prompt](#IV English version prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4e112",
   "metadata": {},
   "source": [
    "## 1. New parameters of OpenAI functions\n",
    "\n",
    "First, we directly define and use `OPENAI_API_KEY` to facilitate subsequent calls to OpenAI's API interface and use its functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b323d9",
   "metadata": {
    "id": "68b323d9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR_API_KEY\"\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba14f39",
   "metadata": {
    "id": "fba14f39"
   },
   "source": [
    "### 1.1 Simple example: get the current weather\n",
    "\n",
    "First, we use the first example of OpenAI and define a function called `get_current_weather`. Normally, getting the current weather is something that the language model itself cannot fully do. Therefore, we hope to be able to combine the language model with such a function to enhance it with current information.\n",
    "\n",
    "In this function, we fix the returned value, such as the temperature is fixed to 22 degrees Celsius, but in actual applications, this may involve calling a weather API or some external knowledge source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "041aacb9",
   "metadata": {
    "id": "041aacb9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define a function to get the current weather for a given location\n",
    "def get_current_weather(location, unit=\"摄氏度\"):\n",
    "\"\"\"Get the current weather at the specified location\"\"\"\n",
    "# Example function that simulates and returns the same weather conditions\n",
    "# In a real application environment, this could be a weather API\n",
    "# Create a dictionary of weather information\n",
    "    weather_info = {\n",
    "        \"location\": location,  # 天气的位置\n",
    "        \"temperature\": \"22\",  # 温度\n",
    "        \"unit\": unit,  # 温度单位，默认为摄氏度\n",
    "        \"forecast\": [\"晴\", \"多云\"],  # 天气预报\n",
    "    }\n",
    "# Convert weather information to a string in JSON format and return it\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba1a92",
   "metadata": {
    "id": "a7ba1a92"
   },
   "source": [
    "### 1.2 New parameter: functions\n",
    "\n",
    "So how do we pass such functions to the language model? OpenAI introduced a new parameter called `functions`, through which we can pass a list of function definitions. Since we only have one function, there is only one element in the list. This is a JSON object with several different parameters.\n",
    "\n",
    "- name: the name of the function\n",
    "- description: the description of the function\n",
    "- parameters: the parameter object, which has some property settings\n",
    "- type: type\n",
    "- properties: It is an object itself, and the description of the corresponding parameters is passed in. In the example we can see that there are two elements, `location` and `unit`. Each of these elements has a type, which is a string, and then a description. `unit` is an external parameter setting, for example, here we want it to be Celsius or Fahrenheit, so we can define its type and enumerated parameter values ​​here.\n",
    "- required: required parameters, for example, the parameter we need here is `location`.\n",
    "\n",
    "In the function definition, `description` and the parameters in `properties` are very important because these will be passed directly to the language model, and the language model will use these descriptions to decide whether to use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d00a4ad8",
   "metadata": {
    "id": "d00a4ad8"
   },
   "outputs": [],
   "source": [
    "# Define a function\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"获取指定位置的当前天气情况\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"城市和省份，例如：北京，北京市\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"摄氏度\", \"华氏度\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb5fd6",
   "metadata": {
    "id": "ecfb5fd6"
   },
   "source": [
    "### 1.3 Related prompts and call results\n",
    "\n",
    "Next, we can define a question about the weather, such as \"What's the weather like in Beijing?\", and then use OpenAI's function to call the conversation API. First, we have to select a newer model, such as `gpt-3.5-turbo-0613`, and then we pass in the function defined above to see the final response result.\n",
    "\n",
    "From the results, the role of the returned message is the assistant, the content is empty, but there is a function call parameter `function_call`, which contains two objects, `name` and `arguments`. `Name` is `get_current_weather`, which is the same name as the function we passed, and then `arguments` is this JSON format dictionary, which contains the parameters we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d6e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"function_call\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"arguments\": \"{\\n  \\\"location\\\": \\\"\\u5317\\u4eac\\uff0c\\u5317\\u4eac\\u5e02\\\",\\n  \\\"unit\\\": \\\"\\u6444\\u6c0f\\u5ea6\\\"\\n}\",\n",
      "          \"name\": \"get_current_weather\"\n",
      "        },\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1709602992,\n",
      "  \"id\": \"chatcmpl-8zE7kIlpxzjsiuxH4q6wtRh8CLDH3\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 30,\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"total_tokens\": 125\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Define input message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"北京的天气怎么样?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call OpenAI's ChatCompletion API to get a response\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions # 传入function参数\n",
    ")\n",
    "\n",
    "# Print the response result\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c7dd5",
   "metadata": {},
   "source": [
    "We can also see the response parameters, for example, here there are two parameters, namely `location` and `unit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88851464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"location\": \"北京，北京市\",\n",
      "  \"unit\": \"摄氏度\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print the incoming parameters\n",
    "print(response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f92a6",
   "metadata": {
    "id": "c04f92a6"
   },
   "source": [
    "If we look closely at the response, we will find that the content is now empty, `function_call` is a dictionary, and the `arguments` parameter in `function_call` is also a JSON dictionary. Therefore, we can use `json.loads` to load it into a Python dictionary. The parameters it returns can be passed directly to the `get_current_weather` function we defined above.\n",
    "\n",
    "We will find that using OpenAI to make function calls does not directly call the tool function. We still need to call the tool function. It just tells us which function to call, that is, the name, and what the parameters of the function should be. And because it does not execute the function, if we encounter some problems when using `json.loads` to decode, it is actually a problem with the model, so this part can consider taking some protective measures when writing tool functions, and this point will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c71606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": null,\n",
      "  \"function_call\": {\n",
      "    \"arguments\": \"{\\n  \\\"location\\\": \\\"\\u5317\\u4eac\\uff0c\\u5317\\u4eac\\u5e02\\\",\\n  \\\"unit\\\": \\\"\\u6444\\u6c0f\\u5ea6\\\"\\n}\",\n",
      "    \"name\": \"get_current_weather\"\n",
      "  },\n",
      "  \"role\": \"assistant\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get the message information in the response\n",
    "response_message = response[\"choices\"][0][\"message\"]\n",
    "\n",
    "print(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f47fc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"location\": \"北京，北京市\",\n",
      "  \"unit\": \"摄氏度\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print parameters\n",
    "print(response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03007bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": {\"location\": \"\\\\u5317\\\\u4eac\\\\uff0c\\\\u5317\\\\u4eac\\\\u5e02\", \"unit\": \"\\\\u6444\\\\u6c0f\\\\u5ea6\"}, \"temperature\": \"22\", \"unit\": \"\\\\u6444\\\\u6c0f\\\\u5ea6\", \"forecast\": [\"\\\\u6674\", \"\\\\u591a\\\\u4e91\"]}'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert a JSON string to a Python object\n",
    "args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "\n",
    "# Call the get_current_weather function and pass in the parameter args\n",
    "get_current_weather(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31263afb",
   "metadata": {
    "id": "31263afb"
   },
   "source": [
    "### 1.4 Unrelated prompt call results\n",
    "\n",
    "Next, let's discuss what will happen if the question asked is unrelated to the function, that is, unrelated to the weather, and what kind of information will be returned? From the results, we can see that the content returned is normal and there is no `function_call` parameter, that is, the language model determines that the tool function is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ea679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\u4f60\\u597d\\uff01\\u6709\\u4ec0\\u4e48\\u6211\\u53ef\\u4ee5\\u5e2e\\u52a9\\u4f60\\u7684\\u5417\\uff1f\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1709602993,\n",
      "  \"id\": \"chatcmpl-8zE7lZ3keV5JGVTFqvAWOGxit4cXj\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 19,\n",
      "    \"prompt_tokens\": 88,\n",
      "    \"total_tokens\": 107\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Not related to weather reminder call\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"你好!\",\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed07bb6",
   "metadata": {},
   "source": [
    "Due to encoding issues, we can print the corresponding returned text information separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89df974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67fac41",
   "metadata": {
    "id": "c67fac41"
   },
   "source": [
    "## 2. Function Call parameter mode\n",
    "\n",
    "There are 3 modes for the `function_call` parameter. We can pass other parameters `function_call` to force the model to use or not use a function.\n",
    "\n",
    "1. By default, it is set to `auto`, which means the model chooses by itself.\n",
    "\n",
    "2. In the second mode, we can force it to call a function. If we want to always return a function\n",
    "\n",
    "3. Another mode is `none`. This forces the language model not to use any function provided.\n",
    "\n",
    "### 2.1 Automatically determine whether to call\n",
    "\n",
    "The `auto` mode is that the large model chooses whether to return parameters by itself. This part is also the default. All the above methods are `auto` modes\n",
    "\n",
    "### 2.2 Force not to call\n",
    "\n",
    "The `none` mode is not important for this example, because the content `hello` does not require a function call, so we see that it is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba400aa9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba400aa9",
    "outputId": "8b563e4f-c447-45a7-eb42-6390488a80b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8nfuv5tlp6UaSwUfmeHppJkQyFpCW\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1706849893,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\\u4f60\\u597d\\uff01\\u6709\\u4ec0\\u4e48\\u53ef\\u4ee5\\u5e2e\\u52a9\\u4f60\\u7684\\u5417\\uff1f\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 88,\n",
      "    \"completion_tokens\": 17,\n",
      "    \"total_tokens\": 105\n",
      "  },\n",
      "  \"system_fingerprint\": null\n",
      "}\n",
      "你好！有什么可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "# Irrelevant prompts are forced not to be called\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"你好\",\n",
    "    }\n",
    "]\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"none\", # 传入参数强制不调用\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd0bb1",
   "metadata": {
    "id": "3bcd0bb1"
   },
   "source": [
    "What will happen if we force not to call the tool function when we need it (that is, use the second mode)? From the result, it still has normal `role` and `content`, but because we force not to call the function, it tries to return the correct parameters, but it is not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d19386d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d19386d5",
    "outputId": "0f206c8e-21e9-4aca-c6b2-7674650d723e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8nfv0wFX6tqcaoU2gT4KqICymXDxa\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1706849898,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\\u8bf7\\u7a0d\\u7b49\\uff0c\\u6211\\u4e3a\\u60a8\\u67e5\\u627e\\u5317\\u4eac\\u7684\\u5929\\u6c14\\u60c5\\u51b5\\u3002\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 18,\n",
      "    \"total_tokens\": 113\n",
      "  },\n",
      "  \"system_fingerprint\": null\n",
      "}\n",
      "请稍等，我为您查找北京的天气情况。\n"
     ]
    }
   ],
   "source": [
    "# Related prompts are forced not to be called\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"北京天气怎么样?\",\n",
    "    }\n",
    "]\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"none\", # 传入参数强制不调用\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed657cb",
   "metadata": {
    "id": "4ed657cb"
   },
   "source": [
    "### 2.3 Forced Calls\n",
    "\n",
    "The last pattern of the `function_call` parameter is to force a function call, and the method is also very simple. Just pass the name of the function in the parameter. In this part, we specify `name` as `get_current_weather`, which will force it to use the `get_current_weather` function. If we look at the results, we can actually see that this `function_call` object is returned, and there are some parameters with `name` as `get_current_weather`.\n",
    "\n",
    "Just for fun, it passes a parameter that has nothing to do with weather, and then forces it to use the `get_current_weather` function, but there is absolutely no information in the parameters we pass in about how it should call the function. So here, it made up the parameters of Beijing, Beijing, and if we run it again, it will keep calling the parameters of Beijing.\n",
    "\n",
    "We can also try to use different model functions to call, different parameter values, different input messages, but there is a thing to note here. First of all, the function itself and the description will count towards the token usage limit passed to OpenAI. So if we run this, we can see that the prompt token returned is 95. If we comment out `functions` and `function_call`, we can see the promptThe token count is reduced to 15. Because the OpenAI model has a limit on tokens, when constructing a message to pass to OpenAI, you now need to pay attention not only to the length of the message, but also the length of the function you pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c432ee4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c432ee4c",
    "outputId": "507ca457-04ae-4a97-f7cd-6c7efd278ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8nfvkQDayf7kmYNdx4G84dkl1L5YC\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1706849944,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"get_current_weather\",\n",
      "          \"arguments\": \"{\\n  \\\"location\\\": \\\"\\u5317\\u4eac\\uff0c\\u5317\\u4eac\\u5e02\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 95,\n",
      "    \"completion_tokens\": 12,\n",
      "    \"total_tokens\": 107\n",
      "  },\n",
      "  \"system_fingerprint\": null\n",
      "}\n",
      "{\n",
      "  \"location\": \"北京，北京市\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# No need to prompt for mandatory function call\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"你好!\",\n",
    "    }\n",
    "]\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call={\"name\": \"get_current_weather\"}, # 强制调用函数get_current_weather\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"message\"][\"function_call\"][\"arguments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3306ca8",
   "metadata": {
    "id": "f3306ca8"
   },
   "source": [
    "## 3. Function calls and execution functions\n",
    "\n",
    "Finally, let's look at how to pass these function calls and the results of actually executing the function calls back to the language model. This is important because usually we want to use the language model to determine the function to call, then run the function, but then pass it back to the language model to get the final response.\n",
    "\n",
    "We will go through this process, first we ask a question to get a response with the `function call` parameter, and then we add this message to our message list. Then we can simulate calling the `get_current_weather` function with the parameters provided by the language model and save it to a variable `observation`. Then we define a new message list to represent the result of the function just called. An important point here is that `role` is equal to `function`, which tells the language model that this is the response of calling the function. In addition, the name of the function `name` and the `content` variable are passed, set to the `observation` calculated above.\n",
    "\n",
    "If we then use this message list to call the language model, we can see that the language model answers very well: the weather in Beijing is currently 22 degrees Celsius, and the weather is mainly sunny, but there are also cloudy conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1636ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\u4f60\\u597d\\uff01\\u6709\\u4ec0\\u4e48\\u6211\\u53ef\\u4ee5\\u5e2e\\u52a9\\u4f60\\u7684\\u5417\\uff1f\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1709602993,\n",
      "  \"id\": \"chatcmpl-8zE7lZ3keV5JGVTFqvAWOGxit4cXj\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 19,\n",
      "    \"prompt_tokens\": 88,\n",
      "    \"total_tokens\": 107\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Not related to weather reminder call\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"你好!\",\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1577b5b",
   "metadata": {},
   "source": [
    "Due to encoding issues, we can only accept the corresponding returned text information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6dff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940bf6d",
   "metadata": {},
   "source": [
    "## 4. English version tips\n",
    "\n",
    "**1.1 Simple example: Get the current weather**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61a23996",
   "metadata": {
    "id": "61a23996"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "\"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5d2d2",
   "metadata": {
    "id": "44c5d2d2"
   },
   "source": [
    "**1.2 Define function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4152b105",
   "metadata": {
    "id": "4152b105"
   },
   "outputs": [],
   "source": [
    "# define a function\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668eb81c",
   "metadata": {
    "id": "668eb81c"
   },
   "source": [
    "**1.3 Related prompt call results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6bd2994",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6bd2994",
    "outputId": "ae7da376-c365-4177-a1d0-a4df92f6815e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8nfyFSmPyo4RoV79D5Urk1QzLCf1C\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1706850099,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"get_current_weather\",\n",
      "          \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"function_call\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 82,\n",
      "    \"completion_tokens\": 18,\n",
      "    \"total_tokens\": 100\n",
      "  },\n",
      "  \"system_fingerprint\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60020d4",
   "metadata": {
    "id": "a60020d4"
   },
   "source": [
    "**1.4 Irrelevant prompt call results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b04b3318",
   "metadata": {
    "id": "b04b3318"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdbfe18",
   "metadata": {
    "id": "bfdbfe18"
   },
   "source": [
    "**2.1 Automatically determine whether to call**\n",
    "\n",
    "Automatically determine whether to call a function and whether the prompt is related to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8c48d2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8c48d2b",
    "outputId": "cf492711-60a5-42a7-e154-09a3b3174da1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8nfyHfC32MVE2gpE6HZxtmC7f7Mvi\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1706850101,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Hello! How can I assist you today?\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 76,\n",
      "    \"completion_tokens\": 10,\n",
      "    \"total_tokens\": 86\n",
      "  },\n",
      "  \"system_fingerprint\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb9b08",
   "metadata": {
    "id": "32eb9b08"
   },
   "source": [
    "**2.2 Forced not to call**\n",
    "\n",
    "Irrelevant prompts Forced not to call, the result is normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba53560e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba53560e",
    "outputId": "c529ab29-8c49-4b2c-9645-10ae8c018f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8nfyfrOoacEQCPO3O2sYLukAdbxX4\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1706850125,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Hello! How can I assist you today?\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 77,\n",
      "    \"completion_tokens\": 9,\n",
      "    \"total_tokens\": 86\n",
      "  },\n",
      "  \"system_fingerprint\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"none\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e037da0",
   "metadata": {
    "id": "0e037da0"
   },
   "source": [
    "**2.3 Forced call**\n",
    "\n",
    "Forced call irrelevant to the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32b16792",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32b16792",
    "outputId": "37b06217-4632-46e4-ce07-453e73814081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8nfyzFFeUyzJDYVbdiVDgPEtNrnKE\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1706850145,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"get_current_weather\",\n",
      "          \"arguments\": \"{\\n  \\\"location\\\": \\\"San Francisco, CA\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 83,\n",
      "    \"completion_tokens\": 12,\n",
      "    \"total_tokens\": 95\n",
      "  },\n",
      "  \"system_fingerprint\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b85a74",
   "metadata": {
    "id": "65b85a74"
   },
   "source": [
    "Forces the problem call to work, the result is the same as `auto`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6f955de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6f955de",
    "outputId": "b8134f2e-1746-4139-83f4-ac51327549cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8nfzav6WbYIDLPHXYwaBXTM0byxaj\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1706850182,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"get_current_weather\",\n",
      "          \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 89,\n",
      "    \"completion_tokens\": 11,\n",
      "    \"total_tokens\": 100\n",
      "  },\n",
      "  \"system_fingerprint\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston!\",\n",
    "    }\n",
    "]\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
