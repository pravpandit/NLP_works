{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f7fce3",
   "metadata": {
    "id": "c5f7fce3"
   },
   "source": [
    "# Chapter 3 LangChain Expression Language LangChain Expression Language\n",
    "\n",
    "In this chapter, we will introduce LangChain Expression Language (or LCEL), which is called Langchain's expression language. LCEL is a new syntax and an important addition to the LangChain toolkit. It has many advantages that make it easier and more convenient for us to handle LangChain and agents.\n",
    "\n",
    "1. LCEL provides asynchronous, batch, and stream processing support, which makes the code versatile and can be quickly applied and run in different servers.\n",
    "\n",
    "- Asynchronous: The program can execute multiple tasks at the same time, rather than one after another in sequence\n",
    "\n",
    "- Batch processing: It is a method of processing a group of tasks or data as a batch, rather than one by one\n",
    "\n",
    "- Streaming processing: It is a method of processing data continuously. Data will continuously enter the system and be processed. Streaming processing can process data immediately when it arrives, and can process data in a continuous and low-latency manner.\n",
    "\n",
    "2. LCEL has fallbacks, also known as fallback safety mechanism. Sometimes the results obtained by LLM are uncontrollable. In this case, you can roll back the results and even attach them to the entire chain.\n",
    "\n",
    "3. LCEL adds LLM parallelism, LLM operation is usually time-consuming, parallelism can speed up the speed of getting results.\n",
    "\n",
    "4. LCEL has built-in logging to record the operation of the agent. Even if the agent is complex, the log helps to understand the operation of complex chains and agents.\n",
    "\n",
    "In the previous course, we know that LangChain provides a component chain (chain) that can combine components to play a more powerful role in LLM, but the syntax is very complex. Here, LCEL provides a pipeline syntax that makes it easy to build complex chains from basic components. We can use LangChain to complete the combination of `Chain = prompt | LLM | OutputParser`. We will discuss the specific use in the following content. Chains usually combine large language models (LLM) with prompts, based on which we can perform a series of operations on text or data.\n",
    "\n",
    "![image.png](../../figures/LCEL.png)\n",
    "\n",
    "- [1. Simple Chain](#1. Simple Chain-Simple-Chain)\n",
    "- [2. More complex chain](#2. More complex chain-More-complex-chain)\n",
    "- [1.1 Building a simple vector database](#1.1-Build a simple vector database)\n",
    "- [1.2 Use RunnableMap](#1.2-Use RunnableMap)\n",
    "- [Three, Bind Bind](#Three, Bind-Bind)\n",
    "- [3.1 Single function binding](#3.1-Single function binding)\n",
    "- [3.2 Multiple function binding](#3.2-Multiple function binding)\n",
    "- [Four, Fallbacks](#Four, Fallbacks)\n",
    "- [4.1 Use the early model to format the output](#4.1-Use the early model to format the output)\n",
    "- [4.2 Use the new model to format the output](#4.2-Use the new model to format the output)\n",
    "- [4.3 fallbacks method](#4.3-fallbacks method)\n",
    "- [Five, Interface Interface](#Five, Interface-Interface)\n",
    "- [5.1 invoke interface](#5.1-invoke interface)\n",
    "- [5.2 batch interface](#5.2-batch interface)\n",
    "- [5.3 stream interface](#5.3-stream interface)\n",
    "- [5.4 asynchronous interface](#5.4-asynchronous interface)\n",
    "- [VI. English version tips](#VI. English version tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631e459",
   "metadata": {},
   "source": [
    "## 1. Simple Chain\n",
    "\n",
    "Next we will still use OpenAI's API, so first we need to initialize our API_Key, the method is the same as the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb572a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cacb572a",
    "outputId": "1acef39b-494e-445e-bde0-2e8f83a2a84c"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install openai==0.28\n",
    "# !pip install \"langchain[docarray]\"\n",
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52952e6",
   "metadata": {
    "id": "c52952e6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR_API_KEY\"\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f48e9",
   "metadata": {
    "id": "0a7f48e9"
   },
   "source": [
    "Next, we first import the LangChain library and define a simple chain, which includes a prompt template, a large language model, and an output parser. We can see that the result of the large language model is successfully output, completing a simple chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ac70fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "81ac70fe",
    "outputId": "00302a8a-42eb-4eaa-ff0f-8540245932cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'为什么熊不喜欢玩扑克牌？因为他总是把两个熊掌都露出来！'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import modules required by LangChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Use ChatPromptTemplate to create a prompt from a template. The {topic} in the template will be replaced with the actual topic in subsequent code.\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"告诉我一个关于{topic}的短笑话\"\n",
    ")\n",
    "\n",
    "# Create a ChatOpenAI model instance, using the gpt-3.5-turbo model by default\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a StrOutputParser instance to parse the output\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create a chain call to connect prompt, model and output_parser together\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "#Call chain call and pass in parameters\n",
    "chain.invoke({\"topic\": \"熊\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563f727",
   "metadata": {
    "id": "7563f727"
   },
   "source": [
    "If we look at the output of `Chain`, we will find that it is the same as what we defined, and it consists of three parts, namely `Chain = prompt | LLM |OutputParser`. The `|` symbol is similar to the unix pipe operator, which links different components together and provides the output of one component as input to the next component. In this chain, the user input is passed to the prompt template, then the prompt template output is passed to the model, and then the model output is passed to the output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a351d14a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a351d14a",
    "outputId": "e9a7f195-fdfb-4523-eb34-a708ce0e0b24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='告诉我一个关于{topic}的短笑话'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the value of Chain\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8c80e",
   "metadata": {
    "id": "35f8c80e"
   },
   "source": [
    "## 2. More complex chain\n",
    "\n",
    "Next, we will create a more complex chain. In the previous course, we have touched upon how to perform retrieval enhancement generation. So next we use LCEL to repeat the previous process, combining the user's question with the vector database retrieval results, and use RunnableMap to build a more complex chain.\n",
    "\n",
    "### 2.1 Build a simple vector database\n",
    "First, we build a vector database. This simple vector database contains only two sentences. We use OpenAI's Embedding as the embedding model, and then we create a retriever through `vector store.as_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308eab74",
   "metadata": {
    "id": "308eab74"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# Create a DocArrayInMemorySearch object to store and search document vectors\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"哈里森在肯肖工作\", \"熊喜欢吃蜂蜜\"],\n",
    "    embedding=OpenAIEmbeddings() # 使用OpenAI的Embedding\n",
    ")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c1f68",
   "metadata": {
    "id": "768c1f68"
   },
   "source": [
    "As we learned earlier, if we call `retriever.get_relevant_documents`, we will get the relevant retrieved documents. First, we ask \"Where does Harrison work?\" We will find that a list of documents is returned. It will return a list of documents sorted by similarity, so the most relevant ones are placed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83e9118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f83e9118",
    "outputId": "4b3e4b1f-c378-43ec-8a2e-4b6a6a0d88b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='哈里森在肯肖工作'), Document(page_content='熊喜欢吃蜂蜜')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get documents related to the question \"Where does Harrison work?\"\n",
    "retriever.get_relevant_documents(\"哈里森在哪里工作？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5eacb",
   "metadata": {
    "id": "a4b5eacb"
   },
   "source": [
    "If we change the question to, say, \"What do bears like to eat?\", we can see that the order of the questions changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d76aaf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d76aaf5",
    "outputId": "8e79e2d9-8572-44e0-edf7-70cd254e0252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='熊喜欢吃蜂蜜'), Document(page_content='哈里森在肯肖工作')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get documents related to the question \"What do bears like to eat?\"\n",
    "retriever.get_relevant_documents(\"熊喜欢吃什么\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34293a",
   "metadata": {
    "id": "fb34293a"
   },
   "source": [
    "### 3.2 Using RunnableMap\n",
    "\n",
    "The above example returns two results because there are only two document lists, which is completely applicable to more documents. Next, we will add `RunnableMap`. In this `RunnableMap`, there are not only user questions, but also document lists corresponding to the questions. This is equivalent to adding context to the documents of the big model, so that retrieval enhancement can be completed. If we ask a question normally, we can see that the big model correctly returns the results in the document and obtains the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a9b652",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e8a9b652",
    "outputId": "d615b86a-a14b-4917-833f-e70c91730477"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'肯肖'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "# Define a template string template\n",
    "template = \"\"\"仅根据以下上下文回答问题：\n",
    "{context}\n",
    "\n",
    "问题：{question}\n",
    "\"\"\"\n",
    "\n",
    "# Use template as a template\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create a processing chain chain, including RunnableMap, prompt, model and output_parser components\n",
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser\n",
    "\n",
    "# Call the invoke method of chain\n",
    "chain.invoke({\"question\": \"哈里森在哪里工作?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394af88e",
   "metadata": {
    "id": "394af88e"
   },
   "source": [
    "If we want to dig deeper into the working mechanism behind the scenes, we can look at `RunnableMap`, which we created as an input and operated in the same way. We can see that in it, `RunnableMap` provides two variables, `context` and `question`, one is the list of queried documents, and the other is the corresponding question. This large model can summarize and answer the corresponding questions based on the provided documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759d2af6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "759d2af6",
    "outputId": "fa16fa6c-dcb0-46e9-e538-428e711db372"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='哈里森在肯肖工作'),\n",
       "  Document(page_content='熊喜欢吃蜂蜜')],\n",
       " 'question': '哈里森在哪里工作?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RunnableMap object containing two key-value pairs\n",
    "# The key \"context\" corresponds to a lambda function, which is used to retrieve relevant documents. The function input parameter is x, which is the input dictionary. The function return value is retriever.get_relevant_documents(x[\"question\"])\n",
    "# The key \"question\" corresponds to a lambda function, which is used to get the question. The function input parameter is x, which is the input dictionary. The function return value is x[\"question\"]\n",
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})\n",
    "\n",
    "# Call the invoke method of inputs and pass it a dictionary as a parameter. The dictionary contains a key-value pair, the key is \"question\" and the value is \"Where does Harrison work?\"\n",
    "inputs.invoke({\"question\": \"哈里森在哪里工作?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01b704",
   "metadata": {
    "id": "2c01b704"
   },
   "source": [
    "## 3. Bind\n",
    "\n",
    "In the previous chapter, we introduced the call of OpenAI function. The new `function` parameter can automatically determine whether to use the tool function. If necessary, it will return the parameters needed. Next, we also use LangChain to implement the new function of OpenAI function call. First, we need a function description information and define the function. The function here still uses the `get_current_weather` function in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b00f651",
   "metadata": {
    "id": "1b00f651"
   },
   "outputs": [],
   "source": [
    "# Define a function\n",
    "functions = [\n",
    "  {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"获取指定位置的当前天气情况\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"location\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"城市和省份，例如：北京，北京市\",\n",
    "        },\n",
    "        \"unit\": {\"type\": \"string\", \"enum\": [\"摄氏度\", \"华氏度\"]},\n",
    "      },\n",
    "      \"required\": [\"location\"],\n",
    "    },\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ff71d",
   "metadata": {
    "id": "5d2ff71d"
   },
   "source": [
    "### 3.1 Single function binding\n",
    "\n",
    "Next, we use the `bind` method to bind the tool function to the large model and build a simple chain. After calling, we can see that an `AIMessage` is returned, in which the returned `content` is empty, but the parameters we need to call the tool function are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f693ff9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f693ff9",
    "outputId": "9ebdd754-906f-4178-a31a-899ed3c7d304"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"北京，北京市\"\\n}'}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a ChatPromptTemplate object using the ChatPromptTemplate.from_messages method\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the bind method to bind functions parameters\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "\n",
    "runnable = prompt | model\n",
    "\n",
    "# Call the invoke method\n",
    "runnable.invoke({\"input\": \"北京天气怎么样？\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5fb09",
   "metadata": {
    "id": "89a5fb09"
   },
   "source": [
    "### 3.2 Multiple function bindings\n",
    "\n",
    "At the same time, we can also define multiple `function`s, and the big model can automatically determine which function to use during the conversation. Here we define two functions. The first function is similar to the previous `weather_search`, which searches for the weather of a given airport. Then we also define a sports news search `sports_search`. The weather query function `weather_search` accepts the parameter airport_code, which is the airport code, and the sports news search function `sports_search` accepts the parameter team_name, which is the sports team name. Since we don't need to run these functions here, the big model automatically determines whether to call these functions by asking questions and returns the parameters, and will not call them directly for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b95f4029",
   "metadata": {
    "id": "b95f4029"
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"weather_search\",\n",
    "        \"description\": \"搜索给定机场代码的天气\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"airport_code\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"要获取天气的机场代码\"\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"airport_code\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sports_search\",\n",
    "        \"description\": \"搜索最近体育赛事的新闻\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"team_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"要搜索的体育队名\"\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"team_name\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a77d53",
   "metadata": {
    "id": "d4a77d53"
   },
   "source": [
    "Then we can use the function to bind the big model and define a simple chain. We can see that after we ask relevant questions, the big model can automatically judge and correctly return the parameters and know that the function needs to be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "519e761d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "519e761d",
    "outputId": "7ad43d65-bd62-4339-c8f8-e5c74a414dac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'sports_search', 'arguments': '{\\n  \"team_name\": \"爱国者队\"\\n}'}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bind large model\n",
    "model = model.bind(functions=functions)\n",
    "runnable = prompt | model\n",
    "\n",
    "runnable.invoke({\"input\": \"爱国者队昨天表现的怎么样?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b988820",
   "metadata": {
    "id": "0b988820"
   },
   "source": [
    "## 4. Fallbacks\n",
    "\n",
    "When using early OpenAI models such as \"text-davinci-001\", these models do not support formatted output results during the conversation, that is, they all output results in the form of strings, which sometimes brings some trouble to us when we need to parse the output of LLM. For example, the following example uses the early model \"text-davinci-001\" to answer user questions. We hope that llm can output results in json format.\n",
    "\n",
    "We define the OpenAI model and create a simple chain to add json to output results in json format. We let simple_model write three poems and output them in josn format. Each poem must contain: `title, author and first sentence of the poem`. We will find that the result is only a string, and the content in the specified format cannot be output. Although there are some `[` in it, it is essentially a large string, which makes it impossible for us to parse the output.\n",
    "\n",
    "> Since OpenAI retired the model text-davinci-001 on January 4, 2024, you will use the replacement model gpt-3.5-turbo-instruct recommended by OpenAI.\n",
    "\n",
    "When using language models, you may often encounter problems from the underlying API, whether these problems are rate limits or downtime. Therefore, when you move your LLM application into a real production environment, it becomes increasingly important to guard against these problems. That's why we introduced the concept of `Fallbacks`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d93a1",
   "metadata": {},
   "source": [
    "### 4.1 Formatting output using early models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11e2b2e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "11e2b2e2",
    "outputId": "1294676f-7fbf-4185-ac16-49fa5d0512f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\n{\\n  \"title\": \"春风\",\\n  \"author\": \"李白\",\\n  \"first_line\": \"春风又绿江南岸\",\\n  \"content\": [\\n    \"春风又绿江南岸\",\\n    \"花开满树柳如丝\",\\n    \"鸟儿欢唱天地宽\",\\n    \"人间春色最宜人\"\\n  ]\\n}\\n\\n{\\n  \"title\": \"夜雨\",\\n  \"author\": \"杜甫\",\\n  \"first_line\": \"夜雨潇潇\",\\n  \"content\": [\\n    \"夜雨潇潇\",\\n    \"孤灯照旧\",\\n    \"思念如潮\",\\n    \"泛滥心头\"\\n  ]\\n}\\n\\n{\\n  \"title\": \"山行\",\\n  \"author\": \"王维\",\\n  \"first_line\": \"远上寒山石径斜\",\\n  \"content\": [\\n    \"远上寒山石径斜\",\\n    \"白云生处有人家\",\\n    \"停车坐爱枫林晚\",\\n    \"霜叶红于二月花\"\\n  ]\\n}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "# Using an early OpenAI model\n",
    "simple_model = OpenAI(\n",
    "    temperature=0,\n",
    "    max_tokens=1000,\n",
    "    model=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "simple_chain = simple_model | json.loads\n",
    "\n",
    "challenge = \"写三首诗，并以josn格式输出，每首诗必须包含:标题，作者和诗的第一句。\"\n",
    "\n",
    "simple_model.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22ae8e",
   "metadata": {
    "id": "6a22ae8e"
   },
   "source": [
    "If we use `simple_chain` to run, we will find that there is a json decoding error, because the returned result is a string and cannot be parsed, so the following code will report an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ee6ba5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "08ee6ba5",
    "outputId": "2ef7f71f-4398-47ed-a8d8-bcaa75bd91d8"
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 15 column 1 (char 147)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7b2363c45b31>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchallenge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2051\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   2054\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3505\u001b[0m         \u001b[0;34m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3507\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m   3508\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m             output = cast(\n\u001b[1;32m   1245\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1247\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3381\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3383\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m   3384\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3385\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 15 column 1 (char 147)"
     ]
    }
   ],
   "source": [
    "simple_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c534366",
   "metadata": {
    "id": "1c534366"
   },
   "source": [
    "### 4.2 Formatting output using the new model\n",
    "\n",
    "So we will find that the early version of the OpenAI model does not support formatted output, so even if LangChain is used and `json.load` is added, errors will still occur, but if we use the new `gpt-3.5-turbo` model, this problem will not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e34ed7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0e34ed7",
    "outputId": "f3ae66a2-b0c7-4557-8914-687ac20b2a08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': '春风',\n",
       "  'author': '李白',\n",
       "  'first_line': '春风又绿江南岸。',\n",
       "  'content': '春风又绿江南岸，明月何时照我还。'},\n",
       " 'poem2': {'title': '静夜思',\n",
       "  'author': '杜甫',\n",
       "  'first_line': '床前明月光，',\n",
       "  'content': '床前明月光，疑是地上霜。'},\n",
       " 'poem3': {'title': '登鹳雀楼',\n",
       "  'author': '王之涣',\n",
       "  'first_line': '白日依山尽，',\n",
       "  'content': '白日依山尽，黄河入海流。'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the new model by default\n",
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads\n",
    "\n",
    "chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a2605",
   "metadata": {
    "id": "603a2605"
   },
   "source": [
    "### 4.3 Fallbacks method\n",
    "\n",
    "At this point, you may wonder if there is any way to enable the early model to achieve the formatted output effect without changing too much code, rather than writing complex formatted output code to operate on the results. At this time, we can use the `fallbacks` method to give the early model such formatting capabilities. From the results, we can also see that we have successfully used `fallbacks` to give the simple model the ability to format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b1aede1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b1aede1",
    "outputId": "3823946c-a875-4807-9b0d-ba2910fe584d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': '春风',\n",
       "  'author': '李白',\n",
       "  'first_line': '春风又绿江南岸。',\n",
       "  'content': '春风又绿江南岸，明月何时照我还。'},\n",
       " 'poem2': {'title': '静夜思',\n",
       "  'author': '杜甫',\n",
       "  'first_line': '床前明月光，',\n",
       "  'content': '床前明月光，疑是地上霜。'},\n",
       " 'poem3': {'title': '登鹳雀楼',\n",
       "  'author': '王之涣',\n",
       "  'first_line': '白日依山尽，',\n",
       "  'content': '白日依山尽，黄河入海流。'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using with_fallbacks mechanism\n",
    "final_chain = simple_chain.with_fallbacks([chain])\n",
    "\n",
    "# Call the invoke method of final_chain and pass the challenge parameter\n",
    "final_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f0aac",
   "metadata": {},
   "source": [
    "### 4.4 How are fallbacks implemented?\n",
    "\n",
    "When we call LLM, it is often impossible to run LLM successfully due to underlying API problems, rate problems, or network problems. In this case, we can use fallback to solve this problem. Specifically, it uses another LLM to replace the original non-operable LLM to produce results. See the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_models.openai import ChatOpenAI\n",
    "from langchain_core.chat_models.anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic().with_fallbacks([ChatOpenAI()])\n",
    "model.invoke('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8fb2aa",
   "metadata": {},
   "source": [
    "In this case, ChatAnthropic will usually be used to answer first, but if calling ChatAnthropic fails, it will fall back to using the ChatOpenAI model to generate the response. If both LLMs fail, it will fall back to a hardcoded response. Hardcoded default responses are used to handle unusual situations or provide a fallback option when the required information cannot be obtained from external resources, such as \"Looks like our LLM providers are down. Here's a nice 🦜️ emoji for you instead.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e1841",
   "metadata": {},
   "source": [
    "If you want to learn more about fallbacks, please refer to the [official documentation](https://python.langchain.com/docs/guides/fallbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e3229d",
   "metadata": {
    "id": "96e3229d"
   },
   "source": [
    "## 5. Interface\n",
    "\n",
    "When using LangChain, there are many interfaces, among which the public standard interfaces include:\n",
    "\n",
    "- stream: stream returns output content\n",
    "- invoke: input calls chain\n",
    "- batch: call chain in parallel in the input list\n",
    "\n",
    "These also have corresponding asynchronous methods:\n",
    "\n",
    "- astream: asynchronous stream returns output content\n",
    "- ainvoke: asynchronously call chain on input\n",
    "- abatch: asynchronously call chain in parallel in the input list\n",
    "\n",
    "First, we define a simple prompt template, that is, \"Tell me a short joke about {topic}\", and then define a simple chain `Chain = prompt | LLM | OutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e228da",
   "metadata": {
    "id": "72e228da"
   },
   "outputs": [],
   "source": [
    "# Create a ChatPromptTemplate object, using the template \"Tell me a short joke about {topic}\"\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"给我讲一个关于{topic}的短笑话\"\n",
    ")\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a StrOutputParser object\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create a chain to connect prompt, model and output_parser\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b02924",
   "metadata": {
    "id": "e8b02924"
   },
   "source": [
    "### 5.1 Invoke interface\n",
    "\n",
    "Next, we use the corresponding interfaces respectively. For example, we first use the conventional `invoke` call, which is also the method shown above, and we get the corresponding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e339d019",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "e339d019",
    "outputId": "6893312f-8473-411f-d175-c351532e5646"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'当熊在森林里遇到一只兔子时，他问：“兔子先生，你有没有问题？”兔子回答道：“当然，先生熊，我有一个问题。你怎么会拉这么长的尾巴？”熊听后笑了起来：“兔子先生，这不是尾巴，这是我的领带！”'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"熊\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520f449",
   "metadata": {
    "id": "2520f449"
   },
   "source": [
    "### 5.2 batch interface\n",
    "\n",
    "Let’s try to use the `batch` interface again. We will find that the large model can return the answers to two questions. We will give the chain an input list, which can contain multiple questions, and finally return the answers to multiple questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d549ac8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d549ac8a",
    "outputId": "fbd34c84-26b2-4576-d7ce-aa73be98d755"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['好的，这是一个关于熊的短笑话：\\n\\n有一天，一只熊走进了一家酒吧。他走到吧台前，对酒保说：“请给我一杯……蜂蜜啤酒。”\\n\\n酒保疑惑地看着熊，说：“对不起，我们这里没有蜂蜜啤酒。”\\n\\n熊有些失望地叹了口气，然后说：“好吧，那就给我来一杯……草莓酒吧。”\\n\\n酒保摇摇头，说：“抱歉，我们也没有草莓酒。”\\n\\n熊又叹了口气，然后说：“那请给我来一杯……蜜糖红酒吧。”\\n\\n酒保实在无法忍受了，他对熊说：“对不起，我们这里没有这些奇怪的酒，你是熊，你应该知道熊只能喝蜂蜜。”\\n\\n熊听后一愣，然后脸色一变，说：“原来你们这里没有蜂蜜啤酒，草莓酒和蜜糖红酒？那请给我来一杯……白开水吧。”',\n",
       " '有一天，一只狐狸在森林里遇到了一只兔子。狐狸笑嘻嘻地对兔子说：“喂，兔子，我有一个好消息和一个坏消息，你想先听哪个？”兔子有些好奇地问：“那就先告诉我好消息吧。”狐狸眯起眼睛说：“好消息是，你的智商比我高。”兔子高兴地跳了起来：“太好了，那坏消息是什么？”狐狸一脸轻松地说：“坏消息是，你的智商还比不过胡萝卜。”']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"熊\"}, {\"topic\": \"狐狸\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a41d2",
   "metadata": {
    "id": "307a41d2"
   },
   "source": [
    "### 5.3 stream interface\n",
    "\n",
    "Next, let's take a look at the `stream` interface, which is streaming output content. This function is very necessary. Sometimes it can save users the trouble of waiting and let users see words pop up one by one instead of an empty screen, which will bring a better user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f934e46d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f934e46d",
    "outputId": "872109e2-47cb-4da6-a439-eb010ed3530d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "好\n",
      "的\n",
      "，\n",
      "这\n",
      "是\n",
      "一个\n",
      "关\n",
      "于\n",
      "熊\n",
      "的\n",
      "短\n",
      "笑\n",
      "话\n",
      "：\n",
      "\n",
      "\n",
      "有\n",
      "一\n",
      "天\n",
      "，\n",
      "一\n",
      "只\n",
      "小\n",
      "熊\n",
      "走\n",
      "进\n",
      "了\n",
      "一\n",
      "家\n",
      "酒\n",
      "吧\n",
      "。\n",
      "他\n",
      "走\n",
      "到\n",
      "吧\n",
      "台\n",
      "前\n",
      "，\n",
      "对\n",
      "酒\n",
      "保\n",
      "说\n",
      "：“\n",
      "酒\n",
      "保\n",
      "，\n",
      "给\n",
      "我\n",
      "一\n",
      "杯\n",
      "牛\n",
      "奶\n",
      "。”\n",
      "\n",
      "\n",
      "酒\n",
      "保\n",
      "惊\n",
      "讶\n",
      "地\n",
      "问\n",
      "道\n",
      "：“\n",
      "小\n",
      "熊\n",
      "，\n",
      "你\n",
      "怎\n",
      "么\n",
      "会\n",
      "来\n",
      "这\n",
      "里\n",
      "喝\n",
      "牛\n",
      "奶\n",
      "？\n",
      "”\n",
      "\n",
      "\n",
      "小\n",
      "熊\n",
      "深\n",
      "情\n",
      "地\n",
      "回\n",
      "答\n",
      "：“\n",
      "因\n",
      "为\n",
      "我的\n",
      "妈\n",
      "妈\n",
      "说\n",
      "，\n",
      "每\n",
      "当\n",
      "我\n",
      "喝\n",
      "酒\n",
      "的\n",
      "时\n",
      "候\n",
      "，\n",
      "我\n",
      "都\n",
      "会\n",
      "变\n",
      "得\n",
      "熊\n",
      "样\n",
      "！”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in chain.stream({\"topic\": \"熊\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda98771",
   "metadata": {
    "id": "fda98771"
   },
   "source": [
    "### 5.4 Asynchronous interface\n",
    "\n",
    "We can also try to call asynchronously, using `ainvoke`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd372fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "cd372fec",
    "outputId": "1232ac0b-7900-493c-aec6-7b99abb928d8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'好的，以下是一个关于熊的短笑话：\\n\\n有一只熊走进了一家餐厅，他走到柜台前，对着服务员说：“我想要一杯咖啡和......嗯，一块...牛肉三明治。”\\n服务员疑惑地看着熊，然后问道：“对不起，先生，你是真的想要一块牛肉三明治吗？”\\n熊点了点头。\\n服务员又问：“那请问为什么你要来这里点餐呢？”\\n熊回答道：“因为我是个熊啊！”'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await chain.ainvoke({\"topic\": \"熊\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c23d0",
   "metadata": {
    "id": "926c23d0"
   },
   "source": [
    "## 6. English prompt words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nf-C5Y-9TR2d",
   "metadata": {
    "id": "nf-C5Y-9TR2d"
   },
   "source": [
    "**1. Build a simple chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "urymAcXkTOQ2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "urymAcXkTOQ2",
    "outputId": "97d1aee3-86a0-44cb-c534-6db4ffb6a11b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Why did the bear bring a flashlight to the party?\\n\\nBecause he wanted to be the \"light\" of the bearbecue!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b6bd6",
   "metadata": {},
   "source": [
    "**2.1 Building a simple document database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "J-IpScccTHtL",
   "metadata": {
    "id": "J-IpScccTHtL"
   },
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4nWE8nLjTJuT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nWE8nLjTJuT",
    "outputId": "65742853-5146-4a1c-fb0b-d1e456532b95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='harrison worked at kensho'),\n",
       " Document(page_content='bears like to eat honey')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hygSW50bTXzV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hygSW50bTXzV",
    "outputId": "2f88f2d7-781d-41d0-c50e-fa18e0420c3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='bears like to eat honey'),\n",
       " Document(page_content='harrison worked at kensho')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"what do bears like to eat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "-6iG51i5TZGJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-6iG51i5TZGJ",
    "outputId": "29de378b-2151-43fc-d2fc-f6a1b8d16896"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabda125",
   "metadata": {},
   "source": [
    "**3.2 Using RunnableMap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "JoLBmwfETg1L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoLBmwfETg1L",
    "outputId": "73127639-c19e-400c-e26e-8f68dfd93635"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='bears like to eat honey')],\n",
       " 'question': 'where did harrison work?'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})\n",
    "\n",
    "inputs.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HqjgfGT8Tj2Z",
   "metadata": {
    "id": "HqjgfGT8Tj2Z"
   },
   "source": [
    "**3.1 Single function binding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "_XZPKC1_Th6S",
   "metadata": {
    "id": "_XZPKC1_Th6S"
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0UPE0nWDTm3T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UPE0nWDTm3T",
    "outputId": "a4762623-c42f-4950-a21e-da73f7efe0c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'weather_search', 'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}'}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "\n",
    "runnable = prompt | model\n",
    "\n",
    "runnable.invoke({\"input\": \"what is the weather in sf\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a3fd8",
   "metadata": {},
   "source": [
    "**3.2 Multiple function binding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "U2R9nKncTpy6",
   "metadata": {
    "id": "U2R9nKncTpy6"
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "        {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "i9yES-wwTrYO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9yES-wwTrYO",
    "outputId": "c4e1ae94-d087-454b-a8c1-7fab9df2b790"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'sports_search', 'arguments': '{\\n  \"team_name\": \"patriots\"\\n}'}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.bind(functions=functions)\n",
    "\n",
    "runnable = prompt | model\n",
    "\n",
    "runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a9405",
   "metadata": {},
   "source": [
    "**4.1 Formatting output using early models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4lECkukwT4ua",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "4lECkukwT4ua",
    "outputId": "88c67d19-6d26-406a-9d26-08827794dc4e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\n{\\n    \"title\": \"Autumn Leaves\",\\n    \"author\": \"Emily Dickinson\",\\n    \"first_line\": \"The leaves are falling, one by one\"\\n}\\n\\n{\\n    \"title\": \"The Ocean\\'s Song\",\\n    \"author\": \"Pablo Neruda\",\\n    \"first_line\": \"I hear the ocean\\'s song, a symphony of waves\"\\n}\\n\\n{\\n    \"title\": \"A Winter\\'s Night\",\\n    \"author\": \"Robert Frost\",\\n    \"first_line\": \"The snow falls softly, covering the ground\"\\n}'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model = OpenAI(\n",
    "    temperature=0,\n",
    "    max_tokens=1000,\n",
    "    model=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "simple_chain = simple_model | json.loads\n",
    "\n",
    "challenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
    "\n",
    "simple_model.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c60800",
   "metadata": {},
   "source": [
    "**Early models are not supported and will result in decoding errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aX0oo-25T9hE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "aX0oo-25T9hE",
    "outputId": "8425f5f7-5891-4448-d67d-be89208e05a9"
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 9 column 1 (char 125)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-7b2363c45b31>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchallenge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2051\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   2054\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3505\u001b[0m         \u001b[0;34m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3507\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m   3508\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m             output = cast(\n\u001b[1;32m   1245\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1247\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3381\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3383\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m   3384\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3385\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 9 column 1 (char 125)"
     ]
    }
   ],
   "source": [
    "simple_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a60b7",
   "metadata": {},
   "source": [
    "**4.2 Newer models can format output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "oLFppvMMT-w7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLFppvMMT-w7",
    "outputId": "3fa0a0c1-3e9b-44ca-acaa-4b5fe5915c43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'Whispers of the Wind',\n",
       "  'author': 'Emily Rivers',\n",
       "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
       " 'poem2': {'title': 'Silent Serenade',\n",
       "  'author': 'Jacob Moore',\n",
       "  'first_line': 'In the stillness of night, a silent serenade'},\n",
       " 'poem3': {'title': 'Dancing Shadows',\n",
       "  'author': 'Sophia Anderson',\n",
       "  'first_line': 'Shadows dance upon the walls, a secret ballet'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads\n",
    "\n",
    "chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fe472",
   "metadata": {},
   "source": [
    "**4.3 Fallback mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "mWQ6qEwSUA0y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWQ6qEwSUA0y",
    "outputId": "1ca395fa-976f-4f5d-c0f4-f2bcbd0ba325"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'Whispers of the Wind',\n",
       "  'author': 'Emily Rivers',\n",
       "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
       " 'poem2': {'title': 'Silent Serenade',\n",
       "  'author': 'Jacob Moore',\n",
       "  'first_line': 'In the stillness of night, a silent serenade'},\n",
       " 'poem3': {'title': 'Dancing Shadows',\n",
       "  'author': 'Sophia Anderson',\n",
       "  'first_line': 'Shadows dance upon the moonlit floor'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = simple_chain.with_fallbacks([chain])\n",
    "\n",
    "final_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd87734",
   "metadata": {},
   "source": [
    "**V. Interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "oenACTHsUFlm",
   "metadata": {
    "id": "oenACTHsUFlm"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5feb063",
   "metadata": {},
   "source": [
    "**5.1 invoke interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "uhpMBbexUHK2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "uhpMBbexUHK2",
    "outputId": "bd63b7f7-7d11-474e-bdd7-ad59a45323ff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6e2de",
   "metadata": {},
   "source": [
    "**5.2 Batch interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ptSp8UhQUILV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptSp8UhQUILV",
    "outputId": "b3125d7e-c35b-4ea3-999f-75543f36ac34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.language_models.llms:Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-GXqDV5Kltm74g4hGPYEjLdb0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\",\n",
       " 'Why did the frog take the bus to work?\\n\\nBecause his car got toad away!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b9ed7",
   "metadata": {},
   "source": [
    "**5.3 Stream Interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "Ehj9MzGPUI_I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ehj9MzGPUI_I",
    "outputId": "239b158c-6283-4892-d6f5-bcbe69329cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " don\n",
      "'t\n",
      " bears\n",
      " wear\n",
      " shoes\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " they\n",
      " have\n",
      " bear\n",
      " feet\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2858691",
   "metadata": {},
   "source": [
    "**5.4 Asynchronous Interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "xucFn07uUJ7Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xucFn07uUJ7Z",
    "outputId": "54545335-949b-463d-b8e2-41bb7c1076c1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
