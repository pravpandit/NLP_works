# अध्याय 7 प्रोजेक्ट अभ्यास

<!-- यह अध्याय शिक्षार्थियों को बेहतर ढंग से समझने में मदद करने के लिए एक व्यापक परियोजना अभ्यास के माध्यम से पहले शुरू की गई मॉडल संपीड़न विधियों को एकीकृत करेगा -->

&emsp;&emsp;इस ट्यूटोरियल में उल्लिखित मॉडल संपीड़न एल्गोरिदम, प्रूनिंग, क्वांटिज़ेशन, डिस्टिलेशन और न्यूरल नेटवर्क आर्किटेक्चर खोज, व्यावहारिक अनुप्रयोगों में परस्पर लंबवत हैं, और प्रत्येक के अपने स्वयं के लागू परिदृश्य और ट्रेड-ऑफ़ हैं, हालांकि, हम कई को जोड़ सकते हैं modelcompreबेहतर परिणाम प्राप्त करने के लिए ssion एल्गोरिदम।

&emsp;&emsp;वर्तमान में, अच्छे परिणाम, समृद्ध अनुसंधान और अनुप्रयोग के साथ मॉडल संपीड़न एल्गोरिदम के संयोजन में मुख्य रूप से शामिल हैं:
- छंटाई और परिमाणीकरण का संयोजन
- ज्ञान आसवन और परिमाणीकरण का संयोजन
- काट-छांट और ज्ञान आसवन का संयोजन
- तंत्रिका वास्तुकला खोज और ज्ञान आसवन का संयोजन

&emsp;&emsp;इसके अलावा, कुछ हालिया अध्ययनों ने कुछ अधिक व्यापक संयोजन का भी प्रस्ताव दिया हैमॉडल संपीड़न एल्गोरिदम की राशन विधियाँ, जैसे:
- छंटाई और परिमाणीकरण के साथ ज्ञान आसवन का संयोजन
- छंटाई और परिमाणीकरण के साथ तंत्रिका नेटवर्क वास्तुकला खोज का संयोजन

&emsp;&emsp;यह ट्यूटोरियल ResNet-18 मॉडल में विभिन्न संपीड़न विधियों को लागू करके, सामान्य छवि वर्गीकरण कार्यों के लिए व्यावहारिक अनुप्रयोगों को संयोजित करेगा, और हार्डवेयर दक्षता में सुधार, मॉडल क्षमता प्रतिधारण (कठिनाई पर विचार करते हुए) के परिप्रेक्ष्य से तुलना करेगाकार्यान्वयन की, मुख्य रूप से CIFAR-10 डेटासेट पर आधारित), और एल्गोरिदम कार्यान्वयन की कठिनाई

<!-- इसके अलावा, कोड के दो संस्करण, केवल-सीपीयू और क्यूडा, प्रदान किए गए हैं -->

<!-- कार्य: एलएलएम मॉडल संपीड़न का अभ्यास अनुभाग जोड़ें -->

## 7.1 छंटाई और परिमाणीकरण का संयोजन

&emsp;&emsp;प्रूनिंग और परिमाणीकरण दोनों प्रभावी ढंग से मॉडल वजन और सक्रियण मूल्यों में अतिरेक को कम कर सकते हैं, जिससे मॉडल का आकार और गणना कम हो जाती है।

&emsp;&emsp;पिछला अध्यायउल्लिखित:

>कांट-छांट (समय के आधार पर) को निम्न में विभाजित किया जा सकता है:
> - प्रशिक्षण के बाद छंटाई (स्थैतिक विरलता)
> - प्रशिक्षण के दौरान काट-छाँट (गतिशील विरलता)
> - प्रशिक्षण से पहले काट-छाँट

>और परिमाणीकरण को मुख्य रूप से विभाजित किया गया है:
> - प्रशिक्षण के बाद परिमाणीकरण पीटीक्यू
> - परिमाणीकरण-जागरूक प्रशिक्षण QAT

&emsp;&emsp;मौजूदा मॉडलों की तैनाती और संपीड़न त्वरण के लिए, सामान्य समाधान बड़े भाषा मॉडल के लिए, प्रशिक्षण के बाद छंटाई और प्रशिक्षण के बाद परिमाणीकरण को संयोजित करना हैएस एलएलएम, क्यूएलओआरए का उपयोग मेमोरी ओवरहेड और फाइन-ट्यूनिंग की डेटा आवश्यकताओं को कम करने के लिए पुनरावृत्त छंटाई के दौरान भी किया जा सकता है।

### 7.1.1 प्रारंभिक प्रयास (सरल संयोजन)
&emsp;&emsp;सरल और सहज रूप से, छंटाई और परिमाणीकरण के संयोजन की विधियाँ हो सकती हैं:
&emsp;&emsp;**पहले छँटाई करें, फिर परिमाणित करें**: पहले मॉडल की छंटाई करें, फिर काँटे गए मॉडल की मात्रा निर्धारित करें।
&emsp;&emsp;**पहले मात्रा निर्धारित करें, फिर काट-छाँट करें**: पहले मॉडल की मात्रा निर्धारित करें, फिर परिमाणित मॉडल की छँटाई करें।

&emsp;&emsp;हालाँकि,ऐसे सरल मॉडलों को एक साथ जोड़ने पर समस्याओं की एक श्रृंखला उत्पन्न होगी:
**पहले छंटाई करें, फिर मात्रा निर्धारित करें**
- काट-छाँट किया गया मॉडल सूचना चैनलों की संख्या को कम कर देता है, जिसके परिणामस्वरूप मॉडल की मजबूती कम हो जाती है, और इस प्रकार परिमाणीकरण द्वारा शुरू किए गए शोर के प्रति सहनशीलता कम हो जाती है, जिसके परिणामस्वरूप परिमाणित मॉडल का प्रदर्शन कम हो जाता है।
- प्रूनिंग आमतौर पर वजन को शून्य पर सेट करती है, जिसके परिणामस्वरूप स्पार्सिटी होती है, हालांकि, परिमाणीकरण इन शून्य मानों को गैर-शून्य परिमाणित मानों में मैप कर सकता हैविरलता को कम करके। विशेष रूप से, यदि परिमाणीकरण में फाइन-ट्यूनिंग की जाती है और छंटाई के बाद परिमाणीकरण-जागरूक प्रशिक्षण (क्यूएटी) किया जाता है, तो इनमें से कुछ शून्य मानों को गैर-शून्य मानों में भी समायोजित किया जाएगा। प्रशिक्षण के दौरान (यदि कोई विशेष उपचार नहीं है), जिससे विरलता कम हो जाती है।

**पहले परिमाणीकरण करें, फिर छंटाई** के लिए
- चूँकि परिमाणित तंत्रिका नेटवर्क पर बैकप्रॉपैगेशन करना अधिक कठिन है, बेहतर प्रदर्शन (पुनरावृत्त प्रूनिंग, आदि) के साथ प्रूनिंग विधियाँ लगभग असंभव हैंपुनर्प्रशिक्षण (फाइन-ट्यूनिंग, रिवाइंडिंग...) से दृष्टांत।

&emsp;&emsp;इसलिए, हमें इन समस्याओं को हल करने के लिए छंटाई और परिमाणीकरण के संयोजन पर गहन अध्ययन करने की आवश्यकता है।

### 7.1.2 समस्या विश्लेषण एवं समाधान

फ्रेडरिक तुंग और ग्रेग मोरी (2018) ने पेपर में [CLIP-Q: इन-पैरेलल प्रूनिंग-क्वांटाइजेशन द्वारा डीप नेटवर्क कंप्रेशन लर्निंग] (https://doi.org/10.1109/cvpr.2018.00821) में प्रूनिंग और क्वांटाइजेशन को संयोजित करने का प्रस्ताव रखा है। एक सीखने की रूपरेखा (बायेसियन अनुकूलन), प्रू का संयोजनविभाजन के माध्यम से निंग और परिमाणीकरण, और समानांतर में फाइन-ट्यूनिंग। अंतराल)

शोकाई ये एट अल (2018) ने पेपर में एडीएमएम के आधार पर वेट प्रूनिंग और क्लस्टरिंग/क्वांटाइजेशन का एक एकीकृत ढांचा प्रस्तावित किया है [एडीएमएम का उपयोग करके डीएनएन वेट प्रूनिंग और वेट क्लस्टरिंग/क्वांटाइजेशन का एक एकीकृत ढांचा](https://arxiv.org/abs/1811.01907), और वजन परिवर्तन के कारण होने वाली त्रुटि को कम करने की उप-समस्याओं में मॉडल त्रुटि को कम करने की गैर-उत्तल अनुकूलन समस्या को परिवर्तित करने के लिए वैकल्पिक दिशा गुणक विधि (एडीएमएम) का उपयोग किया। और पूर्वाग्रह के कारण होने वाली त्रुटियाँ बदलती हैं, और उन्हें प्रभावी ढंग से हल करते हैं।

हाईचुआन यांग एट अल (2020) ने पेपर में एडीएमएम के आधार पर वेट प्रूनिंग और क्लस्टरिंग/क्वांटिज़ेशन का एक एकीकृत ढांचा प्रस्तावित किया है [स्पार्सिटी-क्वांटाइज़ेशन द्वारा स्वचालित तंत्रिका नेटवर्क संपीड़नसंयुक्त शिक्षण: एक प्रतिबंधित अनुकूलन-आधारित दृष्टिकोण [बायेसियन बिट्स: यूनिफाइंग क्वांटाइजेशन एंड प्रूनिंग] (https://arxiv.org/abs/1910.05897), लक्ष्य मॉडल आकार के अनुसार DNN को स्वचालित रूप से संयुक्त रूप से प्रून और क्वांटाइज करने के लिए एक विधि प्रस्तावित है प्रत्येक परत के संपीड़न अनुपात को मैन्युअल रूप से सेट करने के लिए किसी भी हाइपरपैरामीटर का उपयोग किए बिना, एडीएमएम का उपयोग करके एकीकृत छंटाई और परिमाणीकरण पर।

मार्ट वैन बालेन एट अल (2020) ने एक ग्रेडिएंट-अनुकूलित छंटाई और मिश्रित-सटीक परिमाणीकरण विधि का प्रस्ताव दियापेपर [बायेसियन बिट्स: यूनिफाइंग क्वांटाइजेशन एंड प्रूनिंग] (https://arxiv.org/abs/2005.07093), जो क्वांटाइजेशन समस्या को बिट चौड़ाई दोहरीकरण समस्याओं की एक श्रृंखला में विघटित करता है, प्रत्येक नई बिट चौड़ाई पर, पूर्ण के बीच का अवशिष्ट -परिशुद्धता मान और पिछले गोल मान को परिमाणित किया जाएगा। फिर यह तय किया जाता है कि उच्च प्रभावी बिट चौड़ाई और कम परिमाणीकरण शोर प्राप्त करने के लिए इस परिमाणीकरण अवशिष्ट को जोड़ा जाए या नहीं, क्योंकि पावर-ऑफ-2 बिट चौड़ाई से शुरू होकर, यह अपघटन होता हैमैं हमेशा एक हार्डवेयर-अनुकूल कॉन्फ़िगरेशन तैयार करूंगा और अतिरिक्त 0-बिट विकल्प के माध्यम से प्रूनिंग और क्वांटाइजेशन के एकीकृत दृश्य के रूप में काम करूंगा।

गिल शोम्रॉन एट अल (2021) ने पेपर [पोस्ट-ट्रेनिंग स्पार्सिटी-अवेयर क्वांटाइजेशन] (https://arxiv.org/abs/2105.11010) में काटे गए मॉडल के कुशल क्वांटाइजेशन के लिए एक विधि प्रस्तावित की, सबसे पहले, वजन और सक्रियण शून्य मानों को छोड़कर 8 बिट्स तक परिमाणित किया जाता है, फिर, युग्मित सक्रियण मानों का विश्लेषण यह देखने के लिए किया जाता है कि क्या शून्य हैंयदि नहीं, तो सभी को 4 बिट्स तक परिमाणित किया जाता है, इस प्रकार सटीकता और व्यावहारिक हार्डवेयर प्रदर्शन में थोड़ी कमी आती है (इस पेपर में काट-छाँट मुख्य रूप से असंरचित और गतिशील रूप से विरल है)।

शिपेंग बाई एट अल (2023) ने पेपर में एकीकृत डेटा-मुक्त संपीड़न (यूडीएफसी) नामक एक नई रूपरेखा का प्रस्ताव दिया है [एकीकृत डेटा-मुक्त संपीड़न: फाइन-ट्यूनिंग के बिना प्रूनिंग और क्वांटाइजेशन] (https://arxiv.org/abs/) 2308.07209), जो बिना किसी डेटा के एक साथ छंटाई और परिमाणीकरण कर सकता हैए और फाइन-ट्यूनिंग प्रक्रिया, सबसे पहले, यह माना जाता है कि क्षतिग्रस्त चैनल की जानकारी का हिस्सा (उदाहरण के लिए, काटे गए या परिमाणित) अन्य चैनलों के रैखिक संयोजनों द्वारा संरक्षित किया जा सकता है, और फिर पुनर्निर्माण फॉर्म पुनर्प्राप्ति की धारणा से प्राप्त होता है। संपीड़न के कारण खोई गई जानकारी अंत में, हम मूल नेटवर्क और उसके संपीड़ित नेटवर्क के बीच पुनर्निर्माण त्रुटि तैयार करते हैं, और सैद्धांतिक रूप से बंद-फॉर्म समाधान प्राप्त करते हैं।

&emsp;&emsp;