# अध्याय 7 प्रोजेक्ट अभ्यास

<!-- यह अध्याय शिक्षार्थियों को बेहतर ढंग से समझने में मदद करने के लिए एक व्यापक परियोजना अभ्यास के माध्यम से पहले शुरू की गई मॉडल संपीड़न विधि को एकीकृत करेगा -->

&emsp;&emsp;इस ट्यूटोरियल में उल्लिखित मॉडल संपीड़न एल्गोरिदम, प्रूनिंग, क्वांटिज़ेशन, डिस्टिलेशन और न्यूरल नेटवर्क आर्किटेक्चर खोज, व्यावहारिक अनुप्रयोगों में एक-दूसरे के लंबवत हैं, और प्रत्येक के अपने स्वयं के लागू परिदृश्य और ट्रेड-ऑफ़ हैं। हालाँकि, हम बेहतर परिणाम प्राप्त करने के लिए कई मॉडल संपीड़न एल्गोरिदम को जोड़ सकते हैं।

&emsp;&emsp;वर्तमान में, अच्छे परिणाम और समृद्ध अनुसंधान और अनुप्रयोग के साथ मॉडल संपीड़न एल्गोरिदम के संयोजन में मुख्य रूप से शामिल हैं:
- काट-छांट और परिमाणीकरण का संयोजन
- ज्ञान आसवन और परिमाणीकरण का संयोजन
- काट-छांट और ज्ञान आसवन का संयोजन
- तंत्रिका वास्तुकला खोज और ज्ञान आसवन का संयोजन

इसके अलावा, कुछ हालिया अध्ययनों ने मॉडल संपीड़न एल्गोरिदम के कुछ और व्यापक संयोजन तरीकों का भी प्रस्ताव दिया है, जैसे:
- ज्ञान आसवन और छंटाई, मात्रात्मक संयोजन
- तंत्रिका नेटवर्क वास्तुकला खोज और छंटाई, मात्रात्मक संयोजन

&emsp;&emsp;इस ट्यूटोरियल को वास्तविक अनुप्रयोगों के साथ जोड़ा जाएगा, सामान्य छवि वर्गीकरण कार्यों के लिए, ResNet-18 मॉडल में विभिन्न संपीड़न विधियों को लागू करके, और हार्डवेयर दक्षता में सुधार करके, मॉडल क्षमताओं को बरकरार रखा जाता है (कार्यान्वयन की कठिनाई को ध्यान में रखते हुए)। एल्गोरिथ्म कार्यान्वयन की कठिनाई की तुलना में मुख्य रूप से CIFAR-10 डेटा सेट पर आधारित है)।

<!-- इसके अलावा, कोड के दो संस्करण, केवल-सीपीयू और क्यूडा, प्रदान किए जाते हैं -->

<!-- कार्य: एलएलएम मॉडल संपीड़न अभ्यास भाग जोड़ें -->

## 7.1 छंटाई और परिमाणीकरण का संयोजन

&emsp;&emsp;प्रूनिंग और परिमाणीकरण दोनों मॉडल भार और सक्रियण मूल्यों में अतिरेक को कम करने के लिए अच्छी तरह से काम कर सकते हैं, जिससे मॉडल आकार और कम्प्यूटेशनल प्रयास कम हो जाते हैं।

&emsp;&emsp;पिछले अध्याय में उल्लेख किया गया है:

>कांट-छांट (समय के नजरिए से) को निम्न में विभाजित किया जा सकता है:
> - प्रशिक्षण के बाद छंटाई (स्थैतिक विरलता)
> - प्रशिक्षण के दौरान काट-छाँट (गतिशील विरलता)
> - प्रशिक्षण से पहले काट-छाँट

>परिमाणीकरण को मुख्य रूप से विभाजित किया गया है:
> - प्रशिक्षण के बाद मात्रा का ठहराव पीटीक्यू
> - मात्रात्मक जागरूकता प्रशिक्षण QAT

&emsp;&emsp;मौजूदा मॉडलों को तैनात करने और संपीड़ित करने और तेज करने के लिए, एक सामान्य समाधान प्रशिक्षण के बाद की छंटाई और प्रशिक्षण के बाद के परिमाणीकरण को संयोजित करना है। इसके अलावा, बड़े भाषा मॉडल एलएलएम के लिए, क्यूएलओआरए का उपयोग पुनरावृत्त छंटाई करते समय मेमोरी ओवरहेड और फाइन-ट्यूनिंग की डेटा आवश्यकताओं को कम करने के लिए भी किया जा सकता है।

### 7.1.1 प्रारंभिक प्रयास (सरल संयोजन)
&emsp;&emsp;सरल और सहज शब्दों में, छंटाई और मात्रा निर्धारण को संयोजित करने वाली विधियाँ इस प्रकार हो सकती हैं:
&emsp;&emsp;**पहले काट-छाँट करें, फिर मात्रा निर्धारित करें**: पहले मॉडल की काट-छाँट करें, और फिर काँटे गए मॉडल की मात्रा निर्धारित करें।
&emsp;&emsp;**पहले मात्रा निर्धारित करें, फिर काट-छाँट करें**: पहले मॉडल की मात्रा निर्धारित करें, और फिर मात्राबद्ध मॉडल की काट-छाँट करें।

&emsp;&emsp;हालाँकि, इतने सरल के लिएजब इन्हें एक साथ रखा जाएगा, तो समस्याओं की एक श्रृंखला उत्पन्न होगी:
**पहले छंटाई करें, फिर मात्रा निर्धारित करें**
- प्रूनिंग के बाद मॉडल सूचना के चैनलों की संख्या कम कर देता है, जिसके परिणामस्वरूप मॉडल की मजबूती में कमी आती है, इस प्रकार परिमाणीकरण द्वारा शुरू किए गए शोर के प्रति सहनशीलता कम हो जाती है, जिसके परिणामस्वरूप परिमाणित मॉडल के प्रदर्शन में गिरावट आती है।
-कांट-छांट आमतौर पर वजन को शून्य पर सेट कर देती है, जिससे विरलता पैदा होती है। हालाँकि, परिमाणीकरण इन शून्य मानों को गैर-शून्य परिमाणित मानों में मैप कर सकता है, जिससे विरलता कम हो जाती है। विशेष रूप से यदि परिमाणीकरण में फाइन-ट्यूनिंग की जाती है और प्रूनिंग के बाद परिमाणीकरण-जागरूक प्रशिक्षण (क्यूएटी) किया जाता है, तो इनमें से कुछ शून्य मानों को प्रशिक्षण प्रक्रिया के दौरान गैर-शून्य मानों में भी समायोजित किया जाएगा (यदि कोई विशेष नहीं है) प्रसंस्करण), जिससे विरलता कम हो जाती है।

**पहले परिमाण निर्धारित करें, फिर काट-छाँट** के लिए
- चूँकि क्वांटाइज़्ड न्यूरल नेटवर्क को बैकप्रॉपैगेशन करना अधिक कठिन होता है, बेहतर प्रदर्शन (पुनरावृत्त प्रूनिंग, आदि) के साथ प्रूनिंग विधियाँ रिट्रेनिंग (फाइन-ट्यूनिंग, रिवाइंडिंग ...) से लगभग अविभाज्य हैं।

&emsp;&emsp;इसलिए, हमें इन समस्याओं को हल करने के लिए छंटाई और परिमाणीकरण के संयोजन पर अधिक गहन शोध करने की आवश्यकता है।

### 7.1.2 समस्या विश्लेषण एवं समाधान

फ्रेडरिक तुंग और ग्रेग मोरी (2018) पेपर में [सीएलआईपी-क्यू: इन-पैरेलल प्रूनिंग-क्वांटाइजेशन द्वारा डीप नेटवर्क कंप्रेशन लर्निंग](https://doi.org/10.1109/cvpr.2018.00821) ने सीखने के ढांचे (बायेसियन ऑप्टिमाइजेशन) में प्रूनिंग और क्वांटाइजेशन को संयोजित करने, विभाजन के माध्यम से प्रूनिंग और क्वांटाइजेशन को संयोजित करने और समानांतर में फाइन-ट्यूनिंग करने का प्रस्ताव दिया। (इस आलेख में छंटाई असंरचित छंटाई का उपयोग करती है, और मॉडल के आकार के बारे में अधिक चिंतित है। इस आलेख में मात्रा का ठहराव सीधे पूर्णांकों के लिए निर्धारित नहीं है, लेकिन विभाजन अंतराल का मतलब लेता है)

शाओकाई ये एट अल (2018) ने पेपर में एडीएमएम के आधार पर वेट प्रूनिंग और क्लस्टरिंग का प्रस्ताव दिया है [एडीएमएम का उपयोग करके डीएनएन वेट प्रूनिंग और वेट क्लस्टरिंग/क्वांटाइजेशन का एक एकीकृत ढांचा] (https://arxiv.org/abs/1811.01907) / ए परिमाणीकरण के लिए एकीकृत ढाँचा, वैकल्पिक दिशा गुणक विधि (एडीएमएम) का उपयोग करके, मॉडल त्रुटि को कम करने की गैर-उत्तल अनुकूलन समस्या को वजन परिवर्तन के कारण होने वाली त्रुटि और पूर्वाग्रह परिवर्तन के कारण होने वाली त्रुटि को कम करने की उप-समस्या में परिवर्तित करने के लिए, और प्रभावी ढंग से समाधान करें.

हाईचुआन यांग एट अल (2020) पेपर में [स्पार्सिटी-क्वांटिज़ेशन संयुक्त शिक्षण द्वारा स्वचालित तंत्रिका नेटवर्क संपीड़न: एक विवश अनुकूलन-आधारित दृष्टिकोणch](https://arxiv.org/abs/1910.05897), प्रूनिंग और क्वांटाइजेशन को एकीकृत करने के लिए एडीएमएम के उपयोग के आधार पर, एक विधि प्रस्तावित है जो किसी भी हाइपरपैरामीटर का उपयोग किए बिना लक्ष्य मॉडल आकार के अनुसार स्वचालित रूप से संयुक्त रूप से डीएनएन को प्रून और क्वांटाइज कर सकती है। प्रत्येक परत के संपीड़न अनुपात को मैन्युअल रूप से सेट करने के लिए।

मार्ट वैन बालेन एट अल (2020) ने पेपर में ग्रेडिएंट ऑप्टिमाइज़ेशन के आधार पर प्रूनिंग और मिश्रित परिशुद्धता परिमाणीकरण की एक विधि प्रस्तावित की है [बायेसियन बिट्स: यूनिफाइंग क्वांटाइजेशन एंड प्रूनिंग] (https://arxiv.org/abs/2005.07093), विघटित करें बिट-चौड़ाई दोहरीकरण समस्याओं की एक श्रृंखला में परिमाणीकरण समस्या। प्रत्येक नई बिटविड्थ पर, पूर्ण-सटीक मान और पिछले गोलाकार मान के बीच अवशिष्ट अंतर को परिमाणित किया जाता है। फिर यह निर्णय लिया जाता है कि उच्च प्रभावी बिट चौड़ाई और कम परिमाणीकरण शोर प्राप्त करने के लिए इस परिमाणीकरण अवशिष्ट को जोड़ा जाए या नहीं। क्योंकि 2 बिट चौड़ाई की शक्ति से शुरू होकर, यह अपघटन हमेशा एक हार्डवेयर-अनुकूल कॉन्फ़िगरेशन का उत्पादन करेगा और अतिरिक्त 0-बिट विकल्प के साथ, छंटनी और परिमाणीकरण के एकीकृत दृश्य के रूप में कार्य करता है।

गिल शोम्रॉन एट अल (2021) पेपर में [पोस्ट-ट्रेनिंग स्पार्सिटी-अवेयर क्वांटाइजेशन](https://arxiv.org/abs/2105.1101)0) ने काटे गए मॉडलों के कुशल परिमाणीकरण के लिए एक विधि प्रस्तावित की, सबसे पहले, वजन और सक्रियणों को 8 बिट्स तक परिमाणित करने के लिए शून्य मान छोड़ें, और फिर विश्लेषण करें कि क्या युग्मित सक्रियण मानों में शून्य हैं, अन्यथा 8 बिट्स बनाए रखें। सभी को 4 बिट्स में परिमाणित किया गया है, जिसके परिणामस्वरूप सटीकता और व्यावहारिक हार्डवेयर प्रदर्शन में थोड़ी कमी आई है। (इस लेख में प्रूनिंग मुख्य रूप से असंरचित और गतिशील विरल है)

शिपेंग बाई एट अल (2023) ने एकीकृत डेटा-मुक्त संपीड़न (यूडीएफसी) नामक एक विधि का प्रस्ताव दिया, एक नया ढांचा जो बिना किसी डेटा और फाइन-ट्यूनिंग प्रक्रिया के एक साथ छंटाई और परिमाणीकरण कर सकता है। पहले यह माना जाता है कि दूषित (उदाहरण के लिए, काटे गए या परिमाणित) चैनलों की जानकारी का हिस्सा अन्य चैनलों के रैखिक संयोजनों द्वारा संरक्षित किया जा सकता है, और फिर पुनर्निर्मित रूप संपीड़न के कारण होने वाली सूचना हानि को पुनर्प्राप्त करने की धारणा से प्राप्त होता है। अंत में, हम मूल नेटवर्क और उसके संपीड़ित नेटवर्क के बीच पुनर्निर्माण त्रुटि तैयार करते हैं और सैद्धांतिक रूप से एक बंद-फॉर्म समाधान प्राप्त करते हैं।

&emsp;&emsp;