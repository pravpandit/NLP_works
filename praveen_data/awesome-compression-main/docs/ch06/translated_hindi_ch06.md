# अध्याय 6 ज्ञान आसवन


![](images/Distillation.jpeg)



## 6.1 परिचय


![](छवियां/1-1.png)


&emsp;&emsp;आमतौर पर, एक ही मशीन सीखने के कार्य के लिए, जटिल नेटवर्क में अधिक पैरामीटर और अधिक मात्रा में गणना होती है, लेकिन आमतौर पर बेहतर प्रदर्शन करते हैं। इसके विपरीत, सरल नेटवर्क में कम पैरामीटर और कम गणना होती है, लेकिन उनका प्रदर्शन अपेक्षाकृत खराब हो सकता है। नॉलेज डिस्टिलेशन (केडी) एक मॉडल संपीड़न तकनीक है जो ज्ञान को बड़े (शिक्षक) मॉडल से छोटे (छात्र) मॉडल में स्थानांतरित करती है। यह शिक्षक मॉडल के आउटपुट (लॉगिट्स) वितरण या फीचर मैप (फीचर मैप) को सीखकर छात्र मॉडल को बेहतर सीखने में मदद करता है, जिससे छात्र मॉडल की सटीकता और दक्षता में सुधार होता है।

### मुख्य अवधारणाएँ


#### शिक्षक मॉडल
- एक पूर्व-प्रशिक्षित जटिल मॉडल, आमतौर पर उत्कृष्ट प्रदर्शन लेकिन उच्च कम्प्यूटेशनल ओवरहेड के साथ।
- "ज्ञान" के रूप में जानकारी के सॉफ्ट लेबल या मध्यवर्ती परतें प्रदान करें।

#### छात्र मॉडल
- शिक्षक मॉडल के व्यवहार को सीखने और उसका अनुकरण करने के लिए डिज़ाइन किया गया एक छोटा, सरल मॉडल।
- शिक्षक मॉडल के "ज्ञान" को सीखकर, छात्र मॉडल छोटे पैमाने पर शिक्षक मॉडल के करीब प्रदर्शन हासिल करने में सक्षम होता है।

####नरम लक्ष्य
- शिक्षक मॉडल द्वारा संभाव्यता वितरण आउटपुट में आमतौर पर हार्ड लेबल (यानी वन-हॉट लेबल) की तुलना में अधिक जानकारी होती है।
- तापमान समायोजन (तापमान) के माध्यम से आउटपुट संभाव्यता वितरण को नरम करें, ताकि छात्र मॉडल संभावनाओं के बीच सापेक्ष संबंध को बेहतर ढंग से सीख सके।


## 6.2 ज्ञान आसवन की विशिष्ट प्रक्रिया


![](छवियां/1-2.png)


&emsp;&emsp; ज्ञान आसवन की बुनियादी वास्तुकला को ऊपर दिए गए चित्र में दिखाया गया है, जिसमें शिक्षक मॉडल एक मॉडल है जिसे प्रशिक्षित किया गया है, और छात्र मॉडल एक मॉडल है जिसे प्रशिक्षित करने की आवश्यकता है।


&emsp;&emsp; मान लें कि वर्तमान कार्य एक छवि वर्गीकरण कार्य है, प्रशिक्षण चरण में, एक प्रशिक्षण छवि $\mathbf x$ और इसके संबंधित वास्तविक लेबल $y$ (जिसे हार्ड लेबल हार्ड लेबल भी कहा जाता है)। छवि $\mathbf{x}$ को क्रमशः शिक्षक और शिक्षण मॉडल को फ़ीड करें, और एक ही आकार के दो आउटपुट लॉग प्राप्त करें: $\mathbf{u} = [u_1, u_2, ..., u_K]\in \mathbb {R }^{1\times K}$ और $\mathbf{v} = [v_1, v_2, ..., v_K] \in \mathbb{R}^{1\times K}$, जहां $K$ वर्गीकरण कार्य के डेटा सेट के अनुरूप श्रेणियों की कुल संख्या का प्रतिनिधित्व करता है। इसके बाद, सॉफ्टमैक्स फ़ंक्शन द्वारा दो लॉगिट संसाधित होने के बाद, इनपुट $\mathbf{x}$ के बारे में दो मॉडलों की अंतिम भविष्यवाणियां क्रमशः प्राप्त की जाती हैं $\mathbf{p} = [p_1, p_2, ..., p_K ]$ और $\ Mathbf{q} = [q_1, q_2, ..., q_K]$।

&emsp;&emsp; चूंकि शिक्षक मॉडल का आउटपुट आमतौर पर अधिक आश्वस्त होता है, इससे असंसाधित $\mathbf{p}$ एक-हॉट वेक्टर ($[0, 0, 1, 0, ... , 0, 0]$). शिक्षक मॉडल की भविष्यवाणी संभावना को नरम करने के लिए, सामान्य दृष्टिकोण नरम करने के लिए सॉफ्टमैक्स फ़ंक्शन में तापमान $\tau$ को पेश करना है, और नरम लॉगिट्स पर केएल विचलन गणना हानि निष्पादित करना है, जो सॉफ्ट लॉस (सॉफ्ट लॉस) बन जाता है। .


& emsp; }$ और हार्ड लेबल $y$ क्रॉस एन्ट्रापी हानि की गणना कर सकते हैं। हानि के इस भाग को हा कहा जाता हैतीसरा लॉस (हार्ड लॉस), और अंत में सॉफ्ट लॉस और हार्ड लॉस के संयुक्त प्रभाव के तहत मॉडल आसवन निष्पादित करें।


## 6.2 तापमान


### तापमान क्या है?


&emsp;&emsp; छात्र मॉडल के सीखने को बेहतर मार्गदर्शन करने के लिए शिक्षक मॉडल आउटपुट की संभाव्यता वितरण को समायोजित करने के लिए ज्ञान आसवन में तापमान का उपयोग किया जाता है।


### तापमान की आवश्यकता क्यों है

&emsp;&emsp;शिक्षक मॉडल का आउटपुट आमतौर पर एक वर्ग संभाव्यता वितरण होता है, जो सॉफ्टमैक्स फ़ंक्शन द्वारा उत्पन्न होता है। असमायोजित तापमान सॉफ्टमैक्स आउटपुट के लिए, एक वर्ग के लिए संभावना 1 के करीब होना और अन्य वर्गों के लिए संभावना 0 के करीब होना आम बात है। यह तीव्र वितरण छात्र मॉडल के अनुकूल नहीं है क्योंकि यह केवल सही उत्तरों के बारे में जानकारी प्रदान करता है और जानकारी की अन्य श्रेणियों को अनदेखा करता है। (और इस समय, शिक्षक मॉडल द्वारा संभाव्यता वितरण आउटपुट हार्ड लेबल लेबल के बहुत करीब है)। तापमान समायोजन के बाद, एक अपेक्षाकृत "सॉफ्ट" संभाव्यता वितरण प्राप्त किया जा सकता है, जिसे "सॉफ्ट लेबल" कहा जाता है। इस समायोजन प्रक्रिया को "सॉफ्टनिंग" कहा जाता है। उदाहरण के लिए: 


| टैग |. घोड़ा |. कार |
|. :--------: |. :--: |
|img|![](छवियां/2-1-हॉर्सe.webp) | ![](images/2-1-donkey.webp)| ![](images/2-1-car.jpg)|
|. नेटवर्क आउटपुट |. 2.0 |
|.सॉफ़्टमैक्स |.0.018 |
|. हार्ड लेबल (असली लेबल) |. 0 |
|. सॉफ्ट लेबल (तापमान का उपयोग करके नरम करना) |. 0.25 |


&emsp;&emsp; यह पाया जा सकता है कि नेटवर्क के मूल आउटपुट को सॉफ्टमैक्स से पारित करने के बाद प्राप्त संभाव्यता वितरण वास्तविक लेबल के लगभग अनुरूप है। इस बिंदु पर वितरण श्रेणियों के बीच संभावित संबंधों को नजरअंदाज करता है। उदाहरण के लिए, "घोड़ा" "कार" की तुलना में "गधे" की तरह अधिक है। हालाँकि मूल आउटपुट का सॉफ्टमैक्स संभाव्यता वितरण भी इस संबंध को प्रतिबिंबित कर सकता है, अंतर बहुत स्पष्ट नहीं है। तापमान में नरमी के बाद प्राप्त सॉफ्ट लेबल विभिन्न श्रेणियों के बीच अंतर को स्पष्ट रूप से दिखा सकता है और विभिन्न श्रेणियों के बीच सापेक्ष संबंध को अच्छी तरह से दर्शाता है। इस प्रकार का विवरण छात्र मॉडल को केवल हार्ड लेबल सीखने के बजाय इनपुट नमूने की पूरी जानकारी को बेहतर ढंग से समझने में मदद कर सकता है।


### तापमान की गणना प्रक्रिया


&emsp;&emsp; एक निश्चित मॉडलप्रकार का आउटपुट $z = [z_1, z_2, ..., z_K]$ है, जहां $K$ श्रेणियों की संख्या है। पारंपरिक सॉफ़्टमैक्स है

$$
\frac{\exp {z_i}}{\sum _ {j=1} ^ K \exp {z_j}}
$$


&emsp;&emsp;तापमान के साथ सॉफ्टमैक्स की गणना विधि है

$$
\frac{\exp {z_i / \tau}}{\sum _ {j=1} ^ K \exp {z_j / \tau}}
$$


&emsp;&emsp; जहां $\tau$ तापमान का प्रतिनिधित्व करता है। फिर पारंपरिक सॉफ्टमैक्स को एक विशेष मामले के रूप में भी माना जा सकता है जहां तापमान $\tau$ 1 है।


&emsp;&emsp; एक अधिक विशिष्ट उदाहरण, जैसा कि तालिका में दिखाया गया है:


<!-- ![](images/2-2.png) -->


|. श्रेणी |. लॉगिट्स (नेटवर्क का प्रत्यक्ष आउटपुट) |. सॉफ्टमैक्स(टी=4)<br />तापमान के साथ नरमी |
|.:--:|:----------------------: |.:---------------------- ---- ----------------------------------: |. -------------------------------------------------------: |
|. घोड़ा|. 6.0 | $\frac{e^{6.0}}{e ^ {6.0} + e^{-2.0}} = 0.98$ | }{4}}}{e^{\frac{6.0} |
|. गधा |. 2.0 | $\frac{e^{2.0}}{e ^ {6.0} + e^{-2.0}} = 0.018$ | }{4}}}{e^{\frac{6.0}+ e^{\frac{-2.0}{4}} } = 0.25$ |
|. कार|. -2.0 | $\frac{e^{-2.0}}{e ^ {6.0} + e^{-2.0}} = 0.002$ | {-2.0}{4}}}{e^{\frac{6.0} =0.05$ |




&emsp;&emsp;व्यवहार में, हम आम तौर पर छात्र मॉडल के आउटपुट को भी नरम कर देते हैं। अंत में, नरम शिक्षक मॉडल और छात्र मॉडल के सॉफ्ट लेबल $\mathbf{p}(\tau) = [p_1(\tau), p_2(\tau), ..., p_K(\tau)]$ और हैं $\mathbf{q}(\tau) = [q_1(\tau), q_2(\tau), ..., q_K(\tau)]$.


&emsp;&emsp;$p_i(\tau)$ और $q_i(\tau)$ को इस प्रकार परिभाषित किया गया है:
$$
\शुरू{संरेखित}
    p_{i}(\tau) = \frac{\exp(u_i / \tau)}{\sum_{i=1}^K \exp(u_i / \tau)} \\
    q_{i}(\tau) = \frac{\exp(v_i / \tau)}{\sum_{i=1}^K \exp(v_i / \tau)}
\अंत{संरेखित}
$$
&emsp;&emsp;जहां, $\tau$ आसवन प्रक्रिया के दौरान उपयोग किए गए तापमान को दर्शाता है। जब तापमान $\tau$ 1 होता है, तो $\mathbf p(\tau)$ और $\mathbf q(\tau)$ के परिणाम सामान्य सॉफ़्टमैक्स फ़ंक्शन के परिणामों के अनुरूप होते हैं।


### लॉगिट्स की नरमी की डिग्री पर तापमान का प्रभाव


![](छवियां/2-3.png)


&emsp;&emsp; तापमान का आकार $\tau$ अनुमानित संभावना की नरमी को नियंत्रित करता है। छोटे $\tau$ के परिणामस्वरूप तीव्र नरमी प्रभाव होता है, जबकि बड़े $\tau$ के परिणामस्वरूप नरम नरमी प्रभाव पड़ता है। जैसा कि ऊपर दिए गए चित्र में दिखाया गया है, सबसे बाईं ओर ओरिजिन किसी भी चित्र के लिए एक निश्चित वर्गीकरण नेटवर्क के आउटपुट लॉगिट्स के वितरण का प्रतिनिधित्व करता है। क्षैतिज अक्ष विशिष्ट श्रेणी संख्या का प्रतिनिधित्व करता है, और ऊर्ध्वाधर अक्ष एक विशिष्ट प्रकार के लिए नेटवर्क के अनुमानित मूल्य का प्रतिनिधित्व करता है। और सबसे दाहिनी ओर वाला लॉगिट्स पर Arg हैअधिकतम() के बाद परिणाम। 


&emsp;&emsp; यह पाया जा सकता है कि जब तापमान बहुत बड़ा होता है (उदाहरण के लिए $\tau = 14$), नरम लॉगिट्स वितरण लगभग सुसंगत होता है, और जब तापमान बहुत छोटा होता है (उदाहरण के लिए $\tau = 0.5$ ) ,इस समय, नरम लॉगिट्स वितरण लगभग Arg max() के परिणाम के बराबर है। कहने का तात्पर्य यह है कि, जब तापमान बहुत अधिक होता है, तो शिक्षक नेटवर्क के नरम लॉग औसत मूल्य के करीब होते हैं, इस समय, छात्र मॉडल शिक्षक मॉडल से ज्ञान नहीं सीख सकता है, क्योंकि शिक्षक मॉडल के लॉग में होता है प्रत्येक श्रेणी के लिए समान अनुमानित संभावना। जब तापमान बहुत कम होता है, तो इस समय आसवन का अर्थ खो जाता है, क्योंकि शिक्षक मॉडल द्वारा छात्रों को हस्तांतरित ज्ञान को हार्ड लेबल के बराबर माना जा सकता है। इसलिए आसवन के लिए **उचित तापमान** चुनना महत्वपूर्ण है। आम तौर पर, CIFAR-10/100 डेटा सेट पर, उपयोग किया जाने वाला तापमान $4$ होता है, जबकि ImageNet डेटा सेट पर, आमतौर पर उपयोग किया जाने वाला तापमान $1$ होता है।


### विभिन्न तापमानों पर आसवन का परिणाम होता है


![](छवियां/2-4.png)


&emsp;&emsp; चित्र में शिक्षक मॉडल Res32x4 है, और छात्र मॉडल Res8x4 है। प्रयोग के लिए चयनित डेटा सेट CIFAR-100 है। उनमें से क्षैतिजअक्ष आसवन में प्रयुक्त तापमान का प्रतिनिधित्व करता है, और ऊर्ध्वाधर अक्ष CIFAR-100 सत्यापन सेट पर आसवन परिणामों का प्रतिनिधित्व करता है। यह पाया जा सकता है कि अलग-अलग तापमानों का आसवन के परिणामों पर अधिक प्रभाव पड़ता है। पाठकों को केवल अनुभवजन्य सेटिंग्स पर निर्भर रहने के बजाय, स्वयं अभ्यास करते समय अलग-अलग तापमानों का चयन करना चाहिए।


## 6.3 ज्ञान आसवन का हानि कार्य


&emsp;&emsp;ज्ञान आसवन के हानि फ़ंक्शन में नरम हानि और कठोर हानि का एक रैखिक संयोजन होता है। विशिष्ट परिभाषा इस प्रकार है:


&emsp;&emsp;**हल्का नुकसान**


$$
\mathcal{L}_{\operatorname{KL}} = \operatorname {KL}(\mathbf{q}(\tau), \mathbf{p}(\tau)) = \sum_j \ p_j(\tau) \ लॉग \frac{p_j(\tau)}{q_j(\tau)}
$$


&emsp;&emsp;**कठिन हानि**


$$
\mathcal L_{CE} = \operatorname {CE}(\mathbf q(\tau = 1), \mathbf y) = \sum _ {j} - y_j \log q_j(1)
$$


&emsp;&emsp;कहाँ $\mathbf{p}$ और $\mathbf{q}$ क्रमशः शिक्षक मॉडल और छात्र मॉडल के आउटपुट लॉगिट्स का प्रतिनिधित्व करते हैं, जबकि $\tau$ आसवन के लिए उपयोग किए गए तापमान का प्रतिनिधित्व करते हैं। अंतिम हानि फ़ंक्शन है
$$
हानि = \alpha \cdot \mathcal L_{CE} + \beta \cdot \tau ^ 2 \cdot \mathcal L_{KL}\\
= \alpha \cdot \operatorname {CE}(\mathbf q(\tau = 1), \mathbf y) + \beta \cdot \tau ^ 2 \cdot \operatorname {KL}(\mathbf{q}(\tau ), \mathbf{p}(\tau))
$$


&emsp;&emsp; सामान्य परिस्थितियों में, $\alpha + \beta = 1$ बनाए रखा जाना चाहिए। व्यवहार में, $\alpha$ आमतौर पर $0.1$ लेता है, और $\beta$ आमतौर पर $0.9$ लेता है। जहां तक ​​इस बात का सवाल है कि सॉफ्ट लॉस भाग के $\operatorname {KL}$ को $\tau ^ 2$ से गुणा किया जाता है, तो इसका सरल स्पष्टीकरण ग्रेडिएंट पर सॉफ्ट लॉस और हार्ड लॉस के बीच संतुलन बनाए रखना है। कृपया विशिष्ट स्पष्टीकरण के लिए चयनात्मक रहें।पढ़ना [6.4]()


## 6.4 हानि फ़ंक्शन की आंशिक व्युत्पत्ति (वैकल्पिक)


### 6.4.1 सॉफ्टमैक्स फ़ंक्शन का व्युत्पन्न


&emsp;&emsp;मान लीजिए कि एक मनमाने लॉगिट्स वेक्टर के लिए $\mathbf z = [z_1, z_2, ..., z_{K}]\in \mathbb{R}^{1\times K}$, जहां $K$ डेटा सेट में श्रेणियों की संख्या है. तापमान के साथ सॉफ्टमैक्स फ़ंक्शन द्वारा गणना के बाद, वेक्टर $\mathbf s = [s_1(\tau), s_2(\tau), ..., s_K(\tau)]$ प्राप्त होता है, जहां $s_i( की परिभाषा \tau)$ के लिए:
$$
s_i(\tau) = \frac{e^{z_i/ \tau}}{\sum_{j=1}^K e^{z_j / \tau}}
$$


&emsp;&emsp;किसी भी $z_k \in \mathbf z$ के लिए, $s_i(\tau)$ से $z_k$ का आंशिक व्युत्पन्न दो स्थितियों में विभाजित है:


&emsp;&emsp; जब $i = k$, वहाँ है
$$
\शुरू करें{संरेखित करें*}
  \frac{\आंशिक s_i(\tau)}{\आंशिक z_क}
  &= \frac{\partial}{\partial z_k}\frac{e^{z_k / \tau}}{\sum_{j=1}^K e^{z_j / \tau}} \\
  &= \frac{\frac{\partial}{\partial z_k}e^{z_k / \tau}\ \sum_{j=1}^K e^{z_j / \tau} - e^{z_k / \tau 3 ताऊ}\दाएं) ^ 2} \\
  &= \frac{\frac{1}{\tau}\ e^{z_k/ \tau}}{\sum_{j=1}^K e^{z_j / \tau}} - \frac{e^{ z_k/ \tau }\ \frac{1}{\tau}\ e^{z_k/ \tau }}{\left( \sum_{j=1}^K e^{z_j / \tau}\right) ^ 2}\\
  &= \frac{1}{\tau}(s_k(\tau) - s_k(\tau)\ s_k(\tau)) \\
  &= \frac{1}{\tau}\ s_k(\tau)\ (1 - s_k(\tau))
\end{संरेखित करें*}
$$


&emsp;&emsp; जब $i \neq k$, होता है
$$
\शुरू करें{संरेखित करें*}
  \frac{\आंशिक s_i(\tau)}{\आंशिक z_k}
  &= \frac{\partial}{\partial z_k}\frac{e^{z_i / \tau}}{\sum_{j=1}^K e^{z_j / \tau}} \\
  &= \frac{\frac{\partial}{\partial z_k}e^{z_i / \tau}\ \sum_{j=1}^K e^{z_j / \tau} - e^{z_i / \tau 3 ताऊ}\दाएं) ^ 2} \\
  &= 0- \frac{e^{z_i/ \tau }\frac{1}{\tau}e^{z_k/ \tau }}{\left(\sum_{j=1}^K e^{z_j / \tau }\दाएं) ^ 2}\\
  &= -\frac{1}{\tau}\ s_i(\tau)\ s_k(\tau)
\end{संरेखित करें*}
$$


&emsp;&emsp; इसलिए, $\varphi (z_i)$ के लिए $z_k$ का आंशिक व्युत्पन्न है
$$
\frac{\आंशिक s_i(\tau)}{\आंशिक z_k} =
\बाएं\{
\शुरू{मैट्रिक्स}
\frac{1}{\tau}\ s_k(\tau)\ (1 - s_k(\tau))& \text{if } i = k \\
-\frac{1}{\tau}\ s_i(\tau)\ s_k(\tau) & \text{if } i \neq k
\अंत{मैट्रिक्स}
\सही।
$$


### 6.4.2 हार्ड लॉस सीई व्युत्पत्ति


&emsp;&emsp;छात्र मॉडल के आउटपुट लॉगिट के लिए $\mathbf{v} = [v_1, v_2, ..., v_K] \in \mathbb{R}^{1\times K}$ किसी भी $v_k$ के लिए, कठिन हानि का ग्रेडिएंट $ \mathcal L_{CE}$ से $v_k$ है:
$$
\शुरू करें{संरेखित करें*}
\mathcal L_{CE} &= \operatorname {CE}(\mathbf q(\tau = 1), \mathbf y) \\
&= \sum _ {j=1}^K - y_j \log q_j(\tau = 1) \\
\frac{\आंशिक \mathcal L_{CE}}{\आंशिक v_k} &= \frac{\partial}{\आंशिक v_k}\sum _ {j=1}^K - y_j \log q_j(\tau = 1 )\\
&= \frac{\partial}{\partial v_k}\sum _ {j=1, j\neq k}^K - y_{j} \log q_j(\tau = 1) + \frac{\partial}{ \आंशिक v_k} - y_k \log q_k(\tau = 1)\\
&= \योग_ {j=1, j\neq k}^K - y_{j} \frac{1}{q_j(\tau=1)}\ \frac{-1}{\tau=1}q_j(\tau= 1)q_k(\tau=1) \\ &\ \ \ \ \ \ - y_k \frac{1}{q_k(\tau=1)}\frac{1}{\tau=1}q_k(\tau= 1)(1-q_k(\tau=1))\\
&= \frac{1}{1}(1-y_k)q_k(\tau=1) - \frac{1}{1}y_k(1-q_k(\tau=1))\\
&= q_k(\tau=1) - y_k \\
\end{संरेखित करें*}
$$


### 6.4.3 सॉफ्ट लॉस केएल व्युत्पत्ति


&emsp;&emsp;सॉफ्ट लॉस $\mathcal L_{KL}$ शिक्षक मॉडल के सॉफ्ट लेबल $\mathbf p(\tau)$ और सॉफ्ट लेबल $\mathbf q(\tau)$ का केएल विचलन है छात्र मॉडल. छात्र मॉडल के आउटपुट में किसी भी $v_k$ के लिए $\mathcal L_{KL}$ का ग्रेडिएंट है
$$
\शुरू करें{संरेखित करें*}
\mathcal{L}_{\operatorname{KL}} &= \oपेरेटरनाम {KL}(\mathbf{q}(\tau), \mathbf{p}(\tau)) \\
&= \sum_{j=1}^{K} \ p_j(\tau) \log \frac{p_j(\tau)}{q_j(\tau)} \\
\frac{\आंशिक \mathcal{L}_{\operatorname{KL}}}{\आंशिक v_k} &= \frac{\आंशिक}{\आंशिक v_k} \sum_{j=1}^{K} \ p_j (\tau) \log \frac{p_j(\tau)}{q_j(\tau)} \\
&= \frac{\partial}{\partial v_k} \sum_{j=1}^{K} \left( \ p_j(\tau) \log {p_j(\tau)} - p_j(\tau)\log {q_j(\tau)} \दाएं) \\
&= \frac{\partial}{\partial v_k} \left(\sum_{j=1}^{K} - p_j(\tau)\log{q_j(\tau)}\right) \\
&= \frac{\आंशिक}{\आंशिक v_k} \left( \sum_{j=1, j\neq k} ^ K -p_j(\tau) \log q_j(\tau) -p_k(\tau) \log q_k( \ताउ) \दाएं) \\
&= \sum_{j=1, j\neq k}^K \left( -p_j(\tau) \frac{\partial}{\partial v_k} \log q_j(\tau)\right) - \frac{ \आंशिक}{\आंशिक v_k} p_k(\tau) \log q_k(\tau)\\
&= \sum_{j=1, j\neq k}^K -\frac{p_j(\tau)}{q_j(\tau)}\left[ -\frac{1}{\tau}q_j(\tau )q_k(\tau) \right] - \frac{p_k(\tau)}{q_k(\tau)}\left[ \frac{1}{\tau} q_k(\tau)(1 - q_k(\tau) ))\सही]\\
&= \frac{1}{\tau} \sum_{j=1, j\neq k}^K p_j(\tau) q_k(\tau) - \frac{1}{\tau} p_k(\tau)(1 - q_k(\tau)) \ \ \ \text{where} \sum_{j= 1}^K p_j(\tau)=1\\
&= \frac{1}{\tau} (1 - p_k(\tau))q_k(\tau) - \frac{1}{\tau} p_k(\tau)(1 - q_k(\tau)) \ \
&= \frac{1}{\tau} \left[ q_k(\tau) - p_k(\tau)q_k(\tau) - p_k(\tau) + p_k(\tau)q_k(\tau)\right] \\
&= \frac{q_k(\tau) - p_k(\tau)}{\tau}
\end{संरेखित करें*}
$$

### 6.4.4 टेलर दृष्टिकोण


&emsp;&emsp;$e^x$ के लिए, जब $x$ की प्रवृत्ति 0 की ओर होती है, तो $e^x \लगभग 1 + x + ...$ होता है


&emsp;&emsp;अंत में, $v_k$ के लिए हार्ड लॉस CE और सॉफ्ट लॉस KL के ग्रेडिएंट हैं:
$$
\बाएं\{
\शुरुआत{मैटरिक्स}
\frac{\आंशिक \mathcal L_{CE}}{\आंशिक v_k} = q_k(\tau=1) - y_k
\\
\frac{\आंशिक \mathcal{L}_{\operatorname{KL}}}{\आंशिक v_k} = \frac{1}{\tau}(q_k(\tau) - p_k(\tau))
\अंत{मैट्रिक्स}
\सही।
$$

&emsp;&emsp;$\frac{\partial \mathcal L_{CE}}{\partial v_k}$ के लिए विस्तार है:
$$
\शुरू करें{संरेखित करें*}
\frac{\आंशिक \mathcal L_{CE}}{\आंशिक v_k} &= q_k(\tau=1)-y_k\\
&= \frac{e^{v_k}}{\sum_{j=1}^K e^{v_j}} - y_k \\
&\लगभग \frac{1+v_k}{\sum_{j=1}^K 1 + v_j} - y_k , \text{where} \sum v_j = 0\\
&= \frac{1+v_k}{के} - y_k\\
\end{संरेखित करें*}
$$
​
&emsp;&emsp;$\frac{\partial \mathcal{L}_{\operatorname{KL}}}{\partial v_k}$ के लिए विस्तार है:
$$
\शुरू करें{संरेखित करें*}
\frac{\आंशिक \mathcal{L}_{\operatorname{KL}}}{\आंशिक v_k} &= \frac{1}{\tau}(q_k(\tau) - p_k(\tau))\\
&= \frac{1}{\tau} (\frac{e^{v_k/\tau}}{\sum_{j=1}^K e^{v_j / \tau}} - \frac{e^{ u_k/ \tau}}{\sum_{j=1}^K e^{u_k/\tau}}) \\
&\लगभग \frac{1}{\tau}(\frac{1 + v_k/\tau}{\sum_{j=1}^K (1 + v_j/\tau)} - \frac{1+u_k/ \tau}{\sum_{j=1}^K (1+u_j/ \tau)})\\
&= \frac{1}{\tau}(\frac{v_k/\tau - u_k}{K}) \\
&= \frac{1}{K \ \tau^2} v_k - \frac{u_k}{K\tau}
\end{संरेखित करें*}
$$

&emsp;&emsp;इस बिंदु पर, यह पाया जा सकता है कि $v_k$ के लिए हार्ड लॉस में ग्रेडिएंट का हिस्सा $\tau ^ $v_k$ के लिए सॉफ्ट लॉस के ग्रेडिएंट के 2$ गुना है, इसलिए जब हानि फ़ंक्शन हानि की अंतिम गणना आवश्यक है, दो हानियों के बीच ग्रेडिएंट को संतुलित करने के लिए $\mathcal{L}_{\operatorname{KL}}$ को $\tau ^ 2$ से गुणा करें।


## 6.5 अधिक प्रकार के ज्ञान आसवन


### 6.5.1 वियुग्मित ज्ञान आसवन

![](छवियां/6-1.png)


&emsp;&emsp;डिकॉउल्ड नॉलेज डिस्टिलेशन([DKD](https://arxiv.org/abs/2203.08679))। सामान्य केडी (ज्ञान आसवन) हानि को अलग करने से लक्ष्य वर्ग ज्ञान आसवन (टीसीकेडी) और गैर-लक्ष्य वर्ग में परिणाम होता हैएस ज्ञान आसवन (एनसीकेडी) दो भाग। इस प्रकार एनसीकेडी भाग को तीव्र करने के लिए नए आसवन हानियों का प्रस्ताव किया गया है। 


### 6.5.2 फिटनेट


![](छवियां/6-2.पीएनजी)


&emsp;&emsp; [FitNet](https://arxiv.org/abs/1412.6550) को केवल अंतिम आउटपुट का मिलान करके नहीं, बल्कि मध्यवर्ती परतों पर शिक्षक नेटवर्क और छात्र नेटवर्क की सुविधाओं का मिलान करके प्रशिक्षित किया जाता है। आसवन के लिए मध्यवर्ती परतों का उपयोग करने की यह विधि केवल सोच परिणामों को सूचित करने के बजाय इनपुट पर शिक्षक मॉडल की सोच प्रक्रिया को छात्र मॉडल को सिखाने जैसी है। हालाँकि, चूंकि दोनों की मध्य परतों के आयाम अक्सर मेल नहीं खाते हैं, इसलिए छात्र मॉडल के आयामों को समायोजित करने के लिए नए संदर्भ जोड़ना अक्सर आवश्यक होता है।


### 6.5.3 समीक्षाकेडी


&emsp;&emsp; [ReviewKD](https://arxiv.org/abs/2104.09044) का मानना ​​है कि प्रशिक्षण के शुरुआती चरणों में, छात्र मॉडल को शिक्षक मॉडल की गहरी अमूर्त अर्थ संबंधी जानकारी सीखने देना अनुचित है। यह पेपर ज्ञान समीक्षा पद्धति की शुरुआत करके इस समस्या का समाधान करता है। विवरण के लिए, [Zhihu](https://zhuanlan.zhihu.com/p/363994781) देखें।


###6.5.4 परिणामों की तुलना


&emsp;&emsp; CIFAR-100 डेटा सेट पर इस अनुभाग में उल्लिखित आसवन विधियों की तुलना करें।


| | शिक्षक: ResNet32x4, Acc: 79.42 >छात्र: शफलनेटवी2, एसीसी: 71.82 |
|.:------: |. :------------------------------------------------- ------------------: |. :--------------------------------- ----------------------------------: |:---------------------- ----------------------------------: |
|.केडी |.73.33|                          72.98                          |                            74.45                             |
|  FitNet  |                            73.50                             |                          71.02                          |                            73.54                             |
| ReviewKD |                            75.63                             |                        **74.84**                        |**77.78** |
|. डीकेडी | **76.32** |. 77.07 |


### 6.5.5 लक्ष्य का पता लगाने के लिए ज्ञान आसवन


![](छवियां/6-3.पीएनजी)


&emsp;&emsp; वर्गीकरण कार्यों के ज्ञान आसवन की तुलना में, लक्ष्य का पता लगाने का ज्ञान आसवन अधिक जटिल है। क्योंकि 1) वर्गीकरण कार्य में लेबल श्रेणियां अपेक्षाकृत संतुलित होती हैं, जबकि लक्ष्य पहचान कार्य में श्रेणी असंतुलन की समस्या होती है। 2) लक्ष्य का पता लगाने का कार्य अधिक जटिल है, जिसमें वर्गीकरण कार्य और स्थिति प्रतिगमन कार्य दोनों शामिल हैं। जैसा कि चित्र में दिखाया गया है, आसवन मुख्य रूप से हानि फ़ंक्शन के तीन भागों के माध्यम से किया जाता है, साथ ही, सामने और पीछे की पृष्ठभूमि असंतुलन की समस्या को संबोधित करने के लिए एक भारित क्रॉस-एन्ट्रॉपी हानि का भी प्रस्ताव है।


## 6.6 अभ्यास


- [केडी ज्ञान आसवन](https://github.com/datawhalechina/awesome-compression/blob/main/docs/notebook/ch06/1.kd.ipynb)
- [डीकेडी डिकौपल्ड ज्ञान आसवन](https://github.com/datawhalechina/awesome-compression/blob/main/docs/notebook/ch06/2.dkd.ipynb)


## 6.7 अधिक केडी जानकारी


- [कोड बेस](https://github.com/megvii-research/mdistiller)