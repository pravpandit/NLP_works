{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# मॉडल प्रूनिंग टॉर्च अभ्यास\n",
    "पाइटोरच ने संस्करण 1.4.0 के बाद से प्रूनिंग ऑपरेशंस को जोड़ा है। `torch.nn.utils.prune` मॉड्यूल में, यह ट्यूटोरियल प्रूनिंग रेंज को निम्नलिखित प्रूनिंग विधियों में विभाजित करता है:\n",
    "- स्थानीय छंटाई\n",
    "-संरचित छंटाई\n",
    "- यादृच्छिक संरचित छंटाई (यादृच्छिक_संरचित)\n",
    "- नॉर्म स्ट्रक्चर्ड प्रूनिंग (ln_structured)\n",
    "-असंरचित छंटाई\n",
    "- यादृच्छिक असंरचित छंटाई (यादृच्छिक_असंरचित)\n",
    "- सामान्य असंरचित छंटाई (l1_असंरचित)\n",
    "- ग्लोबल प्रूनिंग\n",
    "- असंरचित छंटाई (वैश्विक_uसंरचित)\n",
    "- कस्टम प्रूनिंग (कस्टम प्रूनिंग)\n",
    "\n",
    "**ध्यान दें:** वैश्विक छंटाई में केवल असंरचित छंटाई विधियां हैं।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. स्थानीय काट-छाँट\n",
    "सबसे पहले, हम स्थानीय प्रूनिंग विधि का परिचय देंगे, जो नेटवर्क की एकल परत या स्थानीय श्रेणी की प्रूनिंग को संदर्भित करता है।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 संरचित छंटाई\n",
    "प्रूनिंग विधि के अनुसार, इसे संरचित प्रूनिंग और असंरचित प्रूनिंग में विभाजित किया जा सकता है। असंरचित प्रूनिंग यादृच्छिक रूप से कुछ वजन मापदंडों को 0 में बदल देगी, जबकि संरचित प्रूनिंग एक निश्चित आयाम के कुछ चैनलों को 0 में बदल देगी।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 यादृच्छिक संरचित छंटाई (यादृच्छिक_संरचित)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "एक क्लासिक LeNet नेटवर्क बनाएं"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet नेटवर्क को परिभाषित करें\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 120]          30,840\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# मॉडल संरचना प्रिंट करें\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[[[ 0.0220,  0.1789, -0.0544, -0.0713,  0.0478],\n",
      "          [ 0.1995, -0.0415,  0.0288, -0.1431,  0.1057],\n",
      "          [ 0.1600,  0.0248, -0.1903, -0.0242, -0.1961],\n",
      "          [-0.0211,  0.0257, -0.1116, -0.1678,  0.0611],\n",
      "          [ 0.0012,  0.0420, -0.1725, -0.1265, -0.1075]]],\n",
      "\n",
      "\n",
      "        [[[-0.0540, -0.1928, -0.0355, -0.0075, -0.1481],\n",
      "          [ 0.0135,  0.0192,  0.0082, -0.0120, -0.0164],\n",
      "          [-0.0435, -0.1488,  0.1092, -0.0041,  0.1960],\n",
      "          [-0.1045, -0.0136,  0.0398, -0.1286,  0.0617],\n",
      "          [-0.0091,  0.0466,  0.1827,  0.1655,  0.0727]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1216, -0.0833, -0.1491, -0.1143,  0.0113],\n",
      "          [ 0.0452,  0.1662, -0.0425, -0.0904, -0.1235],\n",
      "          [ 0.0565,  0.0933, -0.0721,  0.0909,  0.1837],\n",
      "          [-0.1739,  0.0263,  0.1339,  0.0648, -0.0382],\n",
      "          [-0.1667,  0.1478,  0.0448, -0.0892,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1976,  0.0123,  0.1523, -0.1207,  0.1493],\n",
      "          [-0.1799,  0.0580,  0.1490,  0.1647, -0.0572],\n",
      "          [-0.0908,  0.1094, -0.0676, -0.0023,  0.0624],\n",
      "          [-0.0320, -0.1794,  0.1706, -0.0486,  0.0557],\n",
      "          [ 0.1482, -0.1306,  0.1213, -0.1090, -0.1267]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1278,  0.1037, -0.0323, -0.1504,  0.1080],\n",
      "          [ 0.0266, -0.0996,  0.1499, -0.0845,  0.0609],\n",
      "          [-0.0662, -0.1405, -0.0586, -0.0615, -0.0462],\n",
      "          [-0.1118, -0.0961, -0.1325, -0.0417, -0.0741],\n",
      "          [ 0.1842, -0.1040, -0.1786, -0.0593,  0.0186]]],\n",
      "\n",
      "\n",
      "        [[[-0.0889, -0.0737, -0.1655, -0.1708, -0.0988],\n",
      "          [-0.1787,  0.1127,  0.0706, -0.0352,  0.1238],\n",
      "          [-0.0985, -0.1929, -0.0062,  0.0488, -0.1152],\n",
      "          [-0.1659, -0.0448,  0.0821, -0.0956, -0.0262],\n",
      "          [ 0.1928,  0.1767, -0.1792, -0.1364,  0.0507]]]], requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([-0.0893, -0.1464, -0.1101, -0.0076,  0.1493, -0.0418],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# पहली कनवल्शनल परत के मापदंडों को प्रिंट करें\n",
    "module = model.conv1\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# मॉड्यूल में विशेषता टेंसर नामित_बफ़र्स प्रिंट करें, जो प्रारंभ में एक खाली सूची है\n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# मॉडल के राज्य शब्दकोश को प्रिंट करें, जिसमें सभी पैरामीटर शामिल हैं\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# पहला पैरामीटर: मॉड्यूल, कांट-छांट किए जाने वाले विशिष्ट मॉड्यूल का प्रतिनिधित्व करता है, यहां यह मॉड्यूल = मॉडल.conv1 को संदर्भित करता है,\n",
    "# इंगित करता है कि पहली संकेंद्रित परत पर छंटाई की जानी है।\n",
    "# दूसरा पैरामीटर: नाम, दर्शाता है कि चयनित मॉड्यूल में किन मापदंडों को काटा जाना है।\n",
    "# यहां name='weight' सेट है, जिसका अर्थ है पूर्वाग्रह के बजाय नेटवर्क में वजन को कम करना।\n",
    "# तीसरा पैरामीटर: राशि, मॉडल में एक विशिष्ट अनुपात या मापदंडों की पूर्ण संख्या की छंटाई का प्रतिनिधित्व करता है।\n",
    "# राशि 0.0-1.0 के बीच एक फ्लोट मान है, जो अनुपात का प्रतिनिधित्व करता है, या एक सकारात्मक पूर्णांक दर्शाता है कि कितने पैरामीटर क्लिप करने हैं।\n",
    "# चौथा पैरामीटर: मंद, कांट-छांट किए जाने वाले चैनल के आयाम सूचकांक का प्रतिनिधित्व करता है।\n",
    "#            \n",
    "\n",
    "prune.random_structured(module, name=\"weight\", amount=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# मॉडल के राज्य शब्दकोश को फिर से प्रिंट करें और conv1 परत का निरीक्षण करें\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([-0.0893, -0.1464, -0.1101, -0.0076,  0.1493, -0.0418],\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0220,  0.1789, -0.0544, -0.0713,  0.0478],\n",
      "          [ 0.1995, -0.0415,  0.0288, -0.1431,  0.1057],\n",
      "          [ 0.1600,  0.0248, -0.1903, -0.0242, -0.1961],\n",
      "          [-0.0211,  0.0257, -0.1116, -0.1678,  0.0611],\n",
      "          [ 0.0012,  0.0420, -0.1725, -0.1265, -0.1075]]],\n",
      "\n",
      "\n",
      "        [[[-0.0540, -0.1928, -0.0355, -0.0075, -0.1481],\n",
      "          [ 0.0135,  0.0192,  0.0082, -0.0120, -0.0164],\n",
      "          [-0.0435, -0.1488,  0.1092, -0.0041,  0.1960],\n",
      "          [-0.1045, -0.0136,  0.0398, -0.1286,  0.0617],\n",
      "          [-0.0091,  0.0466,  0.1827,  0.1655,  0.0727]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1216, -0.0833, -0.1491, -0.1143,  0.0113],\n",
      "          [ 0.0452,  0.1662, -0.0425, -0.0904, -0.1235],\n",
      "          [ 0.0565,  0.0933, -0.0721,  0.0909,  0.1837],\n",
      "          [-0.1739,  0.0263,  0.1339,  0.0648, -0.0382],\n",
      "          [-0.1667,  0.1478,  0.0448, -0.0892,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1976,  0.0123,  0.1523, -0.1207,  0.1493],\n",
      "          [-0.1799,  0.0580,  0.1490,  0.1647, -0.0572],\n",
      "          [-0.0908,  0.1094, -0.0676, -0.0023,  0.0624],\n",
      "          [-0.0320, -0.1794,  0.1706, -0.0486,  0.0557],\n",
      "          [ 0.1482, -0.1306,  0.1213, -0.1090, -0.1267]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1278,  0.1037, -0.0323, -0.1504,  0.1080],\n",
      "          [ 0.0266, -0.0996,  0.1499, -0.0845,  0.0609],\n",
      "          [-0.0662, -0.1405, -0.0586, -0.0615, -0.0462],\n",
      "          [-0.1118, -0.0961, -0.1325, -0.0417, -0.0741],\n",
      "          [ 0.1842, -0.1040, -0.1786, -0.0593,  0.0186]]],\n",
      "\n",
      "\n",
      "        [[[-0.0889, -0.0737, -0.1655, -0.1708, -0.0988],\n",
      "          [-0.1787,  0.1127,  0.0706, -0.0352,  0.1238],\n",
      "          [-0.0985, -0.1929, -0.0062,  0.0488, -0.1152],\n",
      "          [-0.1659, -0.0448,  0.0821, -0.0956, -0.0262],\n",
      "          [ 0.1928,  0.1767, -0.1792, -0.1364,  0.0507]]]], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "# मॉड्यूल में विशेषता टेंसर नामांकित_बफ़र्स को फिर से प्रिंट करें\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]]))]\n"
     ]
    }
   ],
   "source": [
    "# मॉड्यूल में विशेषता टेंसर नामांकित_बफ़र्स को फिर से प्रिंट करें\n",
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "निष्कर्ष: प्रूनिंग के बाद, मूल वेट मैट्रिक्स वेट वेट_ओरिग बन जाता है। और मॉड्यूल.नाम_बफ़र्स (), जिसे प्रूनिंग से पहले एक खाली सूची के रूप में मुद्रित किया गया था, अब एक अतिरिक्त वेट_मास्क पैरामीटर है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0540, -0.1928, -0.0355, -0.0075, -0.1481],\n",
      "          [ 0.0135,  0.0192,  0.0082, -0.0120, -0.0164],\n",
      "          [-0.0435, -0.1488,  0.1092, -0.0041,  0.1960],\n",
      "          [-0.1045, -0.0136,  0.0398, -0.1286,  0.0617],\n",
      "          [-0.0091,  0.0466,  0.1827,  0.1655,  0.0727]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1216, -0.0833, -0.1491, -0.1143,  0.0113],\n",
      "          [ 0.0452,  0.1662, -0.0425, -0.0904, -0.1235],\n",
      "          [ 0.0565,  0.0933, -0.0721,  0.0909,  0.1837],\n",
      "          [-0.1739,  0.0263,  0.1339,  0.0648, -0.0382],\n",
      "          [-0.1667,  0.1478,  0.0448, -0.0892,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1278,  0.1037, -0.0323, -0.1504,  0.1080],\n",
      "          [ 0.0266, -0.0996,  0.1499, -0.0845,  0.0609],\n",
      "          [-0.0662, -0.1405, -0.0586, -0.0615, -0.0462],\n",
      "          [-0.1118, -0.0961, -0.1325, -0.0417, -0.0741],\n",
      "          [ 0.1842, -0.1040, -0.1786, -0.0593,  0.0186]]],\n",
      "\n",
      "\n",
      "        [[[-0.0889, -0.0737, -0.1655, -0.1708, -0.0988],\n",
      "          [-0.1787,  0.1127,  0.0706, -0.0352,  0.1238],\n",
      "          [-0.0985, -0.1929, -0.0062,  0.0488, -0.1152],\n",
      "          [-0.1659, -0.0448,  0.0821, -0.0956, -0.0262],\n",
      "          [ 0.1928,  0.1767, -0.1792, -0.1364,  0.0507]]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# मॉड्यूल.वेट प्रिंट करें और देखें कि हमें क्या मिलता है?\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "निष्कर्ष: प्रूनिंग के बाद, मूल वजन वेट_ओरिग बन जाता है और नामित_पैरामीटर में संग्रहीत होता है। संबंधित प्रूनिंग मैट्रिक्स को वेट_मास्क में संग्रहीत किया जाता है, और वेट_ओरिग के साथ इसे गुणा करने का परिणाम वजन में संग्रहीत होता है। ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ध्यान दें:** छंटाई के बाद, वजन अब मॉड्यूल का एक पैरामीटर नहीं है, बल्कि केवल मॉड्यूल की एक विशेषता है।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "प्रत्येक प्रूनिंग ऑपरेशन के लिए, मॉडल प्रूनिंग के लिए एक विशिष्ट _forward_pre_hooks फ़ंक्शन के अनुरूप होगा, जो निष्पादित प्रूनिंग ऑपरेशन को संग्रहीत करता है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(327, <torch.nn.utils.prune.RandomStructured object at 0x00000235D8EFF1C0>)])\n"
     ]
    }
   ],
   "source": [
    "# प्रिंट_फॉरवर्ड_प्री_हुक\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 सामान्य संरचित छंटाई (एलएन_संरचित)\n",
    "एक मॉडल के मापदंडों को कई बार काटा जा सकता है, जिसे पुनरावृत्तीय छंटाई कहा जाता है। उपरोक्त चरणों ने conv1 पर यादृच्छिक संरचित छंटाई की है। इसके बाद, इस पर फिर से मानक संरचित छंटाई करें। देखें कि क्या होता है?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model state_dict keys:\n",
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " module named_parameters:\n",
      "[('bias', Parameter containing:\n",
      "tensor([-0.0893, -0.1464, -0.1101, -0.0076,  0.1493, -0.0418],\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0220,  0.1789, -0.0544, -0.0713,  0.0478],\n",
      "          [ 0.1995, -0.0415,  0.0288, -0.1431,  0.1057],\n",
      "          [ 0.1600,  0.0248, -0.1903, -0.0242, -0.1961],\n",
      "          [-0.0211,  0.0257, -0.1116, -0.1678,  0.0611],\n",
      "          [ 0.0012,  0.0420, -0.1725, -0.1265, -0.1075]]],\n",
      "\n",
      "\n",
      "        [[[-0.0540, -0.1928, -0.0355, -0.0075, -0.1481],\n",
      "          [ 0.0135,  0.0192,  0.0082, -0.0120, -0.0164],\n",
      "          [-0.0435, -0.1488,  0.1092, -0.0041,  0.1960],\n",
      "          [-0.1045, -0.0136,  0.0398, -0.1286,  0.0617],\n",
      "          [-0.0091,  0.0466,  0.1827,  0.1655,  0.0727]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1216, -0.0833, -0.1491, -0.1143,  0.0113],\n",
      "          [ 0.0452,  0.1662, -0.0425, -0.0904, -0.1235],\n",
      "          [ 0.0565,  0.0933, -0.0721,  0.0909,  0.1837],\n",
      "          [-0.1739,  0.0263,  0.1339,  0.0648, -0.0382],\n",
      "          [-0.1667,  0.1478,  0.0448, -0.0892,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1976,  0.0123,  0.1523, -0.1207,  0.1493],\n",
      "          [-0.1799,  0.0580,  0.1490,  0.1647, -0.0572],\n",
      "          [-0.0908,  0.1094, -0.0676, -0.0023,  0.0624],\n",
      "          [-0.0320, -0.1794,  0.1706, -0.0486,  0.0557],\n",
      "          [ 0.1482, -0.1306,  0.1213, -0.1090, -0.1267]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1278,  0.1037, -0.0323, -0.1504,  0.1080],\n",
      "          [ 0.0266, -0.0996,  0.1499, -0.0845,  0.0609],\n",
      "          [-0.0662, -0.1405, -0.0586, -0.0615, -0.0462],\n",
      "          [-0.1118, -0.0961, -0.1325, -0.0417, -0.0741],\n",
      "          [ 0.1842, -0.1040, -0.1786, -0.0593,  0.0186]]],\n",
      "\n",
      "\n",
      "        [[[-0.0889, -0.0737, -0.1655, -0.1708, -0.0988],\n",
      "          [-0.1787,  0.1127,  0.0706, -0.0352,  0.1238],\n",
      "          [-0.0985, -0.1929, -0.0062,  0.0488, -0.1152],\n",
      "          [-0.1659, -0.0448,  0.0821, -0.0956, -0.0262],\n",
      "          [ 0.1928,  0.1767, -0.1792, -0.1364,  0.0507]]]], requires_grad=True))]\n",
      "**************************************************\n",
      " module named_buffers:\n",
      "[('weight_mask', tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]]))]\n",
      "**************************************************\n",
      " module weight:\n",
      "tensor([[[[ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1216, -0.0833, -0.1491, -0.1143,  0.0113],\n",
      "          [ 0.0452,  0.1662, -0.0425, -0.0904, -0.1235],\n",
      "          [ 0.0565,  0.0933, -0.0721,  0.0909,  0.1837],\n",
      "          [-0.1739,  0.0263,  0.1339,  0.0648, -0.0382],\n",
      "          [-0.1667,  0.1478,  0.0448, -0.0892,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0889, -0.0737, -0.1655, -0.1708, -0.0988],\n",
      "          [-0.1787,  0.1127,  0.0706, -0.0352,  0.1238],\n",
      "          [-0.0985, -0.1929, -0.0062,  0.0488, -0.1152],\n",
      "          [-0.1659, -0.0448,  0.0821, -0.0956, -0.0262],\n",
      "          [ 0.1928,  0.1767, -0.1792, -0.1364,  0.0507]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      " module _forward_pre_hooks:\n",
      "OrderedDict([(328, <torch.nn.utils.prune.PruningContainer object at 0x00000235D919EE50>)])\n"
     ]
    }
   ],
   "source": [
    "# पहला पैरामीटर: मॉड्यूल, कांट-छांट किए जाने वाले विशिष्ट मॉड्यूल का प्रतिनिधित्व करता है, यहां यह मॉड्यूल = मॉडल.conv1 को संदर्भित करता है,\n",
    "# इंगित करता है कि पहली संकेंद्रित परत पर छंटाई की जानी है।\n",
    "# दूसरा पैरामीटर: नाम, दर्शाता है कि चयनित मॉड्यूल में किन मापदंडों को काटा जाना है।\n",
    "# यहां name='weight' सेट है, जिसका अर्थ है पूर्वाग्रह के बजाय नेटवर्क में वजन को कम करना।\n",
    "# तीसरा पैरामीटर: राशि, मॉडल में एक विशिष्ट अनुपात या मापदंडों की पूर्ण संख्या की छंटाई का प्रतिनिधित्व करता है।\n",
    "# राशि 0.0-1.0 के बीच एक फ्लोट मान है, जो अनुपात का प्रतिनिधित्व करता है, या एक सकारात्मक पूर्णांक दर्शाता है कि कितने पैरामीटर क्लिप करने हैं।\n",
    "# चौथा पैरामीटर: n, मानक प्रकार का प्रतिनिधित्व करता है, यहां n=2 L2 मानक का प्रतिनिधित्व करता है।\n",
    "# पांचवां पैरामीटर: मंद, कांट-छांट किए जाने वाले चैनल के आयाम सूचकांक का प्रतिनिधित्व करता है।\n",
    "\n",
    "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "\n",
    "# मॉडल पैरामीटर फिर से प्रिंट करें\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module weight:\")\n",
    "print(module.weight)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module _forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "निष्कर्ष: पुनरावृत्त प्रूनिंग कई प्रूनिंग कोर को एक प्रूनिंग कोर में क्रमबद्ध करने के बराबर है। नए मास्क मैट्रिक्स को PruningContainer में कंप्यूट_मास्क विधि का उपयोग करके पुराने मास्क मैट्रिक्स के साथ जोड़ा जाता है। अंत में, केवल एक वेट_ओरिग और वेट_मास्क होता है।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "मैं संपूर्ण प्रूनिंग इतिहास कैसे देख सकता हूं? मॉड्यूल._forward_pre_hooks मॉडल के आगे प्रसार से पहले कस्टम संचालन निष्पादित करने के लिए एक तंत्र है। निष्पादित प्रूनिंग विधियां यहां दर्ज की गई हैं।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torch.nn.utils.prune.RandomStructured object at 0x00000235D8EFF1C0>, <torch.nn.utils.prune.LnStructured object at 0x00000235D9381F10>]\n"
     ]
    }
   ],
   "source": [
    "# प्रिंट प्रूनिंग इतिहास\n",
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == \"weight\":  \n",
    "        break\n",
    "\n",
    "print(list(hook))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 यादृच्छिक असंरचित छंटाई (यादृच्छिक_असंरचित)\n",
    "आप मॉडल के किसी भी उपसंरचना को काट-छाँट कर सकते हैं, वज़न को काटने के अलावा, आप पूर्वाग्रह को भी काट सकते हैं।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model state_dict keys:\n",
      "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " module named_parameters:\n",
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0220,  0.1789, -0.0544, -0.0713,  0.0478],\n",
      "          [ 0.1995, -0.0415,  0.0288, -0.1431,  0.1057],\n",
      "          [ 0.1600,  0.0248, -0.1903, -0.0242, -0.1961],\n",
      "          [-0.0211,  0.0257, -0.1116, -0.1678,  0.0611],\n",
      "          [ 0.0012,  0.0420, -0.1725, -0.1265, -0.1075]]],\n",
      "\n",
      "\n",
      "        [[[-0.0540, -0.1928, -0.0355, -0.0075, -0.1481],\n",
      "          [ 0.0135,  0.0192,  0.0082, -0.0120, -0.0164],\n",
      "          [-0.0435, -0.1488,  0.1092, -0.0041,  0.1960],\n",
      "          [-0.1045, -0.0136,  0.0398, -0.1286,  0.0617],\n",
      "          [-0.0091,  0.0466,  0.1827,  0.1655,  0.0727]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1216, -0.0833, -0.1491, -0.1143,  0.0113],\n",
      "          [ 0.0452,  0.1662, -0.0425, -0.0904, -0.1235],\n",
      "          [ 0.0565,  0.0933, -0.0721,  0.0909,  0.1837],\n",
      "          [-0.1739,  0.0263,  0.1339,  0.0648, -0.0382],\n",
      "          [-0.1667,  0.1478,  0.0448, -0.0892,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1976,  0.0123,  0.1523, -0.1207,  0.1493],\n",
      "          [-0.1799,  0.0580,  0.1490,  0.1647, -0.0572],\n",
      "          [-0.0908,  0.1094, -0.0676, -0.0023,  0.0624],\n",
      "          [-0.0320, -0.1794,  0.1706, -0.0486,  0.0557],\n",
      "          [ 0.1482, -0.1306,  0.1213, -0.1090, -0.1267]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1278,  0.1037, -0.0323, -0.1504,  0.1080],\n",
      "          [ 0.0266, -0.0996,  0.1499, -0.0845,  0.0609],\n",
      "          [-0.0662, -0.1405, -0.0586, -0.0615, -0.0462],\n",
      "          [-0.1118, -0.0961, -0.1325, -0.0417, -0.0741],\n",
      "          [ 0.1842, -0.1040, -0.1786, -0.0593,  0.0186]]],\n",
      "\n",
      "\n",
      "        [[[-0.0889, -0.0737, -0.1655, -0.1708, -0.0988],\n",
      "          [-0.1787,  0.1127,  0.0706, -0.0352,  0.1238],\n",
      "          [-0.0985, -0.1929, -0.0062,  0.0488, -0.1152],\n",
      "          [-0.1659, -0.0448,  0.0821, -0.0956, -0.0262],\n",
      "          [ 0.1928,  0.1767, -0.1792, -0.1364,  0.0507]]]], requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([-0.0893, -0.1464, -0.1101, -0.0076,  0.1493, -0.0418],\n",
      "       requires_grad=True))]\n",
      "**************************************************\n",
      " module named_buffers:\n",
      "[('weight_mask', tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]])), ('bias_mask', tensor([1., 1., 0., 1., 1., 1.]))]\n",
      "**************************************************\n",
      " module bias:\n",
      "tensor([-0.0893, -0.1464, -0.0000, -0.0076,  0.1493, -0.0418],\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      " module _forward_pre_hooks:\n",
      "OrderedDict([(328, <torch.nn.utils.prune.PruningContainer object at 0x00000235D919EE50>), (329, <torch.nn.utils.prune.RandomUnstructured object at 0x00000235D8F4C310>)])\n"
     ]
    }
   ],
   "source": [
    "# पहला पैरामीटर: मॉड्यूल, कांट-छांट किए जाने वाले विशिष्ट मॉड्यूल का प्रतिनिधित्व करता है, यहां यह मॉड्यूल = मॉडल.conv1 को संदर्भित करता है,\n",
    "# इंगित करता है कि पहली संकेंद्रित परत पर छंटाई की जानी है।\n",
    "# दूसरा पैरामीटर: नाम, दर्शाता है कि चयनित मॉड्यूल में किन मापदंडों को काटा जाना है।\n",
    "# यहां name='weight' सेट है, जिसका अर्थ है पूर्वाग्रह के बजाय नेटवर्क में वजन को कम करना।\n",
    "# तीसरा पैरामीटर: राशि, मॉडल में एक विशिष्ट अनुपात या मापदंडों की पूर्ण संख्या की छंटाई का प्रतिनिधित्व करता है।\n",
    "# राशि 0.0-1.0 के बीच एक फ्लोट मान है, जो अनुपात का प्रतिनिधित्व करता है, या एक सकारात्मक पूर्णांक दर्शाता है कि कितने पैरामीटर क्लिप करने हैं।\n",
    "\n",
    "prune.random_unstructured(module, name=\"bias\", amount=1)\n",
    "\n",
    "# मॉडल पैरामीटर फिर से प्रिंट करें\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module bias:\")\n",
    "print(module.bias)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module _forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "निष्कर्ष: मॉड्यूल के विभिन्न पैरामीटर सेटों पर अलग-अलग प्रूनिंग रणनीतियों को लागू करते हुए, हम पा सकते हैं कि मॉडल पैरामीटर स्टेट_डिक्ट और नेम_पैरामीटर में, न केवल वेट_ओरिग हैं, बल्कि बायस_ओरिग भी हैं, नेम_बफ़र्स के पैरामीटर में, वेट_मास्क और बायस_मास्क भी एक ही समय में दिखाई देते हैं।\n",
    "अंत में, क्योंकि हम दो प्रकार के मापदंडों पर दो अलग-अलग प्रूनिंग फ़ंक्शन लागू करते हैं, _forward_pre_hooks दो अलग-अलग फ़ंक्शन परिणाम भी प्रिंट करते हैं।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 सामान्य असंरचित छंटाई (एल1_असंरचित)\n",
    "पहले, हमने निर्दिष्ट conv1 परत के वजन और पूर्वाग्रह की छंटाई के लिए अलग-अलग तरीके पेश किए थे, तो क्या एक ही समय में मल्टी-लेयर नेटवर्क के विशिष्ट मापदंडों की छंटाई का समर्थन करना संभव है?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model state_dict keys:\n",
      "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias_orig', 'conv2.bias_mask', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " module named_parameters:\n",
      "[('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.0220,  0.1789, -0.0544, -0.0713,  0.0478],\n",
      "          [ 0.1995, -0.0415,  0.0288, -0.1431,  0.1057],\n",
      "          [ 0.1600,  0.0248, -0.1903, -0.0242, -0.1961],\n",
      "          [-0.0211,  0.0257, -0.1116, -0.1678,  0.0611],\n",
      "          [ 0.0012,  0.0420, -0.1725, -0.1265, -0.1075]]],\n",
      "\n",
      "\n",
      "        [[[-0.0540, -0.1928, -0.0355, -0.0075, -0.1481],\n",
      "          [ 0.0135,  0.0192,  0.0082, -0.0120, -0.0164],\n",
      "          [-0.0435, -0.1488,  0.1092, -0.0041,  0.1960],\n",
      "          [-0.1045, -0.0136,  0.0398, -0.1286,  0.0617],\n",
      "          [-0.0091,  0.0466,  0.1827,  0.1655,  0.0727]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1216, -0.0833, -0.1491, -0.1143,  0.0113],\n",
      "          [ 0.0452,  0.1662, -0.0425, -0.0904, -0.1235],\n",
      "          [ 0.0565,  0.0933, -0.0721,  0.0909,  0.1837],\n",
      "          [-0.1739,  0.0263,  0.1339,  0.0648, -0.0382],\n",
      "          [-0.1667,  0.1478,  0.0448, -0.0892,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1976,  0.0123,  0.1523, -0.1207,  0.1493],\n",
      "          [-0.1799,  0.0580,  0.1490,  0.1647, -0.0572],\n",
      "          [-0.0908,  0.1094, -0.0676, -0.0023,  0.0624],\n",
      "          [-0.0320, -0.1794,  0.1706, -0.0486,  0.0557],\n",
      "          [ 0.1482, -0.1306,  0.1213, -0.1090, -0.1267]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1278,  0.1037, -0.0323, -0.1504,  0.1080],\n",
      "          [ 0.0266, -0.0996,  0.1499, -0.0845,  0.0609],\n",
      "          [-0.0662, -0.1405, -0.0586, -0.0615, -0.0462],\n",
      "          [-0.1118, -0.0961, -0.1325, -0.0417, -0.0741],\n",
      "          [ 0.1842, -0.1040, -0.1786, -0.0593,  0.0186]]],\n",
      "\n",
      "\n",
      "        [[[-0.0889, -0.0737, -0.1655, -0.1708, -0.0988],\n",
      "          [-0.1787,  0.1127,  0.0706, -0.0352,  0.1238],\n",
      "          [-0.0985, -0.1929, -0.0062,  0.0488, -0.1152],\n",
      "          [-0.1659, -0.0448,  0.0821, -0.0956, -0.0262],\n",
      "          [ 0.1928,  0.1767, -0.1792, -0.1364,  0.0507]]]], requires_grad=True)), ('bias_orig', Parameter containing:\n",
      "tensor([-0.0893, -0.1464, -0.1101, -0.0076,  0.1493, -0.0418],\n",
      "       requires_grad=True))]\n",
      "**************************************************\n",
      " module named_buffers:\n",
      "[('weight_mask', tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1.]]]])), ('bias_mask', tensor([1., 1., 0., 0., 1., 1.]))]\n",
      "**************************************************\n",
      " module weight:\n",
      "tensor([[[[ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1216, -0.0833, -0.1491, -0.1143,  0.0113],\n",
      "          [ 0.0452,  0.1662, -0.0425, -0.0904, -0.1235],\n",
      "          [ 0.0565,  0.0933, -0.0721,  0.0909,  0.1837],\n",
      "          [-0.1739,  0.0263,  0.1339,  0.0648, -0.0382],\n",
      "          [-0.1667,  0.1478,  0.0448, -0.0892,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0889, -0.0737, -0.1655, -0.1708, -0.0988],\n",
      "          [-0.1787,  0.1127,  0.0706, -0.0352,  0.1238],\n",
      "          [-0.0985, -0.1929, -0.0062,  0.0488, -0.1152],\n",
      "          [-0.1659, -0.0448,  0.0821, -0.0956, -0.0262],\n",
      "          [ 0.1928,  0.1767, -0.1792, -0.1364,  0.0507]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      " module bias:\n",
      "tensor([-0.0893, -0.1464, -0.0000, -0.0000,  0.1493, -0.0418],\n",
      "       grad_fn=<MulBackward0>)\n",
      "**************************************************\n",
      " module _forward_pre_hooks:\n",
      "OrderedDict([(328, <torch.nn.utils.prune.PruningContainer object at 0x00000235D919EE50>), (330, <torch.nn.utils.prune.PruningContainer object at 0x00000235DA526AF0>)])\n"
     ]
    }
   ],
   "source": [
    "# मॉडल के मॉड्यूल मापदंडों को छाँटें\n",
    "for n, m in model.named_modules():\n",
    "# मॉडल में सभी संकेंद्रित परतों पर l1_असंरचित प्रूनिंग करें, और प्रूनिंग के लिए 20% मापदंडों का चयन करें\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(m, name=\"bias\", amount=0.2)\n",
    "# मॉडल में पूरी तरह से जुड़ी सभी परतों पर ln_structured प्रूनिंग करें, और प्रूनिंग के लिए 40% मापदंडों का चयन करें\n",
    "# एलिफ़ इज़इंस्टेंस(मॉड्यूल, टॉर्च.एनएन.रैखिक):\n",
    "# prune.random_structured(मॉड्यूल, नाम = \"वजन\", राशि = 0.4, मंद = 0)\n",
    "\n",
    "# मॉडल पैरामीटर फिर से प्रिंट करें\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module weight:\")\n",
    "print(module.weight)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module bias:\")\n",
    "print(module.bias)\n",
    "print('*'*50)\n",
    "\n",
    "print(\" module _forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "इसके बाद, हम छंटाई करते हैं और मॉडल के वजन को स्थायी रूप से हटा देते हैं। पिछले छंटाई चरणों के बाद, मूल वजन 'वेट_ओरिग' बन गया है, और वजन 'वेट_ओरिग' और मास्क मैट्रिक्स को गुणा करने का परिणाम है, जो एक विशेषता बन गया है हटाने के बाद हुआ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      " model state_dict keys:\n",
      "odict_keys(['conv1.bias', 'conv1.weight', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      " model named_parameters:\n",
      "[('bias', Parameter containing:\n",
      "tensor([-0.0893, -0.1464, -0.0000, -0.0000,  0.1493, -0.0418],\n",
      "       requires_grad=True)), ('weight', Parameter containing:\n",
      "tensor([[[[ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1216, -0.0833, -0.1491, -0.1143,  0.0113],\n",
      "          [ 0.0452,  0.1662, -0.0425, -0.0904, -0.1235],\n",
      "          [ 0.0565,  0.0933, -0.0721,  0.0909,  0.1837],\n",
      "          [-0.1739,  0.0263,  0.1339,  0.0648, -0.0382],\n",
      "          [-0.1667,  0.1478,  0.0448, -0.0892,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0889, -0.0737, -0.1655, -0.1708, -0.0988],\n",
      "          [-0.1787,  0.1127,  0.0706, -0.0352,  0.1238],\n",
      "          [-0.0985, -0.1929, -0.0062,  0.0488, -0.1152],\n",
      "          [-0.1659, -0.0448,  0.0821, -0.0956, -0.0262],\n",
      "          [ 0.1928,  0.1767, -0.1792, -0.1364,  0.0507]]]], requires_grad=True))]\n",
      "**************************************************\n",
      " model named_buffers:\n",
      "[]\n",
      "**************************************************\n",
      " model forward_pre_hooks:\n",
      "OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "# मॉड्यूल वेट रिमूवल पर प्रूनिंग और स्थायी ऑपरेशन करें\n",
    "for n, m in model.named_modules():\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        prune.remove(m, 'bias')\n",
    "\n",
    "# कन्व1 वजन हटाने पर छंटाई और स्थायी संचालन करें\n",
    "prune.remove(module, 'weight')\n",
    "print('*'*50)\n",
    "\n",
    "# काटे गए मॉडल का राज्य शब्दकोश प्रिंट करें\n",
    "print(\" model state_dict keys:\")\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "# मॉडल पैरामीटर फिर से प्रिंट करें\n",
    "print(\" model named_parameters:\")\n",
    "print(list(module.named_parameters()))\n",
    "print('*'*50)\n",
    "\n",
    "# मॉडल मास्क बफ़र्स पैरामीटर को फिर से प्रिंट करें\n",
    "print(\" model named_buffers:\")\n",
    "print(list(module.named_buffers()))\n",
    "print('*'*50)\n",
    "\n",
    "# मॉडल के _फॉरवर्ड_प्री_हुक को फिर से प्रिंट करें\n",
    "print(\" model forward_pre_hooks:\")\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "निष्कर्ष: मॉडल के वजन और पूर्वाग्रह पर हटाने का ऑपरेशन करने के बाद, मॉडल पैरामीटर सेट में वजन_ओरिग और पूर्वाग्रह_ओरिग गायब हो जाते हैं और वजन और पूर्वाग्रह बन जाते हैं, यह दर्शाता है कि नामित_बफ़र्स टेंसर प्रिंटिंग के लिए प्रूनिंग स्थायी हो गई है, यह देखा जा सकता है कि केवल [] छोड़ दिया गया है, क्योंकि वेट_मास्क और बायस-मास्क, जो वजन और पूर्वाग्रह के लिए मास्क हैं, प्रभावी हो गए हैं और अब इन्हें बनाए रखने की आवश्यकता नहीं है।\n",
    "इसी प्रकार, _forward_pre_hooks में केवल एक खाली शब्दकोश बचा हैआईएएस फिर से पैरामीटर बन गए हैं, और काट-छांट स्थायी हो गई है।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. वैश्विक काट-छाँट"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "चार स्थानीय प्रूनिंग विधियाँ ऊपर प्रस्तुत की गई हैं, लेकिन काफी हद तक, आपको अपने अनुभव के आधार पर नेटवर्क की एक निश्चित परत को प्रून करने का निर्णय लेने की आवश्यकता है।\n",
    "एक अधिक सामान्य प्रूनिंग रणनीति वैश्विक प्रूनिंग का उपयोग करना है, जो पूरे नेटवर्क के परिप्रेक्ष्य से प्रूनिंग करती है, वैश्विक प्रूनिंग के बाद, विभिन्न परतों पर प्रूनिंग का प्रतिशत भिन्न हो सकता है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "**************************************************\n",
      "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.bias', 'conv2.weight_orig', 'conv2.weight_mask', 'fc1.bias', 'fc1.weight_orig', 'fc1.weight_mask', 'fc2.bias', 'fc2.weight_orig', 'fc2.weight_mask', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device=device)\n",
    "\n",
    "# सबसे पहले आरंभिक मॉडल का राज्य शब्दकोश प्रिंट करें\n",
    "print(model.state_dict().keys())\n",
    "print('*'*50)\n",
    "\n",
    "# यह निर्धारित करने के लिए पैरामीटर सेट बनाएं कि कौन सी परतें और पैरामीटर सेट प्रूनिंग में भाग लेते हैं\n",
    "parameters_to_prune = (\n",
    "            (model.conv1, 'weight'),\n",
    "            (model.conv2, 'weight'),\n",
    "            (model.fc1, 'weight'),\n",
    "            (model.fc2, 'weight'))\n",
    "\n",
    "# प्रूनिंग ऑपरेशन करने के लिए प्रून में ग्लोबल प्रूनिंग फ़ंक्शन ग्लोबल_अनस्ट्रक्चर्ड को कॉल करें\n",
    "prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.2)\n",
    "\n",
    "# काटे गए मॉडल का राज्य शब्दकोश प्रिंट करें\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "मॉडल को काटने के बाद, अलग-अलग परतों में वजन मापदंडों के अलग-अलग अनुपात होंगे, इसे प्रिंट करने के लिए कोड का उपयोग करें और देखें:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 5.33%\n",
      "Sparsity in conv2.weight: 17.25%\n",
      "Sparsity in fc1.weight: 22.03%\n",
      "Sparsity in fc2.weight: 14.67%\n",
      "Global sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv1.weight == 0))\n",
    "    / float(model.conv1.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv2.weight == 0))\n",
    "    / float(model.conv2.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.fc1.weight == 0))\n",
    "    / float(model.fc1.weight.nelement())\n",
    "    ))\n",
    "\n",
    "print(\n",
    "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.fc2.weight == 0))\n",
    "    / float(model.fc2.weight.nelement())\n",
    "    ))\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "    100. * float(torch.sum(model.conv1.weight == 0)\n",
    "               + torch.sum(model.conv2.weight == 0)\n",
    "               + torch.sum(model.fc1.weight == 0)\n",
    "               + torch.sum(model.fc2.weight == 0))\n",
    "         / float(model.conv1.weight.nelement()\n",
    "               + model.conv2.weight.nelement()\n",
    "               + model.fc1.weight.nelement()\n",
    "               + model.fc2.weight.nelement())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "निष्कर्ष: जब वैश्विक प्रूनिंग रणनीति अपनाई जाती है (यह मानते हुए कि 20% पैरामीटर प्रूनिंग में शामिल हैं), मॉडल के कुल मापदंडों में से केवल 20% की छंटाई की जाती है, और प्रत्येक परत की विशिष्ट स्थिति विशिष्ट पैरामीटर वितरण द्वारा निर्धारित की जाती है मॉडल का."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. कस्टम प्रूनिंग।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "प्रूनिंग मॉडल को प्रूनिंग करने के लिए क्लास BasePruningMethod() विरासत में मिली है। इसके अंदर कई विधियाँ हैं: कॉल, अप्लाई_मास्क, अप्लाई, प्रून, रिमूव, आदि। कस्टम प्रूनिंग नियम सेटिंग को पूरा करने के लिए __init__ (कन्स्ट्रक्टर) और कंप्यूट_मास्क फ़ंक्शंस को लागू किया जाना चाहिए।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# कस्टम प्रूनिंग विधि का वर्ग prune.BasePruningMethod को इनहेरिट करना चाहिए\n",
    "class custom_prune(prune.BasePruningMethod):\n",
    "#इस तकनीक द्वारा कार्यान्वित छंटाई के प्रकार को निर्दिष्ट करें (समर्थित विकल्प वैश्विक, संरचित और असंरचित हैं)\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "# आंतरिक रूप से कंप्यूट_मास्क फ़ंक्शन को कार्यान्वित करें और प्रूनिंग नियमों को परिभाषित करें, जो अनिवार्य रूप से वजन मापदंडों को कैसे छिपाया जाए\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "# यहां परिभाषित नियम हर दूसरे पैरामीटर को छुपाने के लिए है, और अंततः प्रूनिंग में शामिल 50% पैरामीटर को छुपा दिया जाता है\n",
    "        mask.view(-1)[::2] = 0\n",
    "        return mask\n",
    "\n",
    "# प्रूनिंग विधि फ़ंक्शन को कस्टमाइज़ करें, और सीधे प्रूनिंग क्लास विधि को आंतरिक रूप से लागू करें\n",
    "def custome_unstructured_pruning(module, name):\n",
    "    custom_prune.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "2.996683120727539 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# मॉडल क्लास को इंस्टेंट करें\n",
    "model = LeNet().to(device=device)\n",
    "\n",
    "start = time.time()\n",
    "# मॉडल में पहली पूरी तरह से कनेक्टेड परत fc1 में पूर्वाग्रह पर कस्टम प्रूनिंग करने के लिए कस्टम प्रूनिंग विधि के फ़ंक्शन को कॉल करें\n",
    "custome_unstructured_pruning(model.fc1, name=\"bias\")\n",
    "\n",
    "# सफल प्रूनिंग का सबसे बड़ा संकेत बायस_मास्क पैरामीटर का होना है\n",
    "print(model.fc1.bias_mask)\n",
    "\n",
    "# कस्टम प्रूनिंग में लगने वाले समय को प्रिंट करें\n",
    "duration = time.time() - start\n",
    "print(duration * 1000, 'ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "निष्कर्ष: बायस_मास्क टेंसर प्रत्येक दूसरे बिट को पूर्वनिर्धारित तरीके से पूरी तरह से मास्क कर देता है, जिसमें 0 और 1 बारी-बारी से दिखाई देते हैं। जब रिमूवल ऑपरेशन बाद में किया जाता है, तो मूल बायस_ओरिग में वजन भी हर दूसरे बिट में काट दिया जाएगा।"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
