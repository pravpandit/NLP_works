{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1 Customizing LLM based on LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provides an efficient development framework for developing custom applications based on LLM, which makes it easy for developers to quickly stimulate the powerful capabilities of LLM and build LLM applications. LangChain also supports a variety of large models, and has built-in call interfaces for large models such as OpenAI and LLAMA. However, LangChain does not have all large models built in. It provides powerful scalability by allowing users to customize LLM types.\n",
    "\n",
    "In this section, we take the Baidu Wenxin large model as an example to describe how to customize LLM based on LangChain, so that the applications built on LangChain can support domestic large models such as Baidu Wenxin and iFlytek Spark.\n",
    "\n",
    "This section involves relatively more technical details of LangChain and large model calls. Students with energy can learn to deploy, and if you don’t have energy, you can directly use the subsequent code to support calls.\n",
    "\n",
    "To implement custom LLM, you need to define a custom class that inherits from LangChain's LLM base class, and then define two functions: \n",
    "① _call method, which accepts a string and returns a string, which is the core call of the model; \n",
    "② _identifying_params method, used to print LLM information. \n",
    "\n",
    "First, we import the required third-party libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from typing import Any, List, Mapping, Optional, Dict, Union, Tuple\n",
    "import requests\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "from pydantic import Field, model_validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Baidu Wenxin uses dual secret keys for authentication, users need to first obtain access_token based on API_Key and Secret_Key, and then use access_token to call the model (see \"3. Calling Baidu Wenxin\" for details). Therefore, we need to define a get_access_token method to obtain access_token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token(api_key : str, secret_key : str):\n",
    "    \"\"\"\n",
    "    使用 API Key，Secret Key 获取access_token，替换下列示例中的应用API Key、应用Secret Key\n",
    "    \"\"\"\n",
    "# Specify the URL\n",
    "    url = f\"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={api_key}&client_secret={secret_key}\"\n",
    "# Set up POST access\n",
    "    payload = json.dumps(\"\")\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "# Access the corresponding access_token of the account through POST\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    return response.json().get(\"access_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a custom LLM class that inherits from the LLM class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherited from langchain.llms.base.LLM\n",
    "class Wenxin_LLM(LLM):\n",
    "# Native interface address\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/eb-instant\"\n",
    "# The default model is ERNIE-Bot-turbo, which is currently known as the Baidu Wenxin model.\n",
    "    model_name: str = Field(default=\"ERNIE-Bot-turbo\", alias=\"model\")\n",
    "# Access delay limit\n",
    "    request_timeout: Optional[Union[float, Tuple[float, float]]] = None\n",
    "# Temperature Coefficient\n",
    "    temperature: float = 0.1\n",
    "# API_Key\n",
    "    api_key: str = None\n",
    "# Secret_Key\n",
    "    secret_key : str = None\n",
    "# access_token\n",
    "    access_token: str = None\n",
    "# Required optional parameters\n",
    "    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above initialization covers the parameters we usually use. You can also add more parameters according to actual needs and Wenxin's API.\n",
    "\n",
    "Next, we implement an initialization method init_access_token, which is called when the model's access_token is empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_access_token(self):\n",
    "    if self.api_key != None and self.secret_key != None:\n",
    "# Both keys must be non-empty to obtain access_token\n",
    "        try:\n",
    "            self.access_token = get_access_token(self.api_key, self.secret_key)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"获取 access_token 失败，请检查 Key\")\n",
    "    else:\n",
    "        print(\"API_Key 或 Secret_Key 为空，请检查 Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement the core method - calling the model API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any):\n",
    "# Except for the prompt parameter, other parameters are not used, but when we call it through LangChain, these parameters will be passed in, so they must be set\n",
    "# If access_token is empty, initialize access_token\n",
    "    if self.access_token == None:\n",
    "        self.init_access_token()\n",
    "# API call url\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/eb-instant?access_token={}\".format(self.access_token)\n",
    "# Configure POST parameters\n",
    "    payload = json.dumps({\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",# user prompt\n",
    "                \"content\": \"{}\".format(prompt)# 输入的 prompt\n",
    "            }\n",
    "        ],\n",
    "        'temperature' : self.temperature\n",
    "    })\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "# Make a request\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, timeout=self.request_timeout)\n",
    "    if response.status_code == 200:\n",
    "# The returned value is a Json string\n",
    "        js = json.loads(response.text)\n",
    "        return js[\"result\"]\n",
    "    else:\n",
    "        return \"请求失败\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we also need to define the description method of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a method that returns default parameters\n",
    "@property\n",
    "def _default_params(self) -> Dict[str, Any]:\n",
    "\"\"\"Get the default parameters for calling the Ennie API.\"\"\"\n",
    "    normal_params = {\n",
    "        \"temperature\": self.temperature,\n",
    "        \"request_timeout\": self.request_timeout,\n",
    "        }\n",
    "    return {**normal_params}\n",
    "\n",
    "\n",
    "@property\n",
    "def _identifying_params(self) -> Mapping[str, Any]:\n",
    "\"\"\"Get the identifying parameters.\"\"\"\n",
    "    return {**{\"model_name\": self.model_name}, **self._default_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the above steps, we can define the calling method of Baidu Wenxin based on LangChain. We encapsulate this code in the wenxin_llm.py file and use this LLM directly in the notebook that describes how to call Baidu Wenxin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
