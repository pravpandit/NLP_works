{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e3f209-1b47-41aa-bb33-d0e7b564203c",
   "metadata": {
    "height": 30
   },
   "source": [
    "# Deploy the knowledge base assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896c72f-1aa0-4b93-aea6-45908a6e42a1",
   "metadata": {},
   "source": [
    "Now that we have a basic understanding of the knowledge base and LLM, it is time to combine them skillfully and create a visual interface that is not only more convenient for operation, but also easier to share with others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c975542-100a-431f-bfdb-2e948fd1e360",
   "metadata": {
    "height": 30
   },
   "source": [
    "Streamlit is a quick and easy way to demonstrate machine learning models through a friendly web interface directly in Python. In this course, we will learn how to use it to build user interfaces for generative AI applications. After building a machine learning model, if you want to build a demo for others to see, maybe to get feedback and drive improvements to the system, or just because you think the system is cool, you want to demonstrate it: Streamlit allows you to quickly achieve this goal through a Python interface program without writing any front-end, web page, or JavaScript code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa74cbe-96ed-4652-a761-8740615597ed",
   "metadata": {
    "height": 30
   },
   "source": [
    "## 1. Introduction to Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf25020-22a5-435d-925a-26cbe71a5f59",
   "metadata": {},
   "source": [
    "`Streamlit` is an open source Python library for quickly creating data applications. It is designed to enable data scientists to easily transform data analysis and machine learning models into interactive web applications without having to deeply understand web development. Unlike conventional web frameworks such as Flask/Django, it does not require you to write any client-side code (HTML/CSS/JS). You only need to write ordinary Python modules to create beautiful and highly interactive interfaces in a short time, thereby quickly generating data analysis or machine learning results; on the other hand, unlike those tools that can only be generated by dragging and dropping, you still have full control over the code.\n",
    "\n",
    "Streamlit provides a set of simple and powerful basic modules for building data applications:\n",
    "\n",
    "- st.write(): This is one of the most basic modules used to present text, images, tables, and other content in the application.\n",
    "\n",
    "- st.title(), st.header(), st.subheader(): These modules are used to add titles, subtitles, and grouped titles to organize the layout of the application.\n",
    "\n",
    "- st.text(), st.markdown(): Used to add text content, supporting Markdown syntax.\n",
    "\n",
    "- st.image():Used to add images to the application.\n",
    "\n",
    "- st.dataframe(): used to render Pandas data frames.\n",
    "\n",
    "- st.table(): used to render simple data tables.\n",
    "\n",
    "- st.pyplot(), st.altair_chart(), st.plotly_chart(): used to render charts drawn by Matplotlib, Altair or Plotly.\n",
    "\n",
    "- st.selectbox(), st.multiselect(), st.slider(), st.text_input(): used to add interactive widgets that allow users to select, enter or slide in the application.\n",
    "\n",
    "- st.button(), st.checkbox(), st.radio(): used to add buttons, checkboxes and radio buttons to trigger specific actions.\n",
    "\n",
    "These basic modules make it easy to build interactive data applications through Streamlit, and they can be combined and customized as needed when used. For more information, please check the [official documentation](https://docs.streamlit.io/get-started)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3209b0",
   "metadata": {},
   "source": [
    "## 2. Build the application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b23f9f",
   "metadata": {},
   "source": [
    "First, create a new Python file and save it streamlit_app.py in the root of your working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e3845",
   "metadata": {},
   "source": [
    "1. Import necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4172dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d95cb",
   "metadata": {},
   "source": [
    "2. Create the application title `st.title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562cd2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title('ü¶úüîó Âä®ÊâãÂ≠¶Â§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèë')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0465f6",
   "metadata": {},
   "source": [
    "3. Add a text input box for users to enter their OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = st.sidebar.text_input('OpenAI API Key', type='password')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e882d27",
   "metadata": {},
   "source": [
    "4. Define a function that uses the user's key to authenticate to the OpenAI API, send a prompt, and get the AI-generated response. The function accepts the user's prompt as a parameter and uses `st.info` to display the AI-generated response in a blue box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    llm = ChatOpenAI(temperature=0.7, openai_api_key=openai_api_key)\n",
    "    st.info(llm(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c18041",
   "metadata": {},
   "source": [
    "5. Finally, use `st.form()` to create a text box (st.text_area()) for the user to enter input. When the user clicks `Submit`, `generate-response()` will call the function with the user's input as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with st.form('my_form'):\n",
    "    text = st.text_area('Enter text:', 'What are the three key pieces of advice for learning how to code?')\n",
    "    submitted = st.form_submit_button('Submit')\n",
    "    if not openai_api_key.startswith('sk-'):\n",
    "        st.warning('Please enter your OpenAI API key!', icon='‚ö†')\n",
    "    if submitted and openai_api_key.startswith('sk-'):\n",
    "        generate_response(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f024f5b",
   "metadata": {},
   "source": [
    "6. Save the current file `streamlit_app.py`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887beac1",
   "metadata": {},
   "source": [
    "7. Return to your computer's terminal to run the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25271c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit run streamlit_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84b80b",
   "metadata": {},
   "source": [
    "The results are shown below: \n",
    "\n",
    "![](../../figures/streamlit_app.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c38d3b",
   "metadata": {},
   "source": [
    "However, only a single round of conversation is currently possible. We make some modifications to the above. By using `st.session_state` to store the conversation history, the context of the entire conversation can be retained when the user interacts with the application.\n",
    "\n",
    "![](../../figures/streamlit_app2.png)\n",
    "\n",
    "The specific code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit API\n",
    "def main():\n",
    "    st.title('ü¶úüîó Âä®ÊâãÂ≠¶Â§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèë')\n",
    "    openai_api_key = st.sidebar.text_input('OpenAI API Key', type='password')\n",
    "\n",
    "# Used to track conversation history\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    messages = st.container(height=300)\n",
    "    if prompt := st.chat_input(\"Say something\"):\n",
    "# Add user input to the conversation history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"text\": prompt})\n",
    "\n",
    "# Call the respond function to get the answer\n",
    "        answer = generate_response(prompt, openai_api_key)\n",
    "# Check if the answer is None\n",
    "        if answer is not None:\n",
    "# Add LLM's answer to the conversation history\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"text\": answer})\n",
    "\n",
    "# Display the entire conversation history\n",
    "        for message in st.session_state.messages:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                messages.chat_message(\"user\").write(message[\"text\"])\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                messages.chat_message(\"assistant\").write(message[\"text\"])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c244a",
   "metadata": {},
   "source": [
    "## 3. Add search questions and answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916ca5e",
   "metadata": {},
   "source": [
    "First, encapsulate the code in the section `2. Build a search question and answer chain`:\n",
    "- The get_vectordb function returns the vector knowledge base persisted in the C3 section\n",
    "- The get_chat_qa_chain function returns the result of calling a search question and answer chain with historical records\n",
    "- The get_qa_chain function returns the result of calling a search question and answer chain without historical records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectordb():\n",
    "# Define Embeddings\n",
    "    embedding = ZhipuAIEmbeddings()\n",
    "# Vector database persistence path\n",
    "    persist_directory = '../C3 Êê≠Âª∫Áü•ËØÜÂ∫ì/data_base/vector_db/chroma'\n",
    "# Load the database\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,  # ÂÖÅËÆ∏Êàë‰ª¨Â∞Üpersist_directoryÁõÆÂΩï‰øùÂ≠òÂà∞Á£ÅÁõò‰∏ä\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "#Question and answer chain with history\n",
    "def get_chat_qa_chain(question:str,openai_api_key:str):\n",
    "    vectordb = get_vectordb()\n",
    "    llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0,openai_api_key = openai_api_key)\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",  # ‰∏é prompt ÁöÑËæìÂÖ•ÂèòÈáè‰øùÊåÅ‰∏ÄËá¥„ÄÇ\n",
    "        return_messages=True  # Â∞Ü‰ª•Ê∂àÊÅØÂàóË°®ÁöÑÂΩ¢ÂºèËøîÂõûËÅäÂ§©ËÆ∞ÂΩïÔºåËÄå‰∏çÊòØÂçï‰∏™Â≠óÁ¨¶‰∏≤\n",
    "    )\n",
    "    retriever=vectordb.as_retriever()\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory\n",
    "    )\n",
    "    result = qa({\"question\": question})\n",
    "    return result['answer']\n",
    "\n",
    "#Q&A chain without history\n",
    "def get_qa_chain(question:str,openai_api_key:str):\n",
    "    vectordb = get_vectordb()\n",
    "    llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0,openai_api_key = openai_api_key)\n",
    "    template = \"\"\"‰ΩøÁî®‰ª•‰∏ã‰∏ä‰∏ãÊñáÊù•ÂõûÁ≠îÊúÄÂêéÁöÑÈóÆÈ¢ò„ÄÇÂ¶ÇÊûú‰Ω†‰∏çÁü•ÈÅìÁ≠îÊ°àÔºåÂ∞±ËØ¥‰Ω†‰∏çÁü•ÈÅìÔºå‰∏çË¶ÅËØïÂõæÁºñÈÄ†Á≠î\n",
    "        Ê°à„ÄÇÊúÄÂ§ö‰ΩøÁî®‰∏âÂè•ËØù„ÄÇÂ∞ΩÈáè‰ΩøÁ≠îÊ°àÁÆÄÊòéÊâºË¶Å„ÄÇÊÄªÊòØÂú®ÂõûÁ≠îÁöÑÊúÄÂêéËØ¥‚ÄúË∞¢Ë∞¢‰Ω†ÁöÑÊèêÈóÆÔºÅ‚Äù„ÄÇ\n",
    "        {context}\n",
    "        ÈóÆÈ¢ò: {question}\n",
    "        \"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073fce21",
   "metadata": {},
   "source": [
    "Then, add a radio button component `st.radio` to select the mode for question and answer:\n",
    "- None: Normal mode without search and answer\n",
    "- qa_chain: Search and answer mode without history\n",
    "- chat_qa_chain: Search and answer mode with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_method = st.radio(\n",
    "        \"‰Ω†ÊÉ≥ÈÄâÊã©Âì™ÁßçÊ®°ÂºèËøõË°åÂØπËØùÔºü\",\n",
    "        [\"None\", \"qa_chain\", \"chat_qa_chain\"],\n",
    "        captions = [\"‰∏ç‰ΩøÁî®Ê£ÄÁ¥¢ÈóÆÁ≠îÁöÑÊôÆÈÄöÊ®°Âºè\", \"‰∏çÂ∏¶ÂéÜÂè≤ËÆ∞ÂΩïÁöÑÊ£ÄÁ¥¢ÈóÆÁ≠îÊ®°Âºè\", \"Â∏¶ÂéÜÂè≤ËÆ∞ÂΩïÁöÑÊ£ÄÁ¥¢ÈóÆÁ≠îÊ®°Âºè\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5917f6",
   "metadata": {},
   "source": [
    "The final effect is as follows: \n",
    "\n",
    "![](../../figures/streamlit_app3.png)\n",
    "\n",
    "Enter the page, first enter OPEN_API_KEY (default), then click the radio button to select the question and answer mode, and finally enter your question in the input box and press Enter! \n",
    "\n",
    "> For the complete code, refer to [streamlit_app.py](./streamlit_app.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a43de5",
   "metadata": {},
   "source": [
    "## 4. Deploy your app \n",
    "\n",
    "To deploy your app to Streamlit Cloud, follow these steps: \n",
    "\n",
    "1. Create a GitHub repository for your app. Your repository should contain two files: \n",
    "\n",
    "your-repository/ \n",
    "‚îú‚îÄ‚îÄ streamlit_app.py \n",
    "‚îî‚îÄ‚îÄ requirements.txt \n",
    "\n",
    "2. Go to [Streamlit Community Cloud](http://share.streamlit.io/), click the `New app` button in your workspace, and specify the repository, branch, and master file path. Optionally, you can customize the URL of your app by selecting a custom subdomain\n",
    "\n",
    "3. Click the `Deploy!` button \n",
    "\n",
    "Your app will now be deployed to Streamlit Community Cloud and accessible from all over the world! üåé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c13465",
   "metadata": {},
   "source": [
    "Our project deployment is basically completed. We have simplified it for demonstration purposes. There are still many areas that can be further optimized. We look forward to learners making various modifications!\n",
    "\n",
    "Optimization direction:\n",
    "- Add the function of uploading local documents and establishing vector database in the interface\n",
    "- Add buttons for selecting multiple LLM and embedding methods\n",
    "- Add buttons for modifying parameters\n",
    "- More......"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
