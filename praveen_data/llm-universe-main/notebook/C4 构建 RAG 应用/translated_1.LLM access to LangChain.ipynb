{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd27580d-a1f0-4548-a77d-368312687f8e",
   "metadata": {},
   "source": [
    "# 4.1 Connect LLM to LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b0303-2d62-469c-a457-6cfcd42eb14e",
   "metadata": {},
   "source": [
    "LangChain provides an efficient development framework for developing custom applications based on LLM, which allows developers to quickly stimulate the powerful capabilities of LLM and build LLM applications. LangChain also supports a variety of large models, and has built-in calling interfaces for large models such as OpenAI and LLAMA. However, LangChain does not have all large models built in. It provides strong scalability by allowing users to customize LLM types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6dae23-b591-4c26-99b8-db33a77693f9",
   "metadata": {},
   "source": [
    "## 1. Call ChatGPT based on LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073e6df-8d24-46fe-86b3-ac622177500a",
   "metadata": {},
   "source": [
    "LangChain provides encapsulation for a variety of large models. Based on the LangChain interface, ChatGPT can be easily called and integrated into personal applications built on the LangChain framework. Here we briefly describe how to use the LangChain interface to call ChatGPT.\n",
    "\n",
    "Note that calling ChatGPT based on the LangChain interface also requires configuring your personal key, and the configuration method is the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4bf770",
   "metadata": {},
   "source": [
    "### 1.1 Models\n",
    "Import `OpenAI`'s dialogue model `ChatOpenAI` from `langchain.chat_models`. In addition to OpenAI, `langchain.chat_models` also integrates other dialogue models. For more details, please refer to the [Langchain official documentation](https://api.python.langchain.com/en/latest/langchain_api_reference.html#module-langchain.chat_models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09245292-6ab4-4a45-b5c4-42f60acb6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Read local/project environment variables.\n",
    "\n",
    "# find_dotenv() finds and locates the path of the .env file\n",
    "# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment\n",
    "# If you set a global environment variable, this line of code will have no effect.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get the environment variable OPENAI_API_KEY\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecc1e6",
   "metadata": {},
   "source": [
    "If langchain-openai is not installed, please run the following code first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1f024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08314bf7-7ab0-4bc9-b7eb-6dbf88f04a05",
   "metadata": {},
   "source": [
    "Next you need to instantiate a ChatOpenAI class. You can pass in hyperparameters to control the answer, such as the `temperature` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db5579b-72be-4eda-a217-a9f30ad75b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001B17F799BD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001B17F79BA60>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_api_base='https://api.chatgptid.net/v1', openai_proxy='')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we set the parameter temperature to 0.0 to reduce the randomness of the generated answers.\n",
    "# If you want to get different and innovative answers every time, you can try adjusting this parameter.\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d3aad-f100-415b-b19b-a673242d18a0",
   "metadata": {},
   "source": [
    "The cell above assumes that your OpenAI API key is set in an environment variable. If you wish to manually specify your API key, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93de6b9c-2cab-4716-81e5-b674fd714562",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, openai_api_key=\"YOUR_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504c768-416e-48fb-85ed-21c8f05b3e2e",
   "metadata": {},
   "source": [
    "As you can see, the ChatGPT-3.5 model is called by default. In addition, several commonly used hyperparameter settings include:\n",
    "\n",
    "· model_name: the model to be used, the default is ‘gpt-3.5-turbo’, and the parameter settings are consistent with the OpenAI native interface parameter settings.\n",
    "\n",
    "· temperature: temperature coefficient, the value is the same as the native interface.\n",
    "\n",
    "· openai_api_key: OpenAI API key, if you do not use environment variables to set the API Key, you can also set it during instantiation.\n",
    "\n",
    "· openai_proxy: Set the proxy, if you do not use environment variables to set the proxy, you can also set it during instantiation.\n",
    "\n",
    "· streaming: whether to use streaming, that is, output the model answer word by word, the default is False, which is not repeated here.\n",
    "\n",
    "· max_tokens: the maximum number of tokens output by the model, the meaning and value are the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb1b84",
   "metadata": {},
   "source": [
    "Once we have initialized the `LLM` of your choice, we can try using it! Let's ask \"Please tell me about yourself!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef54a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(\"请你自我介绍一下自己！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddb976f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好，我是一个智能助手，专注于为用户提供各种服务和帮助。我可以回答问题、提供信息、解决问题，帮助用户更高效地完成工作和生活。如果您有任何疑问或需要帮助，请随时告诉我，我会尽力帮助您。感谢您的使用！', response_metadata={'token_usage': {'completion_tokens': 104, 'prompt_tokens': 20, 'total_tokens': 124}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa226b2",
   "metadata": {},
   "source": [
    "### 1.2 Prompt (Prompt Template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7199b8c8-4299-462b-aee1-109f66d49ca0",
   "metadata": {},
   "source": [
    "When we develop large model applications, most of the time we don't pass user input directly to the LLM. Usually, they add the user input to a larger text, called a `prompt template`, which provides additional context about the specific task at hand.\n",
    "PromptTemplates help with this! They bundle all the logic from user input to a fully formatted prompt. This can start very simple - for example, the prompt that generates the above string is:\n",
    "\n",
    "We need to construct a personalized Template first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9acbd1ef-17f6-4513-a51b-2f489bbe7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Here we ask the model to translate the given text into Chinese\n",
    "prompt = \"\"\"请你将由三个反引号分割的文本翻译成英文！\\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d459b03",
   "metadata": {},
   "source": [
    "Next, let's take a look at the complete prompt template that has been constructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c106f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'请你将由三个反引号分割的文本翻译成英文！text: ```我带着比身体重的行李，游入尼罗河底，经过几道闪电 看到一堆光圈，不确定是不是这里。```\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"我带着比身体重的行李，\\\n",
    "游入尼罗河底，\\\n",
    "经过几道闪电 看到一堆光圈，\\\n",
    "不确定是不是这里。\\\n",
    "\"\n",
    "prompt.format(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d1867",
   "metadata": {},
   "source": [
    "We know that the interface of the chat model is based on messages, not raw text. PromptTemplates can also be used to generate message lists. In this example, `prompt` contains not only the input content information, but also the information of each `message` (role, position in the list, etc.). Usually, a `ChatPromptTemplate` is a list of `ChatMessageTemplate`. Each `ChatMessageTemplate` contains instructions for formatting the chat message (its role and content).\n",
    "\n",
    "Let's take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a4414f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个翻译助手，可以帮助我将 中文 翻译成 英文.'),\n",
       " HumanMessage(content='我带着比身体重的行李，游入尼罗河底，经过几道闪电 看到一堆光圈，不确定是不是这里。')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = \"你是一个翻译助手，可以帮助我将 {input_language} 翻译成 {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "text = \"我带着比身体重的行李，\\\n",
    "游入尼罗河底，\\\n",
    "经过几道闪电 看到一堆光圈，\\\n",
    "不确定是不是这里。\\\n",
    "\"\n",
    "messages  = chat_prompt.format_messages(input_language=\"中文\", output_language=\"英文\", text=text)\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab2c62",
   "metadata": {},
   "source": [
    "Next, let's call the defined `llm` and `messages` to output the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3129e2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I carried luggage heavier than my body and dived into the bottom of the Nile River. After passing through several flashes of lightning, I saw a pile of halos, not sure if this is the place.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output  = llm.invoke(messages)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44004f",
   "metadata": {},
   "source": [
    "### 1.3 Output parser\n",
    "\n",
    "OutputParsers convert the raw output of the language model into a format that can be used downstream. There are several main types of OutputParsers, including:\n",
    "- Convert LLM text to structured information (such as JSON)\n",
    "- Convert ChatMessage to string\n",
    "- Convert extra information returned by calls other than messages (such as OpenAI function calls) to string\n",
    "\n",
    "Finally, we pass the model output to `output_parser`, which is a `BaseOutputParser`, which means it accepts **strings or BaseMessages as input**. StrOutputParser in particular simply converts any input to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c64afdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I carried luggage heavier than my body and dived into the bottom of the Nile River. After passing through several flashes of lightning, I saw a pile of halos, not sure if this is the place.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44bd56",
   "metadata": {},
   "source": [
    "As can be seen from the above results, we successfully parsed the output of type `ChatMessage` into `string` through the output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5fe94a",
   "metadata": {},
   "source": [
    "### 1.4 Complete Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a49104",
   "metadata": {},
   "source": [
    "We can now combine all of this into a chain. This chain will take input variables, pass those variables to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser. Next we'll use the LCEL syntax to quickly implement a chain. Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc6dae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I carried luggage heavier than my body and dived into the bottom of the Nile River. After passing through several flashes of lightning, I saw a pile of halos, not sure if this is the place.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | llm | output_parser\n",
    "chain.invoke({\"input_language\":\"中文\", \"output_language\":\"英文\",\"text\": text})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532af28",
   "metadata": {},
   "source": [
    "Let’s test another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0440e251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我扛着比我的身体还重的行李，潜入尼罗河的底部。穿过几道闪电后，我看到一堆光环，不确定这是否就是目的地。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I carried luggage heavier than my body and dived into the bottom of the Nile River. After passing through several flashes of lightning, I saw a pile of halos, not sure if this is the place.'\n",
    "chain.invoke({\"input_language\":\"英文\", \"output_language\":\"中文\",\"text\": text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa23207",
   "metadata": {},
   "source": [
    "> What is LCEL? \n",
    "LCEL (LangChain Expression Language) is a new syntax and an important addition to the LangChain toolkit. It has many advantages that make it easier and more convenient to work with LangChain and agents.\n",
    "\n",
    "- LCEL provides asynchronous, batch, and stream processing support, allowing code to be quickly ported across different servers.\n",
    "- LCEL has fallback measures to solve the problem of LLM format output.\n",
    "- LCEL increases the parallelism of LLM and improves efficiency.\n",
    "- LCEL has built-in logging, which helps understand the operation of complex chains and agents even when agents become complex.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "`chain = prompt | model | output_parser`\n",
    "\n",
    "In the above code, we use LCEL to piece together different components into a chain, in which user input is passed to the prompt template, then the prompt template output is passed to the model, and then the model output is passed to the output parser. The symbol | is similar to the Unix pipe operator, which links different components together and uses the output of one component as the input of the next component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6fbbb-996b-4d77-b9ff-ecddbf055dcb",
   "metadata": {},
   "source": [
    "## 2. Use LangChain to call Baidu Wenxin Yiyan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e2c64-37a7-4e2d-8bfd-9fcf4bd753a9",
   "metadata": {},
   "source": [
    "We can also call Baidu Wenxin model through the LangChain framework to integrate the Wenxin model into our application framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddc109",
   "metadata": {},
   "source": [
    "### 2.1 Customize LLM to access langchain\n",
    "In the old version, LangChain does not directly support Wenxin calls, and we need to customize an LLM that supports Wenxin model calls. Here, in order to show users how to customize LLM, we briefly describe this method in \"Appendix 1. LangChain Custom LLM\", and you can also refer to [source document](https://python.langchain.com/v0.1/docs/modules/model_io/llms/custom_llm/).\n",
    "\n",
    "Here, we can directly call the customized Wenxin_LLM. For details on how to encapsulate Wenxin_LLM, see `wenxin_llm.py`.\n",
    "\n",
    "**Note: The following code needs to download our encapsulated code [wenxin_llm.py](./wenxin_llm.py) to the same directory as this Notebook before it can be used directly. Because the new version of LangChain can directly call the Wenxin Qianfan API, we recommend using the code in the next section to call the Wenxin Yiyan model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8505d726-4d2a-4bd4-a603-5edb1b204e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to download source code\n",
    "from wenxin_llm import Wenxin_LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e3020-7fd2-4f8a-87e6-c088b1b773ab",
   "metadata": {},
   "source": [
    "We want to store the secret key directly in the .env file and load it into the environment variable like calling ChatGPT, so as to hide the specific details of the secret key and ensure security. Therefore, we need to configure `QIANFAN_AK` and `QIANFAN_SK` in the .env file and load it using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc0dd3cf-f0a3-46cc-8eff-465523acfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# Read local/project environment variables.\n",
    "\n",
    "# find_dotenv() finds and locates the path of the .env file\n",
    "# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment\n",
    "# If you set a global environment variable, this line of code will have no effect.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get the environment variable API_KEY\n",
    "wenxin_api_key = os.environ[\"QIANFAN_AK\"]\n",
    "wenxin_secret_key = os.environ[\"QIANFAN_SK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ced8d04-8f34-42eb-8eb3-0171ac5db0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Wenxin_LLM(api_key=wenxin_api_key, secret_key=wenxin_secret_key, system=\"你是一个助手！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e909be-889f-4ce9-9164-0410ff4ad73d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-31 22:12:53] openapi_requestor.py:316 [t:27812]: requesting llm api endpoint: /chat/eb-instant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好！我是助手，负责协助您完成各种任务。我具备快速响应、高效执行和灵活适应的能力，致力于为您提供优质的服务。无论您需要什么帮助，我都会尽力满足您的需求。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"你好，请你自我介绍一下！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2167d07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-31 22:12:41] openapi_requestor.py:316 [t:27812]: requesting llm api endpoint: /chat/eb-instant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好！我是助手，负责协助您完成各种任务。我具备快速学习和处理信息的能力，能够根据您的需求提供帮助和回答问题。无论您需要什么帮助，我都会尽力提供支持。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or use\n",
    "llm(prompt=\"你好，请你自我介绍一下！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b81514-3735-4e34-b4a8-2c43240307ca",
   "metadata": {},
   "source": [
    "Therefore, we can add the Wenxin model to the LangChain architecture and implement the call of the Wenxin model in the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266bbe8",
   "metadata": {},
   "source": [
    "### 2.2 Directly call Wenxinyiyan in langchain\n",
    "\n",
    "We can also use the new version of LangChain to directly call the Wenxinyiyan model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7312bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# Read local/project environment variables.\n",
    "\n",
    "# find_dotenv() finds and locates the path of the .env file\n",
    "# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment\n",
    "# If you set a global environment variable, this line of code will have no effect.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get the environment variable API_KEY\n",
    "QIANFAN_AK = os.environ[\"QIANFAN_AK\"]\n",
    "QIANFAN_SK = os.environ[\"QIANFAN_SK\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65168734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install required dependencies\n",
    "%pip install -qU langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a934e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\miniconda3\\envs\\llm2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "[INFO] [03-31 22:40:14] openapi_requestor.py:316 [t:3684]: requesting llm api endpoint: /chat/eb-instant\n",
      "[INFO] [03-31 22:40:14] oauth.py:207 [t:3684]: trying to refresh access_token for ak `MxBM7W***`\n",
      "[INFO] [03-31 22:40:15] oauth.py:220 [t:3684]: sucessfully refresh access_token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是文心一言，英文名是ERNIE Bot。我是一款人工智能语言模型，可以协助你完成范围广泛的任务并提供有关各种主题的信息，比如回答问题，提供定义和解释及建议，还能提供上下文知识和对话管理。如果你有任何问题或需要帮助，随时向我提问，我会尽力回答。\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import QianfanLLMEndpoint\n",
    "\n",
    "llm = QianfanLLMEndpoint(streaming=True)\n",
    "res = llm(\"你好，请你自我介绍一下！\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e8955d-530f-40cb-bbae-c90b9c1a4a67",
   "metadata": {},
   "source": [
    "## 3. iFlytek Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8d4ac-5519-46a7-b6cf-7ebe3511e607",
   "metadata": {},
   "source": [
    "We can also call the iFlytek Spark model through the LangChain framework. For more information, refer to [SparkLLM](https://python.langchain.com/docs/integrations/llms/sparkllm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e831c3-b9b8-4617-ab95-6de61c7422b9",
   "metadata": {},
   "source": [
    "We want to store the secret key directly in the .env file and load it into the environment variable like calling ChatGPT, so as to hide the specific details of the secret key and ensure security. Therefore, we need to configure `IFLYTEK_SPARK_APP_ID`, `IFLYTEK_SPARK_API_KEY` and `IFLYTEK_SPARK_API_SECRET` in the .env file and load it using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d91f2b-f78a-4dc1-b1bf-09ce96a76a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# Read local/project environment variables.\n",
    "\n",
    "# find_dotenv() finds and locates the path of the .env file\n",
    "# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment\n",
    "# If you set a global environment variable, this line of code will have no effect.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get the environment variable API_KEY\n",
    "IFLYTEK_SPARK_APP_ID = os.environ[\"IFLYTEK_SPARK_APP_ID\"]\n",
    "IFLYTEK_SPARK_API_KEY = os.environ[\"IFLYTEK_SPARK_API_KEY\"]\n",
    "IFLYTEK_SPARK_API_SECRET = os.environ[\"IFLYTEK_SPARK_API_SECRET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6b1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_spark_params(model):\n",
    "    '''\n",
    "    构造星火模型请求参数\n",
    "    '''\n",
    "\n",
    "    spark_url_tpl = \"wss://spark-api.xf-yun.com/{}/chat\"\n",
    "    model_params_dict = {\n",
    "# v1.5 version\n",
    "        \"v1.5\": {\n",
    "            \"domain\": \"general\", # 用于配置大模型版本\n",
    "            \"spark_url\": spark_url_tpl.format(\"v1.1\") # 云端环境的服务地址\n",
    "        },\n",
    "# v2.0 version\n",
    "        \"v2.0\": {\n",
    "            \"domain\": \"generalv2\", # 用于配置大模型版本\n",
    "            \"spark_url\": spark_url_tpl.format(\"v2.1\") # 云端环境的服务地址\n",
    "        },\n",
    "# v3.0 version\n",
    "        \"v3.0\": {\n",
    "            \"domain\": \"generalv3\", # 用于配置大模型版本\n",
    "            \"spark_url\": spark_url_tpl.format(\"v3.1\") # 云端环境的服务地址\n",
    "        },\n",
    "# v3.5 version\n",
    "        \"v3.5\": {\n",
    "            \"domain\": \"generalv3.5\", # 用于配置大模型版本\n",
    "            \"spark_url\": spark_url_tpl.format(\"v3.5\") # 云端环境的服务地址\n",
    "        }\n",
    "    }\n",
    "    return model_params_dict[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f0359c-325e-43fc-911a-0fc1679fc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import SparkLLM\n",
    "\n",
    "spark_api_url = gen_spark_params(model=\"v1.5\")[\"spark_url\"]\n",
    "\n",
    "# Load the model (v3.0 is used by default)\n",
    "llm = SparkLLM(spark_api_url = spark_api_url)  #指定 v1.5版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dbd0649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好，我是科大讯飞研发的认知智能大模型，我的名字叫讯飞星火认知大模型。我可以和人类进行自然交流，解答问题，高效完成各领域认知智能需求。\n"
     ]
    }
   ],
   "source": [
    "res = llm(\"你好，请你自我介绍一下！\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a2428-d220-4da8-8ab1-45e29617768a",
   "metadata": {},
   "source": [
    "Therefore, we can add the Spark model to the LangChain architecture and implement the call to the Wenxin model in the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe38818-65c9-42f8-b3e3-f997e1e3cffa",
   "metadata": {},
   "source": [
    "## 4. Use LangChain to call Zhipu GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26b48b-ddd7-4cba-86c7-07f2046e3270",
   "metadata": {},
   "source": [
    "We can also call the Zhipu AI big model through the LangChain framework to connect it to our application framework. Since the [ChatGLM](https://python.langchain.com/docs/integrations/llms/chatglm) provided in langchain is no longer available, we need to customize an LLM.\n",
    "\n",
    "If you are using the Zhipu GLM API, you need to download our encapsulated code [zhipuai_llm.py](./zhipuai_llm.py) to the same directory as this Notebook before you can run the following code to use GLM in LangChain.\n",
    "\n",
    "According to Zhipu’s official announcement, the following models will be deprecated soon. After these models are deprecated, they will be automatically routed to new models. Please note that before the deprecation date, update your model code to the latest version to ensure a smooth transition of services. For more information about the model, please visit [model](https://open.bigmodel.cn/dev/howuse/model)\n",
    "\n",
    "| Model code | Deprecation date | Point to model |\n",
    "| ---- | ---- | ---- |\n",
    "|chatglm_pro|December 31, 2024|glm-4|\n",
    "|chatglm_std|December 31, 2024|glm-3-turbo|\n",
    "|chatglm_lite|December 31, 2024|glm-3-turbo|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe7cf9",
   "metadata": {},
   "source": [
    "### 4.1 Customize chatglm to access langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96644f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai_llm import ZhipuAILLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec52f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# Read local/project environment variables.\n",
    "\n",
    "# find_dotenv() finds and locates the path of the .env file\n",
    "# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment\n",
    "# If you set a global environment variable, this line of code will have no effect.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get the environment variable API_KEY\n",
    "api_key = os.environ[\"ZHIPUAI_API_KEY\"] #填写控制台中获取的 APIKey 信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef49232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhipuai_model = ZhipuAILLM(model = \"glm-4\", temperature = 0.1, api_key = api_key)  #model=\"glm-4-0520\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73328a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！我是智谱清言，是清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型。我的目标是通过回答用户提出的问题来帮助他们解决问题。由于我是一个计算机程序，所以我没有自我意识，也不能像人类一样感知世界。我只能通过分析我所学到的信息来回答问题。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhipuai_model(\"你好，请你自我介绍一下！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77fd1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
