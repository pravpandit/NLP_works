{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Embedding API\n",
    "Note: To facilitate embedding API calls, the key should be filled in the .env file under llm_universe, and the code will automatically read and load the environment variables.\n",
    "## 1. Use OpenAI API\n",
    "GPT has a packaged interface, we can simply package it. Currently, there are three GPT embedding modes, and the performance is as follows:\n",
    "|Model | Pages per dollar | [MTEB](https://github.com/embeddings-benchmark/mteb) score | [MIRACL](https://github.com/project-miracl/miracl) score|\n",
    "| --- | --- | --- | --- |\n",
    "|text-embedding-3-large|9,615|64.6|54.9|\n",
    "|text-embedding-3-small|62,500|62.3|44.0|\n",
    "|text-embedding-ada-002|12,500|61.0|31.4|\n",
    "* MTEB score is the average score of eight tasks such as classification, clustering, and pairing of the embedding model.\n",
    "* MIRACL score is the average score of the embedding model inAverage score on the retrieval task. \n",
    "\n",
    "From the above three embedding models, we can see that `text-embedding-3-large` has the best performance and the most expensive price. We can use it when the application we build needs better performance and the cost is sufficient; `text-embedding-3-small` has better performance and price. We can choose this model when our budget is limited; and `text-embedding-ada-002` is the previous generation model of OpenAI. It is inferior to the previous two in terms of performance and price, so it is not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "# Read local/project environment variables.\n",
    "# find_dotenv() finds and locates the path of the .env file\n",
    "# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment\n",
    "# If you set a global environment variable, this line of code will have no effect.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# If you need to access through the proxy port, you need to configure as follows\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ[\"HTTP_PROXY\"] = 'http://127.0.0.1:7890'\n",
    "\n",
    "def openai_embedding(text: str, model: str=None):\n",
    "# Get the environment variable OPENAI_API_KEY\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "# embedding model: 'text-embedding-3-small', 'text-embedding-3-large', 'text-embedding-ada-002'\n",
    "    if model == None:\n",
    "        model=\"text-embedding-3-small\"\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return response\n",
    "\n",
    "response = openai_embedding(text='要生成 embedding 的输入文本，字符串形式。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data returned by the API is in `json` format. In addition to the `object` vector type, there are also data `data` for storing data, the embedding model model `model`, and the usage of this token `usage`, as shown below:\n",
    "```json\n",
    "{\n",
    "\"object\": \"list\",\n",
    "\"data\": [\n",
    "{\n",
    "\"object\": \"embedding\",\n",
    "\"index\": 0,\n",
    "\"embedding\": [\n",
    "-0.006929283495992422,\n",
    "... (omitted)\n",
    "-4.547132266452536e-05,\n",
    "],\n",
    "}\n",
    "],\n",
    "\"model\": \"text-embedding-3-small\",\n",
    "\"usage\": {\n",
    "\"prompt_tokens\": 5,\n",
    "\"total_tokens\": 5\n",
    "}\n",
    "}\n",
    "```\n",
    "We can call the response object to get the type of embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "返回的embedding类型为：list\n"
     ]
    }
   ],
   "source": [
    "print(f'返回的embedding类型为：{response.object}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding is stored in data, and we can check the length of the embedding and the generated embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding长度为：1536\n",
      "embedding（前10）为：[0.03884002938866615, 0.013516489416360855, -0.0024250170681625605, -0.01655769906938076, 0.024130908772349358, -0.017382603138685226, 0.04206013306975365, 0.011498954147100449, -0.028245486319065094, -0.00674333656206727]\n"
     ]
    }
   ],
   "source": [
    "print(f'embedding长度为：{len(response.data[0].embedding)}')\n",
    "print(f'embedding（前10）为：{response.data[0].embedding[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the model and token usage of this embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本次embedding model为：text-embedding-3-small\n",
      "本次token使用情况为：Usage(prompt_tokens=12, total_tokens=12)\n"
     ]
    }
   ],
   "source": [
    "print(f'本次embedding model为：{response.model}')\n",
    "print(f'本次token使用情况为：{response.usage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Wenxin Qianfan API\n",
    "Embedding-V1 is a text representation model based on Baidu Wenxin large model technology. Access token is the credential for calling the interface. When using Embedding-V1, you should first obtain Access token with API Key and Secret Key, and then use Access token to call the interface to embed text. At the same time, Qianfan large model platform also supports embedding models such as bge-large-zh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def wenxin_embedding(text: str):\n",
    "# Get environment variables wenxin_api_key, wenxin_secret_key\n",
    "    api_key = os.environ['QIANFAN_AK']\n",
    "    secret_key = os.environ['QIANFAN_SK']\n",
    "\n",
    "# Use API Key and Secret Key to obtain Access token from https://aip.baidubce.com/oauth/2.0/token\n",
    "    url = \"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={0}&client_secret={1}\".format(api_key, secret_key)\n",
    "    payload = json.dumps(\"\")\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    \n",
    "# Embed text using the obtained Access token\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/embedding-v1?access_token=\" + str(response.json().get(\"access_token\"))\n",
    "    input = []\n",
    "    input.append(text)\n",
    "    payload = json.dumps({\n",
    "        \"input\": input\n",
    "    })\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "    return json.loads(response.text)\n",
    "# text should be List(string)\n",
    "text = \"要生成 embedding 的输入文本，字符串形式。\"\n",
    "response = wenxin_embedding(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Embedding-V1, each embedding has a separate id and a timestamp to record the time of embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本次embedding id为：as-hvbgfuk29u\n",
      "本次embedding产生时间戳为：1711435238\n"
     ]
    }
   ],
   "source": [
    "print('本次embedding id为：{}'.format(response['id']))\n",
    "print('本次embedding产生时间戳为：{}'.format(response['created']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can also get the embedding type and embedding from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "返回的embedding类型为:embedding_list\n",
      "embedding长度为：384\n",
      "embedding（前10）为：[0.060567744076251984, 0.020958080887794495, 0.053234219551086426, 0.02243831567466259, -0.024505289271473885, -0.09820500761270523, 0.04375714063644409, -0.009092536754906178, -0.020122773945331573, 0.015808865427970886]\n"
     ]
    }
   ],
   "source": [
    "print('返回的embedding类型为:{}'.format(response['object']))\n",
    "print('embedding长度为：{}'.format(len(response['data'][0]['embedding'])))\n",
    "print('embedding（前10）为：{}'.format(response['data'][0]['embedding'][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use iFlytek Spark API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not yet open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Zhipu API\n",
    "Zhipu has a packaged SDK, we can just call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "def zhipu_embedding(text: str):\n",
    "\n",
    "    api_key = os.environ['ZHIPUAI_API_KEY']\n",
    "    client = ZhipuAI(api_key=api_key)\n",
    "    response = client.embeddings.create(\n",
    "        model=\"embedding-2\",\n",
    "        input=text,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "text = '要生成 embedding 的输入文本，字符串形式。'\n",
    "response = zhipu_embedding(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is of type `zhipuai.types.embeddings.EmbeddingsResponded`. We can call `object`, `data`, `model`, and `usage` to view the embedding type, embedding, embedding model, and usage of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response类型为：<class 'zhipuai.types.embeddings.EmbeddingsResponded'>\n",
      "embedding类型为：list\n",
      "生成embedding的model为：embedding-2\n",
      "生成的embedding长度为：1024\n",
      "embedding（前10）为: [0.017892399802803993, 0.0644201710820198, -0.009342825971543789, 0.02707476168870926, 0.004067837726324797, -0.05597858875989914, -0.04223804175853729, -0.03003198653459549, -0.016357755288481712, 0.06777040660381317]\n"
     ]
    }
   ],
   "source": [
    "print(f'response类型为：{type(response)}')\n",
    "print(f'embedding类型为：{response.object}')\n",
    "print(f'生成embedding的model为：{response.model}')\n",
    "print(f'生成的embedding长度为：{len(response.data[0].embedding)}')\n",
    "print(f'embedding（前10）为: {response.data[0].embedding[:10]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_universe_2.x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
