# Use LLM API

**Note: The source code corresponding to this article is in [2. Use LLM API.ipynb](https://github.com/datawhalechina/llm-universe/blob/main/notebook/C2%20%E4%BD%BF%E7%94%A8%20LLM%20API%20%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8/2.%20%E4%BD%BF%E7%94%A8%20LLM%20API.ipynb). If you need to reproduce, you can download and run the source code. **

This chapter mainly introduces the API application guidelines and Python version of the native API calling method of four large language models (ChatGPTAPI, Wenxin Yiyan, iFlytek Spark, Zhipu GLM). Readers can choose an API that they can apply for according to their actual situation to read and learn. If you need to use LLM in LangChain, you can refer to the calling method in [LLM access to LangChain](C4/1.LLM access to LangChain.md).

* ChatGPT: Recommended for readers who can access the Internet scientifically;
* Wenxinyiyan: There is currently no activity to give away tokens to new users, and it is recommended to use Wenxin tokens quota users and paid users;
* iFlytek Spark: New users are given tokens, and free users are recommended to use;
* Zhipu GLM: New users are given tokens, and free users are recommended to use.

## 1. Use ChatGPT

ChatGPT, released in November 2022, is a representative product of the currently popular Large Language Model (LLM). At the end of 2022, it was the amazing performance of ChatGPT that triggered the LLM craze. To date, GPT-4 released by OpenAI is still the representative of the upper limit of LLM performance, and ChatGPT is still the LLM product with the largest number of users, the highest popularity, and the most development potential. In fact, in the eyes of outsiders, ChatGPT is a synonym for LLM.

In addition to releasing free web products, OpenAI also provides a variety of ChatGPT APIs, which support developers to call ChatGPT through Python or Request requests and embed the powerful capabilities of LLM into their own services. The main models to choose from include ChatGPT-3.5 and GPT-4, and each model also has multiple context versions. For example, ChatGPT-3.5 has the most original 4K contextLength model, and also 16K context length model gpt-turbo-16k-0613.

### 1.1 API Application Guidelines

#### Get and configure OpenAI API key

OpenAI API call service is paid, and every developer needs to first get and configure OpenAI API key to access ChatGPT in their own application. We will briefly describe how to get and configure OpenAI API key in this section.

Before getting OpenAI API key, we need to register an account on [OpenAI official website](https://openai.com/). Here we assume that we already have an OpenAI account. Log in to the [OpenAI official website](https://openai.com/). After logging in, it will be as shown below:

![](../figures/C2-2-openai-choose.png)

We select `API` and then click `API keys` in the left sidebar, as shown below:

![](../figures/C2-2-openai-get-key.png)

Click the `Create new secret key` button to create an OpenAI API key.Copy the created OpenAI API key and save it in the form of `OPENAI_API_KEY="sk-..."` to the `.env` file, and save the `.env` file in the project root directory.

Below is the code to read the `.env` file:

```python
import os
from dotenv import load_dotenv, find_dotenv

# Read local/project environment variables.

# find_dotenv() Find and locate the path of the .env file
# load_dotenv() Read the .env file and load the environment variables in it into the current running environment
# If you set the global environment variables, this line of code has no effect.
_ = load_dotenv(find_dotenv())

# If you need to access through a proxy port, you also need to configure as follows
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'
os.environ["HTTP_PROXY"] = 'http://127.0.0.1:7890'
```

### 1.2 Call OpenAI API

CallTo use ChatGPT, you need to use the [ChatCompletion API](https://platform.openai.com/docs/api-reference/chat), which provides calls to the ChatGPT series of models, including ChatGPT-3.5, GPT-4, etc.

The ChatCompletion API call method is as follows:

```python
from openai import OpenAI

client = OpenAI(
# This is the default and can be omitted
api_key=os.environ.get("OPENAI_API_KEY"),
)

# Import required libraries
# Note that here we assume that you have configured the OpenAI API Key according to the above. If not, the access will fail
completion = client.chat.completions.create(
# Call model: ChatGPT-3.5
model="gpt-3.5-turbo",
# messages is a list of conversations
messages=[{"role": "system", "content": "You are a helpful assistant."},
{"role": "user", "content": "Hello!"}
]
)
```

Calling this API will return a ChatCompletion object, which includes attributes such as answer text, creation time, and id. What we generally need is the answer text, that is, the content information in the answer object.

```python
completion
```

ChatCompletion(id='chatcmpl-9FA5aO72SD9X0XTpc1HCNZkSFCf7C', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1713400730, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_c2295e73ad', usage=CompletionUsage(completion_tokens=9, prompt_tokens=19, total_tokens=28))

```python
print(completion.choices[0].message.content)
```

Hello! How can I assist you today?

Here we introduce several parameters commonly used in calling APIs:

· model, that is, the model called, generally takes values ​​including "gpt-3.5-turbo" (ChatGPT-3.5), "gpt-3.5-turbo-16k-0613" (ChatGPT-3.5 16K version), “gpt-4” (ChatGPT-4). Note that the cost of different models is different.

· messages, which is our prompt. ChatCompletion's messages need to pass in a list, which includes prompts of multiple different roles. The roles we can choose generally include system: the system prompt mentioned in the previous article; user: the prompt entered by the user; assistant: the assistant, which is generally the historical response of the model as a reference content provided to the model.

· temperature, temperature. That is, the Temperature coefficient mentioned in the previous article.

· max_tokens, the maximum number of tokens, that is, the maximum number of tokens output by the model. The number of tokens calculated by OpenAI is the total number of tokens of Prompt and Completion combined, and the total number of tokens cannot exceed the upper limit of the model (such as the default model token upper limit is 4096). Therefore, if the input prompt is long, a larger max_token value needs to be set, otherwise an error will be reported that the length exceeds the limit.

OpenAI provides sufficient customization space, supporting us to improve the model's answer effect by customizing the prompt. The following is a simple function that encapsulates the OpenAI interface, which supports us to directly pass in the prompt and obtain the output of the model:

```python
from openai import OpenAI

client = OpenAI(
# This is the default and can be omitted
api_key=os.environ.get("OPENAI_API_KEY"),
)

def gen_gpt_messages(prompt):
'''
Construct GPT model request parameters messages

Request parameters:
prompt: corresponding user prompt words
'''
messages = [{"role": "user", "content": prompt}]
return messages

def get_completion(prompt, model="gpt-3.5-turbo", temperature = 0):
'''
Get GPT model call results

Request parameters:
prompt: corresponding prompt words
model: the model called, the default is gpt-3.5-turbo, you can also choose gpt-3.5-turbo as neededpt-4 and other models
temperature: The temperature coefficient of the model output, which controls the randomness of the output, and the value range is 0~2. The lower the temperature coefficient, the more consistent the output content.
'''
response = client.chat.completions.create(
model=model,
messages=gen_gpt_messages(prompt),
temperature=temperature,
)
if len(response.choices) > 0:
return response.choices[0].message.content
return "generate answer error"
```

```python
get_completion("Hello")
```

'Hello! Is there anything I can help you with? '

In the above function, we encapsulate the details of messages and only use user prompt to implement the call. In simple scenarios, this function is sufficient to meet the usage requirements.

## 2.Using Wenxinyiyan

`Wenxinyiyan`, a Chinese large model launched by Baidu on March 27, 2023, is the representative product of the current domestic large language model. Limited by the difference in the quality of Chinese corpus and the bottlenecks of domestic computing resources and computing technology, Wenxinyiyan still has a certain gap from ChatGPT in overall performance, but it has shown a relatively superior performance in the Chinese context. The landing scenarios considered by Wenxinyiyan include multimodal generation, literary creation and other commercial scenarios. Its goal is to surpass ChatGPT in the Chinese context. Of course, Baidu still has a long way to go to truly defeat ChatGPT; but in China, where generative AI supervision is relatively strict, as the first batch of generative AI applications allowed to be open to the public, Wenxinyiyan still has certain commercial advantages over ChatGPT, which cannot be publicly used.

Baidu also provides the API interface of Wenxinyiyan. At the same time as launching the large model, it also launched the `Wenxin Qianfan` enterprise-level large language model service platform, including Baidu's entire large language model development work chain. For small and medium-sized enterprises or traditional enterprises that do not have the ability to implement large models, considering Wenxin Qianfan is a viable option. Of course, this tutorial only includes calling the Wenxin Yiyan API through the Wenxin Qianfan platform, and does not discuss other enterprise-level services.

### 2.1 Qianfan SDK

#### 2.1.1 API Application Guide

#### Get the KeyBaidu Smart Cloud Qianfan Large Model Platform provides [Qianfan SDK](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/wlmhm7vuo) in multiple languages. Developers can use SDK to quickly develop functions and improve development efficiency.

Before using Qianfan SDK, you need to obtain the Wenxin Yiyan call key first. You need to configure your own key in the code to call the model. Below we take [Python SDK](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/7lq3ft3pb) as an example to introduce the process of calling the Wenxin model through Qianfan SDK.

First, you need to have a Baidu account that has been authenticated by real name. Each account can create several applications, and each application will correspond to an API_Key and Secret_Key.

![](../figures/C2-2-baidu_qianfan_1.png)

Enter the [Wenxin Qianfan Service Platform](https://console.bce.baidu.com/qianfan/overview), click the above `Application Access` button, and create an application that calls the Wenxin model.

![](../figures/C2-2-baidu_qianfan_2.png)

Then click the `Go to Create` button to enter the application creation interface:

![](../figures/C2-2-baidu_qianfan_3.png)

Simply enter basic information, select the default configuration, and create the application.

![](../figures/C2-2-baidu_qianfan_4.png)

After the creation is completed, we can see the `API Key` and `Secret Key` of the created application in the console.

**It should be noted that Qianfan currently only has three services, [Prompt template](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Alisj3ard), [Yi-34B-Chat](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/vlpteyv3c) and [Fuyu-8B public cloud online call experience service](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Qlq4l7uw6), which are free to use. If you want to experience other model services, you need to use [Billing Management](https://console.bce.baidu.com/qianfan/chargemanage/list) to open the paid service of the corresponding model to experience it. **

We fill in the `API Key` and `Secret Key` obtained here to the `QIANFAN_AK` and `QIANFAN_SK` parameters of the `.env` file. If you are using parameter verification for security authentication, you need to check the `Access Key` and `Secret Key` on the [Baidu Smart Cloud Console-User Account-Security Authentication](https://console.bce.baidu.com/iam/#/iam/accesslist) page, and fill in the obtained parameters to the `QIANFAN_ACCESS_KEY` and `QIANFAN_SECRET_KEY` of the `.env` file accordingly.

![](../figures/C2-2-baidu_qianfan_5.png)

Then execute the following code to load the key into the environment variable.

```python
from dotenv import load_dotenv, find_dotenv

# Read local/project environment variables.

# find_dotenv() finds and locates the path of the .env file
# load_dotenv() reads the .envfile, and load the environment variables in it into the current running environment 
# If you set the global environment variables, this line of code has no effect.
_ = load_dotenv(find_dotenv())
```

#### 2.1.2 Call Wenxin Qianfan API

Baidu Wenxin also supports configuring the prompts of the two member roles of user and assistant in the messages field of the incoming parameters, but unlike the prompt format of OpenAI, the model personality is passed in through another parameter system field, not in the messages field.

Below we use the SDK to encapsulate a `get_completion` function for subsequent use.

**Remind readers again: If there is no free or purchased credit in the account, when executing the following code to call Wenxin `ERNIE-Bot`, the following error will be reported: `error code: 17, err msg: Open api daily request limit reached`. **

Click [Model Service](https://console.bce.baidu.com/qianfan/ais/console/onlineService) to view the models supported by QianfanList of all models.

```python
import qianfan

def gen_wenxin_messages(prompt):
'''
Construct Wenxin model request parameters messages

Request parameters:
prompt: corresponding user prompt words
'''
messages = [{"role": "user", "content": prompt}]
return messages

def get_completion(prompt, model="ERNIE-Bot", temperature=0.01):
'''
Get Wenxin model call results

Request parameters:
prompt: corresponding prompt words
model: the model called, the default is ERNIE-Bot, you can also select other models such as ERNIE-Bot-4 as needed
temperature: the temperature coefficient of the model output, controls the randomness of the output, the value range is 0~1.0, and cannot be set to 0. The lower the temperature coefficient, the more consistent the output content.
'''

chat_comp = qianfan.ChatCompletion()
message = gen_wenxin_messages(prompt)

resp = chat_comp.do(messages=message, 
model=model,
temperature = temperature,
system="You are a personal assistant-Little Whale")

return resp["result"]
```

If you are a free user, when using the above function, you can specify a free model (such as `Yi-34B-Chat`) in the input parameter and then run:

```python
get_completion("Hello, introduce yourself", model="Yi-34B-Chat")
```

[WARNING] [04-18 08:39:59] base.py:516 [t:8597525056]: This key `system` does not seem to be a parameter that the model `Yi-34B-Chat` will accept
[INFO] [04-18 08:39:59] openapi_requestor.py:316 [t:8597525056]: requesting llm api endpoint: /chat/yi_34b_chat

'Hello! My name is Yi. I am an intelligent assistant developed by Zero One Everything. The research team of Zero One Everything has trained me with a large amount of text data and learned various patterns and associations of language, so that I can generate text, answer questions, and translate languages. I can help you answer questions, provide information, or perform other tasks related to language processing. If you have any questions or need help, please feel free to let me know! '

If you have the usage quota of the Wenxin series model `ERNIE-Bot`, you can directly run the following function:

```python
get_completion("Hello, introduce yourself")
```

[INFO] [04-18 08:40:05] openapi_requestor.py:316 [t:8597525056]: requesting llmapi endpoint: /chat/completions

'Hello! I'm Little Whale, your personal assistant. I'm committed to providing you with accurate and timely information and help, answering your questions, and trying my best to meet your needs. No matter what help you need, I will do my best to provide help and support. '

Baidu Qianfan provides a variety of model interfaces for calling. Among them, the conversation chat interface of the `ERNIE-Bot` model we used above is also known as the Baidu Wenxin model. Here is a brief introduction to the common parameters of the Wenxin model interface:

· messages, that is, the prompt called. The messages configuration of Wenxin is somewhat different from ChatGPT. It does not support the max_token parameter. The maximum number of tokens is controlled by the model itself. The total length of content in messages, the total content of functions and system fields cannot exceed 20480 characters, and cannot exceed 5120 tokens, otherwise the model will forget the previous text in sequence. Wenxin messages have the following requirements: ① One member is a single-round conversation, and multiple members are multi-round conversations; ② The last message is the current conversation, and the previous messages are historical conversations; ③ The number of members must be an odd number, meThe roles in ssage must be user and assistant in order. Note: Here are the character count and tokens limits of the ERNIE-Bot model, and the parameter limits vary from model to model. Please check the parameter description of the corresponding model on the Wenxin Qianfan official website.

· stream, whether to use streaming transmission.

· temperature, temperature coefficient, default is 0.8, the temperature parameter of Wenxin requires the range of (0, 1.0], and cannot be set to 0.

### 2.2 ERNIE SDK

#### 2.2.1 API Application Guide

Here we will use `ERNIE Bot` in `ERNIE SDK` to call Wenxin Yiyan. ERNIE Bot provides developers with a convenient and easy-to-use interface, enabling them to easily call the powerful functions of the Wenxin big model, covering multiple basic functions such as text creation, general dialogue, semantic vectors, and AI mapping. `ERNIE SDK` does not support various large language models like `Qianfan SDK`, but only supports Baidu's own Wenxin big model. Currently, the models supported by ERNIE Bot are:

```
ernie-3.5 Wenxin big model (ernie-3.5)
ernie-liteWenxin Large Model (ernie-lite)
ernie-4.0 Wenxin Large Model (ernie-4.0)
ernie-longtext Wenxin Large Model (ernie-longtext)
ernie-speed Wenxin Large Model (ernie-speed)
ernie-speed-128k Wenxin Large Model (ernie-speed-128k)
ernie-tiny-8k Wenxin Large Model (ernie-tiny-8k)
ernie-char-8k Wenxin Large Model (ernie-char-8k)
ernie-text-embedding Wenxin Baizhong Semantic Model
ernie-vilg-v2 Wenxin Yige Model
```

Before using ERNIE SDK, you need to obtain the authentication (access token) of the AI ​​Studio backend. You need to configure your own key in the code to call the model. Below we use [Ernie Bot](https://ernie-bot-agent.readthedocs.io/zh-cn/Taking [latest/sdk/] as an example, the process of calling the Wenxin model through ERNIE Bot is introduced.

First, you need to register and log in to the [AI Studio Galaxy Community](https://aistudio.baidu.com/index) (new users will be given a free quota of 1 million tokens for 3 months).

![](../figures/C2-2-ernie_bot_1.png)

Click `Access Token` to obtain the account's access token, copy the access token and save it in the form of `EB_ACCESS_TOKEN="..."` to the `.env` file.
![](../figures/C2-2-ernie_bot_2.png)

Then execute the following code to load the key into the environment variable.
```python
from dotenv import load_dotenv, find_dotenv

# Read the local/project environment variables.

# find_dotenv() finds and locates the path of the .env file
# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment
# If you set global environment variables, this line of codeIt has no effect.
_ = load_dotenv(find_dotenv())
```

#### 2.2.2 Call Ernie Bot API

```python
import erniebot
import os

erniebot.api_type = "aistudio"
erniebot.access_token = os.environ.get("EB_ACCESS_TOKEN")

def gen_wenxin_messages(prompt):
'''
Construct Wenxin model request parameters messages

Request parameters:
prompt: Corresponding user prompt words
'''
messages = [{"role": "user", "content": prompt}]
return messages

def get_completion(prompt, model="ernie-3.5", temperature=0.01):
'''
Get Wenxin model call results

Request parameters:
prompt:Corresponding prompt words
model: the model called
temperature: the temperature coefficient of the model output, which controls the randomness of the output. The value range is 0~1.0 and cannot be set to 0. The lower the temperature coefficient, the more consistent the output content.
'''

chat_comp = erniebot.ChatCompletion()
message = gen_wenxin_messages(prompt)

resp = chat_comp.create(messages=message, 
model=model,
temperature = temperature,
system="You are a personal assistant")

return resp["result"]
```

```python
get_completion("Hello, introduce yourself")
```
'Hello! I am a personal assistant. My main task is to help you complete daily tasks, provide information, answer questions, and try my bestImprove your work efficiency and quality of life. No matter what help you need, I will try my best to provide support. '

## 3. Use iFlytek Spark

iFlytek Spark cognitive big model, a Chinese big model launched by iFlytek in May 2023, is also one of the representative products of domestic big models. Similarly, due to the limitations of Chinese context and computing resources, Spark still has differences in user experience with ChatGPT, but as a domestic Chinese big model that is comparable to Wenxin, it is still worth looking forward to and trying. Compared with Baidu, which has significant resource and technical advantages, iFlytek wants to break through the siege and become a leader in domestic big models. It needs to make full use of its relative advantages. At least for now, Spark has not fallen behind.

### 3.1 API Application Guidelines

We can use the [exclusive link provided by Datawhale](https://xinghuo.xfyun.cn/sparkapi?ch=dwKeloHY), through which you can get more free quotas, click `Free Trial`:

![](../figures/C2-2-spark_1.png)

![](../figures/C2-2-spark_2.png)

If you are a user who has not received a free trial package, you can receive a trial of 100,000 tokens. After completing personal identity authentication, you can also receive 2,000,000 t for free.okens trial. After receiving it, click to enter the console and create an application. After the creation is completed, you can see the `APPID`, `APISecret` and `APIKey` we obtained:

![](../figures/C2-2-spark_3.png)

Spark provides two ways to call the model. One is the SDK method, which is easy to use and recommended for beginners; the other is the WebSocket method, which is enterprise-friendly, but difficult for beginners and novice developers. The following will introduce these two calling methods in detail.

### 3.2 Calling through the SDK method (recommended)

First, execute the following code to load the key into the environment variable.

```python
import os

from dotenv import load_dotenv, find_dotenv

# Read the local/project environment variables.

# find_dotenv() finds and locates the path of the .env file
# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment
# If you set global environment variables, this line of code has no effect.
_ = load_dotenv(find_dotenv())```

Then we use the SDK to encapsulate a `get_completion` function for subsequent use.

```python
from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler
from sparkai.core.messages import ChatMessage

def gen_spark_params(model):
'''
Construct Spark model request parameters
'''

spark_url_tpl = "wss://spark-api.xf-yun.com/{}/chat"
model_params_dict = {
# v1.5 version
"v1.5": {
"domain": "general", # Used to configure the large model version
"spark_url": spark_url_tpl.format("v1.1") # Service address of the cloud environment
},
# v2.0 version
"v2.0": {
"domain": "generalv2", # Used to configure the large model version
"spark_url": spark_url_tpl.format("v2.1") # Service address of the cloud environment
},
# v3.0 version
"v3.0": {
"domain": "generalv3", # Used to configure the large model version
"spark_url": spark_url_tpl.format("v3.1") # Service address of the cloud environment
},
# v3.5 version
"v3.5": {
"domain": "generalv3.5", # Used to configure the large model version
"spark_url": spark_url_tpl.format("v3.5") # Service address of the cloud environment
}
}
return model_params_dict[model]

def gen_spark_messages(prompt):
'''
Construct Spark model request parameters messages

Request parameters:
prompt: corresponding user prompt words
'''

messages = [ChatMessage(role="user", content=prompt)]
return messages

def get_completion(prompt, model="v3.5", temperature = 0.1):
'''
Get Spark model call results

Request parameters:
prompt: corresponding prompt words
model: the model called, the default is v3.5, you can also choose other models such as v3.0 as needed
temperature: the temperature coefficient of the model output, controls the randomness of the output, the value range is 0~1.0, and cannot be set to 0. The lower the temperature coefficient, the more consistent the output content.
'''

spark_llm = ChatSparkLLM(
spark_api_url=gen_spark_params(model)["spark_url"],
spark_app_id=os.environ["SPARK_APPID"],
spark_api_key=os.environ["SPARK_API_KEY"],
spark_api_secret=os.environ["SPARK_API_SECRET"],
spark_llm_domain=gen_spark_params(model)["domain"],
temperature=temperature,
streaming=False,
)
messages = gen_spark_messages(prompt)
handler = ChunkPrintHandler()
# callbacks do not work when streaming is set to False
resp = spark_llm.generate([messages], callbacks=[handler])
returnrn resp
```

```python
# Here the normal response content is directly printed out. In the production environment, it is necessary to be compatible with the abnormal response processing
get_completion("你好").generations[0][0].text
```

'Hello! Is there anything I can help you with? '

### 3.3 Calling via WebSocket

The way to connect via WebSocket is relatively complicated to configure. iFlytek provides [calling examples](https://www.xfyun.cn/doc/spark/Web.html#_3-%E8%B0%83%E7%94%A8%E7%A4%BA%E4%BE%8B). Click the corresponding language call example to download. Here we take [Python call example](https://xfyun-doc.xfyun.cn/lc-sp-sparkAPI-1709535448185.zip) as an example. After downloading, we can get a `sparkAPI.py` file, which contains the server encapsulation and client call implementation.

It should be noted that directly running the official example `sparkAPI.py` file will result in an error, and the following modifications need to be made:

(1) Comment out the following line：`import openpyxl` (this package is not used in the code. If it is not installed, ModuleNotFoundError will be prompted);

(2) Modify the `on_close` function (this function receives 3 input parameters). The modified function is as follows:

```python
# Receive the processing of websocket closing
def on_close(ws, close_status_code, close_msg): 
print("### closed ###")
```

Then let's run the modified official sample code. Note: Before running, you also need to assign the API key obtained in the previous section to the input parameters `appid`, `api_secret`, and `api_key` of the `main` function.

Execute `python sparkAPI.py`, and you can get the following output:

![](../figures/C2-2-spark_4.png)

It can be noticed that in addition to the answer of LLM, the output of the official example also contains the print log indicating the end of the answer ("#### close session", "### close ###"). If you only want to keep the original output content, you can optimize it by modifying the source code.

WeBased on the `sparkAPI.py` file, a `get_completion` function is also encapsulated for calling in subsequent chapters.

First, execute the following code to read the key configuration of the `.env` file.

```python
import os
import sparkAPI

from dotenv import load_dotenv, find_dotenv

# Read local/project environment variables.

# find_dotenv() Find and locate the path of the .env file
# load_dotenv() Read the .env file and load the environment variables in it into the current running environment
# If you set the global environment variables, this line of code has no effect.
_ = load_dotenv(find_dotenv())
```

The Spark Big Model API currently has four versions: V1.5, V2.0, V3.0 and V3.5, and the four versions measure tokens independently. The `get_completion` function is encapsulated as follows:

```python
def gen_spark_params(model):
'''
Construct Spark model request parameters
'''

spark_url_tpl = "wss://spark-api.xf-yun.com/{}/chat"
model_params_dict = {
# v1.5 version
"v1.5": {
"domain": "general", # Used to configure the large model version
"spark_url": spark_url_tpl.format("v1.1") # Service address of the cloud environment
},
# v2.0 version
"v2.0": {
"domain": "generalv2", # Used to configure the large model version
"spark_url": spark_url_tpl.format("v2.1") # Service address of the cloud environment
},
# v3.0 version
"v3.0": {
"domain": "generalv3", # Used to configure the large model version
"spark_url": spark_url_tpl.format("v3.1") # Service address of cloud environment
},
# v3.5 version
"v3.5": {
"domain": "generalv3.5", # Used to configure large model version
"spark_url": spark_url_tpl.format("v3.5") # Service address of cloud environment
}
}
return model_params_dict[model]

def get_completion(prompt, model="v3.5", temperature = 0.1):
'''
Get the result of Spark model call

Request parameters:
prompt: corresponding prompt words
model: the model called, the default is v3.5, you can also select other models such as v3.0 as needed
temperature: the temperature coefficient of the model output, which controls the randomness of the output, the value range is 0~1.0, and it cannot be set to 0. The lower the temperature coefficient, the more output contentconsistent.
'''

response = sparkAPI.main(
appid=os.environ["SPARK_APPID"],
api_secret=os.environ["SPARK_API_SECRET"],
api_key=os.environ["SPARK_API_KEY"],
gpt_url=gen_spark_params(model)["spark_url"],
domain=gen_spark_params(model)["domain"],
query=prompt
)
return response
```

```python
get_completion("Hello")
```

Hello! Is there anything I can help you with?

It should be noted that in the official example `sparkAPI.py` file, the `temperature` parameter does not support external input, but is fixed to 0.5. If you do not want to use the default value, you can modify the source code to support external parameter input, which will not be explained here.## 4. Use Zhipu GLM

Zhipu AI is a company transformed from the technological achievements of the Department of Computer Science at Tsinghua University, dedicated to creating a new generation of cognitive intelligence general models. The company has jointly developed a bilingual 100 billion-level ultra-large-scale pre-trained model GLM-130B, and built a high-precision general knowledge graph to form a cognitive engine driven by data and knowledge. Based on this model, ChatGLM (chatglm.cn) was created.

The ChatGLM series of models, including ChatGLM-130B, ChatGLM-6B and ChatGLM2-6B (an upgraded version of ChatGLM-6B) models, support relatively complex natural language instructions and can solve difficult reasoning problems. Among them, the ChatGLM-6B model has been downloaded more than 3 million times from Huggingface (as of June 24, 2023). The model has ranked first in the Hugging Face (HF) global large model download list for 12 consecutive days, and has had a great impact in the open source community at home and abroad.

### 4.1 API Application Guidelines

First, go to [Zhipu AI Open Platform](https://open.bigmodel.cn/overview), click `Start Using` or `Development Workbench` to register:

![](../figures/C2-2-zhipuai_home.png)

Newly registered users can get a free experience package of 1 million tokens with a validity period of one month. After personal real-name authentication, they can also get an additional experience package of 4 million tokens. Zhipu AI provides experience entrances for two different models, GLM-4 and GLM-3-Turbo. You can click the "Experience Now" button to experience it directly.

![Zhipu AI Console](../figures/C2-2-zhipuai_overview.png)

If you need to use API key to build an application, you need to click the "View API key" button on the right to enter our personal API management list. In this interface, you can see the application name and "API key" corresponding to the API we obtained.

![Zhipu AI API Management](../figures/C2-2-zhipuai_api.png)

We can click `Add new API key` and enter the corresponding name to generate a new API key.

### 4.2 Call Zhipu GLM API

Zhipu AI provides SDK and native HTTP to implement model API calls. It is recommended to use SDK for calls to obtain better coding.Program experience.

First we need to configure the key information, set the `API key` obtained earlier to the `ZHIPUAI_API_KEY` parameter in the `.env` file, and then run the following code to load the configuration information.

```python
import os

from dotenv import load_dotenv, find_dotenv

# Read local/project environment variables.

# find_dotenv() Find and locate the path of the .env file

# load_dotenv() Read the .env file and load the environment variables in it into the current running environment

# If you set a global environment variable, this line of code has no effect.
_ = load_dotenv(find_dotenv())
```

Zhipu's call parameter passing is similar to others, and a messages list is also required, including role and prompt. We encapsulate the following `get_completion` function for subsequent use.

```python from zhipuai import ZhipuAI client = ZhipuAI( api_key=os.environ["ZHIPUAI_API_KEY"]
)
def gen_glm_params(prompt):
'''
Construct GLM model request parameters messages

Request parameters:
prompt: corresponding user prompt words
'''
messages = [{"role": "user", "content": prompt}]
return messages

def get_completion(prompt, model="glm-4", temperature=0.95):
'''
Get GLM model call results

Request parameters:
prompt: corresponding prompt words
model: the model called, the default is glm-4, you can also select other models such as glm-3-turbo as needed
temperature: the temperature coefficient of the model output, which controls the randomness of the output, the value range is 0~1.0, and cannot be set to 0. The lower the temperature coefficient, the more consistent the output content.
'''

messages = gen_glm_params(prompt)
response = client.chat.completions.create(
model=model,
messages=messages,
temperature=temperature
)
if len(response.choices) > 0:
return response.choices[0].message.content
return "generate answer error"
```

```python
get_completion("Hello")
```

'Hello! How can I help you? If you have any questions or need advice, please feel free to let me know. '

Here is a brief introduction to the parameters passed to zhipuai:

- `messages (list)`, when calling the conversation model, the current conversation information list is input to the model as a prompt; the parameters are passed in the form of key-value pairs of {"role": "user", "content": "Hello"}; the total length will be automatically truncated if it exceeds the maximum input limit of the model, and it needs to be sorted from old to new by time.- `temperature (float)`, sampling temperature, controls the randomness of the output, must be a positive number, the range is: (0.0, 1.0), cannot be equal to 0, the default value is 0.95. Larger values ​​make the output more random and creative; smaller values ​​make the output more stable or certain

- `top_p (float)`, another method of sampling with temperature, called kernel sampling. The range is: (0.0, 1.0) open interval, cannot be equal to 0 or 1, the default value is 0.7. The model considers the results with top_p probability mass tokens. For example: 0.1 means that the model decoder only considers taking tokens from the candidate set with the top 10% probability

- `request_id (string)`, which is passed by the user and must be unique; it is used to distinguish the unique identifier of each request. If the user does not pass it, the platform will generate it by default

- **It is recommended that you adjust the top_p or temperature parameters according to the application scenario, but do not adjust both parameters at the same time**

**Note: The corresponding source code of this article is in [2. Using LLM API.ipynb](https://github.com/datawhalechina/llm-universe/blob/main/notebook/C2%20%E4%BD%BF%E7%94%A8%20LLM%20API%20%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8/2.%20%E4%BD%BF%E7%94%A8%20LLM%20API.ipynb), if you need to reproduce, you can download and run the source code. **