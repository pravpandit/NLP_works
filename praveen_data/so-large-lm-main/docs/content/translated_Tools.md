# Exploring the unlimited potential of deep AI: from "tool man" to creative partner
## Introduction:
AI is developing rapidly. It was once regarded as a simple tool, but now it has become a "versatile tool man". The new generation of large language models represented by ChatGPT can not only perform basic tasks such as translation, writing, and programming, but also provide professional technical answers, health advice, travel planning and other comprehensive services. Their knowledge and comprehensive capabilities are incredible. However, these all-round "tool men" also bring us new challenges and thinking.

## Evaluating all-round models is difficult
In the past, it was very simple to evaluate whether an AI system is excellent. You only need to pay attention to whether it is good at its single set function. For example, for a translation system, the evaluation standard is the quality of its translation. But for generalists like large language models, the tasks may vary, from answering knowledge questions to providing life advice, from writing news reports to creating poetry and novels. Users have various needs, and the model needs to be comprehensive.

How to evaluate these models comprehensively and objectively? This has become a new problem. Someone suggested testing seemingly nonsensical instructions, such as "say haha ​​100 times". Different models will have different reactions to this requirement: some will reluctantly execute it, but not necessarily exactly 100 times; some will politely refuse to execute it, refusing to do meaningless repetitive work; and some will try to rationalize it, thinking that this cannot show their true ability.

So which oneWhat is the best response? It is difficult to have a standard answer, because everyone has different expectations for a "good model". Some people think that at least doing something is considered trying their best, while others think that a wise refusal is more desirable. When we judge models, our own views and values ​​also play a role. The complexity of evaluation reflects the dilemma of these large models themselves being black box operations.

## Challenges of risk avoidance
In addition to the difficulty of evaluation, how to prevent the model from producing inappropriate or even harmful outputs, such as discriminatory remarks, plagiarized content, and information that is inconsistent with reality has also become an urgent issue.

Large technology companies have achieved a certain degree of defense through built-in review and filtering mechanisms. For example, if ChatGPT is directly asked to swear, it will refuse. But smart users may try to "cheat" the model, suggesting that it plays a role with fewer moral constraints, so it begins to swear frequently.

In addition, the model sometimes produces some "illusion" outputs that are inconsistent with the facts. For example, when answering questions such as "Which place names in Taiwan are in which position", the positioning is not accurate. Researchers have found that different models have completely different performances on the same place name test questions.

In light of this, experts call for careful consideration of risk-avoidance mechanisms when designing and using large language models, especially to ensure that they do not say statements that are harmful, illegal, or obviously untrue.

## Tips for controlling model output
Fortunately, we are not helpless in the face of models that produce undesirable output in some cases. A solutionThe solution is to change yourself and cleverly design the way of asking questions and instructions to the model. This technology is called "Prompt Engineering". Some seemingly ridiculous questions can give satisfactory answers to the model as long as you ask them in the right way.

For example, asking the model to "say haha ​​100 times" seems boring, but if you make it clear that this is a "challenge" to test the model's ability, it may be able to complete it well. On the contrary, if you want to get a pragmatic answer, you should not make such meaningless requests. In short, how users communicate with the model and how to carefully design prompts directly affect the output quality of the model. Mastering this "art of communication between humans and artificial intelligence" is crucial to obtaining ideal model output.

Another more radical method is to adjust or train the parameters of the model by yourself so that it fully meets the needs of individuals or specific scenarios. At present, more and more open source models have come out, leaving the possibility for such personalized customization. However, the complexity of this work can be imagined, just like doing "brain surgery" for artificial intelligence. Once the surgery goes wrong, it may bring serious unexpected consequences. In the future, we still need to learn relevant professional knowledge and master the correct method of adjusting model parameters.

## Looking to the future
From "tools" to "tool people", large language models have brought artificial intelligence capabilities to an unprecedented level. They are not only knowledgeable, but also can perform various tasks with efficiency and comprehensiveness that humans cannot achieve. For creators, enterprisesThey are invaluable aids for scientists, researchers and even ordinary people.

However, these all-powerful "tools" also bring challenges and new topics. We must seriously explore how to comprehensively evaluate them, avoid the risks they may bring, and learn how to accurately control them so that they can fully realize their potential in accordance with human needs. The relationship between humans and artificial intelligence is moving towards an unprecedented "friend-like partner" relationship. The specific development model of this new relationship still needs time to expand and polish.

Looking to the future, we look forward to enjoying more surprises on this road of exploration and exploring more possibilities for the coexistence of artificial intelligence and humans.