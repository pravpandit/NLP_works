# Theoretical foundation of large models {docsify-ignore-all}

## Project introduction

&emsp;&emsp;This project aims to serve as a tutorial for large-scale pre-trained language models, providing open source knowledge from data preparation, model building, training strategies to model evaluation and improvement, as well as the model's security, privacy, environment and legal ethics.

&emsp;&emsp;The project will be based on [Stanford University's Large-Scale Language Model Course](https://stanford-cs324.github.io/winter2022/), combined with the supplements and improvements from open source contributors, as well as the timely update of cutting-edge large model knowledge, to provide readers with a more comprehensive and in-depth theoretical knowledge and practical methods. Through systematic explanations of model building, training, evaluation and improvement, we hope to establish a project with broad reference value.

&emsp;&emsp;Our project team members will be responsible for the content sorting and writing of each chapter, and it is expected to complete the initial version of the content within three months. Subsequently, we will continue to update and optimize the content based on community contributions and feedback to ensure the continuous development of the project and the timeliness of knowledge. We hope that through this project, we can contribute a valuable resource to the research field of large-scale language models and promote the rapid development and widespread application of related technologies.

## Project Audience

1. Artificial intelligence, natural language processing and machine learning1. Researchers and practitioners in the field: This project aims to provide researchers and practitioners with knowledge and techniques for large-scale pre-trained language models, helping them to gain a deeper understanding of the latest developments and research progress in the current field.
2. People in academia and industry who are interested in large-scale language models: The project content covers all aspects of large-scale language models, from data preparation, model building to training and evaluation, as well as security, privacy and environmental impact. This helps broaden the audience's knowledge in this field and deepen their understanding of large-scale language models.
3. People who want to participate in large-scale language model open source projects: This project provides code contributions and theoretical knowledge to lower the threshold for audiences to learn large-scale pre-training.
4. Other industry personnel related to large-scale language models: The project content also involves legal and ethical considerations of large-scale language models, such as copyright law, fair use, fairness, etc., which helps practitioners in related industries better understand the relevant issues of large-scale language models.
![tool_study](content/images/tool_study.jpg)
## Project Content
**Table of Contents**
1. Introduction
- Project Goal: Focus on the current knowledge of large-scale pre-trained language models
- Project Background: The emergence of large language models such as GPT-3, and the development of research in related fields
2. Capabilities of large models
- Model Adaptation Conversion: Migration of large model pre-training to downstream tasks- Model performance evaluation: Evaluation and analysis of the GPT-3 model based on multiple tasks
3. Model architecture
- Model structure: Research and implementation of network structures such as RNN and Transformer
- Details of each layer of Transformer: from position information encoding to attention mechanism
4. New model architecture
- Mixture of Experts (MoE) model
- Retrieval-based model
5. Data for large models
- Data collection: Obtain data required for training and evaluation from public datasets, such as The Pile dataset
- Data preprocessing: data cleaning, word segmentation, etc.
6. Model training
- Objective function: Training method for large models
- Optimization algorithm: Optimization algorithm used for model training
7. Adaptation of large models
- Discuss why adaptation is needed
- Current mainstream adaptation methods (probing/fine-tuning/efficient fine-tuning)
8. Distributed training
- Why distributed training is needed
- Common parallel strategies: data parallelism, model parallelism, pipeline parallelism, hybrid parallelism
9. Harmfulness of large models - Part 1
- Model performance differences: Pre-training or data processing affects the performance of large models
- Social bias: The explicit social bias exhibited by the model
10. LargeHarmfulness of models - Part 2
- Model harmful information: the case of toxic model information
- Model false information: the case of false information of large models
11. Large model law
- Judicial challenges caused by new technologies: the judiciary continues to improve with the emergence of new technologies
- Summary of past judicial cases: a summary of past cases
12. Environmental impact
- Understanding the impact of large language models on the environment
- Estimating emissions generated by model training

## Main contributors

- [Chen Andong](https://github.com/andongBlue):

- [Zhang Fan](https://github.com/zhangfanTJU):

- [Wang Maolin](https://github.com/mlw67)