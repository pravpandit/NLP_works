# 大模型理论基础 {docsify-ignore-all}

## 项目简介

&emsp;&emsp;本项目旨在作为一个大规模预训练语言模型的教程，从数据准备、模型构建、训练策略到模型评估与改进，以及模型在安全、隐私、环境和法律道德方面的方面来提供开源知识。

&emsp;&emsp;项目将以[斯坦福大学大规模语言模型课程](https://stanford-cs324.github.io/winter2022/)为基础，结合来自开源贡献者的补充和完善，以及对前沿大模型知识的及时更新，为读者提供较为全面而深入的理论知识和实践方法。通过对模型构建、训练、评估与改进等方面的系统性讲解，我们希望建立一个具有广泛参考价值的项目。

&emsp;&emsp;我们的项目团队成员将分工负责各个章节的内容梳理和撰写，并预计在三个月内完成初始版本内容。随后，我们将持续根据社区贡献和反馈进行内容的更新和优化，以确保项目的持续发展和知识的时效性。我们期待通过这个项目，为大型语言模型研究领域贡献一份宝贵的资源，推动相关技术的快速发展和广泛应用。

## 项目受众

1. 人工智能、自然语言处理和机器学习领域的研究者和从业者：该项目旨在为研究者和从业者提供大规模预训练语言模型的知识和技术，帮助他们更深入地了解当前领域的最新动态和研究进展。
2. 学术界和产业界对大型语言模型感兴趣的人士：项目内容涵盖了大型语言模型的各个方面，从数据准备、模型构建到训练和评估，以及安全、隐私和环境影响等方面。这有助于拓宽受众在这一领域的知识面，并加深对大型语言模型的理解。
3. 想要参与大规模语言模型开源项目的人士：本项目提供代码贡献和理论知识，降低受众在大规模预训练学习的门槛。
4. 其余大型语言模型相关行业人员：项目内容还涉及大型语言模型的法律和道德考虑，如版权法、合理使用、公平性等方面的分享，这有助于相关行业从业者更好地了解大型语言模型的相关问题。
![tool_study](content/images/tool_study.jpg)
## 项目内容
**目录**
1. 引言
    - 项目目标：目前对大规模预训练语言模型的相关知识的重点讲解
    - 项目背景：GPT-3等大型语言模型的出现，以及相关领域研究的发展
2. 大模型的能力
    - 模型适应转换：大模型预训练往下游任务迁移
    - 模型性能评估：基于多个任务对GPT-3模型进行评估和分析
3. 模型架构
    - 模型结构：研究和实现RNN, Transformer等网络结构
    - Transformer各层细节：从位置信息编码到注意力机制
4. 新的模型架构
    - 混合专家模型（MoE）
    - 基于检索的模型
5. 大模型的数据
    - 数据收集：从公开数据集中获取训练和评估所需数据，如The Pile数据集
    - 数据预处理：数据清洗、分词等
6. 模型训练
    - 目标函数：大模型的训练方法
    - 优化算法：模型训练所使用的优化算法
7. 大模型之Adaptation
    - 讨论为什么需要Adaptation
    - 当前主流的Adaptation方法（Probing/微调/高效微调） 
8. 分布式训练
    - 为什么需要分布式训练
    - 常见的并行策略：数据并行、模型并行、流水线并行、混合并行
9. 大模型的有害性-上
    - 模型性能差异：预训练或数据处理影响大模型性能
    - 社会偏见：模型表现出的显性的社会偏见
10. 大模型的有害性-下
    - 模型有害信息：模型有毒信息的情况
    - 模型虚假信息：大模型的虚假信息情况
11. 大模型法律
    - 新技术引发的司法挑战：司法随着新技术的出现而不断完善
    - 过去司法案例汇总：过去案例的汇总
12. 环境影响
    - 了解大语言模型对环境的影响
    - 估算模型训练产生的排放量

## 主要贡献者

- [陈安东](https://github.com/andongBlue)：
- [张帆](https://github.com/zhangfanTJU)：
- [王茂霖](https://github.com/mlw67)
