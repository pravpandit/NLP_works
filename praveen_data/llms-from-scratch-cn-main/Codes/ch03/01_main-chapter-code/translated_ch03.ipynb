{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27d5425deb10849c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Chapter 3: Writing an Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ce6dff684c41",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The packages used in this notebook are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58f33e8-5dc9-4dd5-ab84-5a011fa11d92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:12.683615Z",
     "start_time": "2024-03-01T07:05:09.675943900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0475ea32ec926b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.1 Problems with long sequence modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f224b96fb1a27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- There is no code in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7a179e9cf96c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 Capturing Data Dependencies Using Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fde64c-6034-421d-81d9-8244932086ea",
   "metadata": {},
   "source": [
    "- There is no code in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fc49dd41e1c19",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 Using self-attention to focus on different parts of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf1532f595d316",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3.1 A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cdd1ec5345c45e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- This section introduces an extremely simplified version of the self-attention mechanism, which does not contain any trainable weights. This is just for illustration purposes and is not the actual attention mechanism used in transformer models. The following section 3.3.2 will expand on this simple attention mechanism and introduce the real self-attention mechanism.\n",
    "- Suppose we have an input sequence from $x^{(1)}$ to $x^{(T)}$.\n",
    "- The input is a piece of text (such as the sentence \"Your journey starts with one step\"), which has been converted into the token embedding form described in Chapter 2.\n",
    "- For example, $x^{(1)}$ is a d-dimensional vector representing the word \"Your\", and so on.\n",
    "- **Goal:** For each element $x^{(i)}$ in the input sequence from $x^{(1)}$ to $x^{(T)}$, where $z$ and $x$ have the same dimension, calculate the context vector $z^{(i)}$.\n",
    "- The context vector $z^{(i)}$ is a weighted average of the inputs $x^{(1)}$ to $x^{(T)}$.\n",
    "- The context vector is the contextual information for a specific input.\n",
    "- Instead of using $x^{(i)}$ as a placeholder for an arbitrary input token, we consider the secondInput, $x^{(2)}$.\n",
    "- To make this concrete, instead of using a placeholder $z^{(i)}$, we consider the context vector for the second output, $z^{(2)}$.\n",
    "- The second context vector $z^{(2)}$ is a weighted average of all inputs $x^{(1)}$ to $x^{(T)}$, with the weights determined based on the second input element $x^{(2)}$. These attention weights determine how much each input element contributes to the final weighted average when computing $z^{(2)}$.\n",
    "- In short, $z^{(2)}$ can be thought of as a variant of $x^{(2)}$ that not only contains the information of $x^{(2)}$, but also incorporates the information of all other input elements that are relevant to the current task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89766c8b4d562a1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- By convention, unnormalized attention weights are called **\"attention scores\"**, while normalized attention scores (they sum to 1) are called **\"attention weights\"**.\n",
    "\n",
    "- The calculation of attention weights and context vectors is summarized in the following diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28531e83-85bd-43a4-8928-57bb0372d9c7",
   "metadata": {},
   "source": [
    "<img src=\"figures/attention.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfcc8a4890c11f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- The code below shows the above diagram step by step.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Step 1:** Calculate the unnormalized attention score $\\omega$.\n",
    "- Assuming we use the second input token as the query, i.e., $q^{(2)} = x^{(2)}$, we calculate the unnormalized attention score by dot product:\n",
    "- $\\omega_{21} = x^{(1)} \\cdot q^{(2)\\top}$\n",
    "- $\\omega_{22} = x^{(2)} \\cdot q^{(2)\\top}$\n",
    "- $\\omega_{23} = x^{(3)} \\cdot q^{(2)\\top}$\n",
    "- ...\n",
    "- $\\omega_{2T} = x^{(T)} \\cdot q^{(2)\\top}$\n",
    "- Here, $\\omega$ is the Greek letter \"omega\" and is used to represent the unnormalized attention score.\n",
    "- The subscript \"21\" in $\\omega_{21}$ means that the second element of the input sequence is used as the query to be compared with the first element of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e29440f-9b77-4966-83aa-d1ff2e653b00",
   "metadata": {},
   "source": [
    "<img src=\"figures/dot-product.png\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c28811e45031fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Suppose we have the following input sentence that has been converted into a 3-dimensional vector as described in Chapter 3 (for illustration purposes, a very small embedding dimension is used here so that it fits on the page without line wrapping):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b9556a-aaf8-4ab4-a5b4-973372b0b2c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:23.298987200Z",
     "start_time": "2024-03-01T07:05:23.284983400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98cfa901b290ae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- We take the second element $x^{(2)}$ in the input sequence as an example to calculate the context vector $z^{(2)}$; in the following section, we will generalize this method to calculate all context vectors.\n",
    "- The first step is to calculate the unnormalized attention score, which is achieved by calculating the dot product between the query $x^{(2)}$ and all other input tokens:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1540227deed6d1da",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb5b2f8-dd2c-4a6d-94ef-a0e9ad163951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:25.477473800Z",
     "start_time": "2024-03-01T07:05:25.459470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Take the second element from the input sequence as the query vector\n",
    "query = inputs[1]\n",
    "\n",
    "# Create an empty tensor to store the attention scores, with the same shape as the batch size of the input sequence\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "# Iterate over each element of the input sequence\n",
    "for i, x_i in enumerate(inputs):\n",
    "# Calculate the dot product of the current element and the query vector as the attention score\n",
    "# No need to transpose here, because the input vector is assumed to be one-dimensional\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "# Print attention scores\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb77f59671f0aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Note: The dot product is actually a shorthand for multiplying corresponding elements of two vectors and then adding these products together:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef00c65e10fddb4",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9842f39b-1654-410e-88bf-d1b899bf0241",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:27.317884600Z",
     "start_time": "2024-03-01T07:05:27.292879100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the result variable to 0\n",
    "res = 0.\n",
    "\n",
    "# Iterate over each element in the first element of the input sequence\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "# Multiply the current element by the corresponding element of the query vector and add the result to res\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "# Print the manually calculated dot product result\n",
    "print(res)\n",
    "\n",
    "# Use PyTorch's torch.dot function to calculate the dot product and print the result\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d1ba5c3db582b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- **Step 2:** Normalize the unnormalized attention scores (called \"omegas\", represented by the Greek letter $\\omega$) so that their sum is equal to 1.\n",
    "- Here is a simple way to normalize these unnormalized attention scores to ensure that they sum to 1 (this is a common practice that helps to understand and is critical to the stability of the training process):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd31c47c7645e04",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ccc99c-33ce-4f11-b7f2-353cf1cbdaba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:29.228312Z",
     "start_time": "2024-03-01T07:05:29.180302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores using the sum of the attention scores\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "# Print normalized attention weights\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "# Verify that the sum of the normalized attention weights is 1\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085c4759c607872",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- However, in practice, it is more common and recommended to use the softmax function for normalization, as it is better at handling extreme values ​​and has more desirable gradient properties during training.\n",
    "- The following is a simple implementation of the softmax function, which is used to scale and normalize the vector elements so that their sum is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b2e58d-a6ed-49f0-a1cd-2463e8d53a20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:30.924690900Z",
     "start_time": "2024-03-01T07:05:30.877680700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Define a simple softmax function implementation\n",
    "def softmax_naive(x):\n",
    "# Applies an exponential function to each element of the input tensor x\n",
    "    exp_x = torch.exp(x)\n",
    "# Calculate the sum of exp_x in the specified dimension (here is the first dimension, dim=0)\n",
    "    sum_exp_x = exp_x.sum(dim=0)\n",
    "# Divide each element of exp_x by their sum to get the softmax result\n",
    "    return exp_x / sum_exp_x\n",
    "\n",
    "# Normalize the attention scores using the naive softmax function\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "# Print normalized attention weights\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "# Verify that the sum of the normalized attention weights is 1\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bbc97e55cdbd1c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- The simple implementation above may cause numerical instability due to input values ​​that are too large or too small, which is mainly due to numerical overflow and underflow.\n",
    "- Therefore, in practical applications, it is recommended to use the `softmax` function provided by PyTorch, which is highly optimized and has better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d99cac4-45ea-46b3-b3c1-e000ad16e158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:32.360025300Z",
     "start_time": "2024-03-01T07:05:32.332020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Use PyTorch's softmax function to normalize the attention scores\n",
    "# dim=0 means to perform softmax calculation on the first dimension (usually the feature dimension)\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "# Print normalized attention weights\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "# Verify that the sum of the normalized attention weights is 1\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26834a0afec960d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- **Step 3**: Compute the context vector $z^{(2)}$ by multiplying the embedded input token $x^{(i)}$ with the attention weight and then adding the resulting vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fcb96f0-14e5-4973-a50e-79ea7c6af99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:33.773341800Z",
     "start_time": "2024-03-01T07:05:33.765339800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Select the second element in the input sequence as the query vector\n",
    "query = inputs[1]\n",
    "\n",
    "# Initialize the context vector, which has the same shape as the query vector and an initial value of 0\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "# Iterate over each element in the input sequence\n",
    "for i, x_i in enumerate(inputs):\n",
    "# Accumulate the product of each input element and its corresponding attention weight\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "# Print the calculated context vector\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7a0e40c6d8d08",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3.2 Calculate the attention weights of all input tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfcbe08825a085b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Generalize to all input sequence tokens:\n",
    "\n",
    "- In the above, we calculated the attention weights and context vectors for input 2 (as shown in the highlighted row in the diagram below).\n",
    "- Next, we will generalize this calculation process to calculate the attention weights and context vectors for all input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0fb55-394f-42f4-ba07-d01ae5c98ab4",
   "metadata": {},
   "source": [
    "<img src=\"figures/attention-matrix.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dcd2e858df4af2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Apply the previous **first step** to calculate all pairs of elements to get the unnormalized attention score matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04004be8-07a1-468b-ab33-32e16a551b45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:36.389926600Z",
     "start_time": "2024-03-01T07:05:36.370922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 6x6 zero tensor to store the attention scores\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "# Iterate over each element in the input sequence\n",
    "for i, x_i in enumerate(inputs):\n",
    "# For the current input element x_i, traverse the entire input sequence again\n",
    "    for j, x_j in enumerate(inputs):\n",
    "# Calculate the dot product of x_i and x_j as the attention score and store it in the corresponding position of the attn_scores matrix\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "# Print the complete attention score matrix\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a64a8236579473",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- We can implement the above calculation more efficiently through matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cea69d0-9a47-45da-8d5a-47ceef2df673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:39.111548400Z",
     "start_time": "2024-03-01T07:05:39.099546300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Use matrix multiplication to calculate the dot product matrix of the input sequence\n",
    "# inputs @ inputs.T is equivalent to multiplying inputs by the transpose of inputs\n",
    "attn_scores = inputs @ inputs.T\n",
    "\n",
    "# Print attention score matrix\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f7ce6c43bf3af",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Similar to the previous **step 2**, we normalize each row so that the sum of the values ​​in each row is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4ef062-de81-47ee-8415-bfe1708c81b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:40.860940800Z",
     "start_time": "2024-03-01T07:05:40.843935200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1207b1f9b38e9c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Quickly verify that the values ​​in each row do sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "112b492c-fb6f-4e6d-8df5-518ae83363d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:42.340270Z",
     "start_time": "2024-03-01T07:05:42.308262700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Define the attention weight list for the second row\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "# Print the sum of the second line\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "\n",
    "# Use PyTorch's sum function to calculate the sum of all rows along the specified dimension (here dimension 1, i.e. rows)\n",
    "# attn_weights.sum(dim=1) returns a 1D tensor containing the sum of each row\n",
    "print(\"All row sums:\", attn_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e3585324e487a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Apply the previous **third step** to calculate all context vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba8eafcf-f7f7-4989-b8dc-61b50c4f81dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:43.990639200Z",
     "start_time": "2024-03-01T07:05:43.971634200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b13b5bb3d62d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- As a sanity check, the previously calculated context vector $z^{(2)} = [0.4419, 0.6515, 0.5683]$ can be found in the second line above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2570eb7d-aee1-457a-a61e-7544478219fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:05:46.564249100Z",
     "start_time": "2024-03-01T07:05:46.548244600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303b6fb-9f7e-42bb-9fdb-2adabf0a6525",
   "metadata": {},
   "source": [
    "## 3.4 Implementing self-attention using trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90a77e-d746-4704-9354-1ddad86e6298",
   "metadata": {},
   "source": [
    "### 3.4.1 Calculate attention weights step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e95a46-1f67-4b71-9e84-8e2db84ab036",
   "metadata": {},
   "source": [
    "- In this section, we are implementing the self-attention mechanism, a technique used in the original transformer architecture, the GPT model, and most other popular large language models (LLMs).\n",
    "- This self-attention mechanism is also known as \"scaled dot-product attention\".\n",
    "- The general idea is similar to before:\n",
    "- We want to compute the context vector as a weighted sum of the input vectors for a particular input element.\n",
    "- To do this, we need attention weights.\n",
    "- You will see that there are only subtle differences compared to the basic attention mechanism introduced before:\n",
    "- The most notable difference is the introduction of weight matrices that are updated during model training.\n",
    "- These trainable weight matrices are crucial so that the model (especially the attention module within the model) can learn to produce \"good\" context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d996671-87aa-45c9-b2e0-07a7bcc9060a",
   "metadata": {},
   "source": [
    "- As we gradually implement the self-attention mechanism, we will first introduce three training weight matrices $W_q$, $W_k$, and $W_v$.\n",
    "- These three matrices are used to project the embedded input token $x^{(i)}$ to the query, key, and value vectors via matrix multiplication:\n",
    "\n",
    "- Query vector: $q^{(i)} = W_q \\cdot x^{(i)}$\n",
    "- Key vector: $k^{(i)} = W_k \\cdot x^{(i)}$\n",
    "- Value vector: $v^{(i)} = W_v \\cdot x^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b29bc6-4bde-4924-9aff-0af1421803f5",
   "metadata": {},
   "source": [
    "<img src=\"figures/weight-selfattn-1.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f334313-5fd0-477b-8728-04080a427049",
   "metadata": {},
   "source": [
    "- The embedding dimension of the input $x$ and the dimension of the query vector $q$ can be the same or different, depending on the design and implementation of the model.\n",
    "- In the GPT model, the dimensions of the input and output are usually the same, but to better illustrate the calculation process, we choose different input and output dimensions here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8250fdc6-6cd6-4c5b-b9c0-8c643aadb7db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:06:05.548505500Z",
     "start_time": "2024-03-01T07:06:05.539503700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the second element in the input sequence as a specific input vector\n",
    "x_2 = inputs[1]\n",
    "\n",
    "# Get the embedding dimension of the input tensor. Here we assume that the dimension of the input vector is 3.\n",
    "d_in = inputs.shape[1]\n",
    "\n",
    "# Set the dimension of the output embedding. Here we assume that the dimension of the output vector is 2\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528cfb3-e226-47dd-b363-cc2caaeba4bf",
   "metadata": {},
   "source": [
    "- Below, we initialize the three weight matrices; note that we set `requires_grad=False` to reduce clutter in the output, this is for illustration purposes. However, if we were to use these weight matrices in model training, we would set `requires_grad` to `True` so that these matrices are updated during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd7259a-f26c-4cea-b8fc-282b5cae1e00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:06:09.001276300Z",
     "start_time": "2024-03-01T07:06:08.956267200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the random seed to ensure reproducibility of results\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create a query weight matrix with shape (d_in, d_out) and set requires_grad=False to indicate that these weights will not be updated during training\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Create a key weight matrix with the same shape as W_query and also set requires_grad=False\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Create a value weight matrix with the same shape as W_query and also set requires_grad=False\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd0b50-7701-4adb-821c-e5433622d9c4",
   "metadata": {},
   "source": [
    "- Next, we compute the query, key, and value vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73cedd62-01e1-4196-a575-baecc6095601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:06:12.380032800Z",
     "start_time": "2024-03-01T07:06:12.363027500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# Project the second input element x_2 into the query space using the query weight matrix W_query\n",
    "query_2 = x_2 @ W_query  # 使用 @ 运算符进行矩阵乘法\n",
    "\n",
    "# Project the second input element x_2 into the key space using the key weight matrix W_key\n",
    "key_2 = x_2 @ W_key\n",
    "\n",
    "# Project the second input element x_2 into the value space using the value weight matrix W_value\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "# Print the calculated query vector\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be308b3-aca3-421b-b182-19c3a03b71c7",
   "metadata": {},
   "source": [
    "- As can be seen from the results below, we have successfully projected the 6 input tokens from a 3D space to a 2D embedding space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c1c3949-fc08-4d19-a41e-1c235b4e631b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:06:15.416710300Z",
     "start_time": "2024-03-01T07:06:15.403706900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Project the input sequence inputs into the key space using the key weight matrix W_key\n",
    "keys = inputs @ W_key\n",
    "\n",
    "# Use the value weight matrix W_value to project the input sequence inputs into the value space\n",
    "values = inputs @ W_value\n",
    "\n",
    "# Print the shape of the key vector\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "\n",
    "# Print the shape of the value vector\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5dfd6-ade8-4e7b-b0c1-bed40aa24481",
   "metadata": {},
   "source": [
    "- In the next step, step 2, we compute the unnormalized attention scores by computing the dot product between the query vector and each key vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0a2b7-5c50-4ede-90cf-7ad74412b3aa",
   "metadata": {},
   "source": [
    "<img src=\"figures/weight-selfattn-2.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64cbc253-a182-4490-a765-246979ea0a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:06:19.620649700Z",
     "start_time": "2024-03-01T07:06:19.603645500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# Extract the second key vector from the key tensor, corresponding to the second element in the input sequence\n",
    "keys_2 = keys[1]  # Python 中的索引是从 0 开始的\n",
    "\n",
    "# Calculate the dot product between the query vector query_2 and the key vector keys_2 to get the attention score\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "\n",
    "# Print the calculated attention score\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d15c0-c24e-4e6f-a160-6349b418f935",
   "metadata": {},
   "source": [
    "- Since we have 6 inputs, for a given query vector we get 6 attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b14e44b5-d170-40f9-8847-8990804af26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:06:22.436278900Z",
     "start_time": "2024-03-01T07:06:22.426277700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# Perform matrix multiplication using the query vector query_2 and the transpose of all key vectors keys (keys.T) to get all attention scores\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "\n",
    "# Print all attention scores\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622cf39-155f-4eb5-a0c0-82a03ce9b999",
   "metadata": {},
   "source": [
    "<img src=\"figures/weight-selfattn-3.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1609edb-f089-461a-8de2-c20c1bb29836",
   "metadata": {},
   "source": [
    "- Next, in **step 3**, we compute the attention weights (normalized attention scores, which sum to 1) using the softmax function mentioned earlier.\n",
    "- Unlike before, we now scale the attention scores by dividing by the square root of the embedding dimension, $\\sqrt{d_k}$ (i.e. `d_k**0.5`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "146f5587-c845-4e30-9894-c7ed3a248153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:06:26.976293500Z",
     "start_time": "2024-03-01T07:06:26.972292100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# Get the dimension of the key vector, that is, the dimension size of each key vector\n",
    "d_k = keys.shape[1]\n",
    "\n",
    "# Use the softmax function to normalize the attention scores\n",
    "# attn_scores_2 / d_k**0.5 is to scale the dot product score to prevent the softmax gradient from being too small due to too large a dot product value\n",
    "# dim=-1 means to perform softmax calculation along the last dimension (i.e. the dimension of attention score)\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "\n",
    "# Print normalized attention weights\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f61a28-b103-434a-aee1-ae7cbd821126",
   "metadata": {},
   "source": [
    "<img src=\"figures/weight-selfattn-4.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890e3f9-db86-4ab8-9f3b-53113504a61f",
   "metadata": {},
   "source": [
    "- In step 4, we now compute the context vector for the input query vector 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e138f033-fa7e-4e3a-8764-b53a96b26397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:06:33.820850800Z",
     "start_time": "2024-03-01T07:06:33.810848200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# Use the normalized attention weights attn_weights_2 and the value vector values ​​to perform matrix multiplication to get the context vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "\n",
    "# Print the calculated context vector\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b2907-e448-473e-b46c-77735a7281d8",
   "metadata": {},
   "source": [
    "### 3.4.2 Implementing a compact SelfAttention class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04313410-3155-4d90-a7a3-2f3386e73677",
   "metadata": {},
   "source": [
    "Putting it all together, we can implement the self-attention mechanism as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51590326-cdbe-4e62-93b1-17df71c11ee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:06:40.953444700Z",
     "start_time": "2024-03-01T07:06:40.916436100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the self-attention module\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "# Call the parent class constructor\n",
    "        super().__init__()\n",
    "# Set the output dimension\n",
    "        self.d_out = d_out\n",
    "# Initialize the weight matrices for queries, keys, and values. These matrices are trainable\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "# Project input x into query, key, and value spaces using weight matrix\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "# Calculate attention scores (unnormalized)\n",
    "        attn_scores = queries @ keys.T  # omega\n",
    "        \n",
    "# Normalize the attention scores using the softmax function and scaling factor\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "# Calculate the context vector using the normalized attention weights and value vector\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "# Set the random seed to ensure reproducibility of results\n",
    "torch.manual_seed(123)\n",
    "# Create a SelfAttention_v1 instance\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "# Use input data inputs to perform forward propagation and print the results\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e0c16-d911-4ec8-b0bc-45ceec75c081",
   "metadata": {},
   "source": [
    "- We can simplify the above implementation using PyTorch's linear layer `nn.Linear`, which is equivalent to matrix multiplication if we turn off the bias unit.\n",
    "- Another important advantage of using `nn.Linear` instead of the `nn.Parameter(torch.rand(...))` method we created manually is that `nn.Linear` comes with a preferred weight initialization scheme, which helps to achieve more stable model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73f411e3-e231-464a-89fe-0a9035e5f839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T07:08:05.227348900Z",
     "start_time": "2024-03-01T07:08:05.210344600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the second version of the self-attention module\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "# Call the parent class constructor\n",
    "        super().__init__()\n",
    "# Set the output dimension\n",
    "        self.d_out = d_out\n",
    "# Initialize the linear layers for query, key, and value, optionally including a bias\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "# Use a linear layer to project the input x into query, key, and value spaces\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "# Calculate attention scores (unnormalized)\n",
    "        attn_scores = queries @ keys.T\n",
    "        \n",
    "# Normalize the attention scores using the softmax function and scaling factor\n",
    "# Note that dim=1 here means normalization along the dimension of the key vector\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "\n",
    "# Calculate the context vector using the normalized attention weights and value vector\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "# Set the random seed to ensure reproducibility of results\n",
    "torch.manual_seed(789)\n",
    "# Create a SelfAttention_v2 instance\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "# Use input data inputs to perform forward propagation and print the results\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915cd8a5-a895-42c9-8b8e-06b5ae19ffce",
   "metadata": {},
   "source": [
    "- Note that `SelfAttention_v1` and `SelfAttention_v2` produce different outputs because they use different initial weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5025b37-0f2c-4a67-a7cb-1286af7026ab",
   "metadata": {},
   "source": [
    "## 3.5 Attention Mechanism to Mask Context Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f405de-cd86-4e72-8f3c-9ea0354946ba",
   "metadata": {},
   "source": [
    "### 3.5.1 Using Causal Attention Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f28d0-8218-48e4-8b9c-bdc5ce489218",
   "metadata": {},
   "source": [
    "- In this section, we convert the previous self-attention mechanism into a causal self-attention mechanism.\n",
    "\n",
    "- The core goal of the causal self-attention mechanism is to ensure that the model's prediction of a position in the sequence depends only on the known output of the previous position (that is, the previous context), and not on the future position (that is, the following context). In other words, ensure that the prediction of each word should only depend on the previous word.\n",
    "\n",
    "- To achieve this, for each given word, we mask out future words (that is, words after the current word in the input text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e91bb5-5aae-4f05-8a95-973b3f988a35",
   "metadata": {},
   "source": [
    "<img src=\"figures/masked.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfaec7a-68f2-4157-a4b5-2aeceed199d9",
   "metadata": {},
   "source": [
    "- To illustrate and implement the causal self-attention mechanism, let’s operate with the attention scores and weights from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1933940d-0fa5-4b17-a3ce-388e5314a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Use the query and key weight matrices of SelfAttention_V2 in the previous section\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "# The attention weights here are the same as in the previous section\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89020a96-b34d-41f8-9349-98c3e23fd5d6",
   "metadata": {},
   "source": [
    "The simplest way to mask future attention weights is to create a mask via PyTorch’s tril function, with elements below the main diagonal (including the diagonal itself) set to 1 and elements above the main diagonal set to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43f3d2e3-185b-4184-9f98-edde5e6df746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# The shape of the mask we create should be consistent with the shape of the attention weight matrix, in one-to-one correspondence\n",
    "block_size = attn_scores.shape[0]\n",
    "# The tril method creates a lower triangular matrix\n",
    "mask_simple = torch.tril(torch.ones(block_size, block_size))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce2b08-3583-44da-b3fc-cabdd38761f6",
   "metadata": {},
   "source": [
    "- We can then multiply the attention weights by this mask to zero out the attention scores above the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f531e2e-f4d2-4fea-a87f-4c132e48b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb35787-cf12-4024-b66d-e7215e175500",
   "metadata": {},
   "source": [
    "- However, if we apply masking after softmax, as we did above, it will destroy the probability distribution created by softmax. Softmax will ensure that all output values ​​sum to 1, but since we set some of the output values ​​to 0, this will cause the output values ​​to sum to different values.\n",
    "\n",
    "- Therefore, masking after softmax will require re-normalizing the outputs to sum to 1 again. However, this complicates the process and may lead to unexpected effects.\n",
    "\n",
    "- To ensure that the output values ​​sum to 1, we can normalize the weight matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d392083-fd81-4f70-9bdf-8db985e673d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# dim = 1 means sum by row\n",
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e7cf4-dc0e-4cec-948e-c7a3c4eb6877",
   "metadata": {},
   "source": [
    "- Although we have now technically completed the causal attention mechanism, there are more efficient ways to achieve the same effect as above.\n",
    "\n",
    "- For example, instead of zeroing the attention weights above the diagonal and renormalizing the result, we can mask the above-diagonal portion with negative infinity before the unnormalized attention scores enter the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2be2f43-9cf0-44f6-8d8b-68ef2fb3cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# That is, by changing the mask from 0 to -inf, the masking operation can be moved before the softmax\n",
    "mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5f803-d735-4543-b9da-00ac10fb9c50",
   "metadata": {},
   "source": [
    "- As we can see, if we then pass the attention matrix through softmax, we can return the sum of each row to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1cd6d7f-16f2-43c1-915e-0824f1a4bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636fc5f-6bc6-461e-ac6a-99ec8e3c0912",
   "metadata": {},
   "source": [
    "### 3.5.2 Masking for additional attention weights via dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dc7ee-6539-4fab-804a-8f31a890c85a",
   "metadata": {},
   "source": [
    "- In addition, we can also apply dropout during training to reduce overfitting.\n",
    "\n",
    "- Dropout can be applied in multiple places such as the following examples:\n",
    "- After calculating the attention weights;\n",
    "- After multiplying the attention weights with the value vector.\n",
    "\n",
    "- Here, we will apply the dropout mask after calculating the attention weights because this is more common.\n",
    "\n",
    "- In addition, in this particular example, we used a dropout rate of 50%, which means randomly masking half of the attention weights. (When we train the GPT model later, we will use a lower dropout rate, such as 0.1 or 0.2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee799cf6-6175-45f2-827e-c174afedb722",
   "metadata": {},
   "source": [
    "<img src=\"figures/dropout.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a575458-a6da-4e54-8688-83e155f2de06",
   "metadata": {},
   "source": [
    "- Note that if we apply a dropout rate of 0.5, then the unmasked values ​​will be scaled accordingly by a factor of 1/0.5 = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0de578db-8289-41d6-b377-ef645751e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Set a random number seed\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # 设置 50% 的 Dropout 比例\n",
    "example = torch.ones(6, 6) # 创建一个全 1 矩阵作为示例\n",
    "\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b16c5edb-942b-458c-8e95-25e4e355381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3843, 0.3293, 0.3303, 0.3100, 0.3442, 0.3019],\n",
      "        [0.0000, 0.3318, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3325, 0.0000, 0.3328, 0.0000],\n",
      "        [0.3738, 0.3334, 0.0000, 0.0000, 0.0000, 0.3128],\n",
      "        [0.3661, 0.0000, 0.0000, 0.0000, 0.0000, 0.3169],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "# Dropout the attention weights\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc14639-5f0f-4840-aa9d-8eb36ea90fb7",
   "metadata": {},
   "source": [
    "## 3.5.3 Implementing a causal self-attention class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c41d29-1933-43dc-ada6-2dbb56287204",
   "metadata": {},
   "source": [
    "- Now, we are ready to implement a causal self-attention class with dropout.\n",
    "\n",
    "- We also need to implement code to handle a batch of samples consisting of multiple inputs, so that our CausalAttention class supports the batch output produced by the dataloader we implemented in Chapter 2.\n",
    "\n",
    "- For simplicity, to simulate such a batch input, we copy the input text example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 2个输入，每个输入有 6个 token，每个 token 的维度为 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0844,  0.0414],\n",
      "         [-0.2264, -0.0039],\n",
      "         [-0.4163, -0.0564],\n",
      "         [-0.5014, -0.1011],\n",
      "         [-0.7754, -0.1867],\n",
      "         [-1.1632, -0.3303]],\n",
      "\n",
      "        [[-0.0844,  0.0414],\n",
      "         [-0.2264, -0.0039],\n",
      "         [-0.4163, -0.0564],\n",
      "         [-0.5014, -0.1011],\n",
      "         [-0.7754, -0.1867],\n",
      "         [-1.1632, -0.3303]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Define a causal self-attention layer with dropout\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, block_size, dropout, qkv_bias=False):\n",
    "        '''\n",
    "        构造函数，输入参数如下：\n",
    "        d_in: 输入的维度\n",
    "        d_out: 输出的维度\n",
    "        block_size: 注意力权重矩阵的大小\n",
    "        dropout: dropout 比例\n",
    "        qkv_bias: 是否对 query、key 和 value 加偏置\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "# According to the previous text, each weight matrix is ​​a linear layer of d_in x d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "# A dropout layer\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "# A mask matrix with 1 in the lower triangle and 0 in the rest\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        前向传播函数，输入参数为 x，维度为 b x num_tokens x d_in，输出维度为 b x num_tokens x d_out\n",
    "        '''\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "# transpose is to implement matrix multiplication\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "# As mentioned above, change the mask from 0 to -inf and then perform the masking operation\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "# After softmax\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "# Perform dropout\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "# Get the final result\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "# Experiment\n",
    "torch.manual_seed(123)\n",
    "\n",
    "block_size = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, block_size, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4333d12-17e4-4bb5-9d83-54b3a32618cd",
   "metadata": {},
   "source": [
    "- Note that dropout is only used during training, not during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bef90f-cfd4-4289-b0e8-6a00dc9be44c",
   "metadata": {},
   "source": [
    "## 3.6 Extending single-head attention to multiple heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11697757-9198-4a1c-9cee-f450d8bbd3b9",
   "metadata": {},
   "source": [
    "### 3.6.1 Directly stack multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70766faf-cd53-41d9-8a17-f1b229756a5a",
   "metadata": {},
   "source": [
    "- The following figure summarizes the self-attention mentioned above (for simplicity, the causal attention mask and dropout are not shown) \n",
    "\n",
    "- Also known as single-head attention:\n",
    "\n",
    "<img src=\"figures/single-head.png\" width=\"600px\">\n",
    "\n",
    "- We can simply stack multiple single-head attention layers together to achieve a multi-head attention layer:\n",
    "\n",
    "<img src=\"figures/multi-head.png\" width=\"600px\">\n",
    "\n",
    "- The main idea of ​​the multi-head attention mechanism is to run the attention mechanism multiple times (in parallel) using different, learned weight matrices. This allows the model to jointly attend to information in different representation subspaces at different locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9a66e11-7105-4bb4-be84-041f1a1f3bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0844,  0.0414,  0.0766,  0.0171],\n",
      "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
      "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
      "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
      "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
      "         [-1.1632, -0.3303,  1.1224,  0.8460]],\n",
      "\n",
      "        [[-0.0844,  0.0414,  0.0766,  0.0171],\n",
      "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
      "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
      "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
      "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
      "         [-1.1632, -0.3303,  1.1224,  0.8460]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Define a multi-head attention layer\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "# Combine num_heads single-head attention layers together to achieve multi-head\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, block_size, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "# Concatenate the outputs of multiple heads together during forward calculation\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "# Experiment\n",
    "torch.manual_seed(123)\n",
    "\n",
    "block_size = batch.shape[1] # token 数量\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d3d2b-2578-40ba-b791-ea2d49328e48",
   "metadata": {},
   "source": [
    "- In the above implementation, the embedding dimension is 4 because we set d_out=2 as the embedding dimension for key, query, and value. Since we have 2 attention heads, the output embedding dimension is 2*2=4.\n",
    "\n",
    "- If we want the output dimension to be 2, like the earlier single-head attention, we can change the projection dimension d_out to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc9a4375-068b-4b2a-aabb-a29347ca5ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-9.1476e-02,  3.4164e-02],\n",
      "         [-2.6796e-01, -1.3427e-03],\n",
      "         [-4.8421e-01, -4.8909e-02],\n",
      "         [-6.4808e-01, -1.0625e-01],\n",
      "         [-8.8380e-01, -1.7140e-01],\n",
      "         [-1.4744e+00, -3.4327e-01]],\n",
      "\n",
      "        [[-9.1476e-02,  3.4164e-02],\n",
      "         [-2.6796e-01, -1.3427e-03],\n",
      "         [-4.8421e-01, -4.8909e-02],\n",
      "         [-6.4808e-01, -1.0625e-01],\n",
      "         [-8.8380e-01, -1.7140e-01],\n",
      "         [-1.4744e+00, -3.4327e-01]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d_out = 1\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836b5da-ef82-4b4c-bda1-72a462e48d4e",
   "metadata": {},
   "source": [
    "### 3.6.2 Multi-head attention through weight splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7",
   "metadata": {},
   "source": [
    "- While the above is the most intuitive and fully functional implementation of multi-head attention (encapsulating the earlier single-head attention CausalAttention implementation), we can also write a separate class called MultiHeadAttention to implement the same functionality.\n",
    "\n",
    "- For this separate MultiHeadAttention class, we do not connect the individual attention heads together. Instead, we create individual W_query, W_key, and W_value weight matrices, and then split them into separate matrices for each attention head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "110b0188-6e9e-4e56-a988-10523c6c8538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, block_size, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "# Because the weight matrix needs to be split according to the number of attention heads, all output dimensions must be integer multiples of the number of heads\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "# head_dim is the dimension that each head should output after splitting\n",
    "        self.head_dim = d_out // num_heads \n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "# shape is (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "# We can split the matrix into each head by adding a dimension of num_heads\n",
    "# Dimension change: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "# Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "# Calculate attention weights\n",
    "# Based on matrix multiplication, simply implement parallel computing of each head\n",
    "        attn_scores = queries @ keys.transpose(2, 3) \n",
    "# Generally we convert the mask matrix to bool values ​​and truncate based on the length of the sequence\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "# The mask matrix needs to be unsqueezed twice, that is, two dimensions are added, so that the dimensions of the mask matrix and the attention weights correspond\n",
    "        mask_unsqueezed = mask_bool.unsqueeze(0).unsqueeze(0)\n",
    "# Use a mask matrix for masking\n",
    "        attn_scores.masked_fill_(mask_unsqueezed, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "# shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "# Reassemble the outputs of multiple heads self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "# Experiment\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, block_size, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334dfb5-2b6c-4c33-82d5-b4e9db5867bb",
   "metadata": {},
   "source": [
    "- Note that the above is actually a more efficient rewrite of MultiHeadAttentionWrapper.\n",
    "- The final output looks a bit different due to differences in the random weight initialization, but both are perfectly usable implementations that will be used in the GPT class implemented in subsequent chapters.\n",
    "- In addition, we added a linear projection layer (self.out_proj) to the MultiHeadAttention class above. This is just a linear transformation that does not change the dimensionality. Using such a projection layer in LLM implementations is a standard practice, but not strictly necessary (recent research shows that it can be removed without affecting modeling performance; see the Further Reading section at the end of this chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ed78c-e8ac-4f8f-a479-a98242ae8f65",
   "metadata": {},
   "source": [
    "If you are interested in a more sophisticated and efficient implementation of multi-head attention, you may consider using PyTorch’s [`torch.nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363701ad-2022-46c8-9972-390d2a2b9911",
   "metadata": {},
   "source": [
    "- The above implementation may look a bit complicated, let's take a look at what happens when we run `attn_scores = queries @ keys.transpose(2, 3)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8cfc1ae-78ab-4faa-bc73-98bd054806c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\n",
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "\n",
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587b946-c8f2-4888-adbf-5a5032fbfd7b",
   "metadata": {},
   "source": [
    "- In this case, the matrix multiplication implementation in PyTorch will process the 4-dimensional input tensor to do a matrix multiplication between the last two dimensions (num_tokens, head_dim), and then repeat for each head.\n",
    "\n",
    "- For example, the above becomes a more compact matrix multiplication that computes each head separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "053760f1-1a02-42f0-b3bf-3d939e407039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08c2a3fd-e674-4d69-9ef4-ea94b788e937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2360064"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 1024\n",
    "d_in, d_out = 768, 768\n",
    "num_heads = 12\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(mha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec671bf-7938-4304-ad1e-75d9920e7f43",
   "metadata": {},
   "source": [
    "# Summary and Gains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e4113-ffca-432c-b3ec-7a50bd15da25",
   "metadata": {},
   "source": [
    "You can check out the [./multihead-attention.ipynb](./multihead-attention.ipynb) code notebook, which is a concise version of DataLoader (Chapter 2), plus the multi-head attention class we implemented in this chapter, which we will use when training the GPT model in subsequent chapters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
