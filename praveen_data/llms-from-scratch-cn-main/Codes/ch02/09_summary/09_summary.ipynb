{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0315c598-701f-46ff-8806-15813cad0e51",
   "metadata": {},
   "source": [
    "\n",
    "# **2.9 总结**\n",
    "\n",
    "- 大型语言模型需要将文本数据转换为向量,称为嵌入embeddings,因为它们无法处理原始文本。嵌入embeddings将离散数据(如单词或图像)转换为连续向量空间,使其可以兼容神经网络的操作。\n",
    "\n",
    "- 第一步,原始文本被分解为tokens,可以是单词或字符。然后,将标记转换为整数表示形式,称为token IDs。\n",
    "\n",
    "- 特殊标记,如<|unk|>和<|endoftext|>,以增强模型的理解能力,并处理各种情况,如未知单词或标记无关文本之间的边界。\n",
    "\n",
    "- 用于GPT-2和GPT-3等大型语言模型的字节对编码(BPE)标记器可以通过将未知单词分解为子词单元或单个字符来有效地处理未知单词。\n",
    "\n",
    "- 我们在标记化数据上使用滑动窗口方法来生成大型语言模型训练所需的输入-目标对。\n",
    "\n",
    "- PyTorch中的嵌入层充当查找操作,检索与标记ID相对应的向量。生成的嵌入向量提供了标记的连续表示,这对于训练深度学习模型(如大型语言模型)至关重要。\n",
    "\n",
    "- 虽然标记嵌入为每个标记提供了一致的向量表示,但它们缺乏标记在序列中位置的概念。为了解决这一问题,存在两种主要类型的位置嵌入:绝对位置嵌入和相对位置嵌入。OpenAI的GPT模型利用绝对位置嵌入,这些嵌入会被添加到标记嵌入向量中,并在模型训练过程中进行优化。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
