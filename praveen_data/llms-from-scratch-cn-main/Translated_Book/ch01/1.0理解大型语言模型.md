## 1 理解大型语言模型（Understanding Large Language Models 「LLMs」）

This chapter covers
1. High-level explanations of the fundamental concepts behind large language models (LLMs)
2. Insights into the transformer architecture from which ChatGPT-like LLMs are derived
3. A plan for building an LLM from scratch

本章包括
1. 大型语言模型 (LLM) 基本概念的深层次解释「其中将会涉及很多大模型相关原理知识的讲解。」
2. 深入了解transformer架构，由此衍生出类似 ChatGPT 的 LLM「包括国外的：GPT、 BERT 、T5、LLaMA，国内的：Qwen、Baichuan等，当然我们也需要知道，虽然Transformer架构是如今的主流大模型架构，但也有非Transformer架构的大模型，比如RWKV。」
3. 从零开始构建 LLM 的计划「这是本书的重点，本书将从原理层面，一步一步的讲解整个大模型的框架构成，并且从零开始构建一个基于Transformer架构的LLM。」

「那我们开始吧，在大语言模型中，我们首先从对LLM的概念进行初步的介绍开始。」

Large language models (LLMs), such as those offered in OpenAI's ChatGPT, are deep neural network models that have been developed over the past few years. They ushered in a new era for Natural Language Processing (NLP). Before the advent of large language models, traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern recognition that could be captured with handcrafted rules or simpler models. However, they typically underperformed in language tasks that demanded complex understanding and generation abilities, such as parsing detailed instructions, conducting contextual analysis, or creating coherent and contextually appropriate original text. For example, previous generations of language models could not write an email from a list of keywords—a task
that is trivial for contemporary LLMs.

大型语言模型（LLM）是最近几年被开发出来的深度神经网络模型，如 OpenAI 所提供的 ChatGPT 模型。它们开创了自然语言处理（NLP）的新时代。在大型语言模型出现之前，传统方法在垃圾邮件分类和简单模式识别等分类任务中表现出色，这是由于这些任务可以用手工规则或更简单的模型来捕捉。然而，在需要复杂理解和生成能力的语言任务中，如解析详细说明、进行上下文分析或创建连贯且与上下文相适应的原始文本时，这些方法通常表现不佳。例如，前几代语言模型无法根据关键字列表编写电子邮件--而这一任务对当代 LLM 来说是微不足道的。

> 本句的注解和说明如下：
1. **大型语言模型（LLM）**：LLM是“Large Language Model”的缩写，指的是一种设计用来处理和生成自然语言的深度学习模型。这些模型通常拥有大量的参数，能够捕捉和学习语言的复杂结构和模式。
2. **深度神经网络模型**：深度神经网络是机器学习领域中的一种算法，它通过模拟人脑处理信息的方式来识别数据中的复杂关系。这些网络由多层构成，每一层都能够学习数据的不同层次的特征。
3. **OpenAI**：OpenAI 是国际上的一个人工智能研究组织，ChatGPT 是他们开发的一种著名的LLM，能够进行各种语言任务，如文本生成、翻译、问答等。
4. **开创了自然语言处理（NLP）的新时代**：意味着LLM的出现标志着NLP领域的一个重要进展，使得机器能够更加精准和自然地处理人类语言。
5. **传统方法**：指的是LLM出现之前用于处理语言任务的技术和模型，如基于规则的系统或简单的统计模型。
6. **垃圾邮件分类和简单模式识别等分类任务**：这些是一些基本的NLP任务，其中垃圾邮件分类是指识别并过滤掉不想要的电子邮件，简单模式识别可能涉及识别文本中的特定模式或关键词。
7. **可以用手工规则或更简单的模型来捕捉**：意味着这些任务的复杂性较低，可以通过预先定义的规则或者结构相对简单的算法来有效完成，而当任务复杂后，手工规则就失效了。
8. **需要复杂理解和生成能力的语言任务**：指的是那些不仅需要识别文本中的信息，还需要对信息进行深入理解和进一步生成文本的任务。这包括理解复杂的指令、进行细致的上下文分析，或者创作流畅且符合上下文的文本。
9.  **前几代语言模型无法根据关键字列表编写电子邮件**：这表明早期的语言模型在生成文本方面有限制，它们不能灵活地使用给定的信息来创建连贯的文本内容。


LLMs have remarkable capabilities to understand, generate, and interpret human language. However, it's important to clarify that when we say language models "understand," we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension.

大语言模型具有理解、生成和解释人类语言的卓越能力。不过，有必要澄清的是，当我们说语言模型 "理解 "时，我们指的是它们能以看起来连贯且与上下文相关的方式处理和生成文本，而不是说它们拥有类似人类的意识或理解能力。

> 本句的注解和说明如下：
1. **具有理解、生成和解释人类语言的卓越能力**：这表明LLM在执行与语言相关的任务时表现出色，它们能够“理解”语言的含义，生成新的语言表达，以及对语言输入进行解释和回应。
2. **"理解"**：这里使用了引号来强调“理解”这个词在技术层面上的含义，它不同于人类对语言的理解。
3. **以看起来连贯且与上下文相关的方式处理和生成文本**：本句说明LLM通过分析大量数据和模式识别，能够生成读起来流畅并符合给定上下文的文本。这种能力让机器在处理语言时表现得更加自然和人性化。
4. **不是说它们拥有类似人类的意识或理解能力**：这是一个重要的区分，意味着尽管LLM在模拟语言理解和生成方面取得了显著进展，但它们并不具备真正的意识或自我意识。它们的“理解”是基于算法和数据处理，而不是基于情感、经验或主观意识的真正理解，从本书后文的构建中我们了解其本质还是一个基于语言文本的统计回归模型。
5. **理解能力**：在人工智能的语境中，通常指的是模型对输入数据的处理能力，以及它根据这些数据做出反应或生成输出的能力。在LLM中，这通常涉及到复杂的算法和大量的参数调整，以模拟对语言的认知处理。
6. **上下文相关**：LLM能够根据文本的上下文环境来生成或解释语言，这意味着它们不仅仅是简单地按照规则生成文本，而是能够考虑到文本的语义内容、语境线索以及可能的隐含意义。
7. **处理和生成文本**：这涉及到LLM的两个主要功能，即文本的理解（处理）和基于该理解生成新的文本内容。


Enabled by advancements in deep learning, which is a subset of machine learning and artificial intelligence (AI) focused on neural networks, LLMs are trained on vast quantities of text data. This allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches. As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more. Another important distinction between contemporary LLMs and earlier NLP models is that the latter were typically designed for specific tasks; whereas those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks. The success behind LLMs can be attributed to the transformer architecture which underpins many LLMs, and the vast amounts of data LLMs are trained on, allowing them to capture a wide variety of linguistic nuances, contexts, and patterns that would be challenging to manually encode.

深度学习是以神经网络为核心的机器学习和人工智能（AI）的一个子集，在深度学习的推动下，LLM 在大量文本数据的基础上进行训练。与以前的方法相比，这使得 LLM 能够捕捉到更深层次的语境信息和人类语言的微妙之处。因此，LLM 在文本翻译、情感分析、问题解答等各种 NLP 任务中的性能都有显著提高。当代 LLM 与早期 NLP 模型的另一个重要区别是，后者通常是为特定任务而设计的；早期的 NLP 模型在其狭窄的应用领域表现出色，而 LLM 则在广泛的 NLP 任务中表现出更广泛的能力。LLMs 的成功可归功于支撑许多 LLMs 的transformer架构，以及 LLMs 所训练的海量数据，这使得它们能够捕捉各种语言细微差别、语境和模式，而这些都是人工编码所面临的挑战。

> 本句的注解和说明如下：
1. **深度学习**：深度学习是机器学习的一个分支，它使用具有多个层级（也称为深度的原因）的神经网络来学习数据中的复杂模式。这些层次结构使得网络能够从原始数据中提取高级特征，用于各种任务。
2. **在深度学习的推动下，LLM 在大量文本数据的基础上进行训练**：这意味着LLM利用深度学习的原理，通过分析和学习大量的文本数据来提高其语言处理能力。
3. **捕捉到更深层次的语境信息和人类语言的微妙之处**：LLM通过深度学习能够理解语言的复杂性，包括语法、句法、语义以及语言使用的社会和文化背景。
4. **文本翻译、情感分析、问题解答等各种 NLP 任务**：这些是自然语言处理领域的常见应用，LLM在这些任务中展现出比以往模型更优秀的性能。
5. **性能都有显著提高**：与早期的NLP模型相比，LLM在理解、生成和处理自然语言方面取得了重大进展。
6. **早期 NLP 模型通常是为特定任务而设计的**：这指的是传统的NLP模型往往专注于解决特定的语言问题，如垃圾邮件过滤或语音识别，而不是通用的语言任务。
7. **早期的 NLP 模型在其狭窄的应用领域表现出色**：这些模型在它们被设计来解决的具体问题上效果很好，但可能不适用于其他类型的语言处理任务。
8.  **LLM 则在广泛的 NLP 任务中表现出更广泛的能力**：与特定任务的模型不同，LLM能够处理多种类型的语言任务，显示出更通用的能力。
9.  **Transformer架构**：这里提到的“Transformer架构”是一种先进的深度学习模型，它在处理序列数据方面特别有效，尤其是在自然语言处理（NLP）领域，它在2017年被提出，并迅速成为处理序列数据，尤其是在自然语言处理（NLP）任务中的主流方法。它在处理序列数据时特别有效，已经成为许多LLM的基础架构。Transformer模型的核心是自注意力机制，它允许模型在处理序列的每个元素时考虑到序列中的所有其他元素，这使得它能够捕捉到序列内部的长距离依赖关系。这部分的内容将会在后续章节中详细说明。
10. **LLMs 所训练的海量数据**：为了训练LLM，需要大量的文本数据来提供足够的信息，让模型学习语言的各种用法和模式。
11. **捕捉各种语言细微差别、语境和模式**：LLM通过学习大量的语言数据，能够识别和再现语言中的复杂细节，包括词义的微妙变化、语言的使用情境以及不同的语言模式。
12. **人工编码所面临的挑战**：指在没有机器学习的帮助下，人类专家很难手工定义和编码语言的所有复杂规则和特性。

This shift towards implementing models based on the transformer architecture and using large training datasets to train LLMs has fundamentally transformed NLP, providing more capable tools for understanding and interacting with human language.

这种基于transformer架构实施模型并使用大型训练数据集来训练 LLM 的转变从根本上改变了 NLP，为理解人类语言并与之互动提供了能力更强的工具。

> 本句的注解和说明如下：
1. **使用大型训练数据集来训练LLM**：LLM（大型语言模型）的训练需要大量的文本数据，这些数据集通常包含了广泛的语言使用情况，包括不同的语言风格、领域特定术语、语法结构和语义信息。通过在这些丰富的数据上训练，LLM能够学习到语言的多样性和复杂性。
2. **转变**：这里指的是从以往使用的较小规模模型和不同架构，到现在使用基于Transformer的架构和大规模数据集的转变。
3. **从根本上改变了NLP**：本句强调了Transformer架构和大规模数据训练对自然语言处理领域带来的深远影响。这种转变不仅仅是技术上的改进，更是对NLP任务理解和处理方式的根本性改变。
4. **为理解人类语言并与之互动提供了能力更强的工具**：LLM通过学习大量的文本数据，能够更好地理解和生成人类语言，从而在人机交互、语言翻译、文本摘要、情感分析等NLP任务中表现得更加出色。这些工具的增强能力使得机器与人类的交流更加自然和高效。
5. **理解人类语言**：指的是LLM能够识别和解释语言中的语义内容、语法结构、语境含义等，从而对语言有一个深层次的理解。
6. **与之互动**：指的是LLM不仅能够理解输入的语言信息，还能够生成回应或执行特定的语言任务，如回答问题、提供建议、撰写文章等。
7. **能力更强的工具**：与以往的NLP模型相比，LLM因其规模和架构的优势，在处理语言时更为精准、灵活和高效。


Beginning with this chapter, we set the foundation to accomplish the primary objective of this book: understanding LLMs by implementing a ChatGPT-like LLM based on the transformer architecture step by step in code.

从本章开始，我们为实现本书的主要目标奠定了基础：通过在代码中逐步实现基于transformer架构的类似 ChatGPT 的 LLM，从而理解 LLM。
