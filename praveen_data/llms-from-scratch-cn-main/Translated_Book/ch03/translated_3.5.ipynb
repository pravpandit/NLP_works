{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Hiding Subsequent Words Using Causal Attention\n",
    "\n",
    "In this section, we will modify the standard self-attention mechanism to create a causal attention mechanism, which is essential for the development of large language models in subsequent chapters.\n",
    "\n",
    "Causal attention, also known as masked attention, is a special form of self-attention. It restricts the model to only consider the previous and current inputs in the sequence when processing any given token. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once.\n",
    "\n",
    "Therefore, when calculating the attention score, the causal attention mechanism ensures that the model only considers tokens that appear before or before the current token in the sequence.\n",
    "\n",
    "In large language models like GPT, to achieve this, we mask the subsequent tokens after the current token in the input text for each processed token, as shown in Figure 3.19.\n",
    "\n",
    "**Figure 3.19 In causal attention, we mask the attention weights above the diagonal so that the large language model cannot access subsequent tokens when calculating the context vector. For example, in the second row, for the word \"journey\", we only keep the attention weights for \"Your\" (the previous word) and \"journey\" (the current position). **\n",
    "\n",
    "![3.19](../img/fig-3-19.jpg)\n",
    "\n",
    "As shown in Figure 3.19, we mask the attention weights above the diagonal and normalize the unmasked attention weights so that the sum of the attention weights in each row is 1. In the next section, we will implement this masking and normalization process in code.\n",
    "\n",
    "## 3.5.1 Applying Causal Attention Masking\n",
    "\n",
    "In this section, we will implement causal attention masking in code. We start with the procedure summarized in Figure 3.20.\n",
    "\n",
    "**Figure 3.20 One way to obtain the masked attention weight matrix in the causal attention mechanism is to apply a softmax function to the attention scores, zero the elements above the diagonal and normalize the resulting matrix. **\n",
    "\n",
    "![3.20](../img/fig-3-20.jpg)\n",
    "\n",
    "To implement the causal attention masking step shown in Figure 3.20 and obtain the masked attention weights, let's encode the causal attention mechanism using the attention scores and weights from the previous section.\n",
    "\n",
    "In the first step, as shown in Figure 3.20, we calculate the attention weights using the softmax function, as we did in the previous sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)  #A\n",
    "keys = sa_v2.W_key(inputs) \n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the following attention weights: \n",
    "```python\n",
    "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
    "[0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
    "[0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
    "[0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
    "[0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
    "[0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "grad_fn=<SoftmaxBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PyTorch’s tril function to implement the second step in Figure 3.20, creating a mask so that the values ​​above the diagonal are zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting mask is as follows:\n",
    "```python\n",
    "tensor([[1., 0., 0., 0., 0., 0.],\n",
    "[1., 1., 0., 0., 0., 0.],\n",
    "[1., 1., 1., 0., 0., 0.],\n",
    "[1., 1., 1., 1., 0., 0.],\n",
    "[1., 1., 1., 1., 0., 0.],\n",
    "[1., 1., 1., 1., 1., 0.],\n",
    "[1., 1., 1., 1., 1., 1.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can multiply this mask with the attention weights to zero out the values ​​above the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the elements above the diagonal have been successfully zeroed: \n",
    "```python\n",
    "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "[0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "[0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
    "[0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
    "[0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
    "[0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "grad_fn=<MulBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step in Figure 3.20 is to renormalize the attention weights so that they sum to 1 again for each row. We can do this by dividing each element in each row by the sum of that row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an attention weight matrix where the attention weights above the diagonal are zeroed and the sum of each row is 1:\n",
    "```python\n",
    "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "[0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "[0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
    "[0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
    "[0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
    "[0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "grad_fn=<DivBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Leakage\n",
    "\n",
    "When we apply masking and then renormalize the attention weights, it may appear that information from subsequent tokens (that we intend to mask) still affects the current token, since their values ​​are part of the softmax function calculation. However, the key point is that when we renormalize the attention weights after masking, we are actually recalculating the softmax function on a smaller subset (since the masked positions do not contribute anything to the softmax value).\n",
    "\n",
    "The mathematical elegance of softmax is that, although all positions are included in the denominator in the initial calculation, after masking and renormalization, the influence of the masked positions is eliminated - they do not affect the softmax score in any meaningful way.\n",
    "\n",
    "In short, after masking and renormalization, the distribution of attention weights is as if they were only calculated on unmasked positions in the beginning. This ensures that information from subsequent (or other masked) tokens does not leak as we might think.\n",
    "\n",
    "Although the implementation of causal attention is technically complete at this point, we can exploit a mathematical property of the softmax function and implement the calculation of the masked attention weights more efficiently in fewer steps, as shown in Figure 3.21.\n",
    "\n",
    "**Figure 3.21 A more efficient way to obtain the masked attention weight matrix in causal attention is to apply the softmax functionPreviously, the attention scores were masked with negative infinity values. **\n",
    "\n",
    "![3.21](../img/fig-3-21.jpg)\n",
    "\n",
    "The softmax function converts its input into a probability distribution. When there is a negative infinity (-∞) value in a row, the softmax function treats its probability as zero. (Mathematically, this is because e^-∞ approaches 0.)\n",
    "\n",
    "We can implement this more efficient masking technique by creating a mask with 1s above the diagonal, and then replacing these 1s with negative infinity (-inf) values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following masking is obtained: \n",
    "```python\n",
    "tensor([[0.2899, -inf, -inf, -inf, -inf, -inf],\n",
    "[0.4656, 0.1723, -inf, -inf, -inf, -inf],\n",
    "[0.4594, 0.1703, 0.1731, -inf, -inf, -inf],\n",
    "[0.2642, 0.1024, 0.1036, 0.0186, -inf, -inf],\n",
    "[0.2183, 0.0874, 0.0882, 0.0177, 0.0786, -inf],\n",
    "[0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
    "grad_fn=<MaskedFillBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to apply a softmax function to these masked results and we are done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the output, the sum of the values ​​in each row is 1, and no further normalization is needed: \n",
    "```python\n",
    "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "[0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "[0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
    "[0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
    "[0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
    "[0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
    "grad_fn=<SoftmaxBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the modified attention weights to compute the context vector via context_vec = attn_weights@values, as we did in Section 3.4.\n",
    "\n",
    "In the next section, we will first introduce another small tweak to the causal attention mechanism, which is useful for reducing overfitting when training large language models.\n",
    "\n",
    "## 3.5.2 Masking Additional Attention Weights via Dropout\n",
    "\n",
    "In deep learning, dropout is a technique where selected hidden layer units are randomly ignored during training, effectively \"dropping\" them. This approach helps prevent overfitting by ensuring that the model does not become overly dependent on any particular group of hidden layer units. It is important to emphasize that dropout is only used during training and not afterwards.\n",
    "\n",
    "In Transformer architectures, including GPT, dropout in the attention mechanism is usually applied in two specific areas: after the attention scores are calculated, or after the attention weights are applied to the value vector.\n",
    "\n",
    "Here, we will apply dropout masking after the attention weights are calculated, as shown in Figure 3.22, which is the more common variant in practice.\n",
    "\n",
    "**Figure 3.22 Using causal attention masking (top left), we apply additional dropout masking (top right) to zero additional attention weights to reduce overfitting during training. **![3.22](../img/fig-3-22.jpg)\n",
    "\n",
    "In the following code example, we use a 50% dropout rate, which means masking out half of the attention weights. (When training the GPT model in later chapters, we will use lower dropout rates, such as 0.1 or 0.2.)\n",
    "\n",
    "In the following code, we first apply PyTorch's dropout implementation to a 6x6 tensor of 1s for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, about half of the values ​​are reset to zero:\n",
    "```python\n",
    "tensor([[2., 2., 0., 2., 2., 0.],\n",
    "[0., 0., 0., 2., 0., 2.],\n",
    "[2., 2., 2., 2., 0., 2.],\n",
    "[0., 2., 2., 0., 0., 2.],\n",
    "[0., 2., 0., 2., 0., 2.],\n",
    "[0., 2., 0., 2., 0., 2.],\n",
    "[0., 2., 2., 2., 2., 0.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a 50% dropout rate is applied to the attention weight matrix, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values ​​of the remaining elements in the matrix are scaled up by a factor of 1/0.5 = 2. This scaling is critical to maintaining the overall balance of the attention weights, ensuring that the average impact of the attention mechanism remains consistent during training and inference.\n",
    "\n",
    "Now, let's apply dropout to the attention weight matrix itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More elements in the processed attention weight matrix are zeroed, and the remaining elements are rescaled:\n",
    "```python\n",
    "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "[0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
    "[0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
    "[0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
    "[0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
    "grad_fn=<MulBackward0>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of Dropout may vary depending on the operating system; you can read more about this inconsistency on the PyTorch issue tracker (https://github.com/pytorch/pytorch/issues/121595).\n",
    "\n",
    "Having learned about causal attention and dropout masking, we will develop a concise Python class in the next section. This class is designed to facilitate the efficient application of both techniques.\n",
    "\n",
    "## 3.5.3 Implementing a compact causal attention class\n",
    "\n",
    "In this section, we integrate causal attention and dropout techniques into the SelfAttention Python class we developed in Section 3.4. This class will then serve as a template for developing multi-head attention in the upcoming section, which is the final attention class we will implement in this chapter.\n",
    "\n",
    "But before we get started, there is one more thing to ensure, and that is that the code can handle batches consisting of multiple inputs so that the CausalAttention class supports the batch outputs generated by the data loader we implemented in Chapter 2.\n",
    "\n",
    "To simplify simulating this batch input, we copy the input text example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) #A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a 3D tensor containing 2 input texts, each with 6 tokens, and each token is a 3D embedding vector:\n",
    "```python\n",
    "torch.Size([2, 6, 3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following CausalAttention class is similar to the SelfAttention class we implemented earlier, except that we have added the Dropout and causal masking parts highlighted in the code below:\n",
    "\n",
    "### Listing 3.3 A compact causal attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While all the new lines of code are similar to the code in the previous section, we now add a self.register_buffer() call in the ‘__init__’ method. Using register_buffer in PyTorch is not necessary in all cases, but it has several advantages here. For example, when we use the CausalAttention class in a large language model, the buffer is automatically moved to the appropriate device (CPU or GPU) along with the model, which will be useful when training large language models in subsequent sections. This means that we do not need to manually ensure that these tensors are on the same device as the model parameters, thus avoiding device mismatch errors.\n",
    "\n",
    "We can use the CausalAttention class in the same way as we used the SelfAttention class before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting context vector is a 3D tensor where each token is now represented by a 2D embedding:\n",
    "```python\n",
    "context_vecs.shape: torch.Size([2, 6, 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3.23 provides a mental model that summarizes what we have accomplished so far.\n",
    "\n",
    "Figure 3.23 A mental model that summarizes the four different attention modules we wrote in this chapter. We started with a simplified attention mechanism, added trainable weights, and then added causal attention masking. In the rest of this chapter, we will expand the causal attention mechanism and write a multi-head attention mechanism, which is the final module we will use in the large language model implementation in the next chapter.\n",
    "\n",
    "![3.23](../img/fig-3-23.jpg)\n",
    "\n",
    "As shown in Figure 3.23, in this section, we focus on the concept and implementation of causal attention in neural networks. In the next section, we will further expand this concept and implement a multi-head attention module that implements multiple such causal attention mechanisms in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minitorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
