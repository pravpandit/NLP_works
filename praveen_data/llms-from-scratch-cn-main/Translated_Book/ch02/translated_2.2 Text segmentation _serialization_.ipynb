{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03fa84bd",
   "metadata": {},
   "source": [
    "# 2.2 Text segmentation (serialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506a3e2",
   "metadata": {},
   "source": [
    "This section covers how we segment the input text into individual tokens, which is a required preprocessing step for creating embeddings for large language models (LLMs).\n",
    "These tokens may be single words or special characters, including punctuation, as shown in Figure 2.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad846a",
   "metadata": {},
   "source": [
    "**Figure 2.4 A view of the text processing steps involved in this section in a large language model (LLM).\n",
    "Here, we split the input text into individual tokens, which may be words or special characters such as punctuation.\n",
    "In the following sections, we will convert text into token IDs and create token embeddings. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df060a",
   "metadata": {},
   "source": [
    "![fig2.4](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-2-4.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee689dac",
   "metadata": {},
   "source": [
    "The text we will be training tokens for our Large Language Model (LLM) is a short story called The Verdict by Edith Wharton, which is in the public domain so we can use it for our LLM training task.\n",
    "This article can be found on Wikisource at https://en.wikisource.org/wiki/The_Verdict and can be copied and pasted into a text file. I have copied it into a text file called “the-verdict.txt” so that it can be loaded using Python’s standard file reading tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df040cd5",
   "metadata": {},
   "source": [
    "### Code Example 2.1: Using Python code to load a short story as a text example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68baa9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "response = requests.get(url)\n",
    "raw_text = response.text\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1183553",
   "metadata": {},
   "source": [
    "Alternatively, you can find the file named \"the-verdict.txt\" in the GitHub repository of this book at: https://github.com/rasbt/LLMs-from-scratch/tree/main/ch02/01_main-chapter-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4109f8",
   "metadata": {},
   "source": [
    "The print command is used to print the total number of characters in a file. We then print the first 100 characters of the file as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b04498",
   "metadata": {},
   "source": [
    "Total number of characters: 20479\\\n",
    "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc9cb7",
   "metadata": {},
   "source": [
    "Our goal is to tokenize this 20,479-character short story into individual words and special characters so that we can convert them into embedding vectors for training a Large Language Model (LLM) in the following chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d0829",
   "metadata": {},
   "source": [
    "### Size of sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9598248",
   "metadata": {},
   "source": [
    "Note that when running large language models (LLMs), it is common to process millions of articles and hundreds of thousands of books - gigabytes of text.\n",
    "However, for educational purposes, it is sufficient to use a small sample of text, such as a single book. This allows the main steps of text processing to be clearly demonstrated while ensuring that it runs in a reasonable amount of time on common consumer hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7b1cd",
   "metadata": {},
   "source": [
    "How can we best split this text into a list of tokens?\n",
    "We’ll do a brief exploration of this, using Python’s regular expression library, the re module, for examples.\n",
    "(Note that you don’t have to learn or remember any regular expression syntax, as we’ll switch to using a prebuilt tokenizer later in this chapter.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914f2ab",
   "metadata": {},
   "source": [
    "Using some simple example text, we can use the following re.split command to split the text on whitespace characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683ea6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3632203",
   "metadata": {},
   "source": [
    "The result is a list containing single words, spaces, and punctuation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209f23a",
   "metadata": {},
   "source": [
    "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', '\n",
    "', 'test.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1cd96",
   "metadata": {},
   "source": [
    "Note that the simple tokenization scheme above is mostly useful for breaking up the sample text into individual words, but there are still some words connected to punctuation marks that we would like to list separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72303a4c",
   "metadata": {},
   "source": [
    "Let's modify the regular expression to split on spaces (\\s) and commas and periods ([,.]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7c3026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8254d5",
   "metadata": {},
   "source": [
    "We can see that the words and punctuation marks are now separate items in the list, just as we wanted:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e0524",
   "metadata": {},
   "source": [
    "['Hello', ',', '', ' ', 'world.', ' ', 'This', ',', '', ' ',\n",
    "'is', ' ', 'a', ' ', 'test.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ef273",
   "metadata": {},
   "source": [
    "There is one minor problem, though, because the list still contains whitespace characters.\n",
    "We can choose to safely remove these extra characters, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504c9f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb7d9d",
   "metadata": {},
   "source": [
    "The resulting output without whitespace is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef4466",
   "metadata": {},
   "source": [
    "['Hello', ',', 'world.', 'This', ',', 'is', 'a', 'test.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a92c0",
   "metadata": {},
   "source": [
    "### Removing whitespaces or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f530be2",
   "metadata": {},
   "source": [
    "### Whether to remove spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6f354",
   "metadata": {},
   "source": [
    "When developing a simple tokenizer, whether we should encode spaces as separate characters or remove them depends on our application and its requirements.\n",
    "Removing spaces can reduce memory and computational requirements. However, keeping spaces can be useful when we are training models that are sensitive to the precise structure of the text\n",
    "(for example, Python code is very sensitive to indentation and spacing).\n",
    "Here, we remove spaces to simplify and concise the token output.\n",
    "Later, we will move on to a tokenization scheme that includes spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9c8ef",
   "metadata": {},
   "source": [
    "The tokenization scheme we designed in the previous section works well on simple sample text.\n",
    "Now, let's modify it further so that it can also handle other types of punctuation,\n",
    "such as question marks, quotation marks, and the double dash we saw in the first 100 characters of Edith Wharton's short story, as well as other additional special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2faa1386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf02c46",
   "metadata": {},
   "source": [
    "The resulting output looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa080e9",
   "metadata": {},
   "source": [
    "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test',\n",
    "'?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ded4a",
   "metadata": {},
   "source": [
    "As we can see from the results summarized in Figure 2.5,\n",
    "our word segmentation scheme can now successfully handle various special characters in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4baf9",
   "metadata": {},
   "source": [
    "**Figure 2.5 The tokenization scheme we have implemented so far segments text into individual words and punctuation marks. In the specific example shown in this figure, the sample text is segmented into 10 individual tokens. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ecff5",
   "metadata": {},
   "source": [
    "![fig2.5](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-2-5.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bddfbd",
   "metadata": {},
   "source": [
    "Now that we have a basic tokenizer up and running, let's deploy it on the entire collection of Edith Wharton's short stories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5356685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if\n",
    "item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c492a63",
   "metadata": {},
   "source": [
    "The print statement above outputs 4649, which is the number of tokens in the text (excluding spaces)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93d099",
   "metadata": {},
   "source": [
    "Let's print the first 30 tokens for a quick visual inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0865898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f42d2f8",
   "metadata": {},
   "source": [
    "The resulting output shows that our tokenizer seems to have processed the text well, as all the words and special characters are neatly separated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d4bfe",
   "metadata": {},
   "source": [
    "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cell)",
   "language": "python",
   "name": "cell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
