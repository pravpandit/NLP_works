{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd0b15a-2694-45e8-806d-7ccc36f71926",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5.3 Decoding strategy to control randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1f734-42c2-41d1-971d-7d7721a6d935",
   "metadata": {},
   "source": [
    "In this section, we will introduce text generation strategies (also called decoding strategies) to generate more original text. First, we will briefly review the generate_text_simple function used in the generate_and_print_sample function in the previous section. Then we will introduce two techniques for optimizing this function: temperature scaling and top-k sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d5d98-b3a0-4a13-b3fb-31816f003d9b",
   "metadata": {},
   "source": [
    "We first transfer the model from the GPU back to the CPU, since a GPU is not required for inference with a relatively small model. We then put the trained model into the evaluation model to turn off random components such as dropout:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69025d0-7895-4bf1-9f0f-3952e465b4f7",
   "metadata": {},
   "source": [
    "```\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186e848-1366-4cdd-a150-868e9a0eec6e",
   "metadata": {},
   "source": [
    "Next we insert the GPTModel instance (model) into the generate_text_simple function, which uses LLM to generate one token at a time:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6cf33e-be4c-4b48-ac03-e6652c3addcc",
   "metadata": {},
   "source": [
    "```python\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "model=model,\n",
    "idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "max_new_tokens=25,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efebc02-c777-465f-9c03-7c9032275e43",
   "metadata": {},
   "source": [
    "The generated text looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075ecc5-404d-4b8e-b2d3-0a895c7d6b62",
   "metadata": {},
   "source": [
    "```\n",
    "Output text:\n",
    "Every effort moves you know,\" was one of the axioms he laid down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36cdea-2ccd-4551-9562-8cf840d7cc76",
   "metadata": {},
   "source": [
    "As mentioned earlier in Section 5.1.2, the generated token chosen at each generation step corresponds to the maximum probability score among all tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad75b490-18ec-4504-88ca-22960118fad5",
   "metadata": {},
   "source": [
    "This means that no matter how many times we run the generate_text_simple function in the same opening context (e.g. “Every effort moves you”), LLM will always generate the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc2760-709f-494e-b2ad-e79488a29b82",
   "metadata": {},
   "source": [
    "In the following sections, we will introduce two concepts for controlling randomness and diversity: temperature scaling and top-k sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b1d5a-5f22-4a3b-b043-dd630594d950",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.3.1 Temperature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f806f31-2541-4be7-b0ab-f618319ee162",
   "metadata": {},
   "source": [
    "Temperature scaling, introduced in this section, is a technique that adds a probabilistic selection process to the next token generation task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0fd042-7c44-444b-86f8-f89fd3ade4e2",
   "metadata": {},
   "source": [
    "Previously in the generate_text_simple function, we always used torch.argmax to extract the token with the highest probability as the next token, a process also known as greedy decoding. In order to generate more diverse text, we can replace argmax with a function that samples from a probability distribution (specifically, the probability score generated by LLM for each vocabulary entry in each token generation step)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7703c5d-e0ea-4fd9-88b0-bce943c0c5ea",
   "metadata": {},
   "source": [
    "To illustrate probabilistic sampling with a concrete example, let’s briefly discuss the next token generation process using a very small vocabulary to illustrate the problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d620c46e-f269-4719-a0e7-346ff5b60568",
   "metadata": {},
   "source": [
    "```\n",
    "vocab = {\n",
    "\"closer\": 0,\n",
    "\"every\": 1,\n",
    "\"effort\": 2,\n",
    "\"forward\": 3,\n",
    "\"inches\": 4,\n",
    "\"moves\": 5,\n",
    "\"pizza\": 6,\n",
    "\"toward\": 7,\n",
    "\"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c77aa-df9e-4b7a-ba1e-963345757899",
   "metadata": {},
   "source": [
    "Below we assume that the LLM opening context is set to \"every efforts moves you\" and generate the logit of the next token as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3a622-299d-4ee2-925f-83f06a49b2b6",
   "metadata": {},
   "source": [
    "```\n",
    "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934cc1f-08b3-4590-bdfb-d8bb05892fb8",
   "metadata": {},
   "source": [
    "As discussed in generate_text_simple in the previous chapter, we convert logits into probabilities through the softmax function and get the token ID corresponding to the generated token through the argmax function, which we can then map back to text by reversing the vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833324af-a23b-4c7e-a938-26b13c3770ab",
   "metadata": {},
   "source": [
    "```\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d84187-de1f-4df9-9e39-87912567882e",
   "metadata": {},
   "source": [
    "Since the largest logit value and the corresponding largest softmax probability score is at the fourth position (index position 3 because Python uses 0-indexing), the generated word is \"forward\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c54fb7-3b18-4024-9590-3609ba0d497e",
   "metadata": {},
   "source": [
    "To implement the probabilistic sampling process, we can now replace argmax with a polynomial function in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366c1f3-d7e9-460b-b259-d40024a177e7",
   "metadata": {},
   "source": [
    "```\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa15ec-825b-4cda-a965-bcbdf27f8e25",
   "metadata": {},
   "source": [
    "But the printout still prints \"forward\". Why is that? This is because the polynomial function samples the next token proportional to its probability score. In other words, \"forward\" is still the most likely token and will be chosen by the polynomial most of the time, but sometimes there are exceptions. To illustrate this, let's implement a function that repeats sampling 1000 times:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34278f-b0fa-4e61-a0a4-fbdfc6b7cfac",
   "metadata": {},
   "source": [
    "```\n",
    "def print_sampled_tokens(probas):\n",
    "torch.manual_seed(123)\n",
    "sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "for i, freq in enumerate(sampled_ids):\n",
    "print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70824b91-ff0b-4b09-9255-299d2dc998b6",
   "metadata": {},
   "source": [
    "The sampling results are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d0c86-193f-404b-b08d-a59cd120370d",
   "metadata": {},
   "source": [
    "```\n",
    "73 x closer \n",
    "0 x every\n",
    "0 x effort\n",
    "582 x forward\n",
    "2 x inches\n",
    "0 x moves\n",
    "0 x pizza\n",
    "343 x toward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e9a4b-c392-400e-8a6a-98ad285351af",
   "metadata": {},
   "source": [
    "Based on the output we can see that the word \"forward\" is sampled most of the time (582 out of 1000 times), but other tokens like \"closer\", \"inch\", and \"toward\" are sometimes sampled. This means that if we replace the argmax function with the polynomial function in the generate_and_print_sample function, LLM will sometimes generate sentences like \"every effort moves you toward\", \"every effort moves you inches\", and \"every effort moves you closer\" instead of \"every effort moves you forward\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa12938-8c5e-45d6-af7c-7a9c45904d60",
   "metadata": {},
   "source": [
    "We can further control the distribution and selection process through the concept of temperature scaling, which is really just a fancy way of saying dividing the logit by a number greater than 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f24dd-9cfc-4799-948e-5da01e6c3606",
   "metadata": {},
   "source": [
    "```\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "scaled_logits = logits / temperature\n",
    "return torch.softmax(scaled_logits, dim=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d507dd-627b-46c5-b566-9e166d99dc94",
   "metadata": {},
   "source": [
    "Temperatures greater than 1 result in a more uniform distribution of tokens, and temperatures less than 1 result in a more robust (cleaner or more peaked) distribution. Let's illustrate this by plotting the original probabilities and the probabilities scaled using different temperature values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147fc9c-fa54-423c-a539-d7b08d1b4cb8",
   "metadata": {},
   "source": [
    "```\n",
    "temperatures = [1, 0.1, 5] # Original, higher, and lower temperature\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
    "bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa08f06-c363-49c1-8b54-a6844c75f654",
   "metadata": {},
   "source": [
    "The drawing results are shown in fig-5-14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5265c-f0f1-4453-a392-77b350d03afd",
   "metadata": {},
   "source": [
    "![fig-5-14 A temperature of 1 represents the unscaled probability score for each token in the vocabulary. Lowering the temperature to 0.1 makes the distribution sharper, so the most likely token (here \"positive\") will have a higher probability score. Vice versa, increasing the temperature to 5 makes the distribution more uniform. ](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-5-14.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476dc931-4e8c-4efa-8b3e-46c5afbcadd3",
   "metadata": {},
   "source": [
    "When the temperature is 1, the logarithm is divided by 1 and then passed to the softmax function to calculate the probability score. In other words, using a temperature value of 1 is the same as not using any temperature scaling. The probability of the selected token in this case is equivalent to the original softmax probability score obtained by the multinomial sampling function in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686035e-cb54-4a01-a4a6-8edd958b5370",
   "metadata": {},
   "source": [
    "For example, as shown in figure-5-14, when the temperature is set to 1, the token corresponding to \"forward\" is selected about 60% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72475f-168a-4b62-9106-2b524033e866",
   "metadata": {},
   "source": [
    "In addition, as shown in figure-5-14, applying a very small temperature (such as 0.1) will lead to a higher difference in the distribution, so that the polynomial function behaves almost 100% like the argmax function, selecting the most likely token (here \"positive\"). Vice versa, when the temperature is 5, the distribution will be more balanced, increasing the probability of selecting other tokens. This can improve the diversity of generated text but also lead to more meaningless text. For example: a temperature setting of 4 will result in text such as \"every effort moves you pizza\" appearing about 4% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f8b82-ade6-49e6-a68d-8c4181234045",
   "metadata": {},
   "source": [
    "**Exercise 5.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b1bd4-41d3-4e98-8c1a-0c9356e9dc45",
   "metadata": {},
   "source": [
    "Use the print_sampled_tokens function to plot the sampling frequency of the softmax probability, which is proportional to the temperature as shown in figure-5-13. In each case, how often is the word \"pizza\" sampled? Can you think of a faster and more accurate way to determine how often the word \"pizza\" is sampled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e77f6-59a1-4488-9bf8-3ca0f73d008b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.3.2 Top-k Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56851ce2-f815-4e02-8bc1-d67d410f5497",
   "metadata": {},
   "source": [
    "In the previous section, we implemented a probabilistic sampling method combined with temperature scaling to increase the diversity of output results. It can be seen that the higher the temperature value, the more balanced the probability distribution of the next token, which reduces the possibility of the model repeatedly selecting the most likely token, resulting in more diverse outputs. This method allows exploration of less likely but perhaps more interesting and creative paths during the generation process. However, this method also has a drawback: this method sometimes leads to grammatical errors or completely meaningless output, such as \"every effort moves you pizza\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18d8c21-d61c-4e37-9b9c-6adb21a9ca34",
   "metadata": {},
   "source": [
    "In this section we will introduce another concept called top-k sampling, which, when combined with probabilistic sampling and temperature scaling, can optimize text generation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f7e3c-2ffb-4187-9802-1be5a8598d0c",
   "metadata": {},
   "source": [
    "In top-k sampling, we limit the extracted tokens to the most likely top-k tokens and exclude all other tokens by masking the probability scores, as shown in fig-5-15."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce97ef6-f347-4a14-94eb-07b26d7b05c5",
   "metadata": {},
   "source": [
    "![fig-5-15 Using top-k sampling with k=3, focus on the 3 tokens with the highest logits and mask all other tokens with negative infinity (-inf) before running the softmax function. This will result in a probability distribution where all non-top-k token probabilities are zero](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-5-15.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9f1a1-1cbc-4ad1-83ec-d762d998c649",
   "metadata": {},
   "source": [
    "The method outlined in fig-5-15 replaces all unselected logits with negative infinity (-inf), so that the probability score of non-top-k tokens is 0 when calculating the softmax value, and the sum of the remaining probabilities is 1. (Careful readers may remember the masking trick from the causal attention module implemented in Section 3.5.1 of Chapter 3, “Applying Causal Attention Masks”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5734a-6594-4724-a163-fbc10240d07f",
   "metadata": {},
   "source": [
    "We can implement the top-k process outlined in fig-5-15 in code as follows, starting with selecting the token with the largest logit value:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e08166-ff1b-4114-892d-11193f6ef208",
   "metadata": {},
   "source": [
    "```\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6627a41-eb46-4ab7-8207-38644816ff51",
   "metadata": {},
   "source": [
    "The logit values ​​and token IDs of the first 3 tokens (in descending order) are shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6bfb02-080c-4bfa-83da-01707b6c00ff",
   "metadata": {},
   "source": [
    "```\n",
    "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
    "Top positions: tensor([3, 7, 0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4f807-870f-4286-ae67-b416d82c3436",
   "metadata": {},
   "source": [
    "We then use PyTorch’s where function to set the logit value of the token with a lower logit value than the lowest logit value among the first three options to negative infinity (-inf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c26586-7fbd-476d-86fb-45427ab7c3ca",
   "metadata": {},
   "source": [
    "```\n",
    "new_logits = torch.where(\n",
    "condition=next_token_logits < top_logits[-1], #A\n",
    "input=torch.tensor(float('-inf')), #B\n",
    "other=next_token_logits #C\n",
    ")\n",
    "print(new_logits)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fab7e4-400c-4826-aa3c-4bcd62b3a33e",
   "metadata": {},
   "source": [
    "The resulting log for the next token in the 9-token vocabulary looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209131b-42a6-464e-ac25-531faa237462",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([4.5100, -inf, -inf, 6.7500, -inf, -inf, -inf, 6.2800, -inf])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b95fd-3399-49ea-a208-cd80ab920c22",
   "metadata": {},
   "source": [
    "Finally, use the softmax function to convert these into the next token probability:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ac3a3-4611-4449-9de8-b4f30988daec",
   "metadata": {},
   "source": [
    "```\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2fa765-bb1c-4c96-b894-89d12fb432a5",
   "metadata": {},
   "source": [
    "You can see that the result of this top-3 approach is 3 non-zero probability scores:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e66cc2-8a97-4cd8-868b-36f9d28c38ed",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160d7e3c-3597-4845-8d59-ae1a69a042e4",
   "metadata": {},
   "source": [
    "Now we can use the temperature scaling and polynomial function of probability sampling introduced in the previous section to select the next token from these three non-zero probability scores to generate the next token. In the next section, we will do this by modifying the text generation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628597d5-7b22-4fa6-9bf3-ea6c5deb1c46",
   "metadata": {},
   "source": [
    "## 5.3.3 Modify the text generation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85aea9a-7204-475e-b49f-73362712cbcd",
   "metadata": {},
   "source": [
    "The previous two sections introduced two concepts for increasing the diversity of text generated by LLM: temperature sampling and top-k sampling. In this section, we will combine these concepts to modify the generate_simple function that we previously used to generate text through LLM, creating a new generation function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a684576-5c2b-43c7-9a47-9df46afe14dc",
   "metadata": {},
   "source": [
    "**Listing 5.4 Modified text generation function with more diversity**\n",
    "```\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n",
    "for _ in range(max_new_tokens): #A\n",
    "idx_cond = idx[:, -context_size:]\n",
    "with torch.no_grad():\n",
    "logits = model(idx_cond)\n",
    "logits = logits[:, -1, :]\n",
    "if top_k is not None: #B\n",
    "top_logits, _ = torch.topk(logits, top_k)\n",
    "min_val = top_logits[:, -1]\n",
    "logits = torch.where(\n",
    "logits < min_val,torch.tensor(float('-inf')).to(logits.device),\n",
    "logits\n",
    ")\n",
    "if temperature > 0.0: #C\n",
    "logits = logits / temperature\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "idx_next = torch.multinomial(probs, num_samples=1)\n",
    "else: #D\n",
    "idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "idx = torch.cat((idx, idx_next), dim=1)\n",
    "return idx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d430038-9b1f-4f1c-88c8-9fcad6bd99d9",
   "metadata": {},
   "source": [
    "Let's see this new generator function in action:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835bb96-fe55-40c5-bf31-4733dee794e0",
   "metadata": {},
   "source": [
    "```\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "model=model,\n",
    "idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "max_new_tokens=15,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "top_k=25,\n",
    "temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7aae98-555c-4c5b-802a-4fa918df4df6",
   "metadata": {},
   "source": [
    "The generated text looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f36b8a-6c57-4268-aae6-b4c4c0eb0248",
   "metadata": {},
   "source": [
    "```\n",
    "Output text:\n",
    "Every effort moves you to stand up to work on surprise, a one of us had gone with random\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf42a8-767c-4872-ba03-f707e8a81f6a",
   "metadata": {},
   "source": [
    "You can see that the generated text is quite different from the text generated by the generate_simple function at the beginning of Section 5.3 (taking a record in the training set as an example, \"Every effort moves you know,\" was one of the axioms he laid...!\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c444a-8f6a-425f-8e92-fdad188fdb10",
   "metadata": {},
   "source": [
    "** Exercise 5.2 **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d620e-208c-4247-8c46-10a5c0a42a45",
   "metadata": {},
   "source": [
    "Experiment with different temperature and top-k settings. Based on your observations, can you think of applications where lower temperature and top-k settings are desirable? Vice versa, can you think of scenarios where higher temperature and top-k settings are preferred? (It is recommended to revisit this exercise at the end of this section after loading pretrained weights from OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbb181-9c71-4250-ae81-32015112e32d",
   "metadata": {},
   "source": [
    "** Exercise 5.3 **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f62496-9fe5-4e21-95ac-e56006016eb8",
   "metadata": {},
   "source": [
    "What are the different combinations of settings for the generating function to achieve deterministic behavior (i.e. disabling random sampling so that it always produces close to the same output of the generate_simple function)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614f26a-e06f-46ed-bea9-8a2ebd0e92de",
   "metadata": {},
   "source": [
    "So far, we have introduced how to pre-train LLMs and use them to generate text. The last two sections of this chapter will discuss how we can save and load trained LLMs, and how to load pre-trained weights from OpenAI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
