{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63ef76e",
   "metadata": {},
   "source": [
    "# 5.2 Training an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d79d88",
   "metadata": {},
   "source": [
    "​ In this section, we finally implemented the code for pre-training LLM, our GPTModel. To this end, we focused on a simple training loop, as shown in Figure 5.11, to keep the code concise and easy to read. However, interested readers can learn more advanced techniques, including learning rate warm-up, cosine annealing, and gradient clipping, in Appendix D, Adding Bells and Whistles to the Training Loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16882c7e",
   "metadata": {},
   "source": [
    "Figure 5.11 A typical training loop for training a deep neural network in PyTorch consists of several steps, iterating over batches in the training set for multiple epochs. In each loop, we compute the loss for each training set batch to determine the loss gradient, which we use to update the model weights in order to minimize the training set loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9791e0ca",
   "metadata": {},
   "source": [
    "![image-20240422143154243](../img/fig-5-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf954d",
   "metadata": {},
   "source": [
    "The flowchart in Figure 5.11 describes a typical PyTorch neural network training workflow, which we use to train the LLM. It outlines eight steps, starting with iterating through each epoch, processing the batch, resetting and computing gradients, updating the weights, and finally monitoring steps such as printing the loss and generating text samples. If you are relatively new to training deep neural networks with PyTorch and are not familiar with any of these steps, consider reading sections A.5 to A.8 in Appendix A, Introduction to PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6748c",
   "metadata": {},
   "source": [
    "In the code, we can implement this training process through the following train_model_simple function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29235aa",
   "metadata": {},
   "source": [
    "**Listing 5.3 Main functions of pre-trained LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1deba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "eval_freq, eval_iter, start_context):\n",
    "    \n",
    "\ttrain_losses, val_losses, track_tokens_seen = [], [], [] #A\n",
    "\ttokens_seen, global_step = 0, -1\n",
    "\tfor epoch in range(num_epochs): #B\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() #C\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() #D\n",
    "            optimizer.step() #E\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            if global_step % eval_freq == 0: #F\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample( #G\n",
    "        \tmodel, train_loader.dataset.tokenizer, device, start_context\n",
    "        )\n",
    "\treturn train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5cf34a",
   "metadata": {},
   "source": [
    "​ Note that the train_model_simple function we just created uses two functions that have not yet been defined: evaluate_model and generate_and_print_sample.\n",
    "\n",
    "​ The evaluate_model function corresponds to step 7 in Figure 5.11. It prints the training and validation set losses after each model update so that we can evaluate whether training has improved the model.\n",
    "\n",
    "​ More specifically, the evaluate_model function computes the losses for the training and validation sets while ensuring that the model is in evaluation mode, disabling gradient tracking and dropout when computing the losses for the training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() #A\n",
    "    with torch.no_grad(): #B\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84e85a",
   "metadata": {},
   "source": [
    "​ Similar to evaluate_model, the generate_and_print_sample function is a convenience function that we use to track whether the model is improving during training. Specifically, the generate_and_print_sample function takes as input a text snippet (start_context), converts it to a token ID, and feeds it to the LLM to generate a text sample using the generate_text_simple function we used earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \")) # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122ad4c",
   "metadata": {},
   "source": [
    "While the evaluate_model function gives us a numerical estimate of the model's training progress, this generate_and_print_sample_text function provides concrete examples of text generated by the model to judge its ability during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb5ef2",
   "metadata": {},
   "source": [
    "Adam W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e324e55",
   "metadata": {},
   "source": [
    "The Adam optimizer is a popular choice for training deep neural networks. However, in our training loop, we chose the AdamW optimizer. AdamW is a variant of Adam that improves the weight decay method, aiming to minimize model complexity and prevent overfitting by penalizing larger weights. This adjustment enables AdamW to achieve more effective regularization and better generalization, and is therefore often used for training LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea4934",
   "metadata": {},
   "source": [
    "Let’s see all this in action by training a GPTModel instance for 10 epochs using the AdamW optimizer and the train_model_simple function we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1) #A\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n",
    "    start_context=\"Every effort moves you\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b89adb6",
   "metadata": {},
   "source": [
    "Executing the training_model_simple function will start the training process, which should take about 5 minutes to complete on a MacBook Air or similar laptop. The output printed during this execution is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
    "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
    "Every effort moves you,,,,,,,,,,,,.\n",
    "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
    "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
    "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
    "[...] #A\n",
    "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
    "Every effort moves you?\" \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\" He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
    "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
    "Every effort moves you know,\" was one of the axioms he laid down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd848ba",
   "metadata": {},
   "source": [
    "As we can see, according to the results printed during training, the training loss improved dramatically, starting from a value of 9.558 and converging to 0.762. The language skills of the model have improved a lot. In the beginning, the model could only append commas to the start context (\"Every effort moves you,,,,,,,,,,,\") or repeat the word \"and\". At the end of training, it could generate grammatically correct text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264949e",
   "metadata": {},
   "source": [
    "Similar to the training set loss, we can see that the validation loss starts high (9.856) and decreases during training. However, it never becomes as small as the training set loss and remains at 6.372 after epoch 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d42eaa",
   "metadata": {},
   "source": [
    "Before discussing validation loss in more detail, let’s create a simple plot showing the training and validation loss side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax2 = ax1.twiny() #A\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0) #B\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa2922",
   "metadata": {},
   "source": [
    "The resulting training and validation loss plots are shown in Figure 5.12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840557b",
   "metadata": {},
   "source": [
    "Figure 5.12 At the beginning of training, we observe that both the training set and validation set losses decrease dramatically, indicating that the model is learning. However, the training set loss continues to decrease after the second epoch, while the validation loss stagnates. This indicates that the model is still learning, but it is overfitting to the training set after epoch 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06408771",
   "metadata": {},
   "source": [
    "![image-20240422144030197](..\\img\\fig-5-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cf1c9",
   "metadata": {},
   "source": [
    "As shown in Figure 5.12, during the first epoch, both training and validation losses begin to improve. However, the losses begin to diverge beyond the second epoch. This divergence, along with the fact that validation loss is much larger than training loss, suggests that the model is overfitting the training data. We can confirm that the model has memorized the training data verbatim by searching for generated text snippets, such as “very insensitive to sarcasm” in the “The Verdict” text file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac40e02",
   "metadata": {},
   "source": [
    "This memorization is expected since we are using a very very small training dataset and training the model for multiple epochs. Normally, models are trained on a much larger dataset for only one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c5d36",
   "metadata": {},
   "source": [
    "As mentioned earlier, the interested reader can try training the model on Project Gutenberg’s 60,000 public domain books, where this kind of overfitting does not occur; see Appendix B for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a2943",
   "metadata": {},
   "source": [
    "In the next section, as shown in Figure 5.13, we will explore the sampling method adopted by LLM to alleviate the memory effect and thus generate more novel text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9604871",
   "metadata": {},
   "source": [
    "Figure 5.13 Our model can generate coherent text after implementing the training function. However, it often memorizes paragraphs from the training set verbatim. The following sections describe strategies for generating more diverse output text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937bae6",
   "metadata": {},
   "source": [
    "![image-20240422144152449](../img/fig-5-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963ca66",
   "metadata": {},
   "source": [
    "As shown in Figure 5.13, the next section introduces the text generation strategy of LLM to reduce training data memory and improve the originality of LLM-generated text. Then we introduce weight loading and saving and loading pre-trained weights of the GPT model from OpenAI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
