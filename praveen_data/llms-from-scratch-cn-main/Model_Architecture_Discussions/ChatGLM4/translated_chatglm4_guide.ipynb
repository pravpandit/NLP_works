{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d6ad19-adff-423b-8177-30a0d4f6ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e86302-68c0-455a-9457-6a4618b95cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/data1/ckw'\n",
    "os.environ['HF_ENDPOINT']='https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45076d6-d882-4450-8596-424d364ba65e",
   "metadata": {},
   "source": [
    "First, we construct the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4f30f9-d442-4079-9f0b-04874e9de217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "from torch import TensorType\n",
    "from typing import List, Optional, Union, Dict, Any\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers.utils import logging, PaddingStrategy\n",
    "from transformers.tokenization_utils_base import EncodedInput, BatchEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7537fb-dcc2-47fc-a4da-a33e8a68f1ce",
   "metadata": {},
   "source": [
    "The `ChatGLM4Tokenizer` class is a custom `PreTrainedTokenizer` class used to handle the tokenization needs of the specific GLM-4 model. This class mainly has the following structure and member functions:\n",
    "\n",
    "### Class Structure\n",
    "- **Attributes**:\n",
    "- `vocab_files_names`: A dictionary containing vocabulary file names.\n",
    "- `model_input_names`: A list containing model input names.\n",
    "- `vocab_file`: Vocabulary file path.\n",
    "- `name`: The name of the tokenizer.\n",
    "- `pat_str`: A regular expression string used for tokenization.\n",
    "- `encode_special_tokens`: Whether to encode special characters.\n",
    "- `mergeable_ranks`: Mergeable vocabulary rankings.\n",
    "- `tokenizer`: Encoder based on the `tiktoken` library.\n",
    "- `decoder`: Decoder, mapping vocabulary rankings to vocabulary.\n",
    "- `n_words`: Vocabulary size.\n",
    "\n",
    "### Member function\n",
    "\n",
    "- **Initialization function**:\n",
    "```python\n",
    "def __init__(self, vocab_file, padding_side=\"left\", clean_up_tokenization_spaces=False, encode_special_tokens=False, **kwargs)\n",
    "```\n",
    "Initialize the `ChatGLM4Tokenizer` class, load the vocabulary file, set the regular expression and tokenizer.\n",
    "\n",
    "- **Vocabulary size property**:\n",
    "```python\n",
    "@property\n",
    "def vocab_size(self)\n",
    "```\n",
    "Return the vocabulary size.\n",
    "\n",
    "- **Get vocabulary**:\n",
    "```python\n",
    "def get_vocab(self)\n",
    "```\n",
    "Return the vocabulary dictionary.\n",
    "\n",
    "- **Convert tokens to strings**:\n",
    "```python\n",
    "def convert_tokens_to_string(self, tokens: List[Union[bytes, str]]) -> str\n",
    "```\n",
    "Convert a sequence of tokens to a string.\n",
    "\n",
    "- **Tokenization**:\n",
    "```python\n",
    "def _tokenize(self, text, **kwargs)\n",
    "```\n",
    "Tokenize text into token columnstable.\n",
    "\n",
    "- **Convert token to ID**:\n",
    "```python\n",
    "def _convert_token_to_id(self, token)\n",
    "```\n",
    "Convert a token (string) to an ID.\n",
    "\n",
    "- **Convert ID to token**:\n",
    "```python\n",
    "def _convert_id_to_token(self, index)\n",
    "```\n",
    "Convert an ID (integer) to a token (string).\n",
    "\n",
    "- **Save vocabulary**:\n",
    "```python\n",
    "def save_vocabulary(self, save_directory, filename_prefix=None)\n",
    "```\n",
    "Save the vocabulary to the specified directory.\n",
    "\n",
    "- **Get prefix tokens**:\n",
    "```python\n",
    "def get_prefix_tokens(self)\n",
    "```\n",
    "Return a list of prefix tokens.\n",
    "\n",
    "- **Build a single message**:\n",
    "```python\n",
    "def build_single_message(self, role, metadata, message, tokenize=True)\n",
    "```\n",
    "Build a single message, including roles, metadata, and message content.\n",
    "\n",
    "- **Apply chat template**:\n",
    "```python\n",
    "def apply_chat_template(self, conversation, add_generation_prompt=False, tokenize=True, padding=False, truncation=False, max_length=None, return_tensors=None, return_dict=False, tokenizer_kwargs=None, add_special_tokens=True, **kwargs)\n",
    "```\n",
    "Apply conversation data to the chat template.\n",
    "\n",
    "- **Build inputs with special tokens**:\n",
    "```python\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None)\n",
    "```\n",
    "Build inputs with special tokens.\n",
    "\n",
    "- **Padding inputs**:\n",
    "```python\n",
    "def _pad(self, encoded_inputs, max_length=None, padding_strategy=PaddingStrategy.DO_NOT_PAD, pad_to_multiple_of=None, return_attention_mask=None)\n",
    "```\n",
    "Pad the encoded input to ensure that the input length is consistent.\n",
    "\n",
    "### Overview of the main functions and methods in the class\n",
    "\n",
    "- **Initialization**: The `__init__` function is used to initialize the tokenizer, including loading the vocabulary, compiling regular expressions, and initializing the encoder.\n",
    "- **Get vocabulary and size**: The `get_vocab` and `vocab_size` attributes are used to get the vocabulary and its size.\n",
    "- **Tokenization and conversion**: The `_tokenize`, `convert_tokens_to_string`, `_convert_token_to_id`, and `_convert_id_to_token` functions are used to implement text tokenization and conversion between tokens and IDs.\n",
    "- **Saving and loading**: `save_vocabulary` function is used to save the vocabulary to a specified directory.\n",
    "- **Session handling**: `get_prefThe ix_tokens, build_single_message and apply_chat_template functions are used to process session data and template application.\n",
    "- **Input processing**: The build_inputs_with_special_tokens and _pad functions are used to process model inputs to ensure that the input format and length meet the requirements.\n",
    "\n",
    "Through the above structure and member functions, the ChatGLM4Tokenizer class implements a complete tokenizer function that can handle the tokenization requirements of specific GLM-4 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a119f91-e04b-4f8b-aa2a-02ca20a88a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGLM4Tokenizer(PreTrainedTokenizer):\n",
    "# Define vocabulary file name and model input name\n",
    "    vocab_files_names = {\"vocab_file\": \"tokenizer.model\"}\n",
    "    model_input_names = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_file,\n",
    "            padding_side=\"left\",\n",
    "            clean_up_tokenization_spaces=False,\n",
    "            encode_special_tokens=False,\n",
    "            **kwargs\n",
    "    ):\n",
    "# Initialize some basic properties\n",
    "        self.name = \"GLM4Tokenizer\"\n",
    "        self.vocab_file = vocab_file\n",
    "        \n",
    "# Regular expression pattern string for word segmentation\n",
    "        pat_str = \"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\n",
    "# Compile regular expression\n",
    "        self.pat_str = regex.compile(pat_str)\n",
    "# self.pat_str = re.compile(pat_str)\n",
    "# Whether to encode special characters\n",
    "        self.encode_special_tokens = encode_special_tokens\n",
    "\n",
    "# Used to store mergeable word rankings\n",
    "        mergeable_ranks = {}\n",
    "# Read vocabulary file\n",
    "        with open(vocab_file) as f:\n",
    "            for line in f:\n",
    "                token, rank = line.strip().split()  # 读取每一行，获取词汇和其对应的排名\n",
    "                rank = int(rank)  # 将排名转换为整数\n",
    "                token = base64.b64decode(token)  # 解码词汇\n",
    "                mergeable_ranks[token] = rank  # 存储到词汇排名字典中\n",
    "\n",
    "        self.mergeable_ranks = mergeable_ranks\n",
    "\n",
    "# Initialize the encoder, using the tiktoken library\n",
    "        self.tokenizer = tiktoken.Encoding(\n",
    "            name=\"my_tokenizer\",\n",
    "            pat_str=pat_str,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens={}\n",
    "        )\n",
    "# decoder, mapping ranking to vocabulary\n",
    "        self.decoder = {rank: token for token, rank in mergeable_ranks.items()}\n",
    "# Vocabulary size\n",
    "        self.n_words = len(self.decoder)\n",
    "\n",
    "#Call the initialization method of the parent class\n",
    "        super().__init__(\n",
    "            padding_side=padding_side,\n",
    "            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "# Return the vocabulary size\n",
    "        return self.n_words\n",
    "\n",
    "    def get_vocab(self):\n",
    "\"\"\" Returns the vocabulary dictionary \"\"\"\n",
    "        vocab = {self._convert_id_to_token(i): i for i in range(self.vocab_size)}\n",
    "        vocab.update(self.added_tokens_encoder)\n",
    "        return vocab\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens: List[Union[bytes, str]]) -> str:\n",
    "        \"\"\"\n",
    "        将 tokens 序列转换为字符串。\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        temp = b\"\"\n",
    "        for t in tokens:\n",
    "            if isinstance(t, str):\n",
    "                if temp:\n",
    "                    text += temp.decode(\"utf-8\", errors=\"replace\")\n",
    "                    temp = b\"\"\n",
    "                text += t\n",
    "            elif isinstance(t, bytes):\n",
    "                temp += t\n",
    "            else:\n",
    "                raise TypeError(\"token should only be of type bytes or str\")\n",
    "        if temp:\n",
    "            text += temp.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "    def _tokenize(self, text, **kwargs):\n",
    "# Use regular expressions and encoders to tokenize text\n",
    "        tokens = []\n",
    "        ids = self.tokenizer.encode(text)\n",
    "        for t in ids:\n",
    "            tokens.append(self.decoder[t])\n",
    "        return tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "\"\"\" Convert token (string) to id (integer) \"\"\"\n",
    "        return self.mergeable_ranks[token]\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "\"\"\" Convert id (integer) to token (string) \"\"\"\n",
    "        return self.decoder.get(index, \"\")\n",
    "\n",
    "    def save_vocabulary(self, save_directory, filename_prefix=None):\n",
    "        \"\"\"\n",
    "        将词汇表和特殊字符文件保存到指定目录。\n",
    "\n",
    "        Args:\n",
    "            save_directory (`str`): 保存词汇表的目录。\n",
    "            filename_prefix (`str`, *optional*): 保存文件名的前缀。\n",
    "\n",
    "        Returns:\n",
    "            `Tuple(str)`: 保存的文件路径。\n",
    "        \"\"\"\n",
    "        if os.path.isdir(save_directory):\n",
    "            vocab_file = os.path.join(\n",
    "                save_directory, self.vocab_files_names[\"vocab_file\"]\n",
    "            )\n",
    "        else:\n",
    "            vocab_file = save_directory\n",
    "\n",
    "        with open(self.vocab_file, 'rb') as fin:\n",
    "            proto_str = fin.read()\n",
    "\n",
    "        with open(vocab_file, \"wb\") as writer:\n",
    "            writer.write(proto_str)\n",
    "\n",
    "        return (vocab_file,)\n",
    "\n",
    "    def get_prefix_tokens(self):\n",
    "# Return a list of prefix tokens\n",
    "        prefix_tokens = [self.convert_tokens_to_ids(\"[gMASK]\"), self.convert_tokens_to_ids(\"<sop>\")]\n",
    "        return prefix_tokens\n",
    "\n",
    "    def build_single_message(self, role, metadata, message, tokenize=True):\n",
    "# Build a single message, including roles, metadata, and message content\n",
    "        assert role in [\"system\", \"user\", \"assistant\", \"observation\"], role\n",
    "        if tokenize:\n",
    "            role_tokens = [self.convert_tokens_to_ids(f\"<|{role}|>\")] + self.tokenizer.encode(f\"{metadata}\\n\",\n",
    "                                                                                              disallowed_special=())\n",
    "            message_tokens = self.tokenizer.encode(message, disallowed_special=())\n",
    "            tokens = role_tokens + message_tokens\n",
    "            return tokens\n",
    "        else:\n",
    "            return str(f\"<|{role}|>{metadata}\\n{message}\")\n",
    "\n",
    "    def apply_chat_template(\n",
    "            self,\n",
    "            conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]], \"Conversation\"],\n",
    "            add_generation_prompt: bool = False,\n",
    "            tokenize: bool = True,\n",
    "            padding: bool = False,\n",
    "            truncation: bool = False,\n",
    "            max_length: Optional[int] = None,\n",
    "            return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "            return_dict: bool = False,\n",
    "            tokenizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "            add_special_tokens: bool = True,\n",
    "            **kwargs,\n",
    "    ) -> Union[str, List[int], List[str], List[List[int]], BatchEncoding]:\n",
    "    \n",
    "        if return_dict and not tokenize:\n",
    "            raise ValueError(\n",
    "                \"`return_dict=True` is incompatible with `tokenize=False`, because there is no dict \"\n",
    "                \"of tokenizer outputs to return.\"\n",
    "            )\n",
    "    \n",
    "        def handle_single_conversation(conversation):\n",
    "            input_ids = self.get_prefix_tokens() if add_special_tokens else []\n",
    "            input_message = \"[gMASK]<sop>\" if add_special_tokens else \"\"\n",
    "            for item in conversation:\n",
    "                if item.get(\"tools\"):\n",
    "                    tools = item[\"tools\"]\n",
    "                    content = \"你是一个名为 GLM-4 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\"\n",
    "                    for tool in tools:\n",
    "                        if tool[\"type\"] == \"function\":\n",
    "                            function = tool[\"function\"]\n",
    "                            content += f\"\\n\\n## {function['name']}\\n\\n{json.dumps(function, ensure_ascii=False, indent=4)}\"\n",
    "                            content += \"\\n在调用上述函数时，请使用 Json 格式表示调用的参数。\"\n",
    "                        elif tool[\"type\"] == \"python\":\n",
    "                            content += \"\\n\\n## python\\n\\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。\"\n",
    "                        elif tool[\"type\"] == \"simple_browser\":\n",
    "                            content += \"\\n\\n## simple_browser\\n\\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\\n`open_url(url: str)`：打开指定的 URL。\\n\\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\\n\\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。\"\n",
    "                        elif tool[\"type\"] == \"cogview\":\n",
    "                            content += \"\\n\\n## cogview\\n\\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。\"\n",
    "                        else:\n",
    "                            raise NotImplementedError(f\"Unknown tool type {tool['type']}\")\n",
    "                    input = self.build_single_message(\"system\", \"\", content, tokenize=tokenize)\n",
    "                    if tokenize:\n",
    "                        input_ids.extend(input)\n",
    "                    else:\n",
    "                        input_message += input\n",
    "                if item[\"content\"]:\n",
    "                    input = self.build_single_message(\n",
    "                        item[\"role\"],\n",
    "                        item.get(\"metadata\", \"\"),\n",
    "                        item[\"content\"],\n",
    "                        tokenize=tokenize\n",
    "                    )\n",
    "                    if tokenize:\n",
    "                        input_ids.extend(input)\n",
    "                    else:\n",
    "                        input_message += input\n",
    "            if add_generation_prompt:\n",
    "                if tokenize:\n",
    "                    input_ids.extend([self.convert_tokens_to_ids(\"<|assistant|>\")])\n",
    "                else:\n",
    "                    input_message += \"<|assistant|>\"\n",
    "# if tokenize:\n",
    "# input_ids.extend([self.convert_tokens_to_ids(\"[gMASK]\")]) # Use special tokens instead of empty strings\n",
    "# else:\n",
    "# input_message += \"[gMASK]\"\n",
    "    \n",
    "            return input_ids if tokenize else input_message\n",
    "    \n",
    "# Main logic for handling different session formats\n",
    "        if isinstance(conversation, list) and all(isinstance(i, dict) for i in conversation):\n",
    "            result = handle_single_conversation(conversation)\n",
    "        elif isinstance(conversation, list) and all(isinstance(i, list) for i in conversation):\n",
    "            result = [handle_single_conversation(c) for c in conversation]\n",
    "        elif hasattr(conversation, \"messages\"):\n",
    "            result = handle_single_conversation(conversation.messages)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid conversation format\")\n",
    "    \n",
    "        if tokenize:\n",
    "            output = self.batch_encode_plus(\n",
    "                [result] if isinstance(result[0], int) else result,\n",
    "                padding=padding,\n",
    "                truncation=truncation,\n",
    "                max_length=max_length,\n",
    "                return_tensors=return_tensors,\n",
    "                is_split_into_words=True,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            if return_dict:\n",
    "                return output\n",
    "            else:\n",
    "                return output[\"input_ids\"]\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "            self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        构建模型输入，适用于序列分类任务，通过连接和添加特殊 tokens。\n",
    "        BERT 序列格式：\n",
    "        - 单序列: `[CLS] X [SEP]`\n",
    "        - 序列对: `[CLS] A [SEP] B [SEP]`\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`): 要添加特殊 tokens 的 ID 列表。\n",
    "            token_ids_1 (`List[int]`, *optional*): 可选的第二个 ID 列表，表示序列对。\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: 添加了适当特殊 tokens 的输入 ID 列表。\n",
    "        \"\"\"\n",
    "        prefix_tokens = self.get_prefix_tokens()\n",
    "        token_ids_0 = prefix_tokens + token_ids_0\n",
    "        if token_ids_1 is not None:\n",
    "            token_ids_0 = token_ids_0 + token_ids_1 + [self.convert_tokens_to_ids(\"<eos>\")]\n",
    "        return token_ids_0\n",
    "\n",
    "    def _pad(\n",
    "            self,\n",
    "            encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],\n",
    "            max_length: Optional[int] = None,\n",
    "            padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n",
    "            pad_to_multiple_of: Optional[int] = None,\n",
    "            return_attention_mask: Optional[bool] = None,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        填充编码后的输入（左右填充，并预定义长度或批处理中的最大长度）\n",
    "\n",
    "        Args:\n",
    "            encoded_inputs: 包含编码后的输入 (`List[int]`) 或批处理的输入 (`List[List[int]]`) 的字典。\n",
    "            max_length: 返回列表的最大长度，并可选的填充长度。\n",
    "            padding_strategy: 用于填充的策略。\n",
    "                - PaddingStrategy.LONGEST: 填充到批处理中的最长序列\n",
    "                - PaddingStrategy.MAX_LENGTH: 填充到最大长度（默认）\n",
    "                - PaddingStrategy.DO_NOT_PAD: 不填充\n",
    "                填充策略由 self.padding_side 定义：\n",
    "                    - 'left': 在序列左侧填充\n",
    "                    - 'right': 在序列右侧填充\n",
    "            pad_to_multiple_of: (可选) 整数，如果设置将填充序列为该值的倍数。\n",
    "            return_attention_mask: (可选) 设置为 False 以避免返回注意力掩码（默认：根据模型具体情况设置）\n",
    "        \"\"\"\n",
    "# Make sure the fill side is 'left'\n",
    "        assert self.padding_side == \"left\"\n",
    "\n",
    "        required_input = encoded_inputs[self.model_input_names[0]]\n",
    "        seq_length = len(required_input)\n",
    "\n",
    "        if padding_strategy == PaddingStrategy.LONGEST:\n",
    "            max_length = len(required_input)\n",
    "\n",
    "        if max_length is not None and pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "\n",
    "        needs_to_be_padded = padding_strategy != PaddingStrategy.DO_NOT_PAD and len(required_input) != max_length\n",
    "\n",
    "# If there is no attention mask, initialize\n",
    "        if \"attention_mask\" not in encoded_inputs:\n",
    "            encoded_inputs[\"attention_mask\"] = [1] * seq_length\n",
    "\n",
    "        if \"position_ids\" not in encoded_inputs:\n",
    "            encoded_inputs[\"position_ids\"] = list(range(seq_length))\n",
    "\n",
    "        if needs_to_be_padded:\n",
    "            difference = max_length - len(required_input)\n",
    "\n",
    "            if \"attention_mask\" in encoded_inputs:\n",
    "                encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n",
    "            if \"position_ids\" in encoded_inputs:\n",
    "                encoded_inputs[\"position_ids\"] = [0] * difference + encoded_inputs[\"position_ids\"]\n",
    "            encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n",
    "\n",
    "        return encoded_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7c0c2-83eb-4f01-a9c6-0b00ef9d05b2",
   "metadata": {},
   "source": [
    "### Key Points Explanation\n",
    "\n",
    "1. **`mergeable_ranks`**:\n",
    "- Stores ranking information for mergeable words. When reading the vocabulary file, each word has a corresponding rank (integer), which is decoded into the original string and then stored in the `mergeable_ranks` dictionary.\n",
    "\n",
    "2. **`pat_str`**:\n",
    "- Regular expression string used to define the word segmentation pattern. This regular expression is mainly used to match common English abbreviations, words, numbers and other non-alphanumeric characters. The compiled regular expression is stored in `self.pat_str` for the tokenizer.\n",
    "\n",
    "3. **`self.tokenizer`**:\n",
    "- Encoder created using the `tiktoken` library. Initialize a custom tokenizer with the provided regular expression pattern and mergeable vocabulary ranking.\n",
    "\n",
    "4. **`self.decoder`**:\n",
    "- Decoder, mapping vocabulary ranking to vocabulary. The inverse dictionary created using the `mergeable_ranks` dictionary is used to convert from ID back to the original vocabulary.\n",
    "\n",
    "5. **`self.n_words`**:\n",
    "- Vocabulary size, i.e. the number of words. Obtained by calculating the length of `self.decoder`.\n",
    "\n",
    "6. **Initialization process**：\n",
    "- Read the vocabulary file, parse each line to get the vocabulary and its ranking, then decode the vocabulary and store it in the `mergeable_ranks` dictionary. Next, use the `tiktoken` library to initialize the encoder and create the decoder and vocabulary size.\n",
    "\n",
    "There is one thing worth noting:\n",
    "```python\n",
    "def convert_tokens_to_string(self, tokens: List[Union[bytes, str]]) -> str:\n",
    "\"\"\"\n",
    "Convert a sequence of tokens to a string.\n",
    "\"\"\"\n",
    "text = \"\"\n",
    "temp = b\"\"\n",
    "for t in tokens:\n",
    "if isinstance(t, str):\n",
    "if temp:\n",
    "text += temp.decode(\"utf-8\", errors=\"replace\")\n",
    "temp = b\"\"\n",
    "text += telif isinstance(t, bytes):\n",
    "temp += t\n",
    "else:\n",
    "raise TypeError(\"token should only be of type bytes or str\")\n",
    "if temp:\n",
    "text += temp.decode(\"utf-8\", errors=\"replace\")\n",
    "return text\n",
    "```\n",
    "\n",
    "This function decodes `token` to `bytes` because in real applications, tokens may be part of a string or a byte sequence. In some cases, tokens in the vocabulary may be stored in the form of `bytes` instead of directly stored as strings. Here are some detailed reasons and scenarios:\n",
    "\n",
    "1. **Support multiple encoding formats**:\n",
    "- The tokenizer may process multiple data sources, some of which may store tokens in `bytes` format. For example, when the data is compressed, encrypted, or uses a specific encoding, the token may be represented in bytes.\n",
    "\n",
    "2. **DataFlexibility in processing**:\n",
    "- By supporting both `bytes` and `str` types, the tokenizer can more flexibly handle input data from different sources and formats. This is particularly useful when processing text containing binary data, such as some special tokens or characters.\n",
    "\n",
    "3. **Ensure data consistency**:\n",
    "- During the tokenization and decoding process, mixed-type tokens (i.e. both strings and byte sequences) may be encountered. In order to ensure data consistency and correctly concatenate strings, the function needs to handle both `bytes` and `str` types.\n",
    "\n",
    "4. **Compatibility considerations**:\n",
    "- Some NLP tools and libraries may return tokens in byte form when processing text. In order to be compatible with these tools and libraries, the tokenizer needs to be able to process and convert these byte tokens.\n",
    "\n",
    "Let's take a look at the workflow of this function in detail:\n",
    "\n",
    "1. **Initialize empty string and byte sequence**:\n",
    "- `text = \"\"` initializes an empty string to store the final result.\n",
    "- `temp = b\"\"` initializes an empty byte sequence to temporarily store byte tokens.\n",
    "\n",
    "2. **Traverse the token list**:\n",
    "- For each token, check its type.\n",
    "- If the token is a string type (`str`), check `temp` is empty. If `temp` is not empty, decode `temp` into a string and add it to `text`, then clear `temp`. After that, add the current string token to `text`.\n",
    "- If token is of byte type (`bytes`), add it to `temp` for subsequent decoding.\n",
    "- If token is neither a string nor a byte type, throw a type error.\n",
    "\n",
    "3. **Process the remaining byte sequence**:\n",
    "- After the loop ends, if `temp` is not empty, decode it into a string and add it to `text`.\n",
    "\n",
    "Through the above steps, the function can correctly handle a mixed type token list and convert it into a complete string. Here are the comments in the code to better explain these steps:\n",
    "\n",
    "```python\n",
    "def convert_tokens_to_string(self, tokens: List[Union[bytes, str]]) -> str:\n",
    "\"\"\"\n",
    "Convert the tokens sequence to a string.\n",
    "\"\"\"\n",
    "text = \"\" # Initialize an empty string to store the result\n",
    "temp = b\"\" # Initialize an empty byte sequenceTemporarily store byte tokens\n",
    "for t in tokens:\n",
    "if isinstance(t, str):\n",
    "if temp:\n",
    "# If temp is not empty, decode it to a string and add it to text\n",
    "text += temp.decode(\"utf-8\", errors=\"replace\")\n",
    "temp = b\"\" # Clear temp\n",
    "text += t # Add string token to text\n",
    "elif isinstance(t, bytes):\n",
    "temp += t # Add byte token to temp\n",
    "else:\n",
    "raise TypeError(\"token should only be of type bytes or str\") # Throw a type error\n",
    "if temp:\n",
    "# Process the remaining byte sequence and decode it toString and add it to text\n",
    "text += temp.decode(\"utf-8\", errors=\"replace\")\n",
    "return text # Return the final string\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c439bb2-5250-4891-973d-f973bd14aae3",
   "metadata": {},
   "source": [
    "Next, set the config and simply set the relevant properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac8bdb0a-b279-4365-b6bb-224b0fe43d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "class ChatGLMConfig(PretrainedConfig):\n",
    "    model_type = \"chatglm\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_layers=28,\n",
    "            padded_vocab_size=65024,\n",
    "            hidden_size=4096,\n",
    "            ffn_hidden_size=13696,\n",
    "            kv_channels=128,\n",
    "            num_attention_heads=32,\n",
    "            seq_length=2048,\n",
    "            hidden_dropout=0.0,\n",
    "            classifier_dropout=None,\n",
    "            attention_dropout=0.0,\n",
    "            layernorm_epsilon=1e-5,\n",
    "            rmsnorm=True,\n",
    "            apply_residual_connection_post_layernorm=False,\n",
    "            post_layer_norm=True,\n",
    "            add_bias_linear=False,\n",
    "            add_qkv_bias=False,\n",
    "            bias_dropout_fusion=True,\n",
    "            multi_query_attention=False,\n",
    "            multi_query_group_num=1,\n",
    "            rope_ratio=1,\n",
    "            apply_query_key_layer_scaling=True,\n",
    "            attention_softmax_in_fp32=True,\n",
    "            fp32_residual_connection=False,\n",
    "            **kwargs\n",
    "    ):\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = padded_vocab_size\n",
    "        self.padded_vocab_size = padded_vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_hidden_size = ffn_hidden_size\n",
    "        self.kv_channels = kv_channels\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_dropout = hidden_dropout\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.layernorm_epsilon = layernorm_epsilon\n",
    "        self.rmsnorm = rmsnorm\n",
    "        self.apply_residual_connection_post_layernorm = apply_residual_connection_post_layernorm\n",
    "        self.post_layer_norm = post_layer_norm\n",
    "        self.add_bias_linear = add_bias_linear\n",
    "        self.add_qkv_bias = add_qkv_bias\n",
    "        self.bias_dropout_fusion = bias_dropout_fusion\n",
    "        self.multi_query_attention = multi_query_attention\n",
    "        self.multi_query_group_num = multi_query_group_num\n",
    "        self.rope_ratio = rope_ratio\n",
    "        self.apply_query_key_layer_scaling = apply_query_key_layer_scaling\n",
    "        self.attention_softmax_in_fp32 = attention_softmax_in_fp32\n",
    "        self.fp32_residual_connection = fp32_residual_connection\n",
    "        super().__init__(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936d603-79a0-41ae-8777-185c5562e0bb",
   "metadata": {},
   "source": [
    "Then we can start building our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "851a39f0-1cad-48c2-a0ce-b781ed98c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import copy\n",
    "import warnings\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, LayerNorm, MSELoss, BCEWithLogitsLoss\n",
    "from torch.nn.utils import skip_init\n",
    "from typing import Optional, Tuple, Union, List, Callable, Dict, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    "    SequenceClassifierOutputWithPast,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import logging\n",
    "from transformers.generation.logits_process import LogitsProcessor\n",
    "from transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList, GenerationConfig, ModelOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ff61078-3447-4b2d-a77a-034000ce38d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linux'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eaa943a3-5a6a-4f5a-9d1c-a1d2619e090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the system platform is not Darwin (i.e. macOS or Linux), set the following to improve performance\n",
    "if sys.platform != 'darwin':\n",
    "# Disable JIT profiling mode\n",
    "    torch._C._jit_set_profiling_mode(False)\n",
    "# Disable JIT profiling executor\n",
    "    torch._C._jit_set_profiling_executor(False)\n",
    "# Enable tensor fusion on CPU\n",
    "    torch._C._jit_override_can_fuse_on_cpu(True)\n",
    "# Enable tensor fusion on GPU\n",
    "    torch._C._jit_override_can_fuse_on_gpu(True)\n",
    "\n",
    "# Get the logger\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "# Checkpoints for documentation\n",
    "_CHECKPOINT_FOR_DOC = \"THUDM/ChatGLM\"\n",
    "# Configuration for documentation\n",
    "_CONFIG_FOR_DOC = \"ChatGLMConfig\"\n",
    "\n",
    "# Default initialization function\n",
    "def default_init(cls, *args, **kwargs):\n",
    "    return cls(*args, **kwargs)\n",
    "\n",
    "# Invalid Score Logits Processor Class\n",
    "class InvalidScoreLogitsProcessor(LogitsProcessor):\n",
    "# Process input logits\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "# If there are NaN or infinity in logits, reset them\n",
    "        if torch.isnan(scores).any() or torch.isinf(scores).any():\n",
    "            scores.zero_()  # 将所有分数重置为 0\n",
    "            scores[..., 198] = 5e4  # 将特定位置的分数设置为一个大值\n",
    "        return scores\n",
    "\n",
    "# Function to split a tensor along the last dimension\n",
    "def split_tensor_along_last_dim(\n",
    "        tensor: torch.Tensor,\n",
    "        num_partitions: int,\n",
    "        contiguous_split_chunks: bool = False,\n",
    ") -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    沿着最后一个维度分割张量。\n",
    "\n",
    "    参数:\n",
    "        tensor: 输入张量。\n",
    "        num_partitions: 要分割的部分数量。\n",
    "        contiguous_split_chunks: 如果为 True，则使每个分块在内存中是连续的。\n",
    "\n",
    "    返回:\n",
    "        张量的列表\n",
    "    \"\"\"\n",
    "# Get the size of the last dimension\n",
    "    last_dim = tensor.dim() - 1\n",
    "    last_dim_size = tensor.size()[last_dim] // num_partitions\n",
    "# Split tensor\n",
    "    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n",
    "# NOTE: torch.split does not create contiguous tensors by default\n",
    "    if contiguous_split_chunks:\n",
    "        return tuple(chunk.contiguous() for chunk in tensor_list)\n",
    "    return tensor_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52adb283-3a15-4a82-a3e5-b80f2066c891",
   "metadata": {},
   "source": [
    "### Key points explanation\n",
    "\n",
    "1. **System platform check**:\n",
    "- If the system platform is not `Darwin` (macOS), turn off the profiling mode and executor of the JIT, and enable tensor fusion on the CPU and GPU. These settings can improve the performance of the model.\n",
    "\n",
    "2. **Logger**:\n",
    "- Get a logger for logging information and debugging during model building.\n",
    "\n",
    "3. **Document string**:\n",
    "- `_CHECKPOINT_FOR_DOC` and `_CONFIG_FOR_DOC` are string constants for document generation, indicating the checkpoint and configuration files of the model and configuration.\n",
    "\n",
    "4. **Default initialization function**:\n",
    "- The `default_init` function is a general initialization function used to instantiate the class.\n",
    "\n",
    "5. **`InvalidScoreLogitsProcessor` class**:\n",
    "- This is a logits processor class used to handle invalid logits scores. If there are `NaN` or infinite values ​​in the scores, reset them to 0, and set the scores of specific positions (such as 198) to a large value (`5e4`) to ensure that the model outputs valid results.\n",
    "\n",
    "6. **`split_tensor_`along_last_dim` function:\n",
    "- This function is used to split a tensor along its last dimension. The parameters include the input tensor, the number of parts to split, and a boolean value (whether to make each chunk contiguous in memory). If `contiguous_split_chunks` is true, each chunk is contiguous in memory; otherwise, a list of split tensors is returned directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f8005f7-1339-40b4-b103-6bf0396b371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation position embedding class\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, rope_ratio=1, original_impl=False, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "# Calculate the reciprocal frequency\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device).to(dtype=dtype) / dim))\n",
    "# Register the countdown frequency as buffer\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.dim = dim\n",
    "        self.original_impl = original_impl\n",
    "        self.rope_ratio = rope_ratio\n",
    "\n",
    "# Specific method to implement forward propagation\n",
    "    def forward_impl(\n",
    "            self, seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        增强的 Transformer 使用旋转位置嵌入。\n",
    "\n",
    "        参考自:\n",
    "        https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
    "        transformers/rope/__init__.py. MIT 许可证:\n",
    "        https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
    "        \"\"\"\n",
    "# Calculate $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "        base = base * self.rope_ratio\n",
    "        theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=torch.float, device=device) / n_elem))\n",
    "\n",
    "# Create position index `[0, 1, ..., seq_len - 1]`\n",
    "        seq_idx = torch.arange(seq_len, dtype=torch.float, device=device)\n",
    "\n",
    "# Calculate the product of the position index and $\\theta_i$\n",
    "        idx_theta = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "# Cache calculated cosine and sine values\n",
    "        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "\n",
    "# Emulate the behavior of complex32, otherwise you will get different results\n",
    "        if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "            cache = cache.bfloat16() if dtype == torch.bfloat16 else cache.half()\n",
    "        return cache\n",
    "\n",
    "# Forward propagation method\n",
    "    def forward(self, max_seq_len, offset=0):\n",
    "        return self.forward_impl(\n",
    "            max_seq_len, self.dim, dtype=self.inv_freq.dtype, device=self.inv_freq.device\n",
    "        )\n",
    "\n",
    "# Using the TorchScript JIT compiler to optimize the rotation position embedding application function\n",
    "@torch.jit.script\n",
    "def apply_rotary_pos_emb(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:\n",
    "# x: [b, np, sq, hn]\n",
    "    b, np, sq, hn = x.size(0), x.size(1), x.size(2), x.size(3)\n",
    "    rot_dim = rope_cache.shape[-2] * 2\n",
    "    x, x_pass = x[..., :rot_dim], x[..., rot_dim:]\n",
    "# Truncating to support variable sizes\n",
    "    rope_cache = rope_cache[:, :sq]\n",
    "    xshaped = x.reshape(b, np, sq, rot_dim // 2, 2)\n",
    "    rope_cache = rope_cache.view(-1, 1, sq, xshaped.size(3), 2)\n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    return torch.cat((x_out2, x_pass), dim=-1)\n",
    "\n",
    "# RMS normalization layer class\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, device=None, dtype=None, **kwargs):\n",
    "        super().__init__()\n",
    "# Initialize weight parameters\n",
    "        self.weight = torch.nn.Parameter(torch.empty(normalized_shape, device=device, dtype=dtype))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor):\n",
    "# Get the data type of the input tensor\n",
    "        input_dtype = hidden_states.dtype\n",
    "# Calculate variance\n",
    "        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n",
    "# Perform RMS normalization\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n",
    "# Apply weights and return the same data type as input\n",
    "        return (self.weight * hidden_states).to(input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f66720-11b0-4bc5-9468-083e952ce6de",
   "metadata": {},
   "source": [
    "### Key Points Explanation\n",
    "\n",
    "1. **`RotaryEmbedding` class**:\n",
    "- A module for implementing rotational position embedding.\n",
    "- `inv_freq` is a tensor of inverse frequency used for position embedding calculation.\n",
    "- `forward_impl` method calculates rotational position embedding based on sequence length and embedding dimension.\n",
    "- `forward` method calls `forward_impl` for forward propagation.\n",
    "\n",
    "2. **`apply_rotary_pos_emb` function**:\n",
    "- Update the input tensor with rotational position embedding.\n",
    "- `x` is the input tensor, `rope_cache` is the precomputed rotational position embedding.\n",
    "- The function first splits and reshapes the input tensor, then applies the rotational position embedding to each block, and finally splices the result back to the original tensor.\n",
    "\n",
    "3. **`RMSNorm` class**:\n",
    "- A module for implementing RMS normalization.\n",
    "- `weight` is the normalized weight parameter.\n",
    "- `forward` method calculates the variance of the input tensor, normalizes it, and then applies the weights.\n",
    "\n",
    "These comments and explanations can help understand the functions and implementation details of each part, which is very useful for model building and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b76dc446-e37e-4c1f-94f1-96f209bd674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreAttention(torch.nn.Module):\n",
    "    def __init__(self, config: ChatGLMConfig, layer_number):\n",
    "        super(CoreAttention, self).__init__()\n",
    "\n",
    "# Configuration parameters\n",
    "        self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling\n",
    "        self.attention_softmax_in_fp32 = config.attention_softmax_in_fp32\n",
    "        if self.apply_query_key_layer_scaling:\n",
    "            self.attention_softmax_in_fp32 = True\n",
    "        self.layer_number = max(1, layer_number)\n",
    "\n",
    "# Calculate the projection size\n",
    "        projection_size = config.kv_channels * config.num_attention_heads\n",
    "\n",
    "# The value of each attention head and each partition\n",
    "        self.hidden_size_per_partition = projection_size\n",
    "        self.hidden_size_per_attention_head = projection_size // config.num_attention_heads\n",
    "        self.num_attention_heads_per_partition = config.num_attention_heads\n",
    "\n",
    "        coeff = None\n",
    "        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)\n",
    "        if self.apply_query_key_layer_scaling:\n",
    "            coeff = self.layer_number\n",
    "            self.norm_factor *= coeff\n",
    "        self.coeff = coeff\n",
    "\n",
    "# Attention dropout\n",
    "        self.attention_dropout = torch.nn.Dropout(config.attention_dropout)\n",
    "\n",
    "    def forward(self, query_layer, key_layer, value_layer, attention_mask):\n",
    "        pytorch_major_version = int(torch.__version__.split('.')[0])\n",
    "        if pytorch_major_version >= 2:\n",
    "# PyTorch 2.0 and above\n",
    "            if attention_mask is None and query_layer.shape[2] == key_layer.shape[2]:\n",
    "                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,\n",
    "                                                                                 is_causal=True)\n",
    "            else:\n",
    "                if attention_mask is not None:\n",
    "                    attention_mask = ~attention_mask\n",
    "                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,\n",
    "                                                                                 attention_mask)\n",
    "            context_layer = context_layer.transpose(1, 2).contiguous()\n",
    "            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)\n",
    "            context_layer = context_layer.reshape(*new_context_layer_shape)\n",
    "        else:\n",
    "# Handling PyTorch versions below 2.0\n",
    "\n",
    "# Raw attention score\n",
    "# [b, np, sq, sk]\n",
    "            output_size = (query_layer.size(0), query_layer.size(1), query_layer.size(2), key_layer.size(2))\n",
    "\n",
    "# Resize the view [b, np, sq, hn] -> [b * np, sq, hn]\n",
    "            query_layer = query_layer.view(output_size[0] * output_size[1], output_size[2], -1)\n",
    "# Resize the view [b, np, sk, hn] -> [b * np, sk, hn]\n",
    "            key_layer = key_layer.view(output_size[0] * output_size[1], output_size[3], -1)\n",
    "\n",
    "# Pre-allocate input tensor: [b * np, sq, sk]\n",
    "            matmul_input_buffer = torch.empty(\n",
    "                output_size[0] * output_size[1], output_size[2], output_size[3], dtype=query_layer.dtype,\n",
    "                device=query_layer.device\n",
    "            )\n",
    "\n",
    "# Calculate raw attention scores. [b * np, sq, sk]\n",
    "            matmul_result = torch.baddbmm(\n",
    "                matmul_input_buffer,\n",
    "                query_layer,  # [b * np, sq, hn]\n",
    "                key_layer.transpose(1, 2),  # [b * np, hn, sk]\n",
    "                beta=0.0,\n",
    "                alpha=(1.0 / self.norm_factor),\n",
    "            )\n",
    "\n",
    "# Change the view to [b, np, sq, sk]\n",
    "            attention_scores = matmul_result.view(*output_size)\n",
    "\n",
    "# Handling attention scores and dropout\n",
    "            if self.attention_softmax_in_fp32:\n",
    "                attention_scores = attention_scores.float()\n",
    "            if self.coeff is not None:\n",
    "                attention_scores = attention_scores * self.coeff\n",
    "            if attention_mask is None and attention_scores.shape[2] == attention_scores.shape[3]:\n",
    "                attention_mask = torch.ones(output_size[0], 1, output_size[2], output_size[3],\n",
    "                                            device=attention_scores.device, dtype=torch.bool)\n",
    "                attention_mask.tril_()\n",
    "                attention_mask = ~attention_mask\n",
    "            if attention_mask is not None:\n",
    "                attention_scores = attention_scores.masked_fill(attention_mask, float(\"-inf\"))\n",
    "            attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "            attention_probs = attention_probs.type_as(value_layer)\n",
    "\n",
    "# Drop the attention of the entire token, this comes from the original Transformer paper\n",
    "            attention_probs = self.attention_dropout(attention_probs)\n",
    "\n",
    "# Resize the view [b * np, sq, hn]\n",
    "            value_layer = value_layer.view(output_size[0] * output_size[1], value_layer.size(2), -1)\n",
    "# Resize the view [b * np, sq, sk]\n",
    "            attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], -1)\n",
    "# Calculate the context layer [b * np, sq, hn]\n",
    "            context_layer = torch.bmm(attention_probs, value_layer)\n",
    "# Resize the view [b, np, sq, hn]\n",
    "            context_layer = context_layer.view(*output_size)\n",
    "# [b, np, sq, hn] --> [b, sq, np, hn]\n",
    "            context_layer = context_layer.transpose(1, 2).contiguous()\n",
    "# [b, sq, np, hn] --> [b, sq, hp]\n",
    "            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)\n",
    "            context_layer = context_layer.reshape(*new_context_layer_shape)\n",
    "\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f04a8-0373-4a78-baa9-52e665fe604d",
   "metadata": {},
   "source": [
    "### CoreAttention Detailed Explanation and Formula\n",
    "\n",
    "In neural network models, especially Transformer architecture, attention mechanism plays a vital role. CoreAttention implements the core part of the self-attention mechanism, which includes the following steps: calculating attention scores, applying attention masks, calculating attention weights, and calculating context vectors.\n",
    "\n",
    "#### 1. Calculating Attention Scores\n",
    "\n",
    "The calculation of attention scores can be expressed as matrix multiplication. For query vector $Q$ and key vector $K$, the formula for calculating the attention score matrix $A$ is:\n",
    "\\begin{align*} A = \\frac{QK^T}{\\sqrt{d_k}} \\end{align*}\n",
    "Where $d_k$ is the dimension of the key vector, $\\sqrt{d_k}$ is a scaling factor to prevent the score value from being too large.\n",
    "\n",
    "In the code, it is implemented by matrix multiplication:\n",
    "```python\n",
    "matmul_result = torch.baddbmm(\n",
    "matmul_input_buffer,\n",
    "query_layer, # [b * np, sq, hn]\n",
    "key_layer.transpose(1, 2), # [b * np, hn, sk]\n",
    "beta=0.0,\n",
    "alpha=(1.0 / self.norm_factor),\n",
    ")\n",
    "```\n",
    "Where `self.norm_factor` is $\\sqrt{d_k}$.\n",
    "\n",
    "#### 2. Apply Attention Mask\n",
    "\n",
    "In order to prevent the model from focusing on unnecessary parts, use attention mask to mask certain positions. The attention mask is implemented by setting the attention score of the corresponding position to negative infinity:\n",
    "```python\n",
    "if attention_mask is not None:\n",
    "attention_scores = attention_scores.masked_fill(attention_mask, float(\"-inf\"))\n",
    "```\n",
    "\n",
    "#### 3. Calculate Attention Weights\n",
    "\n",
    "Apply the softmax function to convert the attention score into a probability distribution, indicating the degree of attention of each query vector to the key vector:\n",
    "\\begin{align*} \\text{AttentionWeights} = \\text{softmax}(A) \\end{align*}\n",
    "Implemented in code as:\n",
    "```python\n",
    "attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "attention_probs = attention_probs.type_as(value_layer)\n",
    "```\n",
    "\n",
    "#### 4. Calculate context vectors\n",
    "\n",
    "The context vector is obtained by multiplying the attention weights with the value vector and summing them:\n",
    "\\begin{align*} \\text{Context} = \\text{AttentionWeights} \\cdot V \\end{align*}\n",
    "Where $V$ is the value vector.\n",
    "\n",
    "Implemented in code as:\n",
    "```python\n",
    "context_layer = torch.bmm(attention_probs, value_layer)\n",
    "```\n",
    "\n",
    "#### Detailed principle explanation\n",
    "\n",
    "1. **Initialization and setting**:\n",
    "- When initializing the class, the attention scale, number of layers, and other information are set according to the configuration parameters.\n",
    "- `projection_size` defines the projection size, `hidden_size_per_attention_he`ad` and `num_attention_heads_per_partition` define the hidden layer size of each attention head and each partition.\n",
    "\n",
    "2. **Forward propagation steps**:\n",
    "- **Query, key and value calculation**: Calculate the query, key and value vectors.\n",
    "- **Calculate attention scores**: Calculate the attention score matrix through matrix multiplication.\n",
    "- **Apply attention mask**: Set the positions to be masked to negative infinity to avoid affecting subsequent calculations.\n",
    "- **Calculate attention weights**: Apply the softmax function to get the attention weights.\n",
    "- **Calculate context vector**: Multiply the attention weights with the value vector to get the context vector.\n",
    "\n",
    "3. **Specific implementation details**:\n",
    "- Choose the appropriate attention calculation method according to the PyTorch version.\n",
    "- Transform tensors of different dimensions to ensure shape matching.\n",
    "- Use dropout to prevent overfitting.\n",
    "\n",
    "### Correspondence between formula and code\n",
    "\n",
    "- **Attention score**:\n",
    "\\begin{align*}\n",
    "A = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "\\end{align*}\n",
    "Corresponding code:\n",
    "```python\n",
    "matmul_result= torch.baddbmm(\n",
    "matmul_input_buffer,\n",
    "query_layer,\n",
    "key_layer.transpose(1, 2),\n",
    "beta=0.0,\n",
    "alpha=(1.0 / self.norm_factor),\n",
    ")\n",
    "```\n",
    "\n",
    "- **Apply attention mask**:\n",
    "\\begin{align*}\n",
    "A'_{ij} = \\begin{cases} \n",
    "A_{ij} & \\text{if } \\text{mask}_{ij} = 1 \\\\\n",
    "-\\infty & \\text{if } \\text{mask}_{ij} = 0 \n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "Corresponding code:\n",
    "```python\n",
    "if attention_mask is not None:\n",
    "attention_scores = attention_scores.masked_fill(attention_mask, float(\"-inf\"))\n",
    "```\n",
    "\n",
    "- **Attention weight**:\\begin{align*}\n",
    "\\text{AttentionWeights}_{ij} = \\frac{\\exp(A'_{ij})}{\\sum_k \\exp(A'_{ik})}\n",
    "\\end{align*}\n",
    "Corresponding code:\n",
    "```python\n",
    "attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "```\n",
    "\n",
    "- **Context vector**:\n",
    "\\begin{align*}\n",
    "\\text{Context}_{i} = \\sum_j \\text{AttentionWeights}_{ij} V_j\n",
    "\\end{align*}\n",
    "Corresponding code:\n",
    "```python\n",
    "context_layer = torch.bmm(attention_probs, value_layer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fcf489-71b3-4077-af03-990979abc001",
   "metadata": {},
   "source": [
    "Next, let's review the attention mechanism\n",
    "\n",
    "### Principles and formula explanation of the attention mechanism\n",
    "\n",
    "#### 1. What is the attention mechanism?\n",
    "The attention mechanism is a technology in deep learning, especially in the field of natural language processing, which enables the model to dynamically focus on different parts when processing an input sequence. Simply put, the attention mechanism allows the model to selectively focus on other parts of the input sequence when processing a certain element, rather than treating all of them equally.\n",
    "\n",
    "#### 2. The core formula of the attention mechanism\n",
    "Let's look at the formula for the attention score:\n",
    "\\begin{align*} A = \\frac{QK^T}{\\sqrt{d_k}} \\end{align*}\n",
    "\n",
    "The symbols here are explained as follows:\n",
    "- \\( Q \\) is the query vector.\n",
    "- \\( K \\) is the key vector.\n",
    "- \\( d_k \\) is the dimension of the key vector.\n",
    "- \\( A \\) is the attention score matrix.\n",
    "\n",
    "#### 3. Why use this formula to calculate the attention score?\n",
    "\n",
    "The core idea of ​​the attention score formula is to measure the relevance between the query and each key by calculating the dot product of the query vector and the key vector. The larger the dot product result, the stronger the relevance between the query and the key, and the model should pay more attention to the value vector corresponding to the key.\n",
    "\n",
    "Scaling factor in the formula\\(\\sqrt{d_k}\\) is to avoid the dot product result being too large, because if it is not scaled, the larger vector dimension will cause the dot product result to be very large, which will cause the softmax function to output a gradient close to zero, affecting the model training effect.\n",
    "\n",
    "#### 4. Calculate attention weights\n",
    "After obtaining the attention score matrix \\( A \\), convert it to the attention weight matrix through the softmax function:\n",
    "\\begin{align*} \\text{AttentionWeights}_{ij} = \\frac{\\exp(A'_{ij})}{\\sum_k \\exp(A'_{ik})} \\end{align*}\n",
    "Where \\( A' \\) is the attention score matrix after applying the mask.\n",
    "\n",
    "#### 5. Calculate the context vector\n",
    "Finally, use the attention weight matrix to perform weighted summation on the value vector to obtain the context vector:\n",
    "\\begin{align*} \\text{Context}_{i} = \\sum_j \\text{AttentionWeights}_{ij} V_j \\end{align*}\n",
    "Here \\( V \\) is the value vector.\n",
    "\n",
    "### Relationship between attention mechanism and human attention\n",
    "\n",
    "The attention mechanism has certain similarities with human attention, but there are also significant differences.\n",
    "\n",
    "- **Similarities**:\n",
    "- **Selective attention**: Just like humans selectively focus on certain important paragraphs when reading an article and ignore other parts, the attention mechanism also allows the model to selectively focus on different parts when processing a sequence.\n",
    "- **Dynamic adjustment**: Human attention is dynamic and adjusts its focus according to the context. The attention mechanism is also dynamic and can adjust the attention weight according to changes in the input.\n",
    "\n",
    "- **Difference**:\n",
    "- **Different mechanisms**: Human attention is achieved through the complex neural network of the brain, including the comprehensive processing of multiple sensory information such as vision and hearing. The attention mechanism is a mathematical method that is implemented through operations such as dot product and softmax.\n",
    "- **Different purposes**: Human attention is used for understanding and interaction, while the attention mechanism is mainly used to improve the performance and efficiency of the model when processing long sequence data.\n",
    "\n",
    "### Details in the specific code implementation\n",
    "\n",
    "In the `CoreAttention` class, the calculation and application of the attention score is implemented by the following code snippet:\n",
    "```python\n",
    "matmul_result = torch.baddbmm(\n",
    "matmul_input_buffer,\n",
    "query_layer, # [b * np, sq, hn]\n",
    "key_layer.transpose(1, 2), #[b * np, hn, sk]\n",
    "beta=0.0,\n",
    "alpha=(1.0 / self.norm_factor),\n",
    ")\n",
    "```\n",
    "Here, the `torch.baddbmm` function performs batch matrix multiplication, calculates the \\(QK^T\\) part of the formula, and scales it by dividing it by \\(\\sqrt{d_k}\\).\n",
    "\n",
    "Apply softmax to get the attention weights:\n",
    "```python\n",
    "attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "```\n",
    "\n",
    "Finally, calculate the context vector:\n",
    "```python\n",
    "context_layer = torch.bmm(attention_probs, value_layer)\n",
    "```\n",
    "\n",
    "This step multiplies the attention weights with the value vector and sums them to get the final context representation.\n",
    "\n",
    "Through the above explanation, we can better understand the principle and implementation of the attention mechanism, as well as its important role in the model.\n",
    "\n",
    "After completing the core implementation of the attention mechanism, we build the self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b003c-7e42-4e0a-96fe-a052826ea982",
   "metadata": {},
   "source": [
    "### Principle and formula explanation of SelfAttention class\n",
    "\n",
    "#### Function of SelfAttention class\n",
    "\n",
    "The SelfAttention class implements the Self-Attention Mechanism, which is the core part of the Transformer model. The goal of the self-attention mechanism is to allow the representation of each position to dynamically pay attention to other positions of the input sequence, thereby capturing global information.\n",
    "\n",
    "#### Steps and formulas of the self-attention mechanism\n",
    "\n",
    "The self-attention mechanism includes the following key steps:\n",
    "\n",
    "1. **Linear transformation**:\n",
    "The input hidden state \\(X\\) is mapped to the query \\(Q\\), key \\(K\\) and value \\(V\\) spaces through linear layers:\n",
    "\\begin{align*}\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "\\end{align*}\n",
    "Among them, \\(W_Q\\), \\(W_K\\) and \\(W_V\\) are the learned weight matrices.\n",
    "\n",
    "2. **Calculate attention score**:\n",
    "The attention score matrix \\(A\\) is obtained by calculating the dot product of the query \\(Q\\) and the key \\(K\\) and dividing by the scaling factor \\(\\sqrt{d_k}\\):\n",
    "\\begin{align*}\n",
    "A = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "\\end{align*}\n",
    "\n",
    "3. **Apply attention mask**:\n",
    "For self-attention mechanism, if mask is used (for example, in decoding stage), set the positions that do not need to be paid attention to to negative infinity to mask:\n",
    "\\begin{align*}\n",
    "A'_{ij} = \\begin{cases}\n",
    "A_{ij} & \\text{if } \\text{mask}_{ij} = 1 \\\\\n",
    "-\\infty & \\text{if } \\text{mask}_{ij} = 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "4. **Calculate attention weight**:\n",
    "Apply softmax function to the attention score matrix \\(A'\\) to get the attention weight matrix:\n",
    "\\begin{align*}\n",
    "\\text{AttentionWeights}_{ij} = \\frac{\\exp(A'_{ij})}{\\sum_k \\exp(A'_{ik})}\n",
    "\\end{align*}\n",
    "\n",
    "5. **Calculate the context vector**:\n",
    "Use the attention weight to calculate the value \\(V\\)Weighted summation to get the context vector:\n",
    "\\begin{align*}\n",
    "\\text{Context} = \\text{AttentionWeights} \\cdot V\n",
    "\\end{align*}\n",
    "\n",
    "### Specific implementation of the SelfAttention class\n",
    "\n",
    "The following are the implementation details of the core steps and formulas in the SelfAttention class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "691ee97d-c7a7-4a41-846c-eb5f4a33443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "\"\"\"Abstract class for parallel self-attention layers.\n",
    "\n",
    "    自注意力层接受形状为 [s, b, h] 的输入，并返回相同形状的输出。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ChatGLMConfig, layer_number, device=None):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.layer_number = max(1, layer_number)\n",
    "\n",
    "        self.projection_size = config.kv_channels * config.num_attention_heads\n",
    "\n",
    "# The value of each attention head and each partition\n",
    "        self.hidden_size_per_attention_head = self.projection_size // config.num_attention_heads\n",
    "        self.num_attention_heads_per_partition = config.num_attention_heads\n",
    "\n",
    "        self.multi_query_attention = config.multi_query_attention\n",
    "        self.qkv_hidden_size = 3 * self.projection_size\n",
    "        if self.multi_query_attention:\n",
    "            self.num_multi_query_groups_per_partition = config.multi_query_group_num\n",
    "            self.qkv_hidden_size = (\n",
    "                    self.projection_size + 2 * self.hidden_size_per_attention_head * config.multi_query_group_num\n",
    "            )\n",
    "        self.query_key_value = nn.Linear(config.hidden_size, self.qkv_hidden_size,\n",
    "                                         bias=config.add_bias_linear or config.add_qkv_bias,\n",
    "                                         device=device, **_config_to_kwargs(config)\n",
    "                                         )\n",
    "\n",
    "        self.core_attention = CoreAttention(config, self.layer_number)\n",
    "\n",
    "# Output layer\n",
    "        self.dense = nn.Linear(self.projection_size, config.hidden_size, bias=config.add_bias_linear,\n",
    "                               device=device, **_config_to_kwargs(config)\n",
    "                               )\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=None, use_cache=True):\n",
    "# hidden_states: [b, sq, h]\n",
    "\n",
    "        # =====================\n",
    "# Query, Key and Value\n",
    "        # =====================\n",
    "\n",
    "# Attention head [b, sq, h] --> [b, sq, (np * 3 * hn)]\n",
    "        mixed_x_layer = self.query_key_value(hidden_states)\n",
    "\n",
    "        if self.multi_query_attention:\n",
    "            (query_layer, key_layer, value_layer) = mixed_x_layer.split(\n",
    "                [\n",
    "                    self.num_attention_heads_per_partition * self.hidden_size_per_attention_head,\n",
    "                    self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,\n",
    "                    self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "            query_layer = query_layer.view(\n",
    "                query_layer.size()[:-1] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)\n",
    "            )\n",
    "            key_layer = key_layer.view(\n",
    "                key_layer.size()[:-1] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)\n",
    "            )\n",
    "            value_layer = value_layer.view(\n",
    "                value_layer.size()[:-1]\n",
    "                + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)\n",
    "            )\n",
    "        else:\n",
    "            new_tensor_shape = mixed_x_layer.size()[:-1] + \\\n",
    "                               (self.num_attention_heads_per_partition,\n",
    "                                3 * self.hidden_size_per_attention_head)\n",
    "            mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)\n",
    "\n",
    "# [b, sq, np, 3 * hn] --> 3 [b, sq, np, hn]\n",
    "            (query_layer, key_layer, value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)\n",
    "\n",
    "# [b, sq, np, hn] -> [b, np, sq, hn]\n",
    "        query_layer, key_layer, value_layer = [k.transpose(1, 2) for k in [query_layer, key_layer, value_layer]]\n",
    "\n",
    "# Apply relative position encoding (rotation embedding)\n",
    "        if rotary_pos_emb is not None:\n",
    "            query_layer = apply_rotary_pos_emb(query_layer, rotary_pos_emb)\n",
    "            key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)\n",
    "\n",
    "# Adjust key and value for inference\n",
    "        if kv_cache is not None:\n",
    "            cache_k, cache_v = kv_cache\n",
    "            key_layer = torch.cat((cache_k, key_layer), dim=2)\n",
    "            value_layer = torch.cat((cache_v, value_layer), dim=2)\n",
    "        if use_cache:\n",
    "            if kv_cache is None:\n",
    "                kv_cache = torch.cat((key_layer.unsqueeze(0).unsqueeze(0), value_layer.unsqueeze(0).unsqueeze(0)), dim=1)\n",
    "            else:\n",
    "                kv_cache = (key_layer, value_layer)\n",
    "        else:\n",
    "            kv_cache = None\n",
    "\n",
    "        if self.multi_query_attention:\n",
    "            key_layer = key_layer.unsqueeze(2)\n",
    "            key_layer = key_layer.expand(\n",
    "                -1, -1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -1, -1\n",
    "            )\n",
    "            key_layer = key_layer.contiguous().view(\n",
    "                key_layer.size()[:1] + (self.num_attention_heads_per_partition,) + key_layer.size()[3:]\n",
    "            )\n",
    "            value_layer = value_layer.unsqueeze(2)\n",
    "            value_layer = value_layer.expand(\n",
    "                -1, -1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -1, -1\n",
    "            )\n",
    "            value_layer = value_layer.contiguous().view(\n",
    "                value_layer.size()[:1] + (self.num_attention_heads_per_partition,) + value_layer.size()[3:]\n",
    "            )\n",
    "\n",
    "# Core attention calculation\n",
    "        context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)\n",
    "\n",
    "# Output. [sq, b, h]\n",
    "        output = self.dense(context_layer)\n",
    "\n",
    "        return output, kv_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e5da0-10a8-4c65-83ce-072f59c3b376",
   "metadata": {},
   "source": [
    "### Detailed explanation of the principle\n",
    "\n",
    "1. **Linear transformation**:\n",
    "```python\n",
    "mixed_x_layer = self.query_key_value(hidden_states)\n",
    "```\n",
    "Input hidden state `hidden_states` is mapped to the query, key and value space through the linear layer.\n",
    "\n",
    "2. **Split query, key and value**:\n",
    "```python\n",
    "(query_layer, key_layer, value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)\n",
    "```\n",
    "Split the result after linear transformation into query, key and value.\n",
    "\n",
    "3. **Shape transformation**:\n",
    "```python\n",
    "query_layer, key_layer, value_layer = [k.transpose(1, 2) for k in [query_layer, key_layer, value_layer]]\n",
    "```\n",
    "Transform the shape of query, key, and value from `[b, sq, np, hn]` to `[b, np, sq, hn]`.\n",
    "\n",
    "4. **Should**Use rotary position encoding**:\n",
    "```python\n",
    "if rotary_pos_emb is not None:\n",
    "query_layer = apply_rotary_pos_emb(query_layer, rotary_pos_emb)\n",
    "key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)\n",
    "```\n",
    "If rotary position encoding is present, apply position encoding to query and key.\n",
    "\n",
    "5. **Adjust keys and values ​​for inference**:\n",
    "```python\n",
    "if kv_cache is not None:\n",
    "cache_k, cache_v = kv_cache\n",
    "\n",
    "key_layer = torch.cat((cache_k, key_layer), dim=2)\n",
    "value_layer = torch.cat((cache_v, value_layer), dim=2)\n",
    "if use_cache:\n",
    "if kv_cache is None:kv_cache = torch.cat((key_layer.unsqueeze(0).unsqueeze(0), value_layer.unsqueeze(0).unsqueeze(0)), dim=1)\n",
    "else:\n",
    "kv_cache = (key_layer, value_layer)\n",
    "else:\n",
    "kv_cache = None\n",
    "```\n",
    "\n",
    "6. **Core attention calculation**:\n",
    "```python\n",
    "context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)\n",
    "```\n",
    "\n",
    "7. **Output layer**:\n",
    "```python\n",
    "output = self.dense(context_layer)\n",
    "```\n",
    "\n",
    "Through the above detailed principle explanation and formula, we can better understand the implementation of the SelfAttention class and its role in the Transformer model. The self-attention mechanism dynamically adjusts the weights between different positions to make the modelThe model can capture global information more effectively, thereby improving the performance and generalization ability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00c11e28-47e7-437b-aab8-1da8b736443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _config_to_kwargs(args):\n",
    "    common_kwargs = {\n",
    "        \"dtype\": args.torch_dtype,\n",
    "    }\n",
    "    return common_kwargs\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "\"\"\"Multilayer Perceptron (MLP).\n",
    "\n",
    "    MLP 将接受隐藏状态为 h 的输入，将其投影到 4*h 的隐藏维度，执行非线性变换，然后将状态投影回 h 的隐藏维度。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ChatGLMConfig, device=None):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.add_bias = config.add_bias_linear\n",
    "\n",
    "# Project to 4h. If using swiglu, double the output width, see https://arxiv.org/pdf/2002.05202.pdf for details\n",
    "        self.dense_h_to_4h = nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.ffn_hidden_size * 2,\n",
    "            bias=self.add_bias,\n",
    "            device=device,\n",
    "            **_config_to_kwargs(config)\n",
    "        )\n",
    "\n",
    "        def swiglu(x):\n",
    "            x = torch.chunk(x, 2, dim=-1)\n",
    "            return F.silu(x[0]) * x[1]\n",
    "\n",
    "        self.activation_func = swiglu\n",
    "\n",
    "# Project back to h.\n",
    "        self.dense_4h_to_h = nn.Linear(\n",
    "            config.ffn_hidden_size,\n",
    "            config.hidden_size,\n",
    "            bias=self.add_bias,\n",
    "            device=device,\n",
    "            **_config_to_kwargs(config)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "# [s, b, 4hp]\n",
    "        intermediate_parallel = self.dense_h_to_4h(hidden_states)\n",
    "        intermediate_parallel = self.activation_func(intermediate_parallel)\n",
    "# [s, b, h]\n",
    "        output = self.dense_4h_to_h(intermediate_parallel)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GLMBlock(torch.nn.Module):\n",
    "\"\"\"A transformer layer.\n",
    "\n",
    "    Transformer 层接受形状为 [s, b, h] 的输入，并返回相同形状的输出。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ChatGLMConfig, layer_number, device=None):\n",
    "        super(GLMBlock, self).__init__()\n",
    "        self.layer_number = layer_number\n",
    "\n",
    "        self.apply_residual_connection_post_layernorm = config.apply_residual_connection_post_layernorm\n",
    "        self.fp32_residual_connection = config.fp32_residual_connection\n",
    "\n",
    "        LayerNormFunc = RMSNorm if config.rmsnorm else LayerNorm\n",
    "# Layer normalization on input data\n",
    "        self.input_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,\n",
    "                                             dtype=config.torch_dtype)\n",
    "\n",
    "# Self-attention layer\n",
    "        self.self_attention = SelfAttention(config, layer_number, device=device)\n",
    "        self.hidden_dropout = config.hidden_dropout\n",
    "\n",
    "# Layer normalization on attention output\n",
    "        self.post_attention_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,\n",
    "                                                      dtype=config.torch_dtype)\n",
    "\n",
    "# Multilayer Perceptron (MLP)\n",
    "        self.mlp = MLP(config, device=device)\n",
    "\n",
    "    def forward(\n",
    "            self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=None, use_cache=True,\n",
    "    ):\n",
    "# hidden_states: [s, b, h]\n",
    "\n",
    "# Layer normalization at the beginning of transformer layer\n",
    "        layernorm_output = self.input_layernorm(hidden_states)\n",
    "# Self-attention layer\n",
    "        attention_output, kv_cache = self.self_attention(\n",
    "            layernorm_output,\n",
    "            attention_mask,\n",
    "            rotary_pos_emb,\n",
    "            kv_cache=kv_cache,\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "\n",
    "# Residual connection\n",
    "        if self.apply_residual_connection_post_layernorm:\n",
    "            residual = layernorm_output\n",
    "        else:\n",
    "            residual = hidden_states\n",
    "\n",
    "        layernorm_input = torch.nn.functional.dropout(attention_output, p=self.hidden_dropout, training=self.training)\n",
    "        layernorm_input = residual + layernorm_input\n",
    "\n",
    "# Layer normalization after self-attention\n",
    "        layernorm_output = self.post_attention_layernorm(layernorm_input)\n",
    "\n",
    "# Multilayer Perceptron (MLP)\n",
    "        mlp_output = self.mlp(layernorm_output)\n",
    "\n",
    "# The second residual connection\n",
    "        if self.apply_residual_connection_post_layernorm:\n",
    "            residual = layernorm_output\n",
    "        else:\n",
    "            residual = layernorm_input\n",
    "\n",
    "        output = torch.nn.functional.dropout(mlp_output, p=self.hidden_dropout, training=self.training)\n",
    "        output = residual + output\n",
    "\n",
    "        return output, kv_cache\n",
    "\n",
    "\n",
    "class GLMTransformer(torch.nn.Module):\n",
    "\"\"\"Transformer class.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ChatGLMConfig, device=None):\n",
    "        super(GLMTransformer, self).__init__()\n",
    "\n",
    "        self.fp32_residual_connection = config.fp32_residual_connection\n",
    "        self.post_layer_norm = config.post_layer_norm\n",
    "\n",
    "# Number of layers\n",
    "        self.num_layers = config.num_layers\n",
    "\n",
    "Transformer Layer\n",
    "        def build_layer(layer_number):\n",
    "            return GLMBlock(config, layer_number, device=device)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([build_layer(i + 1) for i in range(self.num_layers)])\n",
    "\n",
    "        if self.post_layer_norm:\n",
    "            LayerNormFunc = RMSNorm if config.rmsnorm else LayerNorm\n",
    "# Final layer normalization before output\n",
    "            self.final_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,\n",
    "                                                 dtype=config.torch_dtype)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def _get_layer(self, layer_number):\n",
    "        return self.layers[layer_number]\n",
    "\n",
    "    def forward(\n",
    "            self, hidden_states, attention_mask, rotary_pos_emb, kv_caches=None,\n",
    "            use_cache: Optional[bool] = True,\n",
    "            output_hidden_states: Optional[bool] = False,\n",
    "    ):\n",
    "        if not kv_caches:\n",
    "            kv_caches = [None for _ in range(self.num_layers)]\n",
    "        presents = () if use_cache else None\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        all_self_attentions = None\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        for index in range(self.num_layers):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer = self._get_layer(index)\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_ret = torch.utils.checkpoint.checkpoint(\n",
    "                    layer,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    rotary_pos_emb,\n",
    "                    kv_caches[index],\n",
    "                    use_cache,\n",
    "                    use_reentrant=False\n",
    "                )\n",
    "            else:\n",
    "                layer_ret = layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    rotary_pos_emb,\n",
    "                    kv_cache=kv_caches[index],\n",
    "                    use_cache=use_cache\n",
    "                )\n",
    "            hidden_states, kv_cache = layer_ret\n",
    "            if use_cache:\n",
    "# Token by token decoding, using tuple format\n",
    "                if kv_caches[0] is not None:\n",
    "                    presents = presents + (kv_cache,)\n",
    "# Prefill decode, use tensor format to save CUDA memory\n",
    "                else:\n",
    "                    if len(presents) == 0:\n",
    "                        presents = kv_cache\n",
    "                    else:\n",
    "                        presents = torch.cat((presents, kv_cache.to(presents.device)), dim=0)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "# Final layer normalization\n",
    "        if self.post_layer_norm:\n",
    "            hidden_states = self.final_layernorm(hidden_states)\n",
    "\n",
    "        return hidden_states, presents, all_hidden_states, all_self_attentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad537c5-ceb0-41a7-b889-fc0fe633bab6",
   "metadata": {},
   "source": [
    "## Changes in tensor shape and their meaning.\n",
    "\n",
    "### Tensor shapes in `CoreAttention` class\n",
    "\n",
    "1. **`query_layer`, `key_layer`, `value_layer` shape changes**:\n",
    "```python\n",
    "# Input shape [b, np, sq, hn] and [b, np, sk, hn]\n",
    "# [b, np, sq, hn] -> [b * np, sq, hn]\n",
    "query_layer = query_layer.view(output_size[0] * output_size[1], output_size[2], -1)\n",
    "# [b, np, sk, hn] -> [b * np, sk, hn]\n",
    "key_layer = key_layer.view(output_size[0] * output_size[1], output_size[3], -1)\n",
    "```\n",
    "\n",
    "2. **Calculation of raw attention score**:\n",
    "```python\n",
    "# Input shape [b * np, sq, hn] and [b * np, hn, sk]\n",
    "#Result shape [b * np, sq, sk]\n",
    "matmul_result = torch.baddbmm(matmul_input_buffer, query_layer, key_layer.transpose(1, 2), beta=0.0, alpha=(1.0 / self.norm_factor))\n",
    "```\n",
    "\n",
    "3. **Adjust the view to the original shape**:\n",
    "```python\n",
    "# [b * np, sq, sk] -> [b, np, sq, sk]\n",
    "attention_scores = matmul_result.view(*output_size)\n",
    "```\n",
    "\n",
    "4. **Attention probability shape change**:\n",
    "```python\n",
    "# Input shape [b, np, sq, sk]\n",
    "# Adjusted shape [b * np, sq, sk]\n",
    "attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], -1)\n",
    "```\n",
    "\n",
    "5. **Calculation contextLayer**:\n",
    "```python\n",
    "# Input shapes [b * np, sq, sk] and [b * np, sk, hn]\n",
    "# Result shape [b * np, sq, hn]\n",
    "context_layer = torch.bmm(attention_probs, value_layer)\n",
    "# Adjust view [b * np, sq, hn] -> [b, np, sq, hn]\n",
    "context_layer = context_layer.view(*output_size)\n",
    "# [b, np, sq, hn] -> [b, sq, np, hn]\n",
    "context_layer = context_layer.transpose(1, 2).contiguous()\n",
    "# [b, sq, np, hn] -> [b, sq, hp]\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)\n",
    "context_layer = context_layer.reshape(*new_context_layer_shape)\n",
    "```\n",
    "\n",
    "### Tensor shape in `SelfAttention` class\n",
    "\n",
    "1. **`mixed_x_layer` shape change**:\n",
    "```python\n",
    "# Input shape [b, sq, h]\n",
    "# Result shape [b, sq, (np * 3 * hn)]\n",
    "mixed_x_layer = self.query_key_value(hidden_states)\n",
    "```\n",
    "\n",
    "2. **Multi-query attention shape change**:\n",
    "```python\n",
    "# [b, sq, (np * 3 * hn)] -> [b, sq, np, hn] and [b, sq, np, hn]\n",
    "(query_layer, key_layer, value_layer) = mixed_x_layer.split([...], dim=-1)\n",
    "# Adjust view\n",
    "query_layer = query_layer.view(query_layer.size()[:-1] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head))\n",
    "key_layer = key_layer.view(key_layer.size()[:-1] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head))\n",
    "value_layer = value_layer.view(value_layer.size()[:-1] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head))\n",
    "```\n",
    "\n",
    "3. **Shape change of normal attention**:\n",
    "```python\n",
    "# [b, sq, (np * 3 * hn)] -> [b, sq, np, 3 * hn]\n",
    "new_tensor_shape = mixed_x_layer.size()[:-1] + (self.num_attention_heads_per_partition, 3 * self.hidden_size_per_attention_head)\n",
    "mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)\n",
    "# [b, sq, np, 3 * hn] -> 3 [b, sq, np, hn]\n",
    "(query_layer, key_layer, value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)\n",
    "```\n",
    "\n",
    "4. **Transpose operation**:\n",
    "```python\n",
    "# [b, sq, np, hn] -> [b, np, sq, hn]\n",
    "query_layer, key_layer, value_layer = [k.transpose(1, 2) for k in [query_layer, key_layer, value_layer]]\n",
    "```\n",
    "\n",
    "### Tensor shape in `MLP` class\n",
    "\n",
    "1. **Forward pass**:\n",
    "```python\n",
    "# InputShape [s, b, h]\n",
    "# Shape after dense_h_to_4h linear layer [s, b, 4hp]\n",
    "intermediate_parallel = self.dense_h_to_4h(hidden_states)\n",
    "intermediate_parallel = self.activation_func(intermediate_parallel)\n",
    "# Shape after dense_4h_to_h linear layer [s, b, h]\n",
    "output = self.dense_4h_to_h(intermediate_parallel)\n",
    "```\n",
    "\n",
    "### Tensor shape in `GLMBlock` class\n",
    "\n",
    "1. **Input and output shapes of self-attention layer**:\n",
    "```python\n",
    "# Input shape [s, b, h]\n",
    "# Output shape of self-attention layer [s, b, h]\n",
    "attention_output, kv_cache = self.self_attention(layernorm_output, attention_mask, rotary_pos_emb, kv_cache=kv_cache, use_cache=use_cache)\n",
    "```\n",
    "\n",
    "2. **Residual connection**:\n",
    "```python\n",
    "# Residual connection, shape remains unchanged [s, b, h]\n",
    "layernorm_input = residual + layernorm_input\n",
    "```\n",
    "\n",
    "3. **Input and output shape of multi-layer perceptron (MLP)**:\n",
    "```python\n",
    "# MLP output shape [s, b, h]\n",
    "mlp_output = self.mlp(layernorm_output)\n",
    "```\n",
    "\n",
    "### Tensor shape in `GLMTransformer` class\n",
    "\n",
    "1. **Layer-by-layer processing**:\n",
    "```python\n",
    "# Input and output shape of each layer [s, b, h]\n",
    "for index in range(self.num_layers):\n",
    "layer = self._get_layer(index)\n",
    "hidden_states, kv_cache = layer(hidden_states, attention_mask, rotary_pos_emb, kv_cache=kv_caches[index], use_cache=use_cache)\n",
    "```\n",
    "\n",
    "2. **Final layer normalization**:\n",
    "```python\n",
    "# Final layer normalization, shape [s, b, h]\n",
    "if self.post_layer_norm:\n",
    "hidden_states = self.final_layernorm(hidden_states)\n",
    "```\n",
    "\n",
    "Summarizing these shape changes helps understand how the input and output of each layer are passed and processed, ensuring that the model maintains the correct tensor shape at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59c956-a31d-459c-a22d-ef3920644ab5",
   "metadata": {},
   "source": [
    "### Key Points Explanation\n",
    "\n",
    "1. **`CoreAttention` class**:\n",
    "- Implements the core attention mechanism.\n",
    "- Supports `scaled_dot_product_attention` of PyTorch 2.0 and above, as well as custom attention calculations of older versions.\n",
    "\n",
    "2. **`SelfAttention` class**:\n",
    "- Implements the self-attention layer.\n",
    "- Includes the calculation of query, key, value, and the application of the core attention mechanism.\n",
    "- Supports multi-query attention mechanism.\n",
    "\n",
    "3. **`MLP` class**:\n",
    "- Multilayer perceptron (MLP), including two linear layers and a nonlinear activation function.\n",
    "\n",
    "4. **`GLMBlock` class**:\n",
    "- Implements a Transformer layer, including a self-attention layer and an MLP layer.\n",
    "- Includes layer normalization and residual connections.\n",
    "\n",
    "5. **`GLMTransformer` class**:\n",
    "- Implements the Transformer model, including multiple Transformer layers.\n",
    "- Support for gradient checkpointing and final layer normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48414b7e-3bb1-4a06-b411-5492035dabcf",
   "metadata": {},
   "source": [
    "### Steps for extracting input hidden states from the self-attention mechanism\n",
    "\n",
    "In the SelfAttention class, the self-attention mechanism extracts and processes the input hidden states through a series of steps to finally generate a context vector. These steps include linear transformation, calculating attention scores, applying attention masks, calculating attention weights, and generating context vectors. The following are the detailed steps and explanations:\n",
    "\n",
    "#### 1. Input hidden state\n",
    "\n",
    "The shape of the input hidden state \\( \\text{hidden_states} \\) is usually \\([b, sq, h]\\), where:\n",
    "- \\( b \\) is the batch size.\n",
    "- \\( sq \\) is the sequence length.\n",
    "- \\( h \\) is the hidden layer dimension (hidden size).\n",
    "\n",
    "#### 2. Linear transformation\n",
    "\n",
    "The input hidden state \\( \\text{hidden_states} \\) is projected to the query \\( Q \\), key \\( K \\), and value \\( V \\) space through a linear layer. The purpose of this step is to map the input to different subspaces for attention calculation. The formula is as follows: \\begin{align*} Q = \\text{hidden_states} \\cdot W_Q \\end{align*}\n",
    "\\begin{align*} K = \\text{hidden_states} \\cdot W_K \\end{align*}\n",
    "\\begin{align*} V = \\text{hidden_states} \\cdot W_V \\end{align*}\n",
    "\n",
    "In the code, this is done as follows:\n",
    "```python\n",
    "mixed_x_layer = self.query_key_value(hidden_states)\n",
    "```\n",
    "Here `self.query_key_value` is a linear layer that projects the input hidden state into a larger space, and the resulting shape is \\([b, sq, (3 \\times \\text{hidden_size})]\\).\n",
    "\n",
    "#### 3. Split query, key, and value\n",
    "\n",
    "Split the above result into query, key, and value vectors. After splitting, the shape of each vector is \\([b, sq, \\text{hidden_size}]\\).\n",
    "```python\n",
    "(query_layer, key_layer, value_layer) = split_tensor_along_last_dim(mixed_x_layer,3)\n",
    "```\n",
    "`split_tensor_along_last_dim` splits the tensor into three parts along the last dimension.\n",
    "\n",
    "#### 4. Shape transformation\n",
    "\n",
    "In order to adapt to the subsequent matrix multiplication operation, the query, key and value need to be reshaped. Convert them from \\([b, sq, np, hn]\\) to \\([b, np, sq, hn]\\), where \\(np\\) is the number of attention heads and \\(hn\\) is the dimension of each attention head.\n",
    "```python\n",
    "query_layer, key_layer, value_layer = [k.transpose(1, 2) for k in [query_layer, key_layer, value_layer]]\n",
    "```\n",
    "\n",
    "#### 5. Apply rotational position encoding\n",
    "\n",
    "If there is a rotational position embedding, it is applied to the query and key vectors. Rotational position encoding can help the model better capture position information.\n",
    "```python\n",
    "if rotary_pos_emb is not None:\n",
    "query_layer = apply_rotary_pos_emb(query_layer, rotary_pos_emb)\n",
    "key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)\n",
    "```\n",
    "\n",
    "#### 6. Adjust keys and values ​​for inference\n",
    "\n",
    "During the inference phase, you may need to cache keys and values ​​so that they can be reused in the next time step to improve efficiency. Concatenate the cached keys and values ​​with the keys and values ​​of the current time step.\n",
    "```python\n",
    "if kv_cache is not None:\n",
    "cache_k, cache_v = kv_cache\n",
    "key_layer = torch.cat((cache_k, key_layer), dim=2)\n",
    "value_layer = torch.cat((cache_v, value_layer), dim=2)\n",
    "if use_cache:\n",
    "if kv_cache is None:\n",
    "kv_cache = torch.cat((key_layer.unsqueeze(0).unsqueeze(0), value_layer.unsqueeze(0).unsqueeze(0)), dim=1)\n",
    "else:\n",
    "kv_cache = (key_layer, value_layer)\n",
    "else:\n",
    "kv_cache = None\n",
    "```\n",
    "\n",
    "#### 7. Core Attention Calculation\n",
    "\n",
    "Use the CoreAttention class to calculate the attention score, attention weight, and context vector.\n",
    "```python\n",
    "context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)\n",
    "```\n",
    "\n",
    "#### 8. Output Layer\n",
    "\n",
    "Finally, the context vector is projected back to the original hidden layer dimension through a linear layer. This step is to map the calculated result back to the shape of the input for subsequent processing.\n",
    "```python\n",
    "output = self.dense(context_layer)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "Through the above steps, the self-attention mechanism realizes the process of extracting and processing information from the input hidden state. The detailed explanation of each step is as follows:\n",
    "\n",
    "1. **Input hidden state**: hidden representation of the input sequence.\n",
    "2. **Linear transformation**: Project the hidden representation into the space of query, key and value.\n",
    "3. **Split query, key and value**: Split the projected result into query, key and value vectors.\n",
    "4. **Shape Transformation**: Adjust the shape of query, key and value to adapt to subsequent operations.\n",
    "5. **Apply Rotation Position Encoding**: Enhance the position information of query and key vectors.\n",
    "6. **Adjust keys and values ​​for inference**: Cache keys and values ​​during the inference phase to improve efficiency.\n",
    "7. **Core Attention Computation**: Complete the core part of the attention mechanism by calculating attention scores, weights and context vectors.\n",
    "8. **Output Layer**: Project the context vector back to the original hidden layer dimension.\n",
    "\n",
    "Together, these steps form the self-attention mechanism, which enables the model to dynamically focus on different parts of the input sequence, capture global information and generate richer representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16f0c2-db97-4512-9a09-1e59502d6e19",
   "metadata": {},
   "source": [
    "### ChatGLMPreTrainedModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d1e5174-cc11-4ba0-ad18-b58110bf084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGLMPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    处理权重初始化和下载及加载预训练模型的简单接口的抽象类。\n",
    "    \"\"\"\n",
    "    is_parallelizable = False  # 是否可并行化\n",
    "    supports_gradient_checkpointing = True  # 是否支持梯度检查点\n",
    "    config_class = ChatGLMConfig  # 配置类\n",
    "    base_model_prefix = \"transformer\"  # 基础模型前缀\n",
    "    _no_split_modules = [\"GLMBlock\"]  # 不拆分的模块列表\n",
    "\n",
    "    def _init_weights(self, module: nn.Module):\n",
    "\"\"\"Initialize weights\"\"\"\n",
    "        return\n",
    "\n",
    "    def get_masks(self, input_ids, past_key_values, padding_mask=None):\n",
    "        \"\"\"\n",
    "        获取注意力掩码\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "# Create a lower triangular matrix as the full attention mask\n",
    "        full_attention_mask = torch.ones(batch_size, seq_length, seq_length, device=input_ids.device)\n",
    "        full_attention_mask.tril_()\n",
    "        past_length = 0\n",
    "        if past_key_values:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "        if past_length:\n",
    "            full_attention_mask = torch.cat((torch.ones(batch_size, seq_length, past_length, device=input_ids.device), full_attention_mask), dim=-1)\n",
    "        if padding_mask is not None:\n",
    "            full_attention_mask = full_attention_mask * padding_mask.unsqueeze(1)\n",
    "        if not past_length and padding_mask is not None:\n",
    "            full_attention_mask -= padding_mask.unsqueeze(-1) - 1\n",
    "        full_attention_mask = (full_attention_mask < 0.5).bool()\n",
    "        full_attention_mask.unsqueeze_(1)\n",
    "        return full_attention_mask\n",
    "\n",
    "    def get_position_ids(self, input_ids, device):\n",
    "        \"\"\"\n",
    "        获取位置ID\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        return position_ids\n",
    "\n",
    "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
    "        if not self.supports_gradient_checkpointing:\n",
    "            raise ValueError(f\"{self.__class__.__name__} does not support gradient checkpointing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d19827-5201-4981-9618-520275a14b6c",
   "metadata": {},
   "source": [
    "### Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ede82883-45bb-41f8-b679-a97b2d04c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "\"\"\"Language model embedding layer.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ChatGLMConfig, device=None):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "# Word Embedding Layer (Parallel)\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.padded_vocab_size,\n",
    "            self.hidden_size,\n",
    "            dtype=config.torch_dtype,\n",
    "            device=device\n",
    "        )\n",
    "        self.fp32_residual_connection = config.fp32_residual_connection\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "# Get word embeddings\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        embeddings = words_embeddings\n",
    "# If fp32 residual connection is set, convert to floating point\n",
    "        if self.fp32_residual_connection:\n",
    "            embeddings = embeddings.float()\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7568a-a42f-4053-9939-152a656181e4",
   "metadata": {},
   "source": [
    "### ChatGLMModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21ed53de-5687-4093-9662-efbc224093b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGLMModel(ChatGLMPreTrainedModel):\n",
    "    def __init__(self, config: ChatGLMConfig, device=None, empty_init=True):\n",
    "        super().__init__(config)\n",
    "        if empty_init:\n",
    "            init_method = skip_init  # 跳过初始化\n",
    "        else:\n",
    "            init_method = default_init  # 使用默认初始化方法\n",
    "        init_kwargs = {}\n",
    "        if device is not None:\n",
    "            init_kwargs[\"device\"] = device\n",
    "        self.embedding = init_method(Embedding, config, **init_kwargs)  # 使用 Embedding 类\n",
    "        self.num_layers = config.num_layers\n",
    "        self.multi_query_group_num = config.multi_query_group_num\n",
    "        self.kv_channels = config.kv_channels\n",
    "\n",
    "# Rotation position embedding\n",
    "        self.seq_length = config.seq_length\n",
    "        rotary_dim = (\n",
    "            config.hidden_size // config.num_attention_heads if config.kv_channels is None else config.kv_channels\n",
    "        )\n",
    "\n",
    "        self.rotary_pos_emb = RotaryEmbedding(rotary_dim // 2, rope_ratio=config.rope_ratio, original_impl=config.original_rope, \n",
    "                                              device=device, dtype=config.torch_dtype)  # 使用 RotaryEmbedding 类\n",
    "        self.encoder = init_method(GLMTransformer, config, **init_kwargs)  # 使用 GLMTransformer 类\n",
    "        self.output_layer = init_method(nn.Linear, config.hidden_size, config.padded_vocab_size, bias=False,\n",
    "                                        dtype=config.torch_dtype, **init_kwargs)  # 使用 nn.Linear 类\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embedding.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embedding.word_embeddings = value\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.BoolTensor] = None,\n",
    "            full_attention_mask: Optional[torch.BoolTensor] = None,\n",
    "            past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embedding(input_ids)  # 使用 Embedding 类\n",
    "\n",
    "        if full_attention_mask is None:\n",
    "            if (attention_mask is not None and not attention_mask.all()) or (past_key_values and seq_length != 1):\n",
    "                full_attention_mask = self.get_masks(input_ids, past_key_values, padding_mask=attention_mask)\n",
    "\n",
    "# Rotation position embedding\n",
    "        rotary_pos_emb = self.rotary_pos_emb(self.seq_length)  # 使用 RotaryEmbedding 类\n",
    "        if position_ids is not None:\n",
    "            rotary_pos_emb = rotary_pos_emb[position_ids]\n",
    "        else:\n",
    "            rotary_pos_emb = rotary_pos_emb[None, :seq_length]\n",
    "\n",
    "# Run the encoder\n",
    "        hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(\n",
    "            inputs_embeds, full_attention_mask, rotary_pos_emb=rotary_pos_emb,\n",
    "            kv_caches=past_key_values, use_cache=use_cache, output_hidden_states=output_hidden_states\n",
    "        )  # 使用 GLMTransformer 类\n",
    "        if presents is not None and type(presents) is torch.Tensor:\n",
    "            presents = presents.split(1, dim=0)\n",
    "            presents = list(presents)\n",
    "            presents = [list(x.squeeze(0).split(1, dim=0)) for x in presents]\n",
    "            presents = [tuple([x.squeeze(0) for x in y]) for y in presents]\n",
    "            presents = tuple(presents)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
    "\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=presents,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b86c96-a071-49aa-8676-f1d8be23a31f",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1. **ChatGLMPreTrainedModel**:\n",
    "- **get_masks**: Generates attention masks.\n",
    "- **get_position_ids**: Generates position IDs.\n",
    "\n",
    "2. **Embedding**:\n",
    "- **word_embeddings**: Implements word embeddings.\n",
    "\n",
    "3. **ChatGLMModel**:\n",
    "- **RotaryEmbedding**: For position encoding.\n",
    "- **GLMTransformer**: Implements Transformer encoder.\n",
    "- **forward**: Performs forward propagation, integrating all modules.\n",
    "\n",
    "4. **Previously built modules used**:\n",
    "- **RotaryEmbedding**: For generating rotation position embeddings.\n",
    "- **GLMTransformer**: For the encoder part.\n",
    "- **Embedding**: For generating word embeddings.\n",
    "- **CoreAttention**, **SelfAttention**: Indirectly used through GLMTransformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af49b32-bd82-4b62-9d95-494ae2d78dfb",
   "metadata": {},
   "source": [
    "### ChatGLMForConditionalGeneration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4725aadc-c0e6-4d42-bebb-a4194dd695f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGLMForConditionalGeneration(ChatGLMPreTrainedModel):\n",
    "    def __init__(self, config: ChatGLMConfig, empty_init=True, device=None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.max_sequence_length = config.max_length  # 最大序列长度\n",
    "        self.transformer = ChatGLMModel(config, empty_init=empty_init, device=device)  # 使用 ChatGLMModel 类\n",
    "        self.config = config\n",
    "\n",
    "    def _update_model_kwargs_for_generation(\n",
    "            self,\n",
    "            outputs: ModelOutput,\n",
    "            model_kwargs: Dict[str, Any],\n",
    "            is_encoder_decoder: bool = False,\n",
    "            standardize_cache_format: bool = False,\n",
    "    ) -> Dict[str, Any]:\n",
    "# Update past_key_values\n",
    "        model_kwargs[\"past_key_values\"] = self._extract_past_from_model_output(\n",
    "            outputs, standardize_cache_format=standardize_cache_format\n",
    "        )\n",
    "\n",
    "# Update the attention mask\n",
    "        if \"attention_mask\" in model_kwargs:\n",
    "            attention_mask = model_kwargs[\"attention_mask\"]\n",
    "            model_kwargs[\"attention_mask\"] = torch.cat(\n",
    "                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
    "            )\n",
    "\n",
    "# Update location ids\n",
    "        if \"position_ids\" in model_kwargs:\n",
    "            position_ids = model_kwargs[\"position_ids\"]\n",
    "            new_position_id = position_ids[..., -1:].clone()\n",
    "            new_position_id += 1\n",
    "            model_kwargs[\"position_ids\"] = torch.cat(\n",
    "                [position_ids, new_position_id], dim=-1\n",
    "            )\n",
    "\n",
    "        model_kwargs[\"is_first_forward\"] = False\n",
    "        return model_kwargs\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor,\n",
    "            past_key_values: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            is_first_forward: bool = True,\n",
    "            **kwargs\n",
    "    ) -> dict:\n",
    "# If past_key_values ​​is not empty, only take the last token of input_ids\n",
    "        if position_ids is None:\n",
    "            position_ids = self.get_position_ids(input_ids, device=input_ids.device)\n",
    "        if not is_first_forward:\n",
    "            if past_key_values is not None:\n",
    "                position_ids = position_ids[..., -1:]\n",
    "                input_ids = input_ids[:, -1:]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"return_last_logit\": True,\n",
    "            \"use_cache\": use_cache\n",
    "        }\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            labels: Optional[torch.Tensor] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "            return_last_logit: Optional[bool] = False,\n",
    "    ):\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )  # 使用 ChatGLMModel 类\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        if return_last_logit:\n",
    "            hidden_states = hidden_states[:, -1:]\n",
    "        lm_logits = self.transformer.output_layer(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            lm_logits = lm_logits.to(torch.float32)\n",
    "\n",
    "# Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "# Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            lm_logits = lm_logits.to(hidden_states.dtype)\n",
    "            loss = loss.to(hidden_states.dtype)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(\n",
    "            past: Tuple[Tuple[torch.Tensor, torch.Tensor], ...], beam_idx: torch.LongTensor\n",
    "    ) -> Tuple[Tuple[torch.Tensor, torch.Tensor], ...]:\n",
    "        \"\"\"\n",
    "        重新排序 `past_key_values` 缓存以匹配每个生成步骤中的 `beam_idx`。\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            (\n",
    "                layer_past[0].index_select(0, beam_idx.to(layer_past[0].device)),\n",
    "                layer_past[1].index_select(0, beam_idx.to(layer_past[1].device)),\n",
    "            )\n",
    "            for layer_past in past\n",
    "        )\n",
    "\n",
    "    def process_response(self, output, history):\n",
    "        content = \"\"\n",
    "        history = deepcopy(history)\n",
    "        for response in output.split(\"\"):\n",
    "            if \"\\n\" in response:\n",
    "                metadata, content = response.split(\"\\n\", maxsplit=1)\n",
    "            else:\n",
    "                metadata, content = \"\", response\n",
    "            if not metadata.strip():\n",
    "                content = content.strip()\n",
    "                history.append({\"role\": \"assistant\", \"metadata\": metadata, \"content\": content})\n",
    "                content = content.replace(\"[[训练时间]]\", \"2023年\")\n",
    "            else:\n",
    "                history.append({\"role\": \"assistant\", \"metadata\": metadata, \"content\": content})\n",
    "                if history[0][\"role\"] == \"system\" and \"tools\" in history[0]:\n",
    "                    parameters = json.loads(content)\n",
    "                    content = {\"name\": metadata.strip(), \"parameters\": parameters}\n",
    "                else:\n",
    "                    content = {\"name\": metadata.strip(), \"content\": content}\n",
    "        return content, history\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def chat(self, tokenizer, query: str, history: List[Dict] = None, role: str = \"user\",\n",
    "             max_length: int = 8192, num_beams=1, do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None,\n",
    "             **kwargs):\n",
    "        if history is None:\n",
    "            history = []\n",
    "        if logits_processor is None:\n",
    "            logits_processor = LogitsProcessorList()\n",
    "        logits_processor.append(InvalidScoreLogitsProcessor())\n",
    "        gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams, \"do_sample\": do_sample, \"top_p\": top_p,\n",
    "                      \"temperature\": temperature, \"logits_processor\": logits_processor, **kwargs}\n",
    "        history.append({\"role\": role, \"content\": query})\n",
    "        inputs = tokenizer.apply_chat_template(history, add_generation_prompt=True, tokenize=True,\n",
    "                                               return_tensors=\"pt\", return_dict=True)\n",
    "        inputs = inputs.to(self.device)\n",
    "        eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"\"),\n",
    "                        tokenizer.convert_tokens_to_ids(\"\")]\n",
    "        outputs = self.generate(**inputs, **gen_kwargs, eos_token_id=eos_token_id)\n",
    "        outputs = outputs.tolist()[0][len(inputs[\"input_ids\"][0]):-1]\n",
    "        response = tokenizer.decode(outputs)\n",
    "        response, history = self.process_response(response, history)\n",
    "        return response, history\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def stream_chat(self, tokenizer, query: str, history: List[Dict] = None, role: str = \"user\",\n",
    "                    past_key_values=None, max_length: int = 8192, do_sample=True, top_p=0.8, temperature=0.8,\n",
    "                    logits_processor=None, return_past_key_values=False, **kwargs):\n",
    "        if history is None:\n",
    "            history = []\n",
    "        if logits_processor is None:\n",
    "            logits_processor = LogitsProcessorList()\n",
    "        logits_processor.append(InvalidScoreLogitsProcessor())\n",
    "        eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"\"),\n",
    "                        tokenizer.convert_tokens_to_ids(\"\")]\n",
    "        gen_kwargs = {\"max_length\": max_length, \"do_sample\": do_sample, \"top_p\": top_p,\n",
    "                      \"temperature\": temperature, \"logits_processor\": logits_processor, **kwargs}\n",
    "        if past_key_values is None:\n",
    "            inputs = tokenizer.apply_chat_template(history + [{\"role\": role, \"content\": query}],\n",
    "                                                   add_generation_prompt=True, tokenize=True, return_tensors=\"pt\",\n",
    "                                                   return_dict=True)\n",
    "        else:\n",
    "            inputs = tokenizer.apply_chat_template([{\"role\": role, \"content\": query}], add_special_tokens=False,\n",
    "                                                   add_generation_prompt=True, tokenize=True, return_tensors=\"pt\",\n",
    "                                                   return_dict=True)\n",
    "        inputs = inputs.to(self.device)\n",
    "        if past_key_values is not None:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "            inputs.position_ids += past_length\n",
    "\n",
    "\n",
    "            attention_mask = inputs.attention_mask\n",
    "            attention_mask = torch.cat((attention_mask.new_ones(1, past_length), attention_mask), dim=1)\n",
    "            inputs['attention_mask'] = attention_mask\n",
    "        history.append({\"role\": role, \"content\": query})\n",
    "        for outputs in self.stream_generate(**inputs, past_key_values=past_key_values,\n",
    "                                            eos_token_id=eos_token_id, return_past_key_values=return_past_key_values,\n",
    "                                            **gen_kwargs):\n",
    "            if return_past_key_values:\n",
    "                outputs, past_key_values = outputs\n",
    "            outputs = outputs.tolist()[0][len(inputs[\"input_ids\"][0]):-1]\n",
    "            response = tokenizer.decode(outputs)\n",
    "            if response and response[-1] != \"�\":\n",
    "                response, new_history = self.process_response(response, history)\n",
    "                if return_past_key_values:\n",
    "                    yield response, new_history, past_key_values\n",
    "                else:\n",
    "                    yield response, new_history\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def stream_generate(\n",
    "            self,\n",
    "            input_ids,\n",
    "            generation_config: Optional[GenerationConfig] = None,\n",
    "            logits_processor: Optional[LogitsProcessorList] = None,\n",
    "            stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "            prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "            return_past_key_values=False,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n",
    "\n",
    "        if generation_config is None:\n",
    "            generation_config = self.generation_config\n",
    "        generation_config = copy.deepcopy(generation_config)\n",
    "        model_kwargs = generation_config.update(**kwargs)\n",
    "        model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
    "        bos_token_id, eos_token_id = generation_config.bos_token_id, generation_config.eos_token_id\n",
    "\n",
    "        if isinstance(eos_token_id, int):\n",
    "            eos_token_id = [eos_token_id]\n",
    "        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
    "        if has_default_max_length and generation_config.max_new_tokens is None:\n",
    "            warnings.warn(\n",
    "                f\"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. \"\n",
    "                \"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we\"\n",
    "                \" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "        elif generation_config.max_new_tokens is not None:\n",
    "            generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length\n",
    "            if not has_default_max_length:\n",
    "                logger.warn(\n",
    "                    f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n",
    "                    f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n",
    "                    \"Please refer to the documentation for more information. \"\n",
    "                    \"(https://hf-mirror.com/docs/transformers/main/en/main_classes/text_generation)\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "\n",
    "        if input_ids_seq_length >= generation_config.max_length:\n",
    "            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
    "            logger.warning(\n",
    "                f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n",
    "                f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n",
    "                \" increasing `max_new_tokens`.\"\n",
    "            )\n",
    "\n",
    "# 2. Set generation parameters if not already defined\n",
    "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "\n",
    "        logits_processor = self._get_logits_processor(\n",
    "            generation_config=generation_config,\n",
    "            input_ids_seq_length=input_ids_seq_length,\n",
    "            encoder_input_ids=input_ids,\n",
    "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "            logits_processor=logits_processor,\n",
    "        )\n",
    "\n",
    "        stopping_criteria = self._get_stopping_criteria(\n",
    "            generation_config=generation_config, stopping_criteria=stopping_criteria\n",
    "        )\n",
    "        logits_warper = self._get_logits_warper(generation_config)\n",
    "\n",
    "        unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "        scores = None\n",
    "        while True:\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "# Forward pass to get the next token\n",
    "            outputs = self(\n",
    "                **model_inputs,\n",
    "                return_dict=True,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "            )\n",
    "\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "# Preprocessing distribution\n",
    "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "            next_token_scores = logits_warper(input_ids, next_token_scores)\n",
    "\n",
    "# Sampling\n",
    "            probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "            if generation_config.do_sample:\n",
    "                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "            else:\n",
    "                next_tokens = torch.argmax(probs, dim=-1)\n",
    "# Update generated ids, model inputs and length for next step\n",
    "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n",
    "            )\n",
    "            unfinished_sequences = unfinished_sequences.mul(\n",
    "                next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
    "            )\n",
    "            if return_past_key_values:\n",
    "                yield input_ids, outputs.past_key_values\n",
    "            else:\n",
    "                yield input_ids\n",
    "# Stop when each sentence is complete or exceeds the maximum length\n",
    "            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a7db6f-8889-4e73-b882-edf7e4c72ca7",
   "metadata": {},
   "source": [
    "### ChatGLMForSequenceClassification class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "832d1a09-3f8c-4035-ad73-ec3433a71f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGLMForSequenceClassification(ChatGLMPreTrainedModel):\n",
    "    def __init__(self, config: ChatGLMConfig, empty_init=True, device=None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.num_labels = config.num_labels  # 标签数量\n",
    "        self.transformer = ChatGLMModel(config, empty_init=empty_init, device=device)  # 使用 ChatGLMModel 类\n",
    "\n",
    "        self.classifier_head = nn.Linear(config.hidden_size, config.num_labels, bias=True, dtype=torch.half)\n",
    "        if config.classifier_dropout is not None:\n",
    "            self.dropout = nn.Dropout(config.classifier_dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        self.config = config\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.LongTensor] = None,\n",
    "            position_ids: Optional[torch.LongTensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            full_attention_mask: Optional[torch.Tensor] = None,\n",
    "            past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n",
    "            inputs_embeds: Optional[torch.LongTensor] = None,\n",
    "            labels: Optional[torch.LongTensor] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor, ...], SequenceClassifierOutputWithPast]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            full_attention_mask=full_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )  # 使用 ChatGLMModel 类\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        pooled_hidden_states = hidden_states[:, -1]\n",
    "        if self.dropout is not None:\n",
    "            pooled_hidden_states = self.dropout(pooled_hidden_states)\n",
    "        logits = self.classifier_head(pooled_hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze().float(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits.float(), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels).float(), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits.float(), labels.view(-1, self.num_labels))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss\n",
    "\n",
    "=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415020b6-a65c-4738-aaac-c25b00c84996",
   "metadata": {},
   "source": [
    "### Previously built modules used\n",
    "\n",
    "In this code, multiple modules and methods are based on previously built classes and functions:\n",
    "\n",
    "1. **ChatGLMModel**:\n",
    "- **Used in ChatGLMForConditionalGeneration and ChatGLMForSequenceClassification**, as the core part of the Transformer model.\n",
    "\n",
    "2. **ChatGLMPreTrainedModel**:\n",
    "- **As the base class of ChatGLMForConditionalGeneration and ChatGLMForSequenceClassification**, it provides interfaces for weight initialization and loading pre-trained models.\n",
    "\n",
    "3. **RotaryEmbedding**:\n",
    "- **Used for position encoding in ChatGLMModel**.\n",
    "\n",
    "4. **CoreAttention and SelfAttention**:\n",
    "- **Used in GLMTransformer**, implementing the core part of the attention mechanism.\n",
    "\n",
    "The detailed comments and explanations provide a better understanding of the construction and implementation principles of the code. Together, these modules form the entire ChatGLM model.The whole architecture realizes the functions of conditional generation and sequence classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e2c5d-40de-4c9d-a8b7-e49099306fa7",
   "metadata": {},
   "source": [
    "### Meaning of `past_key_values` variable\n",
    "\n",
    "In Transformer models, especially models used for generation tasks, such as GPT-like models, `past_key_values` is a very important variable. It is used to cache the key and value vectors calculated by the model in the previous time step. These cached data can be reused in subsequent time steps, thereby improving computational efficiency, especially in long sequence generation tasks.\n",
    "\n",
    "### The role of `past_key_values`\n",
    "\n",
    "1. **Cache previous calculation results**:\n",
    "In the process of generating text, a new word is generated at each step. At this time, the query vector (query) of the current time step needs to be calculated with the key and value vectors of all previous time steps. If all keys and values ​​are recalculated every time, it will be very inefficient. `past_key_values` caches the results of these previous time steps to avoid repeated calculations.\n",
    "\n",
    "2. **Speed ​​up the generation process**:\n",
    "In long sequence generation, by caching the key and value vectors of previous time steps, only the current time step needs to be calculated and combined with the cached results, which greatly speeds up the generation process.\n",
    "\n",
    "### Specific implementation of `past_key_values`\n",
    "\n",
    "#### In `ChatGLMForConditionalGeneration` Usage in class\n",
    "\n",
    "```python\n",
    "def prepare_inputs_for_generation(\n",
    "self,\n",
    "input_ids: torch.LongTensor,\n",
    "past_key_values: Optional[torch.Tensor] = None,\n",
    "attention_mask: Optional[torch.Tensor] = None,\n",
    "position_ids: Optional[torch.Tensor] = None,\n",
    "use_cache: Optional[bool] = None,\n",
    "is_first_forward: bool = True,\n",
    "**kwargs\n",
    ") -> dict:\n",
    "# If past_key_values ​​is not empty, only take the last token of input_ids\n",
    "if position_ids is None:\n",
    "position_ids = self.get_position_ids(input_ids, device=input_ids.device)\n",
    "if not is_first_forward:\n",
    "if past_key_values ​​is not None:\n",
    "position_ids = position_ids[..., -1:]\n",
    "input_ids = input_ids[:, -1:]\n",
    "return {\n",
    "\"input_ids\": input_ids,\n",
    "\"past_key_values\": past_key_values,\n",
    "\"position_ids\": position_ids,\n",
    "\"attention_mask\": attention_mask,\n",
    "\"return_last_logit\": True,\n",
    "\"use_cache\": use_cache\n",
    "}\n",
    "```\n",
    "\n",
    "In the `prepare_inputs_for_generation` method, if `past_key_values` is not empty, only the last token of `input_ids` is taken. The purpose of this is to calculate only the data of the current time step when generating a new token, without recalculating the entire sequence.\n",
    "\n",
    "#### In the `forward` method of the `ChatGLMForConditionalGeneration` class\n",
    "\n",
    "```python\n",
    "def forward(\n",
    "self,\n",
    "input_ids: Optional[torch.Tensor] = None,\n",
    "position_ids: Optional[torch.Tensor] = None,\n",
    "attention_mask: Optional[torch.Tensor] = None,\n",
    "past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "inputs_embeds: Optional[torch.Tensor] = None,\n",
    "labels: Optional[torch.Tensor] = None,\n",
    "use_cache: Optional[bool] = None,\n",
    "output_attentions: Optional[bool] = None,\n",
    "output_hidden_states: Optional[bool] = None,\n",
    "return_dict: Optional[bool] = None,\n",
    "return_last_logit: Optional[bool] = False,\n",
    "):\n",
    "use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "transformer_outputs = self.transformer(\n",
    "input_ids=input_ids,position_ids=position_ids,\n",
    "attention_mask=attention_mask,\n",
    "past_key_values=past_key_values,\n",
    "inputs_embeds=inputs_embeds,\n",
    "use_cache=use_cache,\n",
    "output_hidden_states=output_hidden_states,\n",
    "return_dict=return_dict,\n",
    ") # Using ChatGLMModel class\n",
    "\n",
    "hidden_states = transformer_outputs[0]\n",
    "if return_last_logit:\n",
    "hidden_states = hidden_states[:, -1:]\n",
    "lm_logits = self.transformer.output_layer(hidden_states)\n",
    "\n",
    "loss = None\n",
    "if labels is not None:\n",
    "lm_logits = lm_logits.to(torch.float32)\n",
    "\n",
    "# Shift so that tokens < n predict n\n",
    "shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "# Flatten the tokens\n",
    "loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "lm_logits = lm_logits.to(hidden_states.dtype)\n",
    "loss = loss.to(hidden_states.dtype)\n",
    "\n",
    "ifnot return_dict:\n",
    "output = (lm_logits,) + transformer_outputs[1:]\n",
    "return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "return CausalLMOutputWithPast(\n",
    "loss=loss,\n",
    "logits=lm_logits,\n",
    "past_key_values=transformer_outputs.past_key_values,\n",
    "hidden_states=transformer_outputs.hidden_states,\n",
    "attentions=transformer_outputs.attentions,\n",
    ")\n",
    "```\n",
    "\n",
    "In the `forward` method, `past_key_values` is passed as a parameter to the `transformer` model. The `transformer` model uses these cached key and value vectors internally to speed up calculations.\n",
    "`p`ast_key_values` is a caching mechanism used to speed up the Transformer model in generation tasks. It saves the key and value vectors calculated in the previous time step, avoiding repeated calculation of these vectors in each time step, thereby improving the efficiency of the generation process. By using `past_key_values`, the model can generate long sequence data faster, which is very important in practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e361aa-a1b5-4eae-9178-306b888713f4",
   "metadata": {},
   "source": [
    "### Function analysis and its differences and connections\n",
    "\n",
    "In the `ChatGLMForConditionalGeneration` class, there are three important functions related to generation: `_update_model_kwargs_for_generation`, `prepare_inputs_for_generation` and `forward`. Their functions, differences and connections are as follows:\n",
    "\n",
    "#### 1. `_update_model_kwargs_for_generation` function\n",
    "\n",
    "```python\n",
    "def _update_model_kwargs_for_generation(\n",
    "self,\n",
    "outputs: ModelOutput,\n",
    "model_kwargs: Dict[str, Any],\n",
    "is_encoder_decoder: bool = False,\n",
    "standardize_cache_format: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "# Update past_key_values\n",
    "model_kwargsrgs[\"past_key_values\"] = self._extract_past_from_model_output(\n",
    "outputs, standardize_cache_format=standardize_cache_format\n",
    ")\n",
    "# Update attention mask\n",
    "if \"attention_mask\" in model_kwargs:\n",
    "attention_mask = model_kwargs[\"attention_mask\"]\n",
    "model_kwargs[\"attention_mask\"] = torch.cat(\n",
    "[attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
    ")\n",
    "# Update position ids\n",
    "if \"position_ids\" in model_kwargs:\n",
    "position_ids = model_kwargs[\"position_ids\"]on_ids\"]\n",
    "new_position_id = position_ids[..., -1:].clone()\n",
    "new_position_id += 1\n",
    "model_kwargs[\"position_ids\"] = torch.cat(\n",
    "[position_ids, new_position_id], dim=-1\n",
    ")\n",
    "\n",
    "model_kwargs[\"is_first_forward\"] = False\n",
    "return model_kwargs\n",
    "```\n",
    "\n",
    "- **Function**: Updates the model parameters required for the generation process. Specifically:\n",
    "- Update `past_key_values` to cache previously calculated key and value vectors.\n",
    "- Update `attention_mask` to include the newly generated token.\n",
    "- Update `position_ids` to add the new position ID.\n",
    "- **Difference**: This function does not directly perform forward propagation, but updates the model parameters to prepare for the next step of generation.\n",
    "- **Relationship**: This function is called after each step of generating a new token, usingTo update the model parameters and prepare for the next generation.\n",
    "\n",
    "#### 2. `prepare_inputs_for_generation` function\n",
    "\n",
    "```python\n",
    "def prepare_inputs_for_generation(\n",
    "self,\n",
    "input_ids: torch.LongTensor,\n",
    "past_key_values: Optional[torch.Tensor] = None,\n",
    "attention_mask: Optional[torch.Tensor] = None,\n",
    "position_ids: Optional[torch.Tensor] = None,\n",
    "use_cache: Optional[bool] = None,\n",
    "is_first_forward: bool = True,\n",
    "**kwargs\n",
    ") -> dict:\n",
    "# If past_key_values ​​is not empty, only take the last token of input_ids\n",
    "if position_ids is None:\n",
    "position_ids = self.get_position_ids(input_ids, device=input_ids.device)\n",
    "if not is_first_forward:\n",
    "if past_key_values ​​is not None:\n",
    "position_ids = position_ids[..., -1:]\n",
    "input_ids = input_ids[:, -1:]\n",
    "return {\n",
    "\"input_ids\": input_ids,\n",
    "\"past_key_values\": past_key_values,\n",
    "\"position_ids\": position_ids,\n",
    "\"attention_mask\": attention_mask,\n",
    "\"return_last_logit\": True,\n",
    "\"use_cache\": use_cache\n",
    "}```\n",
    "\n",
    "- **Function**: Prepare the input required for the generation process. Specifically:\n",
    "- Get or update `position_ids`.\n",
    "- If it is not the first forward propagation and `past_key_values` exists, only the last token of `input_ids` and `position_ids` is taken.\n",
    "- **Difference**: This function is mainly used to process input data to ensure that the shape and content of the input data are suitable for the current generation step.\n",
    "- **Relationship**: In each step of the generation process, this function will be called to prepare the input data, especially to process `past_key_values` to improve the generation efficiency.\n",
    "\n",
    "#### 3. `forward` function\n",
    "\n",
    "```python\n",
    "def forward(\n",
    "self,\n",
    "input_ids: Optional[torch.Tensor] = None,\n",
    "position_ids: Optional[torch.Tensor] = None,\n",
    "attention_mask: Optional[torch.Tensor] = None,\n",
    "past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "inputs_embeds: Optional[torch.Tensor] = None,\n",
    "labels: Optional[torch.Tensor] = None,\n",
    "use_cache: Optional[bool] = None,\n",
    "output_attentions: Optional[bool] = None,\n",
    "output_hidden_states: Optional[bool] = None,\n",
    "return_dict: Optional[bool] = None,\n",
    "return_last_logit: Optional[bool] = False,\n",
    "):\n",
    "use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "transformer_outputs = self.transformer(\n",
    "input_ids=input_ids,\n",
    "position_ids=position_ids,\n",
    "attention_mask=attention_mask,\n",
    "past_key_values=past_key_values,\n",
    "inputs_embeds=inputs_embeds,\n",
    "use_cache=use_cache,\n",
    "output_hidden_states=output_hidden_states,\n",
    "return_dict=return_dict,\n",
    ") # Use ChatGLMModel class\n",
    "\n",
    "hidden_states = transformer_outputs[0]\n",
    "if return_last_logit:\n",
    "hidden_states = hidden_states[:, -1:]\n",
    "lm_logits = self.transformer.output_layer(hidden_states)\n",
    "\n",
    "loss = None\n",
    "if labels is not None:\n",
    "lm_logits = lm_logits.to(torch.float32)\n",
    "\n",
    "# Shift so that tokens < n predict n\n",
    "shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "shift_labels = labels[..., 1:].contiguous()\n",
    "# Flatten the tokens\n",
    "loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shiftt_labels.view(-1))\n",
    "\n",
    "lm_logits = lm_logits.to(hidden_states.dtype)\n",
    "\n",
    "loss = loss.to(hidden_states.dtype)\n",
    "\n",
    "if not return_dict:\n",
    "\n",
    "output = (lm_logits,) + transformer_outputs[1:]\n",
    "\n",
    "return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "return CausalLMOutputWithPast(\n",
    "\n",
    "loss=loss,\n",
    "\n",
    "logits=lm_logits,\n",
    "\n",
    "past_key_values=transformer_outputs.past_key_values,\n",
    "\n",
    "hidden_states=transformer_outputs.hidden_states,\n",
    "\n",
    "attentions=transformer_outputs.attentions,\n",
    ")\n",
    "```\n",
    "\n",
    "- **Function**: Perform forward propagation and generate the output of the model. Specifically:\n",
    "- Pass the input data to `transformer` (`ChatGLMModel`) for forward calculation.\n",
    "- Calculate the logits of the language model and (if there is a label) calculate the loss.\n",
    "- Return the model output, including logits, `past_key_values`, hidden state, and attention weights.\n",
    "- **Difference**: This is the core forward propagation logic of the model, which directly processes the input data and generates output.\n",
    "- **Relationship**: The `forward` function uses the `ChatGLMModel` class to perform the actual forward propagation and calls the previous `prepare_inputs_for_generation` to prepare the input.\n",
    "\n",
    "### Relationship and process\n",
    "\n",
    "1. **Prepare input data**:\n",
    "- The `prepare_inputs_for_generation` function is used to process the input data, especially the `past_key_values` so that only the necessary last token is passed.\n",
    "\n",
    "2. **Perform forward propagation**:\n",
    "- The `forward` function uses the prepared input data to perform forward propagation.Generate output.\n",
    "\n",
    "3. **Update model parameters**:\n",
    "- The `_update_model_kwargs_for_generation` function updates the key parameters of the model (such as `past_key_values`, `attention_mask`, and `position_ids`) after each generation step to ensure that the latest data is used in the next generation.\n",
    "\n",
    "Through the close cooperation of these functions, the forward propagation and cache management in the generation task can be efficiently implemented, thereby improving the generation efficiency and effect of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbd264-47bc-43bf-853a-4470462e31cd",
   "metadata": {},
   "source": [
    "These three functions are the core functions in the `ChatGLMForConditionalGeneration` class, which are used to handle different generation task requirements. There is a certain connection between them, and they also have their own uses and characteristics. The following is a detailed explanation of them and their differences and connections:\n",
    "\n",
    "### 1. `chat` function\n",
    "\n",
    "```python\n",
    "@torch.inference_mode()\n",
    "def chat(self, tokenizer, query: str, history: List[Dict] = None, role: str = \"user\",\n",
    "max_length: int = 8192, num_beams=1, do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None,\n",
    "**kwargs):\n",
    "if history is None:\n",
    "history = []\n",
    "if logits_processor is None:\n",
    "logits_processor = LogitsProcessorList()\n",
    "logits_processor.append(InvalidScoreLogitsProcessor())\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams, \"do_sample\": do_sample, \"top_p\": top_p,\n",
    "\"temperature\": temperature, \"logits_processor\": logits_processor, **kwargs}\n",
    "history.append({\"role\": role, \"content\": query})\n",
    "inputs = tokenizer.apply_chat_template(history, add_generation_prompt=True, tokenize=True,\n",
    "return_tensors=\"pt\", return_dict=True)inputs = inputs.to(self.device)\n",
    "eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"\"), tokenizer.convert_tokens_to_ids(\"\")]\n",
    "outputs = self.generate(**inputs, **gen_kwargs, eos_token_id=eos_token_id)\n",
    "outputs = outputs.tolist()[0][len(inputs[\"input_ids\"][0]):-1]\n",
    "response = tokenizer.decode(outputs)\n",
    "response, history = self.process_response(response, history)\n",
    "return response, history\n",
    "```\n",
    "\n",
    "- **Purpose**: Execute a complete chat session. Encode the user's query and history into model inputs, generate responses and update history.\n",
    "- **Difference**:This is a high-level interface for generating a complete response in one go. It is suitable for scenarios where a complete answer is needed immediately.\n",
    "- **Contact**: It relies on the `generate` function to actually generate the response, and calls the `process_response` function to process the generated output.\n",
    "\n",
    "### 2. `stream_chat` function\n",
    "\n",
    "```python\n",
    "@torch.inference_mode()\n",
    "def stream_chat(self, tokenizer, query: str, history: List[Dict] = None, role: str = \"user\",\n",
    "past_key_values=None, max_length: int = 8192, do_sample=True, top_p=0.8, temperature=0.8,\n",
    "logits_processor=None, return_past_key_values=False, **kwargs):\n",
    "if history is None:\n",
    "history = []if logits_processor is None:\n",
    "logits_processor = LogitsProcessorList()\n",
    "logits_processor.append(InvalidScoreLogitsProcessor())\n",
    "eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"\"), tokenizer.convert_tokens_to_ids(\"\")]\n",
    "gen_kwargs = {\"max_length\": max_length, \"do_sample\": do_sample, \"top_p\": top_p,\n",
    "\"temperature\": temperature, \"logits_processor\": logits_processor, **kwargs}\n",
    "if past_key_values ​​is None:\n",
    "inputs = tokenizer.apply_chat_template(history + [{\"role\": role, \"content\": query}],\n",
    "add_generation_prompt=True, tokenize=True, return_tensors=\"pt\",\n",
    "return_dict=True)\n",
    "else:\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": role, \"content\": query}], add_special_tokens=False,\n",
    "add_generation_prompt=True, tokenize=True, return_tensors=\"pt\",return_dict=True)\n",
    "inputs = inputs.to(self.device)\n",
    "if past_key_values ​​is not None:\n",
    "past_length = past_key_values[0][0].shape[2]\n",
    "inputs.position_ids += past_length\n",
    "attention_mask = inputs.attention_mask\n",
    "attention_mask = torch.cat((attention_mask.new_ones(1, past_length), attention_mask), dim=1)\n",
    "inputs['attention_mask'] = attention_mask\n",
    "history.append({\"role\": role, \"content\": query})\n",
    "for outputs in self.stream_generate(**inputs,past_key_values=past_key_values,\n",
    "eos_token_id=eos_token_id, return_past_key_values=return_past_key_values,\n",
    "**gen_kwargs):\n",
    "if return_past_key_values:\n",
    "outputs, past_key_values ​​= outputs\n",
    "outputs = outputs.tolist()[0][len(inputs[\"input_ids\"][0]):-1]\n",
    "response = tokenizer.decode(outputs)\n",
    "if response and response[-1] != \"�\":\n",
    "response, new_history = self.process_response(response, history)\n",
    "if return_past_key_values:\n",
    "yield response, new_history, past_key_values\n",
    "else:\n",
    "yield response, new_history\n",
    "```\n",
    "\n",
    "- **Purpose**: Implement a streaming chat session. Similar to the `chat` function, it returns responses gradually through a generator, which is suitable for streaming generation application scenarios.\n",
    "- **Difference**: Supports gradual generation of responses, so that partial responses can be dynamically processed and displayed during the generation process.\n",
    "- **Relationship**: Depends on the `stream_generate` function to gradually generate responses, and calls the `process_response` function to process and update history after each new response fragment is generated.\n",
    "\n",
    "### 3. `stream_generate` function\n",
    "\n",
    "```python\n",
    "@torch.inference_mode()\n",
    "def stream_generate(\n",
    "self,\n",
    "input_ids,generation_config: Optional[GenerationConfig] = None,\n",
    "logits_processor: Optional[LogitsProcessorList] = None,\n",
    "stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "return_past_key_values=False,\n",
    "**kwargs,\n",
    "):\n",
    "batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n",
    "\n",
    "if generation_config is None:\n",
    "generation_config = self.generation_config\n",
    "generation_config = copy.deepcopy(generation_config)\n",
    "model_kwargs = generation_config.update(**kwargs)\n",
    "model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
    "bos_token_id, eos_token_id = generation_config.bos_token_id, generation_config.eos_token_id\n",
    "\n",
    "if isinstance(eos_token_id, int):\n",
    "eos_token_id = [eos_token_id]\n",
    "eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n",
    "\n",
    "has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
    "if has_default_max_length and generation_config.max_new_tokens is None:\n",
    "warnings.warn(\n",
    "f\"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. \"\n",
    "\"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we\"\n",
    "\" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n",
    "UserWarning,\n",
    ")\n",
    "elif generation_config.max_new_tokens is not None:\n",
    "generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length\n",
    "if not has_default_max_length:\n",
    "logger.warn(\n",
    "f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n",
    "f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n",
    "\"Please refer to the documentation forr more information. \"\n",
    "\"(https://hf-mirror.com/docs/transformers/main/en/main_classes/text_generation)\",\n",
    "UserWarning,\n",
    ")\n",
    "\n",
    "if input_ids_seq_length >= generation_config.max_length:\n",
    "input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n",
    "logger.warning(\n",
    "f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n",
    "f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n",
    "\"increasing `max_new_tokens`.\"\n",
    ")\n",
    "\n",
    "# 2. Set generation parameters if not already defined\n",
    "logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
    "stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
    "\n",
    "logits_processor = self._get_logits_processor(\n",
    "generation_config=generation_config,\n",
    "input_ids_seq_length=input_ids_seq_length,\n",
    "encoder_input_ids = input_ids,\n",
    "prefix_allowed_tokens_fn = prefix_allowed_tokens_fn,\n",
    "logits_processor = logits_processor,\n",
    ")\n",
    "stopping_criteria = self._get_stopping_criteria(\n",
    "generation_config = generation_config, stopping_criteria = stopping_criteria\n",
    ")\n",
    "logits_warper = self._get_logits_warper(generation_config)\n",
    "\n",
    "unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n",
    "scores = None\n",
    "while True:\n",
    "model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "# forward pass to get the next token\n",
    "outputs = self(\n",
    "**model_inputs,\n",
    "return_dict=True,\n",
    "output_attentions=False,\n",
    "output_hidden_states=False,\n",
    ")\n",
    "\n",
    "next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "# preprocess distribution\n",
    "next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "next_token_scores = logits_warper(input_ids, next_token_scores)\n",
    "\n",
    "# sampling\n",
    "probs= nn.functional.softmax(next_token_scores, dim=-1)\n",
    "if generation_config.do_sample:\n",
    "next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "else:\n",
    "next_tokens = torch.argmax(probs, dim=-1)\n",
    "\n",
    "# Update generated ids, model inputs, and length of next step\n",
    "input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "model_kwargs = self._update_model_kwargs_for_generation(\n",
    "outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder)\n",
    "unfinished_sequences = unfinished_sequences.mul(\n",
    "next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n",
    ")\n",
    "if return_past_key_values:\n",
    "yield input_ids, outputs.past_key_values\n",
    "else:\n",
    "yield input_ids\n",
    "# Stop when each sentence is completed or exceeds the maximum length\n",
    "if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n",
    "break\n",
    "```\n",
    "\n",
    "- **Purpose**: Generate model output in a streaming manner, and gradually return the generated tokens for real-time processing.\n",
    "- **Difference**: This functionThe generator is used to generate data in a streaming manner, and the result is returned once each token is generated. It is suitable for application scenarios that need to display the generated results step by step.\n",
    "- **Relationship**: The `stream_chat` function relies on `stream_generate` to generate responses step by step, and updates the input and model parameters after each new token is generated.\n",
    "\n",
    "### Summary of differences and connections\n",
    "\n",
    "1. **Relationship**:\n",
    "- `chat` function: used to generate a complete response at one time, suitable for application scenarios that need to get a complete answer immediately.\n",
    "- `stream_chat` function: used to generate responses in a streaming manner, suitable for application scenarios that gradually display the generated results.\n",
    "- `stream_generate` function: implements the core logic of streaming generation, and gradually returns the generated tokens through the generator.\n",
    "\n",
    "2. **Relationship**:\n",
    "- `chat` and `stream_chat` are both high-level interfaces through which users interact with the model.\n",
    "- The `chat` function calls the `generate` function to generate data in one go, while the `stream_chat` function calls the `stream_generate` function to generate data in a streaming manner.\n",
    "- `stream_generate` function uses `prepare_inputs_for_generation` to process the input data, and update the model parameters through `_update_model_kwargs_for_generation` to ensure that the latest data is used during the generation process.\n",
    "\n",
    "Through the coordinated work of these three functions, the model can achieve efficient and flexible generation tasks to meet the needs of different application scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62284871-67f4-47a2-941f-46befd2032b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = ChatGLM4Tokenizer.from_pretrained(\"THUDM/glm-4-9b-chat\", trust_remote_code=True)\n",
    "\n",
    "query = \"你好\"\n",
    "\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": query}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       )\n",
    "\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0801f0ee-2d71-4eda-b2f5-5e00209cd0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bf96f6983b4492b2d95ae8f5eaa7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "你好👋！有什么可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "model = ChatGLMForConditionalGeneration.from_pretrained(\n",
    "    \"THUDM/glm-4-9b-chat\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    load_in_4bit=True,\n",
    "    device_map='auto'\n",
    ").eval()#.to(device)\n",
    "\n",
    "gen_kwargs = {\"max_length\": 300, \"do_sample\": True, \"top_k\": 1}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e395e4-2f03-429b-b233-f612a7482ad5",
   "metadata": {},
   "source": [
    "Time taken: 16 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c190f6-e495-49cc-8c20-70e4e8eceefd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Note: Other test drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c9f19-430f-468d-8387-b73e45badeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(\n",
    "        self,\n",
    "        conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]], \"Conversation\"],\n",
    "        add_generation_prompt: bool = False,\n",
    "        tokenize: bool = True,\n",
    "        padding: bool = False,\n",
    "        truncation: bool = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_dict: bool = False,\n",
    "        tokenizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        **kwargs,\n",
    ") -> Union[str, List[int], List[str], List[List[int]], BatchEncoding]:\n",
    "\n",
    "    if return_dict and not tokenize:\n",
    "        raise ValueError(\n",
    "            \"`return_dict=True` is incompatible with `tokenize=False`, because there is no dict \"\n",
    "            \"of tokenizer outputs to return.\"\n",
    "        )\n",
    "\n",
    "    def handle_single_conversation(conversation):\n",
    "        input_ids = self.get_prefix_tokens() if add_special_tokens else []\n",
    "        input_message = \"[gMASK]<sop>\" if add_special_tokens else \"\"\n",
    "        for item in conversation:\n",
    "            if item.get(\"tools\"):\n",
    "                tools = item[\"tools\"]\n",
    "                content = \"你是一个名为 GLM-4 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\"\n",
    "                for tool in tools:\n",
    "                    if tool[\"type\"] == \"function\":\n",
    "                        function = tool[\"function\"]\n",
    "                        content += f\"\\n\\n## {function['name']}\\n\\n{json.dumps(function, ensure_ascii=False, indent=4)}\"\n",
    "                        content += \"\\n在调用上述函数时，请使用 Json 格式表示调用的参数。\"\n",
    "                    elif tool[\"type\"] == \"python\":\n",
    "                        content += \"\\n\\n## python\\n\\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。\"\n",
    "                    elif tool[\"type\"] == \"simple_browser\":\n",
    "                        content += \"\\n\\n## simple_browser\\n\\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\\n`open_url(url: str)`：打开指定的 URL。\\n\\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\\n\\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。\"\n",
    "                    elif tool[\"type\"] == \"cogview\":\n",
    "                        content += \"\\n\\n## cogview\\n\\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。\"\n",
    "                    else:\n",
    "                        raise NotImplementedError(f\"Unknown tool type {tool['type']}\")\n",
    "                input = self.build_single_message(\"system\", \"\", content, tokenize=tokenize)\n",
    "                if tokenize:\n",
    "                    input_ids.extend(input)\n",
    "                else:\n",
    "                    input_message += input\n",
    "            if item[\"content\"]:\n",
    "                input = self.build_single_message(\n",
    "                    item[\"role\"],\n",
    "                    item.get(\"metadata\", \"\"),\n",
    "                    item[\"content\"],\n",
    "                    tokenize=tokenize\n",
    "                )\n",
    "                if tokenize:\n",
    "                    input_ids.extend(input)\n",
    "                else:\n",
    "                    input_message += input\n",
    "        if add_generation_prompt:\n",
    "            if tokenize:\n",
    "                input_ids.extend([self.convert_tokens_to_ids(\"<|assistant|>\")])\n",
    "            else:\n",
    "                input_message += \"<|assistant|>\"\n",
    "\n",
    "        return input_ids if tokenize else input_message\n",
    "\n",
    "# Main logic to handle different conversation formats\n",
    "    if isinstance(conversation, list) and all(isinstance(i, dict) for i in conversation):\n",
    "        result = handle_single_conversation(conversation)\n",
    "    elif isinstance(conversation, list) and all(isinstance(i, list) for i in conversation):\n",
    "        result = [handle_single_conversation(c) for c in conversation]\n",
    "    elif hasattr(conversation, \"messages\"):\n",
    "        result = handle_single_conversation(conversation.messages)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid conversation format\")\n",
    "\n",
    "    if tokenize:\n",
    "        output = self.batch_encode_plus(\n",
    "            [result] if isinstance(result[0], int) else result,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "            return_tensors=return_tensors,\n",
    "            is_split_into_words=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        if return_dict:\n",
    "            return output\n",
    "        else:\n",
    "            return output[\"input_ids\"]\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece39da8-2b28-4415-b226-0919f2beb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_single_conversation(conversation):\n",
    "    input_ids = self.get_prefix_tokens() if add_special_tokens else []\n",
    "    input_message = \"[gMASK]<sop>\" if add_special_tokens else \"\"\n",
    "    for item in conversation:\n",
    "        if item.get(\"tools\"):\n",
    "            tools = item[\"tools\"]\n",
    "            content = \"你是一个名为 GLM-4 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\"\n",
    "            for tool in tools:\n",
    "                if tool[\"type\"] == \"function\":\n",
    "                    function = tool[\"function\"]\n",
    "                    content += f\"\\n\\n## {function['name']}\\n\\n{json.dumps(function, ensure_ascii=False, indent=4)}\"\n",
    "                    content += \"\\n在调用上述函数时，请使用 Json 格式表示调用的参数。\"\n",
    "                elif tool[\"type\"] == \"python\":\n",
    "                    content += \"\\n\\n## python\\n\\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。\"\n",
    "                elif tool[\"type\"] == \"simple_browser\":\n",
    "                    content += \"\\n\\n## simple_browser\\n\\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\\n`open_url(url: str)`：打开指定的 URL。\\n\\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\\n\\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。\"\n",
    "                elif tool[\"type\"] == \"cogview\":\n",
    "                    content += \"\\n\\n## cogview\\n\\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。\"\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Unknown tool type {tool['type']}\")\n",
    "            input = self.build_single_message(\"system\", \"\", content, tokenize=tokenize)\n",
    "            if tokenize:\n",
    "                input_ids.extend(input)\n",
    "            else:\n",
    "                input_message += input\n",
    "        if item[\"content\"]:\n",
    "            input = self.build_single_message(\n",
    "                item[\"role\"],\n",
    "                item.get(\"metadata\", \"\"),\n",
    "                item[\"content\"],\n",
    "                tokenize=tokenize\n",
    "            )\n",
    "            if tokenize:\n",
    "                input_ids.extend(input)\n",
    "            else:\n",
    "                input_message += input\n",
    "    if add_generation_prompt:\n",
    "        if tokenize:\n",
    "            input_ids.extend([self.convert_tokens_to_ids(\"<|assistant|>\")])\n",
    "        else:\n",
    "            input_message += \"<|assistant|>\"\n",
    "\n",
    "    return input_ids if tokenize else input_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc9551-ad44-4ecd-b7a6-00267fbcc186",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tokens_to_ids(\"<|assistant|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef2589e3-5f81-4c93-bb2d-a240b6041411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_single_conversation(conversation):\n",
    "    input_ids = self.get_prefix_tokens() if add_special_tokens else []\n",
    "    input_message = \"[gMASK]<sop>\" if add_special_tokens else \"\"\n",
    "    for item in conversation:\n",
    "        if item.get(\"tools\"):\n",
    "            tools = item[\"tools\"]\n",
    "            content = \"你是一个名为 GLM-4 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\"\n",
    "            for tool in tools:\n",
    "                if tool[\"type\"] == \"function\":\n",
    "                    function = tool[\"function\"]\n",
    "                    content += f\"\\n\\n## {function['name']}\\n\\n{json.dumps(function, ensure_ascii=False, indent=4)}\"\n",
    "                    content += \"\\n在调用上述函数时，请使用 Json 格式表示调用的参数。\"\n",
    "                elif tool[\"type\"] == \"python\":\n",
    "                    content += \"\\n\\n## python\\n\\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。\"\n",
    "                elif tool[\"type\"] == \"simple_browser\":\n",
    "                    content += \"\\n\\n## simple_browser\\n\\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\\n`open_url(url: str)`：打开指定的 URL。\\n\\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\\n\\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。\"\n",
    "                elif tool[\"type\"] == \"cogview\":\n",
    "                    content += \"\\n\\n## cogview\\n\\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。\"\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Unknown tool type {tool['type']}\")\n",
    "            input = self.build_single_message(\"system\", \"\", content, tokenize=tokenize)\n",
    "            if tokenize:\n",
    "                input_ids.extend(input)\n",
    "            else:\n",
    "                input_message += input\n",
    "        if item[\"content\"]:\n",
    "            input = self.build_single_message(\n",
    "                item[\"role\"],\n",
    "                item.get(\"metadata\", \"\"),\n",
    "                item[\"content\"],\n",
    "                tokenize=tokenize\n",
    "            )\n",
    "            if tokenize:\n",
    "                input_ids.extend(input)\n",
    "            else:\n",
    "                input_message += input\n",
    "    if add_generation_prompt:\n",
    "        if tokenize:\n",
    "            input_ids.extend([self.convert_tokens_to_ids(\"<|assistant|>\")])\n",
    "        else:\n",
    "            input_message += \"<|assistant|>\"\n",
    "# if tokenize:\n",
    "# input_ids.extend([self.convert_tokens_to_ids(\"[gMASK]\")]) # Use special tokens instead of empty strings\n",
    "# else:\n",
    "# input_message += \"[gMASK]\"\n",
    "\n",
    "    return input_ids if tokenize else input_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8400577-cfae-44ab-a1a9-831177ec24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(\n",
    "        self,\n",
    "        conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]], \"Conversation\"],\n",
    "        add_generation_prompt: bool = False,\n",
    "        tokenize: bool = True,\n",
    "        padding: bool = False,\n",
    "        truncation: bool = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_dict: bool = False,\n",
    "        tokenizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        **kwargs,\n",
    ") -> Union[str, List[int], List[str], List[List[int]], BatchEncoding]:\n",
    "\n",
    "    if return_dict and not tokenize:\n",
    "        raise ValueError(\n",
    "            \"`return_dict=True` is incompatible with `tokenize=False`, because there is no dict \"\n",
    "            \"of tokenizer outputs to return.\"\n",
    "        )\n",
    "\n",
    "    def handle_single_conversation(conversation):\n",
    "        input_ids = self.get_prefix_tokens() if add_special_tokens else []\n",
    "        input_message = \"[gMASK]<sop>\" if add_special_tokens else \"\"\n",
    "        for item in conversation:\n",
    "            if item.get(\"tools\"):\n",
    "                tools = item[\"tools\"]\n",
    "                content = \"你是一个名为 GLM-4 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\"\n",
    "                for tool in tools:\n",
    "                    if tool[\"type\"] == \"function\":\n",
    "                        function = tool[\"function\"]\n",
    "                        content += f\"\\n\\n## {function['name']}\\n\\n{json.dumps(function, ensure_ascii=False, indent=4)}\"\n",
    "                        content += \"\\n在调用上述函数时，请使用 Json 格式表示调用的参数。\"\n",
    "                    elif tool[\"type\"] == \"python\":\n",
    "                        content += \"\\n\\n## python\\n\\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。\"\n",
    "                    elif tool[\"type\"] == \"simple_browser\":\n",
    "                        content += \"\\n\\n## simple_browser\\n\\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\\n`open_url(url: str)`：打开指定的 URL。\\n\\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\\n\\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。\"\n",
    "                    elif tool[\"type\"] == \"cogview\":\n",
    "                        content += \"\\n\\n## cogview\\n\\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。\"\n",
    "                    else:\n",
    "                        raise NotImplementedError(f\"Unknown tool type {tool['type']}\")\n",
    "                input = self.build_single_message(\"system\", \"\", content, tokenize=tokenize)\n",
    "                if tokenize:\n",
    "                    input_ids.extend(input)\n",
    "                else:\n",
    "                    input_message += input\n",
    "            if item[\"content\"]:\n",
    "                input = self.build_single_message(\n",
    "                    item[\"role\"],\n",
    "                    item.get(\"metadata\", \"\"),\n",
    "                    item[\"content\"],\n",
    "                    tokenize=tokenize\n",
    "                )\n",
    "                if tokenize:\n",
    "                    input_ids.extend(input)\n",
    "                else:\n",
    "                    input_message += input\n",
    "        if add_generation_prompt:\n",
    "            if tokenize:\n",
    "                input_ids.extend([self.convert_tokens_to_ids(\"<|assistant|>\")])\n",
    "            else:\n",
    "                input_message += \"<|assistant|>\"\n",
    "# if tokenize:\n",
    "# input_ids.extend([self.convert_tokens_to_ids(\"[gMASK]\")]) # Use special tokens instead of empty strings\n",
    "# else:\n",
    "# input_message += \"[gMASK]\"\n",
    "\n",
    "        return input_ids if tokenize else input_message\n",
    "\n",
    "# Main logic for handling different session formats\n",
    "    if isinstance(conversation, list) and all(isinstance(i, dict) for i in conversation):\n",
    "        result = handle_single_conversation(conversation)\n",
    "    elif isinstance(conversation, list) and all(isinstance(i, list) for i in conversation):\n",
    "        result = [handle_single_conversation(c) for c in conversation]\n",
    "    elif hasattr(conversation, \"messages\"):\n",
    "        result = handle_single_conversation(conversation.messages)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid conversation format\")\n",
    "\n",
    "    if tokenize:\n",
    "        output = self.batch_encode_plus(\n",
    "            [result] if isinstance(result[0], int) else result,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length,\n",
    "            return_tensors=return_tensors,\n",
    "            is_split_into_words=True,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        if return_dict:\n",
    "            return output\n",
    "        else:\n",
    "            return output[\"input_ids\"]\n",
    "    else:\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38eb42ba-0628-4f9d-bf29-410607552950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: regex in /data1/ckw/micromamba/envs/kewei-ai/lib/python3.11/site-packages (2023.10.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80613542-7de6-458e-9b93-8dc022fc2801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regex.Regex(\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\", flags=regex.V0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_str = \"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\"\n",
    "regex.compile(pat_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e08863f0-9acd-458b-9af8-4ba37f1d7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\n",
    "pattern = regex.compile(pat_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kewei-ai",
   "language": "python",
   "name": "kewei-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
