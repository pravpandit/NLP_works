{
 "cells": [
  {
   "cell_type": "raw",
   "id": "bcd88fb5-6a0f-4c4b-81fd-34be59ea7903",
   "metadata": {},
   "source": [
    "模型下载链接：https://hf-mirror.com/BlinkDL/rwkv-4-pile-430m/resolve/main/RWKV-4-Pile-430M-20220808-8066.pth?download=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b78b7ef-acc6-46cf-88c2-f90a2835e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n",
    "########################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n",
    "import types, torch\n",
    "from torch.nn import functional as F\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deacc22b-2896-4b77-b595-3284b0c13544",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"20B_tokenizer.json\")\n",
    "\n",
    "args = types.SimpleNamespace()\n",
    "args.MODEL_NAME = '/data1/ckw/RWKV-4-Pile-430M-20220808-8066'\n",
    "args.n_layer = 24\n",
    "args.n_embd = 1024\n",
    "\n",
    "context = \"\\nDataWhalechina is an organization founded at Shanghai Jiao Tong University that helps learners learn artificial intelligence.\"\n",
    "NUM_TRIALS = 3\n",
    "LENGTH_PER_TRIAL = 100\n",
    "TEMPERATURE = 1.0\n",
    "TOP_P = 0.85\n",
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c85bca7-1342-4d8b-870c-baddcf2661d6",
   "metadata": {},
   "source": [
    "### RWKV Time Mixing Implementation\n",
    "\n",
    "In the RWKV model, time mixing is a key step to handle the changes of input sequences over time. The following is a detailed formula description and code comments for the `time_mixing` function.\n",
    "\n",
    "#### Formula Description\n",
    "\n",
    "The core idea of ​​time mixing is to mix the current input with the previous state through the time mixing coefficient to generate new key, value and gating signals. This process involves the following steps:\n",
    "\n",
    "1. **Mixed input**:\n",
    "- Take a weighted average of the current input \\(x \\) and the previous state:\n",
    "$$ x_k = x \\cdot \\text{time\\_mix\\_k} + \\text{state}[5i+1] \\cdot (1 - \\text{time\\_mix\\_k}) $$\n",
    "$$ x_v = x \\cdot \\text{time\\_mix\\_v} + \\text{state}[5i+1] \\cdot (1 - \\text{time\\_mix\\_v}) $$\n",
    "$$ x_r = x \\cdot \\text{time\\_mix\\_r} + \\text{state}[5i+1] \\cdot (1 - \\text{time\\_mix\\_r}) $$\n",
    "\n",
    "2. **State update**:\n",
    "- Update state:\n",
    "$$ \\text{state}[5i+1] = x $$\n",
    "\n",
    "3. **Calculate gate signal**:\n",
    "- Use sigmoid activation function to calculate gate signal \\( r \\):\n",
    "$$ r = \\sigma(\\text{rw} @ x_r) $$\n",
    "\n",
    "4. **Calculate key and value**:\n",
    "- Generate key \\( k \\) and value \\( v \\) through linear transformation:\n",
    "$$ k = \\text{kw} @ x_k $$\n",
    "$$ v = \\text{vw} @ x_v $$\n",
    "\n",
    "5. **Weighted sum calculation**:\n",
    "- Calculate weighted sum \\( wkv \\) according to weighted sum formula:\n",
    "$$ a = e1 \\cdot aa + e2 \\cdot v $$\n",
    "$$ b = e1 \\cdot bb + e2 $$\n",
    "$$ \\text{wkv} = a / b $$\n",
    "\n",
    "Code as follows:\n",
    "\n",
    "```python\n",
    "@torch.jit.script_method\n",
    "def time_mixing(self, x, state, i:int, time_mix_k, time_mix_v, time_mix_r, time_first, time_decay, kw, vw, rw, ow):\n",
    "# Mix the current input with the previous state\n",
    "xk = x * time_mix_k + state[5*i+1] * (1 - time_mix_k)\n",
    "xv = x * time_mix_v + state[5*i+1] * (1 - time_mix_v)\n",
    "xr = x * time_mix_r + state[5*i+1] * (1 - time_mix_r)\n",
    "\n",
    "# Update the state\n",
    "state[5*i+1] = x\n",
    "\n",
    "# Compute the gate signal\n",
    "r = torch.sigmoid(rw @ xr)\n",
    "\n",
    "# Compute the key and value\n",
    "k = kw @ xk\n",
    "v = vw @ xv\n",
    "\n",
    "# Read the previous accumulated value from the state\n",
    "aa = state[5*i+2]\n",
    "bb = state[5*i+3]\n",
    "pp = state[5*i+4]\n",
    "\n",
    "# Calculate the weighted sumPart\n",
    "ww = time_first + k\n",
    "qq = torch.maximum(pp, ww)\n",
    "e1 = torch.exp(pp - qq)\n",
    "e2 = torch.exp(ww - qq)\n",
    "a = e1 * aa + e2 * v\n",
    "b = e1 * bb + e2\n",
    "wkv = a / b\n",
    "\n",
    "# Calculate new weights and states\n",
    "ww = pp + time_decay\n",
    "qq = torch.maximum(ww, k)\n",
    "e1 = torch.exp(ww - qq)\n",
    "e2 = torch.exp(k - qq)\n",
    "state[5*i+2] = e1 * aa + e2 * v\n",
    "state[5*i+3] = e1 * bb + e2\n",
    "state[5*i+4] = qq\n",
    "\n",
    "# Calculate the final output\n",
    "return ow @ (r * wkv)\n",
    "```\n",
    "\n",
    "### Detailed explanation\n",
    "\n",
    "1. **Mixed input**:\n",
    "- `xk`, `xv`, `xr` are the input `x` and the state `state`, used to calculate the key, value and gating signal respectively.\n",
    "\n",
    "2. **State Update**:\n",
    "- Store the current input `x` in the state array for use in the next step.\n",
    "\n",
    "3. **Calculate the gating signal**:\n",
    "- Use `torch.sigmoid` to calculate the gating signal `r`, which determines how much information will be passed.\n",
    "\n",
    "4. **Calculate Keys and Values**:\n",
    "- Use matrix multiplication to calculate the key `k` and value `v`.\n",
    "\n",
    "5. **Weighted Sum Calculation**:\n",
    "- Calculate the weighted sum `wkv` by exponential weighted average, which involves dealing with numerical stability issues (via `torch.maximum` and exponential operations).\n",
    "\n",
    "6. **Update State**:\n",
    "- Update the accumulated values ​​in the state array for use in subsequent time steps.\n",
    "\n",
    "7. **Calculate Final Output**:\n",
    "- Use the gating signal `r` and the weighted sum `wkv` to calculate the final output.\n",
    "\n",
    "In this way, by gradually mixing the current input and the previous state, the RWKV model achieves efficient processing of time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d47e4-1792-47e4-a506-3071f510526e",
   "metadata": {},
   "source": [
    "### RWKV Channel Mixing Implementation and Code Comments\n",
    "\n",
    "In the RWKV model, channel mixing is another key step for handling information exchange between different channels. The following is a detailed formula description and code comments for the `channel_mixing` function.\n",
    "\n",
    "#### Formula Description\n",
    "\n",
    "The core idea of ​​channel mixing is to mix the current input with the previous state through the channel mixing coefficient to generate new key and gating signals. This process involves the following steps:\n",
    "\n",
    "1. **Mixed input**:\n",
    "- Take a weighted average of the current input \\(x \\) and the previous state:\n",
    "$$ x_k = x \\cdot \\text{time\\_mix\\_k} + \\text{state}[5i+0] \\cdot (1 - \\text{time\\_mix\\_k}) $$\n",
    "$$ x_r = x \\cdot \\text{time\\_mix\\_r} + \\text{state}[5i+0] \\cdot (1 - \\text{time\\_mix\\_r}) $$\n",
    "\n",
    "2. **State update**:\n",
    "- Update the state:\n",
    "$$ \\text{state}[5i+0] = x $$3. **Calculate the gate signal**:\n",
    "- Use the sigmoid activation function to calculate the gate signal \\(r\\):\n",
    "$$r = \\sigma(\\text{rw} @ x_r) $$\n",
    "\n",
    "4. **Calculate the key**:\n",
    "- Generate the key \\(k\\) through ReLU and square transformation:\n",
    "$$k = (\\text{ReLU}(\\text{kw} @ x_k))^2 $$\n",
    "\n",
    "5. **Calculate the output**:\n",
    "- Use the gate signal and the key to calculate the final output:\n",
    "$$\\text{output} = r \\cdot (\\text{vw} @ k) $$\n",
    "\n",
    "The code is as follows:\n",
    "\n",
    "```python\n",
    "@torch.jit.script_method\n",
    "def channel_mixing(self, x, state, i:int, time_mix_k, time_mix_r, kw, vw, rw):\n",
    "# Mix the current input and the previous state\n",
    "xk = x * time_mix_k + state[5*i+0] * (1 - time_mix_k)\n",
    "xr = x * time_mix_r + state[5*i+0] * (1 - time_mix_r)\n",
    "\n",
    "# Update state\n",
    "state[5*i+0] = x\n",
    "\n",
    "# Calculate gating signal\n",
    "r = torch.sigmoid(rw @ xr)\n",
    "\n",
    "# Calculate key and pass ReLU and square transformation\n",
    "k = torch.square(torch.relu(kw @ xk)) # square relu, primer paper\n",
    "\n",
    "# Calculate final output\n",
    "return r * (vw @ k)\n",
    "```\n",
    "\n",
    "1. **Mixed input**:\n",
    "- `xk`, `xr` are weighted mixtures of input `x` and state `state`, used to calculate key and gating signal respectively.\n",
    "\n",
    "2. **State update**:\n",
    "- Store the current input `x` in the state array for the next step of calculation.\n",
    "\n",
    "3. **Calculate gating signal**:\n",
    "- Use `torch.sigmoid` to calculate the gating signal `r`, which determines how much information will be passed.\n",
    "\n",
    "4. **Calculate the key**:\n",
    "- Use `torch.relu` to calculate the key `k`, and then perform a square transformation to increase nonlinearity.\n",
    "\n",
    "5. **Calculate the final output**:\n",
    "- Use the gating signal `r` and the key `k` to calculate the final output.\n",
    "\n",
    "Through these steps, the RWKV model realizes the effective exchange of information between channels and enhances the model's ability to process input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1b2e2b-9f0d-4db3-b9d9-d43e3e2537ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWKV_RNN(torch.jit.ScriptModule):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.eval() # set torch to inference mode\n",
    "        \n",
    "        w = torch.load(args.MODEL_NAME + '.pth', map_location='cpu')\n",
    "        for k in w.keys():\n",
    "            if      '.time_' in k: w[k] = w[k].squeeze()\n",
    "            if '.time_decay' in k: w[k] = -torch.exp(w[k].float()) # the real time decay is like e^{-e^x}\n",
    "            else: w[k] = w[k].float() # convert to f32 type\n",
    "        \n",
    "        self.w = types.SimpleNamespace() # set self.w from w\n",
    "        self.w.blocks = {}\n",
    "        for k in w.keys(): # example: \"blocks.0.att.time_first\" => self.w.blocks[0].att.time_first\n",
    "            parts = k.split('.')\n",
    "            last = parts.pop()\n",
    "            here = self.w\n",
    "            for p in parts:\n",
    "                if p.isdigit():\n",
    "                    p = int(p)\n",
    "                    if p not in here: here[p] = types.SimpleNamespace()\n",
    "                    here = here[p]\n",
    "                else:\n",
    "                    if not hasattr(here, p): setattr(here, p, types.SimpleNamespace())\n",
    "                    here = getattr(here, p)\n",
    "            setattr(here, last, w[k])\n",
    "\n",
    "    def layer_norm(self, x, w):\n",
    "        return F.layer_norm(x, (self.args.n_embd,), weight=w.weight, bias=w.bias)\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def channel_mixing(self, x, state, i:int, time_mix_k, time_mix_r, kw, vw, rw):\n",
    "        xk = x * time_mix_k + state[5*i+0] * (1 - time_mix_k)\n",
    "        xr = x * time_mix_r + state[5*i+0] * (1 - time_mix_r)\n",
    "        state[5*i+0] = x\n",
    "        r = torch.sigmoid(rw @ xr)\n",
    "        k = torch.square(torch.relu(kw @ xk)) # square relu, primer paper\n",
    "        return r * (vw @ k)\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def time_mixing(self, x, state, i:int, time_mix_k, time_mix_v, time_mix_r, time_first, time_decay, kw, vw, rw, ow):\n",
    "        xk = x * time_mix_k + state[5*i+1] * (1 - time_mix_k)\n",
    "        xv = x * time_mix_v + state[5*i+1] * (1 - time_mix_v)\n",
    "        xr = x * time_mix_r + state[5*i+1] * (1 - time_mix_r)\n",
    "        state[5*i+1] = x\n",
    "        r = torch.sigmoid(rw @ xr)\n",
    "        k = kw @ xk\n",
    "        v = vw @ xv\n",
    "        \n",
    "        aa = state[5*i+2]\n",
    "        bb = state[5*i+3]\n",
    "        pp = state[5*i+4]\n",
    "        ww = time_first + k\n",
    "        qq = torch.maximum(pp, ww)\n",
    "        e1 = torch.exp(pp - qq)\n",
    "        e2 = torch.exp(ww - qq)\n",
    "        a = e1 * aa + e2 * v\n",
    "        b = e1 * bb + e2\n",
    "        wkv = a / b\n",
    "        ww = pp + time_decay\n",
    "        qq = torch.maximum(ww, k)\n",
    "        e1 = torch.exp(ww - qq)\n",
    "        e2 = torch.exp(k - qq)\n",
    "        state[5*i+2] = e1 * aa + e2 * v\n",
    "        state[5*i+3] = e1 * bb + e2\n",
    "        state[5*i+4] = qq\n",
    "        return ow @ (r * wkv)\n",
    "\n",
    "    def forward(self, token, state):\n",
    "        with torch.no_grad():\n",
    "            if state == None:\n",
    "                state = torch.zeros(self.args.n_layer * 5, self.args.n_embd)\n",
    "                for i in range(self.args.n_layer): state[5*i+4] = -1e30 # -infinity\n",
    "            \n",
    "            x = self.w.emb.weight[token]\n",
    "            x = self.layer_norm(x, self.w.blocks[0].ln0)\n",
    "            for i in range(self.args.n_layer):\n",
    "                att = self.w.blocks[i].att\n",
    "                x = x + self.time_mixing(self.layer_norm(x, self.w.blocks[i].ln1), state, i, \n",
    "                    att.time_mix_k, att.time_mix_v, att.time_mix_r, att.time_first, att.time_decay, \n",
    "                    att.key.weight, att.value.weight, att.receptance.weight, att.output.weight)\n",
    "                ffn = self.w.blocks[i].ffn\n",
    "                x = x + self.channel_mixing(self.layer_norm(x, self.w.blocks[i].ln2), state, i, \n",
    "                    ffn.time_mix_k, ffn.time_mix_r, \n",
    "                    ffn.key.weight, ffn.value.weight, ffn.receptance.weight)\n",
    "            \n",
    "            x = self.w.head.weight @ self.layer_norm(x, self.w.ln_out)\n",
    "            return x.float(), state\n",
    "\n",
    "##########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b457af-77a3-4b5e-a6f3-034b0fc6708d",
   "metadata": {},
   "source": [
    "The sampling method has not changed compared to versions v2 and v3, only some optimization adjustments have been made to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf027b6-7df9-4c0f-818e-013e7c49e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_logits(out, temperature=1.0, top_p=0.8):\n",
    "    probs = F.softmax(out, dim=-1).numpy()\n",
    "    sorted_probs = np.sort(probs)[::-1]\n",
    "    cumulative_probs = np.cumsum(sorted_probs)\n",
    "    cutoff = float(sorted_probs[np.argmax(cumulative_probs > top_p)])\n",
    "    probs[probs < cutoff] = 0\n",
    "    if temperature != 1.0:\n",
    "        probs = probs.pow(1.0 / temperature)\n",
    "    probs = probs / np.sum(probs)\n",
    "    out = np.random.choice(a=len(probs), p=probs)\n",
    "    return out\n",
    "\n",
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298dbbde-6535-406b-bd43-f2d886799f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using CPU. Loading /data1/ckw/RWKV-4-Pile-430M-20220808-8066 ...\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nUsing CPU. Loading {args.MODEL_NAME} ...')\n",
    "model = RWKV_RNN(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d366a89-02cb-4b5e-95ef-52f6376d3607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing context (slow version. see v2/rwkv/model.py for fast version)\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nPreprocessing context (slow version. see v2/rwkv/model.py for fast version)')\n",
    "init_state = None\n",
    "for token in tokenizer.encode(context).ids:\n",
    "    init_out, init_state = model.forward(token, init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5273e7a8-875e-4998-b98e-f81951a7af32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--[ Trial 0 ]----------------- \n",
      "DataWhalechina is an organization founded at Shanghai Jiao Tong University that helps learners learn artificial intelligence. The machine learning solutions applied to the class are called Persona, which consist of several categories:\n",
      "\n",
      "\\begin{tabular}{|c|c|c|}\n",
      "\\hline\n",
      "  Name   & Description  \\\\ \\hline\n",
      "\\hline\n",
      "  \\end{tabular}\n",
      "\n",
      "DataWhalechina organizes the data in two ways:\n",
      "\n",
      "\\begin{tabular}{|c|c|c|}\n",
      "\\hline\n",
      "  \\multicolumn{2}{|c}{\n",
      "\n",
      "--[ Trial 1 ]----------------- \n",
      "DataWhalechina is an organization founded at Shanghai Jiao Tong University that helps learners learn artificial intelligence. The main goal is to allow learners to learn how to use artificial intelligence in an integrated fashion, by using both AI and deep learning techniques. Datawhalechina aims to teach AI algorithms from scratch and teach them from scratch to become competent with many algorithms that humans could not have.\n",
      "\n",
      "Applications\n",
      "\n",
      "Projects \n",
      " DeeplearningAI : Encourage AI algorithms to become competent with many algorithms that humans could not have. Datawhalechina aims to be able to combine knowledge from multiple AI\n",
      "\n",
      "--[ Trial 2 ]----------------- \n",
      "DataWhalechina is an organization founded at Shanghai Jiao Tong University that helps learners learn artificial intelligence. The company was founded in 2016. The company has graduated 1,000 engineers, who work from the companies headquarters in Shanghai.\n",
      "\n",
      "In September 2019, the team was reported to have learned over 400,000 artificial intelligence.\n",
      "\n",
      "In August 2019, the company was reported to have sold 600,000 artificial intelligence to clients in Singapore.\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      " \n",
      "\n",
      "Category:Human machine interaction\n",
      "Category:Learning management systems\n",
      "Category:Learning management systemsTechnologies, industry,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for TRIAL in range(NUM_TRIALS):\n",
    "    print(f'\\n\\n--[ Trial {TRIAL} ]-----------------', context, end=\"\")\n",
    "    all_tokens = []\n",
    "    out_last = 0\n",
    "    out, state = init_out.clone(), init_state.clone()\n",
    "    for i in range(LENGTH_PER_TRIAL):\n",
    "        token = sample_logits(out, TEMPERATURE, TOP_P)\n",
    "        all_tokens += [token]\n",
    "        tmp = tokenizer.decode(all_tokens[out_last:])\n",
    "        if '\\ufffd' not in tmp: # only print when we have a valid utf-8 string\n",
    "            print(tmp, end=\"\", flush=True)\n",
    "            out_last = i + 1\n",
    "        out, state = model.forward(token, state)       \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cdc809-c64d-4861-b540-460bc1097e38",
   "metadata": {},
   "source": [
    "### Note: RWKV's Scaling Law\n",
    "\n",
    "RWKV's scaling laws describe the mathematical relationship between model performance and various factors. These factors include model size ($N$), dataset size ($D$), or optimal computational budget ($C_{\\min}$). Scaling laws are important in two ways:\n",
    "1. **Prediction and Planning**: They allow us to predict and plan costs and performance by interpolation and extrapolation before training large models.\n",
    "2. **Feedback and Research**: They provide important feedback on model failures and guide future research directions.\n",
    "\n",
    "#### Summary of key points:\n",
    "- **Comparison with previous RNN research**: Previous work has shown that LSTM does not follow exactly the same log-linear scaling law as Transformer. However, the training results of the RWKV model show that RWKV follows the same general scaling law form as Transformer.\n",
    "- **Experimental verification**: In the [v4 paper](https://arxiv.org/abs/2305.13048), 45 RWKV models were trained to verify the linear relationship between the loss and the amount of computation. The $r^2$ value of the linear fit was 0.994, and even if it was extrapolated by an order of magnitude, the fit was still very good ($r^2$ was 0.875).\n",
    "\n",
    "These results show the superiority of the RWKV model when scalingand similar performance scaling behavior to Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6025d-faea-4647-be05-8fb4cce05991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
