# ट्रांसफार्मर आरेख

सामग्री संगठन:
- ट्रांसफार्मर आरेख
- ट्रांसफार्मर मैक्रो संरचना
- ट्रांसफार्मर संरचना विवरण
-इनपुट प्रोसेसिंग
-शब्द सदिश
- स्थिति वेक्टर
-एनकोडर
-स्व-ध्यान परत
-मल्टी-हेड ध्यान तंत्र
-ध्यान दें कोड उदाहरण
- अवशिष्ट कनेक्शन
-डिकोडर
-रैखिक परत और सॉफ्टमैक्स
-लॉस फंकशन
-अतिरिक्त सामग्री
-स्वीकृति

[ध्यान आरेख](./2.1-ध्यान आरेख.एमडी) का अध्ययन करने के बाद, हम उन लाभों को जानते हैं जो ध्यान लाते हैंआवर्ती तंत्रिका नेटवर्क। तो क्या कोई तंत्रिका नेटवर्क संरचना है जो सीधे ध्यान पर आधारित है और अब आरएनएन, एलएसटीएम या सीएनएन नेटवर्क संरचनाओं पर निर्भर नहीं है? इसलिए, हम इस खंड में ट्रांसफार्मर में शामिल विवरणों का पता लगाएंगे।

ट्रांसफॉर्मर मॉडल को 2017 में Google द्वारा प्रस्तावित किया गया था, जो सीधे सेल्फ-अटेंशन संरचना पर आधारित था, इसने पिछले एनएलपी कार्यों में आमतौर पर उपयोग की जाने वाली आरएनएन तंत्रिका नेटवर्क संरचना को बदल दिया, और उस समय WMT2 दोनों में SOTA हासिल किया।014 अंग्रेजी से जर्मन और WMT2014 अंग्रेजी से फ्रेंच मशीन अनुवाद कार्य।

आरएनएन जैसे तंत्रिका नेटवर्क संरचनाओं की तुलना में, ट्रांसफार्मर का एक बड़ा लाभ यह है कि: ** अनुक्रम इनपुट को संसाधित करते समय, मॉडल पूरे अनुक्रम इनपुट पर समानांतर गणना कर सकता है, और समय के अनुसार इनपुट अनुक्रम को पुनरावर्ती रूप से संसाधित करने की आवश्यकता नहीं होती है। चरण। ** अध्याय 2.1 में बताया गया है कि आरएनएन तंत्रिका नेटवर्क इनपुट अनुक्रम को कैसे पुनरावर्ती रूप से संसाधित करता है। पाठकों का इसकी समीक्षा करने के लिए स्वागत है।नीचे दिया गया चित्र ट्रांसफार्मर की समग्र संरचना है। अध्याय 2.1 में पेश किए गए seq2seq मॉडल के समान, ट्रांसफार्मर मॉडल संरचना का बायां आधा हिस्सा एनकोडर है, और दायां आधा हिस्सा डिकोडर है आइए चरण दर चरण ट्रांसफार्मर को अलग करें।

![ट्रांसफार्मर](./pictures/2-transformer.png)
चित्र: ट्रांसफार्मर मॉडल संरचना

नोट्स और संदर्भ: यह आलेख सामान्य-विशिष्ट तरीके से ट्रांसफार्मर को अलग करेगा और समझाएगा, जिससे शुरुआती लोगों को ट्रांसफार्मर मॉड को समझने में मदद मिलेगी।ईएल संरचना। यह आलेख मुख्य रूप से [सचित्र-ट्रांसफॉर्मर] (http://jalammar.github.io/ Illustrator-transformer) को संदर्भित करता है।

## ट्रांसफार्मर मैक्रो संरचना

नोट: इस अनुभाग को इस प्रकार समझाया गया है: सामान्य-विशिष्ट, पहले संपूर्ण, फिर भाग।

ट्रांसफार्मर को सबसे पहले मशीन अनुवाद कार्य को हल करने के लिए प्रस्तावित किया गया था, इसलिए इसे एक प्रकार का seq2seq मॉडल माना जा सकता है। यह खंड पहले ट्रांसफार्मर मॉडल में संरचना के विशिष्ट विवरणों को अनदेखा करेगा और पहले मैक्रो एस का अध्ययन करेगाSeq2seq के परिप्रेक्ष्य से ट्रांसफार्मर की संरचना। मशीन अनुवाद कार्य को एक उदाहरण के रूप में लेते हुए, पहले ट्रांसफार्मर के विशेष seqseq मॉडल को एक ब्लैक बॉक्स के रूप में मानें, ब्लैक बॉक्स का इनपुट फ्रेंच टेक्स्ट अनुक्रम है, और आउटपुट अंग्रेजी टेक्स्ट है। अनुक्रम (अध्याय 2.1 में seq2seq फ्रेमवर्क ज्ञान की तुलना में, हम पा सकते हैं कि ट्रांसफार्मर मैक्रो संरचना seq2seq श्रेणी से संबंधित है, सिवाय इसके कि पिछले seq2seq में एनकोडर और डिकोडर को बदल दिया गया हैआरएनएन मॉडल से ट्रांसफार्मर मॉडल तक)।

![इनपुट-आउटपुट](./चित्र/2-इनपुट-आउटपुट.पीएनजी)
चित्र: ट्रांसफार्मर ब्लैक बॉक्स इनपुट और आउटपुट

उपरोक्त आकृति के मध्य भाग को बदलें "ट्रांसफॉर्मर" को मानक seq2seq संरचना में अलग किया गया है, और निम्नलिखित आकृति प्राप्त की गई है: बाईं ओर एनकोडर भाग है, और दाईं ओर डिकोडर भाग है।
![एनकोडर-डिकोडर](./चित्र/2-एनकोडर-डिकोडर.png)
चित्र: एनकोडर-डिकोडर

अगला, उपरोक्त एनकोडर और डिकोडर का विवरणनिम्नलिखित आकृति प्राप्त करने के लिए आकृति तैयार की गई है। हम देख सकते हैं कि एनकोडर भाग (एनकोडर) एनकोडर की कई परतों से बना है (ट्रांसफॉर्मर पेपर 6-लेयर एनकोडर का उपयोग करता है, और यहां परतों 6 की संख्या निश्चित नहीं है, आप कर सकते हैं) प्रायोगिक परिणामों के अनुसार परतों की संख्या को भी संशोधित करें)। इसी तरह, डिकोडिंग भाग (डिकोडर) भी डिकोडर की कई परतों से बना होता है (पेपर भी 6-लेयर डिकोडर का उपयोग करता है)। वही, औरप्रत्येक डिकोडर परत की नेटवर्क संरचना भी समान होती है। विभिन्न परतों पर एनकोडर और डिकोडर की नेटवर्क संरचना पैरामीटर साझा नहीं करती है।
![अनुवाद उदाहरण](./pictures/2-2-encoder-detail.png)

चित्र: 6-लेयर एनकोडर और 6-लेयर डिकोडर

आगे, आइए सिंगल-लेयर एनकोडर पर एक नज़र डालें। एक सिंगल-लेयर एनकोडर मुख्य रूप से निम्नलिखित दो भागों से बना होता है, जैसा कि नीचे दिए गए चित्र में दिखाया गया है
-स्व-ध्यान परत
- फ़ीड फॉरवर्ड न्यूरल नेटवर्कवर्क (फीडफॉरवर्ड न्यूरल नेटवर्क, एबीबीएफएफएनएन के रूप में पुनर्जीवित)

एन्कोडर के इनपुट टेक्स्ट अनुक्रम $w_1, w_2,...,w_n$ को पहले प्रत्येक शब्द $x_1, x_2,...,x_n$ का वेक्टर प्रतिनिधित्व प्राप्त करने के लिए एम्बेडिंग परिवर्तन से गुजरना होगा, जहां $x_i \in \mathbb{ R}^{d}$ आयाम $d$ का एक वेक्टर है, और फिर सभी वैक्टर बदल दिए जाते हैं और $h_1, h_2,...h_n$ प्राप्त करने के लिए सेल्फ-अटेंशन न्यूरल नेटवर्क परत के माध्यम से जानकारी का आदान-प्रदान किया जाता है, जहां $h_i \ \mathbb{R}^{d}$ में आयाम $d$ का एक वेक्टर है जब आत्म-ध्यान परत एक शब्द वेक्टर को संसाधित करती हैया, यह न केवल शब्द की जानकारी का उपयोग करता है, बल्कि वाक्य में अन्य शब्दों की जानकारी का भी उपयोग करता है (आप इसकी तुलना इससे कर सकते हैं: जब हम किसी शब्द का अनुवाद करते हैं, तो हम न केवल वर्तमान शब्द पर ध्यान देते हैं, बल्कि ध्यान भी देते हैं) शब्द के संदर्भ में अन्य शब्दों की जानकारी पर ध्यान दें) सेल्फ-अटेंशन लेयर का आउटपुट नए $x_1, x_2,..,x_n$ प्राप्त करने के लिए फीडफॉरवर्ड न्यूरल नेटवर्क से गुजरेगा, जो अभी भी $n$ हैं। आयाम $d$ के सदिश इन सदिशों को वें पर भेजा जाएगाई एनकोडर की अगली परत और उसी ऑपरेशन को जारी रखें।

![एनकोडर](./pictures/2-encoder.png)

चित्र: सिंगल-लेयर एनकोडर

एनकोडर के अनुरूप, जैसा कि नीचे दिए गए चित्र में दिखाया गया है, डिकोडर एन्कोडिंग कर रहा है। डिकोडर के स्व-ध्यान और एफएफएनएन के बीच एक एनकोडर-डिकोडर ध्यान परत डाली जाती है, जो डिकोडर को इनपुट अनुक्रम के सबसे प्रासंगिक भाग पर ध्यान केंद्रित करने में मदद करती है (समान) Seq2seq मॉडल में ध्यान दें)।

![डिकोडर](./pictures/2-decoder.webp)

चित्र: सिंगल-लेयर डेकोडीईआर

संक्षेप में, हम मूल रूप से समझते हैं कि ट्रांसफार्मर में एक एन्कोडिंग भाग और एक डिकोडिंग भाग होता है, और एन्कोडिंग भाग और डिकोडिंग भाग एक ही नेटवर्क संरचना के साथ कई एन्कोडिंग परतों और डिकोडिंग परतों से बने होते हैं। प्रत्येक एन्कोडिंग परत में स्वयं-ध्यान होता है एफएफएनएन, और प्रत्येक डिकोडिंग परत में आत्म-ध्यान, एफएफएन और एनकोडर-डिकोडर ध्यान शामिल हैं।

ऊपर ट्रांसफार्मर की मैक्रो संरचना है आइए मैक्रो स्ट्रू में मॉडल विवरण देखना शुरू करेंसंरचना।

## ट्रांसफार्मर संरचना विवरण

ट्रांसफार्मर की मैक्रो संरचना को समझने के बाद, आइए देखें कि ट्रांसफार्मर इनपुट टेक्स्ट अनुक्रम को वेक्टर प्रतिनिधित्व में कैसे परिवर्तित करता है, और अंतिम आउटपुट प्राप्त करने के लिए इन वेक्टर प्रतिनिधित्व को परत दर परत कैसे संसाधित करता है।

इसलिए, इस अनुभाग की मुख्य सामग्री में शामिल हैं:
-इनपुट प्रोसेसिंग
- शब्द सदिश
- स्थिति वेक्टर
-एनकोडर
- डिकोडर### इनपुट प्रोसेसिंग

#### शब्द सदिश
सामान्य एनएलपी कार्यों की तरह, हम सबसे पहले इसका उपयोग करते हैंइनपुट टेक्स्ट अनुक्रम में प्रत्येक शब्द को शब्द वेक्टर में बदलने के लिए शब्द एम्बेडिंग एल्गोरिदम। व्यावहारिक अनुप्रयोगों में, वेक्टर आम तौर पर 256 या 512 आयामी होता है, लेकिन सरलता के लिए, हम स्पष्टीकरण के लिए 4-आयामी शब्द वैक्टर का उपयोग करते हैं।

जैसा कि नीचे दिए गए चित्र में दिखाया गया है, यह मानते हुए कि हमारा इनपुट टेक्स्ट 3 शब्दों वाला एक अनुक्रम है, प्रत्येक शब्द शब्द एम्बेडिंग एल्गोरिदम के माध्यम से 4-आयामी वेक्टर प्राप्त कर सकता है, इसलिए संपूर्ण इनपुट को व्यावहारिक अनुप्रयोगों में एक वेक्टर अनुक्रम में परिवर्तित किया जाता है यूआम तौर पर एक ही समय में मॉडल में कई वाक्य इनपुट करें यदि प्रत्येक वाक्य की लंबाई अलग है, तो हम इनपुट टेक्स्ट अनुक्रम की अधिकतम लंबाई के रूप में एक उपयुक्त लंबाई चुनेंगे: यदि कोई वाक्य इस लंबाई तक नहीं पहुंचता है, तो उसे गद्देदार बना दिया जाएगा। एक विशेष "पैडिंग" शब्द के साथ; यदि वाक्य इस लंबाई से अधिक है, तो इसे छोटा कर दिया जाएगा। अधिकतम अनुक्रम लंबाई एक हाइपरपैरामीटर है, और आमतौर पर यह आशा की जाती है कि जितना बड़ा होगा, उतना बेहतर होगा, लेकिन लंबे अनुक्रम अक्सर अधिक प्रशिक्षण मेमोरी लेते हैं।मोरी, इसलिए मॉडल को प्रशिक्षित करते समय इसे स्थिति के अनुसार तय करने की आवश्यकता है।

![शब्द वैक्टर](./चित्र/2-x.png)
चित्र: 3 शब्द और संबंधित शब्द सदिश

इनपुट अनुक्रम में प्रत्येक शब्द को शब्द वेक्टर प्रतिनिधित्व में परिवर्तित किया जाता है और शब्द का अंतिम वेक्टर प्रतिनिधित्व प्राप्त करने के लिए स्थिति वेक्टर जोड़ा जाता है।

#### स्थिति वेक्टर

जैसा कि नीचे दिए गए चित्र में दिखाया गया है, ट्रांसफॉर्मर मॉडल प्रत्येक इनपुट शब्द वेक्टर में एक स्थिति वेक्टर जोड़ता है। ये वैक्टर स्थिति निर्धारित करने में मदद करते हैंप्रत्येक शब्द की विशेषताएँ, या एक वाक्य में विभिन्न शब्दों के बीच की दूरी की विशेषताएँ शब्द वैक्टर में स्थिति वैक्टर जोड़ने के पीछे अंतर्ज्ञान यह है कि शब्द वैक्टर में स्थिति का प्रतिनिधित्व करने वाले इन वैक्टर को जोड़ने से मॉडल को अधिक सार्थक जानकारी मिल सकती है, जैसे शब्दों की स्थिति। , शब्दों के बीच की दूरी, आदि।

![स्थिति एन्कोडिंग](./चित्र/2-स्थिति.png)
चित्र: स्थिति एन्कोडिंग वेक्टर

फिर भी यह मानते हुए कि शब्द सदिश और स्थिति सदिश का आयामs 4 है, हम नीचे दिए गए चित्र में एक संभावित स्थिति वेक्टर + शब्द वेक्टर दिखाते हैं:

![स्थिति एन्कोडिंग](./चित्र/2-स्थिति2.png)
चित्र: स्थिति एन्कोडिंग वेक्टर

तो स्थिति एन्कोडिंग जानकारी वाला वेक्टर किस पैटर्न का अनुसरण करता है? मूल पेपर में दी गई डिज़ाइन अभिव्यक्ति है:
$$
PE_{(pos,2i)} = syn(pos / 10000^{2i/d_{\text{model}}}) \\ PE_{(pos,2i+1)} = cos(pos / 10000^{2i/ d_{\text{मॉडल}}})
$$
उपरोक्त तालिका अभिव्यक्ति में, $pos$ शब्द की स्थिति, $d_{मॉडल को दर्शाती है}$ स्थिति वेक्टर के आयाम का प्रतिनिधित्व करता है, और $i \in [0, d_{model})$ $d_{model}$th स्थिति वेक्टर के $i$वें आयाम का प्रतिनिधित्व करता है, इसलिए उपरोक्त सूत्र के अनुसार, हम $d_{model}$th स्थिति वेक्टर को $pos$वें स्थान पर प्राप्त कर सकते हैं, नीचे दिए गए चित्र में, हम 4वें, 5वें, 6वें स्थान पर स्थिति वेक्टर के संख्यात्मक मानों को आलेखित करते हैं। और 7वां आयाम। क्षैतिज अक्ष स्थिति सबस्क्रिप्ट का प्रतिनिधित्व करता है, और ऊर्ध्वाधर अक्ष संख्यात्मक मान का प्रतिनिधित्व करता है।![स्थिति एन्कोडिंग आरेख](./pictures/2-2-pos-embedding.png)
चित्र: 4थे, 5वें, 6वें और 7वें आयामों में 0-100 स्थिति पर स्थिति एन्कोडिंग का संख्यात्मक आरेख

बेशक, उपरोक्त सूत्र स्थिति एन्कोडिंग वैक्टर उत्पन्न करने का एकमात्र तरीका नहीं है, लेकिन इस पद्धति का लाभ यह है कि इसे अज्ञात अनुक्रम लंबाई तक बढ़ाया जा सकता है, उदाहरण के लिए, जब हमारे मॉडल को एक वाक्य का अनुवाद करने की आवश्यकता होती है, और लंबाई यह वाक्य प्रशिक्षण सेट के सभी वाक्यों की लंबाई से अधिक है,यह स्थिति एन्कोडिंग विधि समान लंबाई के स्थिति एन्कोडिंग वैक्टर भी उत्पन्न कर सकती है।

### एनकोडर

एन्कोडिंग भाग के इनपुट टेक्स्ट अनुक्रम को संसाधित करने के बाद, एक वेक्टर अनुक्रम प्राप्त किया जाता है। यह वेक्टर अनुक्रम पहली परत एनकोडर को भेजा जाएगा। पहली परत एनकोडर का आउटपुट भी एक वेक्टर अनुक्रम होता है, जिसे बाद में भेजा जाता है परत एनकोडर: पहली परत एनकोडर का इनपुट स्थिति वेक्टर के साथ जुड़े शब्द वेक्टर है, * ऊपरी परत एनकोडर इनपुटपिछले एनकोडर* का आउटपुट है।

नीचे दिया गया चित्र एकल-परत एनकोडर में वेक्टर अनुक्रमों के प्रवाह को दर्शाता है: शब्द वैक्टर जो फ्यूज स्थिति की जानकारी को आत्म-ध्यान परत में दर्ज करते हैं, और आत्म-ध्यान द्वारा प्रत्येक स्थिति आउटपुट के वैक्टर को फिर एफएफएन तंत्रिका नेटवर्क में इनपुट किया जाता है प्रत्येक स्थिति के लिए नए वेक्टर प्राप्त करें।

![इनपुट एनकोडर](./pictures/2-x-encoder.png)
चित्र: एकल-परत एनकोडर का अनुक्रम वेक्टर प्रवाह

आइए दो शब्दों का उदाहरण देखें:

![पास ओएक परत से दूसरी परत](./pictures/2-multi-encoder.webp)
चित्र: दो शब्दों का उदाहरण: $x_1, x_2 \to z_1, z_2 \to r_1, r_2$

### आत्म-ध्यान परत

आइए उपरोक्त चित्र में स्व-ध्यान परत के विशिष्ट तंत्र का विश्लेषण करें।

##### आत्म-ध्यान का अवलोकन

मान लीजिए हम वाक्य का अनुवाद करना चाहते हैं:
```
जानवर सड़क पार नहीं कर पाया क्योंकि वह बहुत थका हुआ था
```
इस वाक्य में, *यह* एक सर्वनाम है, तो *यह* क्या संदर्भित करता है? क्या यह *जानवर* या *सड़क* को संदर्भित करता है?यह मनुष्यों के लिए बहुत सरल है, लेकिन मॉडल के लिए इतना आसान नहीं है, हालांकि, यदि मॉडल *सेल्फ अटेंशन* तंत्र का परिचय देता है, तो यह मॉडल इसे जानवर के साथ जोड़ सकता है, जब मॉडल वाक्य में अन्य शब्दों को संसाधित करता है। सेल्फ अटेंशन* तंत्र मॉडल को न केवल वर्तमान स्थिति में शब्द पर, बल्कि वाक्य में अन्य स्थितियों में संबंधित शब्दों पर भी ध्यान दे सकता है, ताकि वर्तमान स्थिति में शब्द को बेहतर ढंग से समझा जा सके।

आरएनएन पुरुषों के साथ तुलना मेंधारा 2.1 में कहा गया है: जब आरएनएन किसी शब्द को एक क्रम में संसाधित करता है, तो यह वाक्य में पिछले शब्द द्वारा पारित *छिपी हुई स्थिति* पर विचार करेगा, और *छिपी हुई स्थिति* में पिछले शब्द की जानकारी होती है; * तंत्र यह है कि वर्तमान शब्द सीधे अपने वाक्य के पहले और बाद के सभी संबंधित शब्दों पर ध्यान देगा, जैसा कि नीचे दिए गए चित्र में *it* के उदाहरण में दिखाया गया है:

![एक शब्द और अन्य शब्दों का ध्यान](./pictures/2-attention-word.png)

आकृति:एक शब्द और दूसरे शब्द का ध्यान

उपरोक्त चित्र में दिखाया गया *यह* एक वास्तविक उदाहरण है, जो वह स्थिति है जब ट्रांसफार्मर 5वीं परत एनकोडर में "इसे" एनकोड करता है, विज़ुअलाइज़ेशन के बाद, यह दिखाता है कि *यह* का ध्यान "जानवर" पर केंद्रित है " और इन दो शब्दों की जानकारी को "इट" में मिला दिया।

##### आत्म-ध्यान विवरण

सबसे पहले, आइए एक सरल उदाहरण के माध्यम से समझें: "आत्म-ध्यान तंत्र" क्या है? मान लीजिए कि एक वाक्य में दो शब्द हैं: थिंकिंग मशीनेंआत्म-ध्यान की स्थिति है: सोच-विचार, सोच-मशीनें, मशीनें-सोच, मशीनें-मशीनें, कुल $2^2$ दो-दो ध्यान तो इसे विशेष रूप से दो शब्दों की गणना कैसे करें? वैक्टर $X_1, X_2$​ प्राप्त करने के लिए मशीनों को वेक्टर एल्गोरिदम शब्द द्वारा वेक्टर किया जाता है:
$$
1: q_1 = X_1 W^Q, q_2 = X_2 W^Q; }\\
2-3: स्कोर_{11} = \frac{q_1 \cdot q_1}{\sqrt{d_k}} , स्कोर_{12} = \frac{q_1 \cdot q_2}{\sqrt{d_k}} ; स्कोर_{21} = \frac{q_2 \cdot q_1}{\sqrt{d_k}}, स्कोर_{22} = \frac{q_2 \cdot q_2}{\sqrt{d_k}};
4: स्कोर_{11} = \frac{e^{score_{11}}}{e^{score_{11}} + e^{score_{12}}},score_{12} = \frac{e^{ स्कोर_{12}}}{e^{स्कोर_{11}} + e^{स्कोर_{12}}}; स्कोर_{21} = \frac{e^{स्कोर_{21}}}{e^{स्कोर_{21 }} + e^{score_{22}}},score_{22} = \frac{e^{score_{22}}}{e^{score_{21} \
5-6: z_1 = v_1 \गुणा स्कोर_{11} + v_2 \गुणा स्कोर_{12};21} + v_2 \गुना स्कोर_{22}
$$
नीचे, हम उपरोक्त आत्म-ध्यान गणना के छह चरणों की कल्पना करते हैं।

चरण 1: प्राप्त करने के लिए इनपुट एनकोडर के शब्द वेक्टर को रैखिक रूप से बदलें: क्वेरी वेक्टर: $q_1, q_2$, मुख्य वेक्टर: $k_1, k_2$, मान वेक्टर: $v_1, v_2$। इन तीन वैक्टर को गुणा करके प्राप्त किया जाता है तीन पैरामीटर मैट्रिक्स के साथ शब्द वेक्टर, और यह मैट्रिक्स मॉडल द्वारा सीखा जाने वाला पैरामीटर भी है।

![Q,K,V](./pictures/2-qkv.png)चित्रा: क्वेरी वेक्टर की गणना: $q_1, q_2$,मुख्य वेक्टर: $k_1, k_2$, मान वेक्टर: $v_1, v_2$।

क्वेरी वेक्टर, कुंजी वेक्टर और वैल्यू वेक्टर का क्या मतलब है?

वास्तव में, वे केवल तीन वैक्टर हैं। उन्हें एक नाम देने से हमें आत्म-ध्यान की गणना प्रक्रिया और तर्क को बेहतर ढंग से समझने में मदद मिल सकती है। ध्यान गणना के तर्क को अक्सर इस प्रकार वर्णित किया जा सकता है: ** प्रश्न और कुंजी सहसंबंध या ध्यान स्कोर की गणना करते हैं। और फिर ध्यान स्कोर के अनुसार मूल्य का योग करें।

चरण 2: ध्यान स्कोर की गणना करें। मान लीजिए कि अब हम गणना करते हैंपहले शब्द *सोच* के ध्यान स्कोर को देखें), हमें *सोच* के अनुरूप शब्द वेक्टर के आधार पर वाक्य में अन्य सभी शब्द वैक्टर के लिए एक स्कोर की गणना करने की आवश्यकता है। ये स्कोर उस वजन को निर्धारित करते हैं जो हमें शब्द को देने की आवश्यकता है *सोच* शब्द को कूटबद्ध करते समय वाक्य में अन्य स्थानों पर सदिश।

ध्यान स्कोर "*सोच*" के अनुरूप क्वेरी वेक्टर और अन्य पदों पर प्रत्येक शब्द के मुख्य वेक्टर के बीच एक डॉट उत्पाद निष्पादित करके प्राप्त किया जाता हैथिंकिंग का ध्यान स्कोर $q_1$ और $k_1$ का आंतरिक उत्पाद है, और दूसरा स्कोर $q_1$ और $k_2$ का डॉट उत्पाद है। यह गणना प्रक्रिया नीचे दिए गए चित्र में दिखाई गई है, जहां विशिष्ट स्कोर डेटा है अभिव्यक्ति में आसानी के लिए अनुकूलित।

![सोच गणना](./pictures/2-think.png)
चित्र: सोच के लिए ध्यान स्कोर गणना

चरण 3: प्रत्येक अंक को $\sqrt{d_k}$ से विभाजित करें, जहां $d_{k}$ कुंजी वेक्टर का आयाम है। आप इसे अन्य संख्याओं से भी विभाजित कर सकते हैंआर का उद्देश्य बैकप्रॉपैगेशन के दौरान ग्रेडिएंट को अधिक स्थिर बनाना है।

चरण 4: फिर इन अंकों को सॉफ्टमैक्स फ़ंक्शन के माध्यम से पास करें, जो अंकों को सामान्य कर सकता है ताकि वे सभी सकारात्मक हों और 1 तक जोड़ें, जैसा कि नीचे दिए गए चित्र में दिखाया गया है।
ये स्कोर थिंकिंग के शब्द वेक्टर और अन्य शब्दों के शब्द वेक्टर को निर्धारित करते हैं कि सभी पदों पर शब्द वेक्टर पर कितना ध्यान दिया जाता है?

![सोच गणना](./pictures/2-think2.png)
चित्र: सोच का ध्यान स्कोर गणना

चरण 5: प्राप्त करने के बादप्रत्येक शब्द वेक्टर के स्कोर को संबंधित वैल्यू वेक्टर से गुणा करें। इस दृष्टिकोण के पीछे सहज समझ यह है: उच्च स्कोर वाले पदों के लिए, गुणन मूल्य जितना बड़ा होगा, हम कम स्कोर वाले पदों के लिए उन पर अधिक ध्यान देते हैं; गुणन मान जितना छोटा होगा, और इन स्थितियों में शब्द बहुत प्रासंगिक नहीं हो सकते हैं।

चरण 6: वर्तमान स्थिति में सेल्फ अटेंशन के अनुरूप आउटपुट प्राप्त करने के लिए चरण 5 में प्राप्त वैल्यू वैक्टर जोड़ें (इस उदाहरण में पहला स्थान)।

अंत में, नीचे दिया गया चित्र पहले स्थान पर शब्द वेक्टर के लिए सेल्फ अटेंशन की गणना करने की पूरी प्रक्रिया को दर्शाता है। वर्तमान स्थिति का अंतिम शब्द वेक्टर (इस उदाहरण में पहला स्थान) फीडफॉरवर्ड न्यूरल नोट में इनपुट होता रहेगा : उपरोक्त 6 चरण एक समय में केवल एक स्थिति के आउटपुट वेक्टर की गणना कर सकते हैं, वास्तविक कोड कार्यान्वयन में, सेल्फ अटेंशन की गणना प्रक्रिया की गणना मैट का उपयोग करके की जाती हैचावल, और सभी स्थितियों के आउटपुट वैक्टर एक समय में प्राप्त किए जाते हैं।

![गणना सोचो](./pictures/2-sum.png)
चित्र: ध्यान $z_1$ के बाद वेक्टर प्रतिनिधित्व पर विचार करना

##### आत्म-ध्यान मैट्रिक्स गणना

स्व-ध्यान गणना के 6 चरणों में वैक्टर को एक साथ रखें, जैसे $X=[x_1;x_2]$​, और आप मैट्रिक्स गणना कर सकते हैं, हम अभी भी चरण दर चरण आत्म-ध्यान की मैट्रिक्स गणना विधि दिखाते हैं .
$$
एक्स = [X_1;X_2] \\
क्यू = एक्स डब्ल्यू^क्यू, के = एक्स डब्ल्यू^के, वी=एक्स डब्ल्यू^वी \\
Z=मुलायमax(\frac{QK^T}{\sqrt{d_k}}) वी
$$
चरण 1: क्वेरी, कुंजी और मान के मैट्रिक्स की गणना करें, सबसे पहले, हम सभी शब्द वैक्टर को एक मैट्रिक्स एक्स में डालते हैं, और फिर उन्हें तीन वेट मैट्रिक्स $W^Q, W^K W^V$ से गुणा करते हैं। Q, K, और V मैट्रिक्स मैट्रिक्स आयाम में प्रत्येक पंक्ति $d_k$ है।

![](./pictures/2-qkv-multi.png) चित्र: QKV मातृx गुणन

चरण 2: चूँकि हम गणना करने के लिए मैट्रिक्स का उपयोग करते हैं, हम उपरोक्त चरण 2 से 6 को एक चरण में संपीड़ित कर सकते हैं और सीधे सेल्फ अटेंशन का आउटपुट प्राप्त कर सकते हैं।

![आउटपुट](./pictures/2-attention-output.webp)
चित्र: आउटपुट $Z$

#### मल्टी-हेड ध्यान तंत्र

ट्रांसफॉर्मर पेपर ने मल्टी-हेड अटेंशन मैकेनिज्म (ध्यान के एक समूह को अटेंशन हेड कहा जाता है) जोड़कर सेल्फ-अटेंशन को और बेहतर बनाया है। यह तंत्र निम्नलिखित दो तरीकों से ध्यान परत की क्षमता को बढ़ाता है:

-**यह विभिन्न स्थितियों पर ध्यान देने की मॉडल की क्षमता का विस्तार करता है**। उपरोक्त उदाहरण में, पहले स्थान पर आउटपुट $z_1$ में वाक्य में प्रत्येक दूसरे स्थिति की जानकारी का एक छोटा सा हिस्सा होता है, लेकिन $z_1$​ सिर्फ एक वेक्टर है, इसलिए इस पर केवल पहली स्थिति की जानकारी का प्रभुत्व हो सकता है: जब हम वाक्य का अनुवाद करते हैं: 'जानवर सड़क पार नहीं कर पाया क्योंकि वह बहुत थका हुआ था', हम चाहते हैं कि मॉडल ध्यान दे। न केवल "इसे" को, बल्कि "द" को भीडी "जानवर", और यहां तक ​​कि "थका हुआ" भी इस समय, बहु-सिर ध्यान तंत्र मदद करेगा।
- **मल्टी-हेड अटेंशन मैकेनिज्म ध्यान परत को कई "उप-प्रतिनिधित्व स्थान" देता है**। जैसा कि हम नीचे देखेंगे, मल्टी-हेड अटेंशन मैकेनिज्म में $W^Q, W^K W^V$ के कई सेट होते हैं​ वेट मैट्रिक्स (ट्रांसफॉर्मर पेपर में, 8ग्रुप अटेंशन), ​​इसलिए $X$​ को प्रतिनिधित्व के लिए अधिक बीज स्थानों में बदला जा सकता है, इसके बाद, हम प्रत्येक समूह के वेट मैट्रिक्स के 8 समूहों का भी उपयोग करते हैंध्यान को यादृच्छिक रूप से प्रारंभ किया जाता है, लेकिन प्रशिक्षण के बाद, ध्यान के प्रत्येक समूह का वजन $W^Q, W^K W^V$​ इनपुट वेक्टर को संबंधित "उप-प्रतिनिधित्व स्थान" पर मैप कर सकता है।

![मल्टी-हेड ध्यान तंत्र](./pictures/2-multi-head.png)
चित्र: बहु-सिर ध्यान तंत्र

मल्टी-हेड ध्यान तंत्र में, हम ध्यान के प्रत्येक समूह के लिए एक अलग WQ, WK, WV पैरामीटर मैट्रिक्स सेट करते हैं, Q, K के 8 समूह प्राप्त करने के लिए ध्यान के प्रत्येक समूह के इनपुट X और WQ, WK, WV को गुणा करते हैं। वीमैटचावल.

इसके बाद, हम K, Q, V के प्रत्येक समूह के Z मैट्रिक्स की गणना करते हैं और 8 Z मैट्रिक्स प्राप्त करते हैं।

![8 Z मैट्रिसेस](./pictures/2-8z.webp)
चित्र: 8 Z मैट्रिक्स

चूंकि फीडफॉरवर्ड न्यूरल नेटवर्क परत को 8 मैट्रिक्स के बजाय 1 मैट्रिक्स (जहां वेक्टर की प्रत्येक पंक्ति एक शब्द का प्रतिनिधित्व करती है) प्राप्त होती है, हम एक बड़े मैट्रिक्स को प्राप्त करने के लिए सीधे 8 उप-मैट्रिसेस को जोड़ते हैं, और फिर इसे दूसरे वेट मैट्रिक्स $W^ के साथ गुणा करते हैं। O$ इसे फीडफॉरवर्ड न्यूरल नेटवर्क द्वारा आवश्यक आयामों में मैप करने के लिए एक परिवर्तन करने के लिएपरत।

![एकीकरण मैट्रिक्स](./pictures/2-to1.webp)
चित्र: 8 उप-मैट्रिस को संयोजित करें और मानचित्रण परिवर्तन करें

संक्षेप में:
1. 8 आव्यूहों को संयोजित करें {Z0, Z1..., Z7}
2. संयोजित मैट्रिक्स को WO भार मैट्रिक्स से गुणा करें
3. अंतिम मैट्रिक्स Z प्राप्त करें, जिसमें सभी ध्यान प्रमुखों की जानकारी शामिल है। यह मैट्रिक्स FFNN (फीड फॉरवर्ड न्यूरल नेटवर्क) परत पर इनपुट होगा।

उपरोक्त सभी बहु-प्रमुख ध्यान के बारे में है। अंत में, सभी सामग्री को एक चित्र में रखें:

![प्रेरित करनाएक साथ](./pictures/2-put-together.webp)
चित्र: मल्टी-हेड ध्यान तंत्र का मैट्रिक्स ऑपरेशन

मल्टी-हेड ध्यान तंत्र को सीखने के बाद, आइए देखें कि जब हमने ऊपर आईटी उदाहरण का उल्लेख किया है तो विभिन्न ध्यान शीर्षों के अनुरूप "यह" किस पर ध्यान देता है। नीचे दिए गए चित्र में हरी और नारंगी रेखाएं दो अलग-अलग सेटों का प्रतिनिधित्व करती हैं प्रमुखों पर ध्यान दें:

![`इस पर` का ध्यान](./pictures/2-it-attention.webp)
चित्र: `यह` ध्यान है

जब हम "इट" शब्द को एन्कोड करते हैं, तो एक ओf ध्यान शीर्ष (नारंगी ध्यान शीर्ष) "जानवर" पर सबसे अधिक ध्यान केंद्रित करता है, और दूसरा हरा ध्यान शीर्ष "थका हुआ" पर केंद्रित होता है, इसलिए एक अर्थ में, मॉडल में "यह" का प्रतिनिधित्व "जानवर" की कुछ अभिव्यक्तियों को जोड़ता है " " और " टायर"।

#### ध्यान कोड उदाहरण
निम्नलिखित कोड कार्यान्वयन में, टेंसर का पहला आयाम बैच आकार है, और दूसरा आयाम वाक्य की लंबाई है। कोड को एनोटेट किया गया है और विस्तार से समझाया गया है।

```
क्लास मल्टीहेडअटेंशन(एनएन.मॉड्यूल):
#n_heads: मल्टी-हेड ध्यान की संख्या
# hid_dim: प्रत्येक शब्द आउटपुट का वेक्टर आयाम
def __init__(स्वयं, hid_dim, n_heads, ड्रॉपआउट):
सुपर(मल्टीहेडअटेंशन, स्वयं).__init__()
स्वयं.hid_dim = hid_dim
self.n_heads = n_heads

# hid_dim को h से विभाज्य होने के लिए बाध्य करें
ज़ोर से hid_dim % n_heads == 0
# W_q मैट्रिक्ससेल्फ.w_q = nn.Linear(hid_dim, hid_dim) को परिभाषित करें
# W_k मैट्रिक्स को परिभाषित करें
self.w_k = nn.रैखिक(hid_dim, hid_dim)
# W_v मैट्रिक्स को परिभाषित करें
self.w_v = nn.रैखिक(hid_dim, hid_dim)
self.fc = nn.रैखिक(hid_मंद, hid_dim)
self.do = nn.ड्रॉपआउट(ड्रॉपआउट)
#स्केलिंग
सेल्फ.स्केल = मशाल.वर्ग(मशाल.फ्लोटटेन्सर([hid_dim // n_heads]))

डीईएफ़ फॉरवर्ड (स्वयं, क्वेरी, कुंजी, मान, मुखौटा = कोई नहीं):
# ध्यान दें कि वाक्य की लंबाई के आयाम में Q, K और V का मान समान या भिन्न हो सकता है।
# K: [64,10,300], बैच आकार 64 मानते हुए, 10 शब्द हैं, प्रत्येक शब्द का क्वेरी वेक्टर 300-आयामी है
# वी: [64,10,300], बैच आकार 64 मानते हुए, 10 शब्द हैं, और प्रत्येक शब्द का क्वेरी वेक्टर 300-डी हैआकार
# प्रश्न: [64,12,300], बैच आकार 64 मानते हुए, 12 शब्द हैं, और प्रत्येक शब्द का क्वेरी वेक्टर 300-आयामी है
bsz = query.shape[0]
Q = self.w_q(क्वेरी)
के = self.w_k(कुंजी)
वी = self.w_v(मूल्य)
# यहां, K Q V मैट्रिक्स को ध्यान के कई समूहों में विभाजित किया गया है
# अंतिम आयाम self.hid_dim // self.n_heads का उपयोग करके प्राप्त किया जाता है, जो ध्यान के प्रत्येक समूह की वेक्टर लंबाई को इंगित करता है: प्रत्येक शीर्ष की वेक्टर लंबाई है: 300/6=50
# 64 बैच आकार को इंगित करता है, 6 6 समूहों को इंगित करता हैध्यान का, और 10 10 शब्दों को इंगित करता है, 50 प्रत्येक ध्यान समूह शब्द की वेक्टर लंबाई का प्रतिनिधित्व करता है
# K: [64,10,300] एकाधिक ध्यान समूहों को विभाजित करें-> [64,10,6,50] स्थानान्तरण -> [64,6,10,50]
# वी: [64,10,300] एकाधिक ध्यान समूहों को विभाजित करें -> [64,10,6,50] स्थानांतरण -> [64,6,10,50]
# प्रश्न: [64,12,300] कई ध्यान समूहों को विभाजित करें -> [64,12,6,50] स्थानांतरण -> [64,6,12,50]
# ट्रांसपोज़ में निम्नलिखित गणनाओं को सुविधाजनक बनाने के लिए ध्यान की संख्या 6 को सामने और 10 और 50 को पीछे रखना है
क्यू= Q.view(bsz, -1, self.n_heads, self.hid_dim //
self.n_heads).permute(0, 2, 1, 3)
K = K.view(bsz, -1, self.n_heads, self.hid_dim //
self.n_heads).permute(0, 2, 1, 3)
V = V.view(bsz, -1, self.n_heads, self.hid_dim //
self.n_heads).permute(0, 2, 1, 3)

# चरण 1: Q को K के स्थानान्तरण से गुणा किया गया, पैमाने से विभाजित किया गया
# [64,6,12,50] * [64,6,50,10] = [64,6,12,10]
# ध्यान: [64,6,12,10]
ध्यान = टार्च.मैटमूल(क्यू, के.परम्यूट(0, 1, 3, 2)) / सेल्फ.स्केल

# अगर मास्क खाली नहीं है तो अटेंशन एससी सेट करेंस्थिति का अयस्क जहां मास्क 0 से -1e10 है, यहां "0" का उपयोग यह इंगित करने के लिए किया जाता है कि किस शब्द वैक्टर पर ध्यान नहीं दिया जा सकता है, जैसे पैडिंग स्थिति, निश्चित रूप से, "1" या अन्य संख्याओं का भी उपयोग किया जा सकता है मुख्य डिज़ाइन कोड की निम्नलिखित दो पंक्तियों में परिवर्तन है।
यदि मास्क कोई नहीं है:
ध्यान = ध्यान.नकाबपोश_भरण(मुखौटा == 0, -1e10)

#चरण 2: पिछले चरण के परिणाम के सॉफ्टमैक्स की गणना करें, और फिर ध्यान आकर्षित करने के लिए ड्रॉपआउट पास करें।
# ध्यान दें कि यहां हम एल पर सॉफ्टमैक्स करते हैंएस्ट डायमेंशन यानी हम इनपुट सीक्वेंस के डायमेंशन पर सॉफ्टमैक्स करते हैं
# ध्यान: [64,6,12,10]
ध्यान = स्व.करें(मशाल.सॉफ्टमैक्स(ध्यान, मंद=-1))

# चरण 3, मल्टी-हेड ध्यान का परिणाम प्राप्त करने के लिए ध्यान परिणाम को V से गुणा करें
# [64,6,12,10] * [64,6,10,50] = [64,6,12,50]
# एक्स: [64,6,12,50]
एक्स = टॉर्च.मैटमुल(ध्यान, वी)

# क्योंकि क्वेरी में 12 शब्द हैं, परिणामों के कई समूहों को संयोजित करने की सुविधा के लिए आगे 12 और पीछे 50 और 6 रखें
# x: [64,6,12,50] स्थानांतरण-> [64,12,6,50]
x = x.permute(0, 2, 1, 3).contiguous()
# यहां मैट्रिक्स रूपांतरण है: एकाधिक ध्यान समूहों के परिणामों को संयोजित करें
# अंतिम परिणाम है [64,12,300]
# x: [64,12,6,50] -> [64,12,300]
x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))
x = self.fc(x)
वापसी एक्स
# बैच_आकार 64 है, 12 शब्द हैं, और प्रत्येक शब्द का क्वेरी वेक्टर 300 आयाम है
क्वेरी = टॉर्च.रैंड(64, 12, 300)
# बैच_आकार 64 है, 12 शब्द हैं, और प्रत्येक शब्द का कुंजी वेक्टर 3 है00 आयाम
कुंजी = टॉर्च.रैंड(64, 10, 300)
# बैच_आकार 64 है, 10 शब्द हैं, और प्रत्येक शब्द का मान वेक्टर 300 आयाम है वेक्टर 300-आयामी है
मूल्य = टॉर्च.रैंड(64, 10, 300)
ध्यान = मल्टीहेडअटेंशन(hid_dim=300, n_heads=6, ड्रॉपआउट=0.1)
आउटपुट = ध्यान (क्वेरी, कुंजी, मूल्य)
## आउटपुट: टॉर्च.आकार([64,12,300])
प्रिंट(आउटपुट.आकार)

```

#### अवशिष्ट कनेक्शन

अब तक, हमने आत्म-ध्यान के आउटपुट वेक्टर की गणना की है। एस में दो महत्वपूर्ण ऑपरेशन हैंएकल-परत एनकोडर: अवशिष्ट कनेक्शन और सामान्यीकरण।

एनकोडर की प्रत्येक उपपरत (सेल्फ अटेंशन लेयर और एफएफएनएन) में एक अवशिष्ट कनेक्शन और परत सामान्यीकरण होता है, जैसा कि नीचे दिए गए चित्र में दिखाया गया है।

![अवशिष्ट कनेक्शन](./pictures/2-resnet.png)
चित्र: अवशिष्ट कनेक्शन

स्व-ध्यान परत की परत सामान्यीकरण और इसमें शामिल वेक्टर गणना का विवरण निम्नानुसार देखा गया है:

![सामान्यीकरण](./चित्र/2-lyn.png)
चित्र: सामान्यीकरण विवरण

एनकोडर और डीकोडर सबलेयर्स में परत सामान्यीकरण होता है। मान लें कि एक ट्रांसफार्मर 2 से बना है, एनकोडर में एक परत एनकोडर और डिकोडर की दो परतें होती हैं, जो सभी आंतरिक विवरण दिखाती हैं जैसा कि नीचे दिए गए चित्र में दिखाया गया है।

![2-परत आरेख](./चित्र/2-2लेयर.png)
चित्र: 2-परत ट्रांसफार्मर आरेख

##डिकोडर

अब हमने एनकोडर में अधिकांश अवधारणाओं को पेश किया है, और हम मूल रूप से एनकोडर के सिद्धांतों को भी जानते हैं। अब आइए देखें कि एनकोडर और डिकोडर एक साथ कैसे काम करते हैं।एनकोडर में आम तौर पर कई परतें होती हैं। पहले एनकोडर का इनपुट एक अनुक्रम टेक्स्ट होता है, और अंतिम एनकोडर का आउटपुट अनुक्रम वैक्टर का एक सेट होता है, जहां अनुक्रम वैक्टर का यह सेट डिकोडर के K और V इनपुट के रूप में उपयोग किया जाएगा K=V=डिकोडर द्वारा अनुक्रम वेक्टर आउटपुट। ये ध्यान वैक्टर प्रत्येक डिकोडर की एनकोडर-डिकोडर ध्यान परत पर इनपुट होंगे, जो डिकोडर को इनपुट अनुक्रम की उचित स्थिति पर ध्यान केंद्रित करने में मदद करता है, जैसा कि नीचे दिए गए चित्र में दिखाया गया है।

![](./चित्र/ट्रांसफॉर्मर_डिकोडिंग_1.gif)

डिकोडिंग चरण में प्रत्येक बार चरण एक अनुवादित शब्द को आउटपुट करता है (उदाहरण यहां अंग्रेजी अनुवाद है), और वर्तमान समय चरण में डिकोडर के आउटपुट को इनपुट क्यू के रूप में और एनकोडर के और वी के आउटपुट को इनपुट के रूप में उपयोग किया जाता है। अगली बार चरण पर डिकोडर। फिर इस प्रक्रिया को तब तक दोहराएं जब तक कि कोई अंतिम प्रतीक आउटपुट न हो जाए।

![डिकोडर डायनेमिक मैप](./pictures/2-decoder.gif)
डायनेमिक ग्राफ़: डिकोडर डायनेमिक ग्राफ़

के अंतरडिकोडर में सेल्फ अटेंशन परत और एनकोडर में सेल्फ अटेंशन परत के बीच:

1. डिकोडर में, सेल्फ अटेंशन परत को आउटपुट अनुक्रम में वर्तमान स्थिति से पहले केवल शब्दों पर ध्यान केंद्रित करने की अनुमति है: विशिष्ट दृष्टिकोण यह है: सेल्फ अटेंशन स्कोर सॉफ्टमैक्स परत से गुजरने से पहले, वर्तमान स्थिति के बाद की स्थिति को छुपाया जाता है (ध्यान स्कोर -inf पर सेट है)।

2. डिकोडर अटेंशन लेयर क्वेरी एम के निर्माण के लिए पिछली लेयर के आउटपुट का उपयोग करती हैएट्रिक्स, जबकि कुंजी मैट्रिक्स और वैल्यू मैट्रिक्स एनकोडर के अंतिम आउटपुट से आते हैं।

## रैखिक परत और सॉफ्टमैक्स

डिकोडर का अंतिम आउटपुट एक वेक्टर है, जिसमें प्रत्येक तत्व एक फ्लोटिंग पॉइंट नंबर है। हम इस वेक्टर को शब्दों में कैसे परिवर्तित करते हैं? यह रैखिक परत और सॉफ्टमैक्स द्वारा किया जाता है।

रैखिक परत एक सामान्य पूरी तरह से जुड़ा हुआ तंत्रिका नेटवर्क है जो डिकोडर द्वारा वेक्टर आउटपुट को एक बड़े वेक्टर में मैप करता है जिसे लॉगिट्स वेक्टर कहा जाता है: मान लें कि हमारे मॉडल में 10,000 अंग्रेजी शब्द हैं (बाहर)मॉडल की शब्दावली डालें), इस लॉग वेक्टर में 10,000 संख्याएँ होंगी, प्रत्येक एक शब्द के स्कोर का प्रतिनिधित्व करेगा।

फिर, सॉफ्टमैक्स कुल्हाड़ी परत इन अंकों को संभावनाओं में परिवर्तित करती है (सभी अंकों को सकारात्मक संख्याओं में परिवर्तित करती है और उन्हें 1 तक जोड़ती है)। फिर उच्चतम संभावना वाली संख्या के अनुरूप शब्द का चयन किया जाता है, जो इस समय चरण का आउटपुट शब्द है।

![रैखिक परत](./pictures/2-linear.png)
चित्र: रैखिक परत

## लॉस फंकशन

ट्रांसफार्मर को प्रशिक्षित करते समय, वें का आउटपुटहानि प्राप्त करने के लिए ई डिकोडर और लेबल को हानि फ़ंक्शन में एक साथ फीड करने की आवश्यकता होती है, और मॉडल अंततः हानि के अनुसार दिशा में प्रचारित होगा, हम प्रशिक्षण की हानि गणना को चित्रित करने के लिए एक सरल उदाहरण का उपयोग करते हैं प्रक्रिया: "दया" का "धन्यवाद" में अनुवाद करना।

हमें उम्मीद है कि मॉडल डिकोडर के अंतिम आउटपुट का संभाव्यता वितरण "धन्यवाद" शब्द को इंगित करेगा (हालांकि, शुरुआत में, "धन्यवाद" शब्द की संभावना सबसे अधिक है)।मॉडल को अच्छी तरह से प्रशिक्षित नहीं किया गया है, और यह जो संभाव्यता वितरण आउटपुट करता है वह उस संभाव्यता वितरण से बहुत दूर हो सकता है जिसकी हम आशा करते हैं। जैसा कि नीचे दिए गए चित्र में दिखाया गया है, सही संभाव्यता वितरण यह होना चाहिए कि "धन्यवाद" शब्द की संभावना सबसे अधिक है। हालाँकि, चूंकि मॉडल के मापदंडों को यादृच्छिक रूप से आरंभ किया गया है, शुरुआत में मॉडल द्वारा अनुमानित सभी शब्दों की संभावना लगभग यादृच्छिक है।

![संभावना वितरण](./चित्र/2-नुकसान.वेबपी)
चित्र: संभाव्यतावितरण

जब तक ट्रांसफार्मर डिकोडर संभावनाओं के एक सेट की भविष्यवाणी करता है, हम संभावनाओं के इस सेट की तुलना सही आउटपुट संभावनाओं के साथ कर सकते हैं और फिर मॉडल के वजन को समायोजित करने के लिए बैकप्रॉपैगेशन का उपयोग कर सकते हैं ताकि आउटपुट संभावना वितरण पूर्णांक आउटपुट के करीब हो।

तो हम दो संभाव्यता वितरणों की तुलना कैसे करते हैं? : हम संभाव्यता वैक्टर के दो सेटों के बीच स्थानिक दूरी का उपयोग हानि (वेक्टर घटाव, फिर वर्ग योग, और फिर वर्ग) के रूप में कर सकते हैं।यूरे रूट), निश्चित रूप से, हम क्रॉस-एन्ट्रॉपी और केएल डाइवर्जेंस का भी उपयोग कर सकते हैं। पाठक संबंधित ज्ञान को आगे खोज और पढ़ सकते हैं, और इस खंड में हानि फ़ंक्शन के ज्ञान का विस्तार नहीं किया गया है।

चूँकि उपरोक्त केवल एक शब्द वाला उदाहरण बहुत सरल है, हम एक अधिक जटिल वाक्य देख सकते हैं: वाक्य इनपुट है: "je suis étudiant" और आउटपुट है: "मैं एक छात्र हूँ"। संभाव्यता वितरण वैक्टर को कई बार आउटपुट करना होगा:

- प्रत्येक संभावनाy वितरण आउटपुट एक वेक्टर है जिसकी लंबाई vocab_size है (अधिकतम vocab आकार, यानी, वेक्टर लंबाई 6 है, इस पर पहले सहमति हुई है, लेकिन वास्तविक vocab आकार 30,000 या 50,000 होने की अधिक संभावना है)
- पहले आउटपुट के संभाव्यता वितरण में, उच्चतम संभावना वाला शब्द "i" है
- दूसरे आउटपुट के संभाव्यता वितरण में, उच्चतम संभावना वाला शब्द "am" है
- और इसी तरह, पांचवें संभाव्यता वितरण तक, उच्चतम संभाव्यता वाला शब्दity "\<eos>" है, जिसका अर्थ है कि यह इंगित करता है कि कोई अगला शब्द नहीं है।

तो हमारे लक्ष्य का संभाव्यता वितरण इस तरह दिखता है:

![संभावना वितरण](./चित्र/2-target.png)
चित्र: लक्ष्य संभाव्यता वितरण

हम चित्र में दिखाए गए संभाव्यता वितरण का उत्पादन करने की उम्मीद में, उदाहरण में वाक्यों के साथ मॉडल को प्रशिक्षित करते हैं

हमारे मॉडल को काफी बड़े डेटासेट पर लंबे समय तक प्रशिक्षित करने के बाद, हम नीचे दिए गए चित्र में दिखाए गए संभाव्यता वितरण को आउटपुट करने की उम्मीद करते हैं:![प्रशिक्षण के बाद संभाव्यता वितरण](./pictures/2-trained.webp)
चित्र: मॉडल प्रशिक्षण के बाद एकाधिक संभाव्यता वितरण आउटपुट

हमें उम्मीद है कि प्रशिक्षण के बाद मॉडल जो संभाव्यता वितरण आउटपुट कर सकता है वह सही अनुवाद के अनुरूप होगा, यदि आप जिस वाक्य का अनुवाद करना चाहते हैं वह प्रशिक्षण सेट का हिस्सा है, तो हमें उम्मीद है कि मॉडल का आउटपुट परिणाम कुछ भी नहीं होगा उन वाक्यों का सटीक अनुवाद कर सकता है जिन्हें उसने कभी नहीं देखा है।

इसके अतिरिक्त सी का उल्लेख करेंलालची डिकोडिंग और बीम खोज की अवधारणाएँ:

- लालची डिकोडिंग: चूंकि मॉडल प्रति समय चरण केवल एक आउटपुट उत्पन्न करता है, हम इसे इस तरह से सोचते हैं: मॉडल संभाव्यता वितरण से उच्चतम संभावना वाले शब्द का चयन करता है और अन्य शब्दों को हटा देता है। इस विधि को लालची डिकोडिंग कहा जाता है।
- बीम खोज: प्रत्येक समय चरण में k उच्चतम संभावना वाले आउटपुट शब्दों को बनाए रखें, और फिर अगले चरण में, निर्धारित करें कि पिछले चरण में बनाए गए k शब्दों के आधार पर कौन से k शब्दों को बनाए रखा जाना चाहिए।समय चरण। मान लें कि k=2, पहली स्थिति में सबसे अधिक संभावना वाले दो शब्द "I" और "a" हैं, जिनमें से दोनों को बरकरार रखा गया है, और फिर दूसरी स्थिति में शब्द की संभाव्यता वितरण की गणना इसके आधार पर की जाती है। पहला शब्द, और फिर दूसरे स्थान पर सबसे अधिक संभावना वाले दो शब्द निकाले जाते हैं, तीसरे और चौथे स्थान के लिए भी हम इस प्रक्रिया को दोहराते हैं। इस विधि को बीम खोज कहा जाता है।

## अतिरिक्त जानकारी

मुझे आशा है कि उपरोक्त सामग्री आपकी मदद कर सकती हैआप ट्रांसफार्मर में मुख्य अवधारणाओं को समझते हैं यदि आप अधिक गहराई से समझना चाहते हैं, तो मेरा सुझाव है कि आप निम्नलिखित का संदर्भ लें:

- ट्रांसफार्मर पेपर पढ़ें:
"आपको केवल ध्यान देने की आवश्यकता है"
लिंक: https://arxiv.org/abs/1706.03762
- ट्रांसफार्मर ब्लॉग पोस्ट पढ़ें:
"ट्रांसफॉर्मर: भाषा समझ के लिए एक नवीन तंत्रिका नेटवर्क वास्तुकला"
लिंक: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
"Tensor2Tensor घोषणा" पढ़ें - लिंक: https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html
- मॉडल और उसके विवरण को समझने के लिए वीडियो देखें [लुकाज़ कैसर की बातचीत]
लिंक: https://www.youtube.com/watch?v=rBCqOTEfxvg
इस कोड को चलाएँ: [ज्यूपिटर नोटबुक Tensor2Tensor रेपो के भाग के रूप में प्रदान किया गया]
- लिंक: https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb।
- इस प्रोजेक्ट को देखें: [Tensor2Tensor रेपो]
लिंक: https://github.com/tensorflow/tensor2tensor

## आभार
यह लेख हैइसका मुख्य रूप से अनुवाद और लेखन हार्बिन इंस्टीट्यूट ऑफ टेक्नोलॉजी के झांग जियान द्वारा किया गया है, और डुओडुओ और डेटाव्हेल प्रोजेक्ट के छात्रों द्वारा पुनर्गठित किया गया है, अंत में, आपकी पढ़ने की प्रतिक्रिया और सितारों की प्रतीक्षा में, धन्यवाद।