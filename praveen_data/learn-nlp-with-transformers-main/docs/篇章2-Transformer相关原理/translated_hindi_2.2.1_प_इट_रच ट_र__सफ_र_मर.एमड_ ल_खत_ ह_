```अजगर
IPython.display से छवि आयात करें
छवि(फ़ाइलनाम='चित्र/ट्रांसफॉर्मर.पीएनजी')
```




    
![png](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_0_0.पीएनजी)
    



यह लेख हार्वर्ड एनएलपी [द एनोटेटेड ट्रांसफॉर्मर] (https://nlp.seas.harvard.edu/2018/04/03/attention.html) से अनुवादित है।
यह लेख मुख्य रूप से 2018 की शुरुआत में हार्वर्ड एनएलपी विद्वानों द्वारा लिखा गया था और लाइन-बाय-लाइन कार्यान्वयन के रूप में पेपर का "एनोटेटेड" संस्करण प्रस्तुत करता है, जिसमें मूल पेपर को पुनर्व्यवस्थित किया गया है और टिप्पणियों और एनोटेशन को जोड़ा गया है। इस लेख की नोट बुक [अध्याय 2](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB) पर पाई जा सकती है।%A02-ट्रांसफॉर्मर%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86) डाउनलोड।

सामग्री संगठन:
-पाइटोरच में एक संपूर्ण ट्रांसफार्मर लिखें
  - पृष्ठभूमि
  - मॉडल वास्तुकला
  - एनकोडर भाग और डिकोडर भाग
    -एनकोडर
    -डिकोडर
    -ध्यान
    - मॉडल में ध्यान का अनुप्रयोग
    - स्थिति-आधारित फ़ीडफ़ॉरवर्ड नेटवर्क
  - एंबेडिंग और सॉफ्टमैक्स
  - स्थान एन्कोडिंग
  - पूरा मॉडल
- रेलगाड़ी
  - बैच प्रोसेसिंग और मास्क
  - ट्रैनिंग लूप
  - प्रशिक्षण डेटा और बैच प्रोसेसिंग
  - हार्डवेयर और प्रशिक्षण का समय
  - अनुकूलक
  -नियमितीकरण
    - लेबल स्मूथिंग
- उदाहरण
  - सिंथेटिक डेटा
  - हानि समारोह गणना
  - लालची डिकोडिंग
-वास्तविक दृश्य उदाहरण
- निष्कर्ष
- आभार


# प्रारंभिक काम


```अजगर
# !pip इंस्टॉल करें http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib स्पेसी टॉर्चटेक्स्ट सीबॉर्न
```


```अजगर
एनपी के रूप में सुन्न आयात करें
मशाल आयात करें
टॉर्च.एनएन को एनएन के रूप में आयात करें
टॉर्च.एनएन.फंक्शनल को एफ के रूप में आयात करें
गणित, कॉपी, समय आयात करें
टॉर्च.ऑटोग्राड से वेरिएबल आयात करें
matplotlib.pyplot को plt के रूप में आयात करें
समुद्री जहाज़ आयात करें
Seaborn.set_context(संदर्भ='बातचीत')
%matplotlib इनलाइन
```

विषयसूची


* विषयसूची
{:टोक}

# पृष्ठभूमि

ट्रांसफार्मर के बारे में अधिक पृष्ठभूमि ज्ञान के लिए, पाठक इस परियोजना का [अध्याय 2.2 इलस्ट्रेटेड ट्रांसफार्मर] पढ़ सकते हैं (https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-ट्रांसफॉर्मर%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90 %86/2.2-%E5%9B%BE%E8%A7%A3transformer.md) सीखने के लिए।

#मॉडल वास्तुकला

अधिकांश अनुक्रम-से-अनुक्रम (seq2seq) मॉडल एक एनकोडर-डिकोडर आर्किटेक्चर [(उद्धरण)] (https://arxiv.org/abs/1409.0473) का उपयोग करते हैं। एनकोडर एक इनपुट अनुक्रम $(x_{1},...x_{n})$ को निरंतर प्रतिनिधित्व $z=(z_{1},...z_{n})$ पर मैप करता है। डिकोडर z में प्रत्येक तत्व के लिए एक आउटपुट अनुक्रम $(y_{1},...y_{m})$ उत्पन्न करता है। डिकोडर प्रत्येक समय चरण में एक आउटपुट उत्पन्न करता है। प्रत्येक चरण पर, मॉडल ऑटोरेग्रेसिव है [(उद्धरण)](https://arxiv.org/abs/1308.0850), और अगला परिणाम उत्पन्न करते समय, पहले उत्पन्न परिणामों को एक साथ भविष्यवाणी करने के लिए इनपुट अनुक्रम में जोड़ा जाता है। अब हम seq2seq आर्किटेक्चर बनाने के लिए सबसे पहले एक EncoderDecoder क्लास बनाते हैं:


```अजगर
classEncoderDecodएर(एनएन.मॉड्यूल):
    """
    मूल एनकोडर-डिकोडर संरचना।
    इसके लिए एक मानक एनकोडर-डिकोडर आर्किटेक्चर और कई
    अन्य मॉडल.
    """
    def __init__(स्वयं, एनकोडर, डिकोडर, src_embed, tgt_embed, जनरेटर):
        सुपर(एनकोडरडिकोडर, स्वयं).__init__()
        स्व.एनकोडर = एनकोडर
        सेल्फ.डिकोडर = डिकोडर
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        स्व.जनरेटर = जनरेटर
        
    डीईएफ़ फॉरवर्ड (स्वयं, स्रोत, टीजीटी, स्रोत_मास्क, टीजीटी_मास्क):"नकाबपोश स्रोत और लक्ष्य अनुक्रम लें और संसाधित करें।"
        self.decode(self.encode(src, src_mask), src_mask लौटाएं,
                            टीजीटी, टीजीटी_मास्क)
    
    डीईएफ़ एनकोड(स्वयं, स्रोत, स्रोत_मास्क):
        self.encoder(self.src_embed(src), src_mask) वापस करें
    
    डीईएफ़ डिकोड(स्वयं, मेमोरी, src_mask, tgt, tgt_mask):
        self.decoder(self.tgt_embed(tgt), मेमोरी, src_mask, tgt_mask) वापस करें
```


```अजगर
क्लास जेनरेटर (एनएन.मॉड्यूल):
    "रैखिक और सॉफ्टमैक्स से युक्त जनरेटर को परिभाषित करें"
    "मानक एल को परिभाषित करेंइनियर + सॉफ्टमैक्स जेनरेशन स्टेप।"
    def __init__(self, d_model, vocab):
        सुपर(जनरेटर, स्वयं).__init__()
        self.proj = nn.Linear(d_model, vocab)

    डीईएफ़ फॉरवर्ड(स्वयं, एक्स):
        वापसी F.log_softmax(self.proj(x), dim=-1)
```

TTransformer के एन्कोडर और डिकोडर दोनों को स्व-ध्यान और पूरी तरह से कनेक्टेड परतों का उपयोग करके स्टैक किया गया है। जैसा कि नीचे दिए गए चित्र के बाएँ और दाएँ तरफ दिखाया गया है।


```अजगर
छवि(फ़ाइलनाम='./चित्र/2-ट्रांसफॉर्मर.पीएनजी')
```




    
![png](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_13_0.पीएनजी)
    



## एन सीओडर भाग और डिकोडर भाग

### एनकोडर

एनकोडर में N = 6 समान परतें होती हैं।


```अजगर
डीईएफ़ क्लोन (मॉड्यूल, एन):
    "एन समान नेटवर्क परतें उत्पन्न करें"
    "एन समान परतें तैयार करें।"
    रिटर्न एनएन.मॉड्यूललिस्ट([कॉपी.डीपकॉपी(मॉड्यूल) फॉर _ इन रेंज(एन)])
```


```अजगर
क्लास एनकोडर (एनएन.मॉड्यूल):
    "संपूर्ण एनकोडर में एन परतें होती हैं"
    def __init__(स्वयं, परत, N):
        सुपर(एनकोडर, स्वयं).__init__()
        स्व.परतें = क्लोन(परत, एन)
        self.norm = LayerNorm(परत.आकार)
        
    डीईएफ़ फॉरवर्ड (स्वयं, एक्स, मास्क):
        "प्रत्येक परत का इनपुट x और मास्क है"
        एफओस्व.परतों में आर परत:
            x = परत(x, मास्क)
        self.norm(x) वापस करें
```

एनकोडर की प्रत्येक एनकोडर परत में सेल्फ अटेंशन उप-परत और एफएफएनएन उप-परत शामिल होती है। प्रत्येक उप-परत अवशिष्ट कनेक्शन [(उद्धरण)](https://arxiv.org/abs/1512.03385), और परत सामान्यीकरण (परत-) का उपयोग करती है। सामान्यीकरण) [(उद्धरण)](https://arxiv.org/abs/1607.06450)। पहले मानकीकरण की निम्नलिखित परत लागू करें:


```अजगर
क्लास लेयरनॉर्म(एनएन.मॉड्यूल):
    "एक लेयरनॉर्म मॉड्यूल का निर्माण करें (विवरण के लिए उद्धरण देखें)।"
    def __init__(स्वयं, सुविधाएँ, eps=1e-6):
        सुपर(लेयरनॉर्म, सेल्फ).__init__()
        self.a_2 = nn.Parameter(torch.ones(featयूरेस))
        self.b_2 = nn.Parameter(मशाल.शून्य(विशेषताएं))
        स्वयं.ईपीएस = ईपीएस

    डीईएफ़ फॉरवर्ड(स्वयं, एक्स):
        माध्य = x.माध्य(-1, मंद रखें=सत्य)
        std = x.std(-1, Keepdim=True)
        वापसी self.a_2 * (x - माध्य) / (std + self.eps) + self.b_2
```

हम सबलेयर को कॉल करते हैं: $\mathrm{Sublayer}(x)$, और प्रत्येक सबलेयर का अंतिम आउटपुट $\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$ है। ड्रॉपआउट [(उद्धरण)](http://jmlr.org/papers/v15/srivastava14a.html) को सबलेयर में जोड़ा गया है।

अवशिष्ट कनेक्शन की सुविधा के लिए, मॉडल और एम्बेडिंग परत में सभी उप-परतों द्वारा उत्पन्न आउटपुट के आयाम $d_{\text{model}}=512$ हैं।

नीचे एसublayerConnection क्लास का उपयोग एकल सबलेयर के आउटपुट को संसाधित करने के लिए किया जाता है, जो अगले सबलेयर में इनपुट होता रहेगा:


```अजगर
क्लास सबलेयरकनेक्शन(एनएन.मॉड्यूल):
    """
    एक परत मानदंड के बाद एक अवशिष्ट कनेक्शन।
    कोड सरलता के लिए नोट करें कि मानक अंतिम के विपरीत प्रथम है।
    """
    def __init__(स्वयं, आकार, ड्रॉपआउट):
        सुपर(सबलेयरकनेक्शन, सेल्फ).__init__()
        self.norm = लेयरनॉर्म(आकार)
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(ड्रॉपआउट)

    डीईएफ़ फॉरवर्ड (स्वयं, एक्स, सबलेयर):
        "किसी भी सबलेयर पर अवशिष्ट कनेक्शन लागू करेंयह एक ही आकार का है।"
        वापसी x + self.dropout(sublayer(self.norm(x)))
```

प्रत्येक एन्कोडर परत में दो उप-परतें होती हैं। पहली परत एक मल्टी-हेड सेल्फ-अटेंशन परत है, और दूसरी परत एक सरल पूरी तरह से कनेक्टेड फीड-फॉरवर्ड नेटवर्क है। दोनों परतों को सबलेयरकनेक्शन क्लास का उपयोग करके संसाधित करने की आवश्यकता है।


```अजगर
क्लास एनकोडरलेयर (एनएन.मॉड्यूल):
    "एनकोडर सेल्फ-एटीएन और फीड फॉरवर्ड से बना है (नीचे परिभाषित)"
    def __init__(स्वयं, आकार, self_attn, फ़ीड_फॉरवर्ड, ड्रॉपआउट):
        सुपर(एनकोडरलेयर, स्वयं).__init__()
        self.self_attn = self_attn
        सेल्फ.फीड_फॉरवर्ड = फीड_फॉरवर्ड
        स्व.उपआयर = क्लोन(सबलेयरकनेक्शन(आकार, ड्रॉपआउट), 2)
        स्व.आकार = आकार

    डीईएफ़ फॉरवर्ड (स्वयं, एक्स, मास्क):
        "कनेक्शन के लिए चित्र 1 (बाएं) का अनुसरण करें।"
        x = self.sublayer[0](x, Lambda x: self.self_attn(x, x, x, मास्क))
        सेल्फ.सबलेयर लौटाएं[1](x, सेल्फ.फीड_फॉरवर्ड)
```

### डिकोडर

डिकोडर भी N = 6 समान डिकोडर परतों से बना है।  


```अजगर
क्लास डिकोडर (एनएन.मॉड्यूल):
    "मास्किंग के साथ जेनेरिक एन लेयर डिकोडर।"
    def __init__(स्वयं, परत, N):
        सुपर(डिकोडर, स्वयं).__init__()स्व.परतें = क्लोन(परत, एन)
        self.norm = LayerNorm(परत.आकार)
        
    डीईएफ़ फॉरवर्ड (स्वयं, एक्स, मेमोरी, src_mask, tgt_mask):
        self.layers में परत के लिए:
            x = परत(x, मेमोरी, src_mask, tgt_mask)
        self.norm(x) वापस करें
```

सिंगल-लेयर एनकोडर की तुलना में, सिंगल-लेयर डिकोडर में एक तीसरी उप-परत होती है, जो एनकोडर के आउटपुट पर ध्यान केंद्रित करती है: यानी, एनकोडर-डिकोडर-ध्यान परत क्यू वेक्टर के आउटपुट से आती है डिकोडर की पिछली परत, और k और v वेक्टर एनकोडर की अंतिम परत के आउटपुट वेक्टर हैं। एनकोडर के समान, हम प्रत्येक उप-परत में अवशिष्ट कनेक्शन का उपयोग करते हैं और फिर परत सामान्यीकरण करते हैं।


```अजगर
क्लास डिकोडरलेयर(एनएन.मॉड्यूल):
    "डिकोडर स्व-ध्यान से बना है, src-attn, और आगे फ़ीड करें (नीचे परिभाषित)"
    def __init__(स्वयं, आकार, self_attn, src_attn, फ़ीड_फॉरवर्ड, ड्रॉपआउट):
        सुपर(डिकोडरलेयर, स्वयं).__init__()
        स्व.आकार = आकार
        self.self_attn = self_attn
        self.src_attn = src_attn
        सेल्फ.फीड_फॉरवर्ड = फीड_फॉरवर्ड
        self.sublayer = क्लोन (SublayerConnection(आकार, ड्रॉपआउट), 3)
 
    डीईएफ़ फॉरवर्ड (स्वयं, एक्स, मेमोरी, src_mask, tgt_mask):
        "कनेक्शन के लिए चित्र 1 (दाएं) का अनुसरण करें।"
        एम=स्मृतिx = self.sublayer[0](x, Lambda x: self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, Lambda x: self.src_attn(x, m, m, src_mask))
        सेल्फ.सबलेयर लौटाएं[2](x, सेल्फ.फीड_फॉरवर्ड)
```

एकल-परत डिकोडर में आत्म-ध्यान उप-परत के लिए, हमें वर्तमान स्थिति में बाद की स्थितियों पर ध्यान देने से रोकने के लिए एक मुखौटा तंत्र का उपयोग करने की आवश्यकता है।


```अजगर
डीईएफ़ अनुवर्ती_मास्क(आकार):
    "बाद की स्थितियों को छुपाएं।"
    attn_shape = (1, आकार, साइज)
    अनुवर्ती_मास्क = np.triu(np.ones(attn_shape), k=1).astype('uint8')
    वापसी मशाल.from_numpy(subsequent_mask) == 0
```

> नीचे दिया गया ध्यान मास्क दिखाता है कि प्रत्येक tgt शब्द (पंक्ति) को कहाँ देखने की अनुमति है (कॉलम)। प्रशिक्षण के दौरान, इस शब्द को अगले शब्दों पर ध्यान देने से रोकने के लिए वर्तमान शब्द की भविष्य की जानकारी को अवरुद्ध कर दिया जाता है।


```अजगर

plt.figure(figsize=(5,5))
plt.imshow(बाद में_मास्क(20)[0])
कोई नहीं
```


    
![svg](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_30_0.एसवीजी)
    


### ध्यान

अटेंशन फ़ंक्शन को मैपिंग क्वेरी और आउटपुट के लिए कुंजी-मानों के एक सेट के रूप में वर्णित किया जा सकता है, जहां क्वेरी, कुंजी, मान और आउटपुट सभी वैक्टर हैं। आउटपुट मानों का भारित योग है, जहां प्रत्येक मान के वजन की गणना क्वेरी और संबंधित कुंजी द्वारा की जाती है।                                                                         
हम विशेष ध्यान देंगेएन्शन को "स्केल्ड डॉट-प्रोडक्ट अटेंशन" (स्केल्ड डॉट-प्रोडक्ट अटेंशन") कहा जाता है। इसके इनपुट क्वेरी, कुंजी (आयाम $d_k$ है) और मान (आयाम $d_v$ है) हैं। हम क्वेरी और सभी की गणना करते हैं कुंजी डॉट उत्पाद, फिर प्रत्येक को $\sqrt{d_k}$ से विभाजित करें, और अंत में मूल्य का वजन प्राप्त करने के लिए सॉफ्टमैक्स फ़ंक्शन का उपयोग करें।```अजगर
छवि(फ़ाइलनाम='./चित्र/ट्रांसफॉर्मर-स्वयं-ध्यान.png')
```




    
![png](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_32_0.पीएनजी)
    




व्यवहार में, हम एक साथ प्रश्नों के एक सेट के ध्यान कार्यों की गणना करते हैं और उन्हें मैट्रिक्स $Q$ में जोड़ते हैं। कुंजी और मान मिलकर मैट्रिक्स $K$ और $V$ भी बनाते हैं। हम जिस आउटपुट मैट्रिक्स की गणना करते हैं वह है:
                                                                 
$$
   \mathrm{ध्यान देंआयन}(क्यू, के, वी) = \mathrm{सॉफ्टमैक्स}(\frac{QK^T}{\sqrt{d_k}})V
$$


```अजगर
ध्यान आकर्षित करें (क्वेरी, कुंजी, मूल्य, मुखौटा = कोई नहीं, ड्रॉपआउट = कोई नहीं):
    "'स्केल्ड डॉट उत्पाद ध्यान' की गणना करें"
    d_k = query.size(-1)
    स्कोर = Torch.matmul(क्वेरी, key.transpose(-2, -1)) \
             / गणित.sqrt(d_k)
    यदि मास्क कोई नहीं है:
        स्कोर = स्कोर.मास्कड_फिल(मास्क == 0, -1e9)
    p_attn = F.softmax(स्कोर, मंद = -1)
    यदि ड्रॉपआउट कोई नहीं है:
        p_attn = ड्रॉपआउट(p_attn)
    वापस करनामशाल.मैटमुल(p_attn, मान), p_attn
```

&#8195;&#8195;दो सबसे अधिक उपयोग किए जाने वाले ध्यान कार्य हैं:
- अतिरिक्त ध्यान[(उद्धरण)](https://arxiv.org/abs/1409.0473)
- डॉट उत्पाद (गुणा) ध्यान

स्केलिंग फ़ैक्टर $\frac{1}{\sqrt{d_k}}$ को छोड़कर, डॉट उत्पाद ध्यान हमारे सामान्य डॉट उत्पाद एल्गोरिदम के समान है। समानता की गणना करने के लिए एडिटिव अटेंशन एकल छिपी हुई परत के साथ फ़ीड-फ़ॉरवर्ड नेटवर्क का उपयोग करता है। यद्यपि सैद्धांतिक रूप से डॉट उत्पाद ध्यान और योगात्मक ध्यान की जटिलता समान है, व्यवहार में, डॉट उत्पाद ध्यान को अत्यधिक अनुकूलित मैट्रिक्स गुणन का उपयोग करके लागू किया जा सकता है, इसलिए डॉट उत्पाद ध्यान गणना तेज और अधिक स्थान बचाने वाली होती है।                                                                                           
जब $d_k$ का मूल्य अपेक्षाकृत छोटा होता है, तो दोनों तंत्रों का प्रदर्शन समान होता है। जब $d_k$ अपेक्षाकृत बड़ा होता है, तो स्केलिंग के बिना योगात्मक ध्यान डॉट उत्पाद ध्यान से बेहतर प्रदर्शन करता है [(उद्धरण)](https://arxiv.org/abs/1703.03906)। हमें संदेह है कि $d_k$ के बड़े मूल्यों के लिए, डॉट उत्पाद काफी हद तक बढ़ता है, सॉफ्टमैक्स फ़ंक्शन को बेहद छोटे ग्रेडिएंट वाले क्षेत्रों में धकेलता है। (यह समझाने के लिए कि डॉट उत्पाद बड़ा क्यों हो जाता है, मान लें कि q और k 0 के माध्य और 1 के प्रसरण के साथ स्वतंत्र यादृच्छिक चर हैं। फिर उनका डॉट उत्पाद $q \cdot k = \sum_{i=1}^{d_k } q_ik_i $, माध्य 0 है और प्रसरण $d_k$ है)। इस प्रभाव का प्रतिकार करने के लिए, हम डॉट उत्पाद को $\frac{1}{\sqrt{d_k}}$ के कारक से छोटा करते हैं।     

यहाँ सु जियानलिन के लेख का एक उद्धरण है ["ट्रांसफॉर्मर के आरंभीकरण, पैरामीटरीकरण और मानकीकरण पर एक संक्षिप्त बातचीत"] (https://zhuanlan.zhihu.com/p/400925524?utm_source=wechat_session&utm_medium=social&utm_oi=1400823417357139968&utm_campaign=shareopn), $\sqrt{d}$ से विभाजित करना ध्यान में इतना महत्वपूर्ण क्यों है?

 


```अजगर
छवि(फ़ाइलनाम='चित्र/ट्रांसफॉर्मर-रेखीय.पीएनजी')
```![png](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_37_0.पीएनजी)
    



मल्टी-हेड ध्यान मॉडल को एक ही समय में विभिन्न स्थानों पर अलग-अलग प्रतिनिधित्व उप-स्थानों से जानकारी पर ध्यान केंद्रित करने की अनुमति देता है, यदि केवल एक ही ध्यान हेड है, तो वेक्टर की प्रतिनिधित्व क्षमता कम हो जाएगी।

$$
\mathrm{मल्टीहेड}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\
    \text{where}~\mathrm{head_i} = \mathrm{ध्यान}(QW^Q_i, KW^K_i, VW^V_i)
$$मैपिंग वेट मैट्रिक्स द्वारा पूरी की जाती है: $W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_ {text{मॉडल}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ और $W^O \in \mathbb{R }^ {hd_v \times d_{\text{model}}}$. 

 इस कार्य में, हम $h=8$ समानांतर ध्यान परतों या शीर्षों का उपयोग करते हैं। इनमें से प्रत्येक शीर्ष के लिए, हम $d_k=d_v=d_{\text{model}}/h=64$ का उपयोग करते हैं। चूँकि प्रत्येक शीर्ष के आयाम कम हो गए हैं, कुल कम्प्यूटेशनल लागत पूर्ण आयामों के साथ एकल शीर्ष ध्यान के समान है। 


```अजगर
क्लास मल्टीहेडेडअटेंशन(एनएन.मॉड्यूल):
    def __init__(स्वयं, h, d_model, ड्रॉपौटी=0.1):
        "मॉडल का आकार और सिरों की संख्या लें।"
        सुपर(मल्टीहेडेडअटेंशन, स्वयं).__init__()
        जोर से d_model % h == 0
        # हम मानते हैं कि d_v हमेशा d_k के बराबर होता है
        self.d_k = d_model // h
        स्वयं.एच = एच
        self.linears = क्लोन्स(nn.Linear(d_model, d_model), 4)
        self.attn = कोई नहीं
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(पी=ड्रॉपआउट)
        
    डीईएफ़ फॉरवर्ड (स्वयं, क्वेरी, कुंजी, मान, मुखौटा = कोई नहीं):
        "चित्रा 2 लागू करें"
        यदि मास्क कोई नहीं है:# सभी सिरों पर एक जैसा मास्क लगाया गया।
            मुखौटा = मुखौटा.खोलना(1)
        nbatches = query.size(0)
        
        # 1) सभी रैखिक प्रक्षेपण d_model => h x d_k से बैच में करें
        क्वेरी, कुंजी, मान = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
             ज़िप में l, x के लिए (self.linears, (क्वेरी, कुंजी, मान))]
        
        #2) बैच में सभी प्रक्षेपित वैक्टरों पर ध्यान दें।
        x, self.attn = ध्यान (प्रश्न, कुंजी, मूल्य, मुखौटा = मुखौटा,ड्रॉपआउट=स्वयं.ड्रॉपआउट)
        
        #3) एक दृश्य का उपयोग करके "कॉनकैट" करें और एक अंतिम रैखिक लागू करें।
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d_k)
        स्व.रेखीय वापसी[-1](x)
```

### मॉडल में ध्यान का अनुप्रयोग

ट्रांसफार्मर में मल्टी-हेड अटेंशन का उपयोग तीन अलग-अलग तरीकों से किया जाता है:
- एनकोडर-डिकोडर ध्यान परत में, क्वेरी पिछली डिकोडर परत से आती हैं, और कुंजी और मान एनकोडर के आउटपुट से आते हैं। यह डिकोडर में प्रत्येक स्थिति को इनपुट अनुक्रम में सभी स्थितियों पर ध्यान केंद्रित करने की अनुमति देता है। यह एक पैरोडी अनुक्रम हैअनुक्रम मॉडल में विशिष्ट एनकोडर-डिकोडर ध्यान तंत्र की सूची बनाएं, जैसे कि [(उद्धरण)](https://arxiv.org/abs/1609.08144)।


- एनकोडर में एक आत्म-ध्यान परत होती है। स्व-ध्यान परत में, सभी कुंजियाँ, मान और प्रश्न एक ही स्थान से आते हैं, यानी एनकोडर में पिछली परत का आउटपुट। इस मामले में, एनकोडर की प्रत्येक स्थिति एनकोडर की ऊपरी परत की सभी स्थितियों पर ध्यान दे सकती है।


-इसी तरह, डिकोडर में स्व-ध्यान परत डिकोडर में प्रत्येक स्थिति को डिकोडर परत (वर्तमान स्थिति सहित) में वर्तमान स्थिति से पहले सभी स्थितियों पर ध्यान देने की अनुमति देती है। डिकोडर के ऑटोरेग्रेसिव गुणों को बनाए रखने के लिए, डिकोडर में जानकारी को बाईं ओर बहने से रोकना आवश्यक है। हम इसे सॉफ्टमैक्स इनपुट ($-\infty$ पर सेट) में सभी अवैध कनेक्शन मानों को मास्क करके स्केल्ड डॉट उत्पाद ध्यान के अंदर प्राप्त करते हैं।### स्थिति-आधारित फ़ीडफ़ॉरवर्ड नेटवर्क

ध्यान उप-परत के अलावा, हमारे एनकोडर और डिकोडर की प्रत्येक परत में प्रत्येक परत में एक ही स्थिति में एक पूरी तरह से जुड़ा हुआ फ़ीड-फ़ॉरवर्ड नेटवर्क होता है (प्रत्येक एन्कोडर-परत या अंत में डिकोडर-परत दोनों में)। फीडफॉरवर्ड नेटवर्क में दो रैखिक परिवर्तन और दो रैखिक परिवर्तनों के बीच एक ReLU सक्रियण फ़ंक्शन शामिल है।

$$\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2$$

हालाँकि दोनों परतें रैखिक परिवर्तन हैं, वे परत दर परत विभिन्न मापदंडों का उपयोग करती हैं। इसका वर्णन करने का दूसरा तरीका 1 आकार की दो गुठली का कनवल्शन है। इनपुट और आउटपुट आयाम दोनों $d_{\text{model}}=512$ हैं, और आंतरिक आयाम $d_{ff}= है2048$. (अर्थात, पहली परत 512 आयामों को इनपुट करती है और 2048 आयामों को आउटपुट करती है; दूसरी परत 2048 आयामों को इनपुट करती है और 512 आयामों को आउटपुट करती है)


```अजगर
क्लास पोजिशनवाइजफीडफॉरवर्ड(एनएन.मॉड्यूल):
    "एफएफएन समीकरण लागू करता है।"
    def __init__(स्वयं, d_model, d_ff, ड्रॉपआउट=0.1):
        सुपर(स्थितिवार फ़ीडफॉरवर्ड, स्वयं).__init__()
        self.w_1 = nn.रैखिक(d_model, d_ff)
        self.w_2 = nn.रैखिक(d_ff, d_model)
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(ड्रॉपआउट)

    डीईएफ़ फॉरवर्ड(स्वयं, एक्स):
        self.w_2(self.dropout(F.relu(self.w_1(x)))) लौटाएं
```

## एंबेडिंग और सॉफ्टमैक्स

अन्य seq2seq के साथमॉडल समान है। हम इनपुट टोकन और आउटपुट टोकन को $d_{\text{model}}$-आयामी वैक्टर में बदलने के लिए सीखे गए एम्बेडिंग का उपयोग करते हैं। हम डिकोडर आउटपुट को अगले टोकन की अनुमानित संभावना में परिवर्तित करने के लिए एक सामान्य रैखिक परिवर्तन और एक सॉफ्टमैक्स फ़ंक्शन का भी उपयोग करते हैं, हमारे मॉडल में, समान वजन मैट्रिक्स दो एम्बेडिंग परतों और प्री-सॉफ्टमैक्स रैखिक परिवर्तन के बीच साझा किया जाता है। [(उद्धरण)](https://arxiv.org/abs/1608.05859) के समान। एम्बेडिंग परत में, हम इन भारों को $\sqrt{d_{\text{model}}}$ से गुणा करते हैं।                                                                                                                               


```अजगर
क्लास एंबेडिंग्स (एनएन.मॉड्यूल):
    def __init__(self, d_model, vocab):
        सुपर(एंबेडिंग, स्वयं).__init__()self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    डीईएफ़ फॉरवर्ड(स्वयं, एक्स):
        self.lut(x) * गणित.sqrt(self.d_model) वापस करें
```

## स्थिति एन्कोडिंग
&#8195;&#8195;चूंकि हमारे मॉडल में लूप और कनवल्शन शामिल नहीं हैं, इसलिए मॉडल को अनुक्रम के क्रम का लाभ उठाने के लिए, हमें अनुक्रम में टोकन की सापेक्ष या पूर्ण स्थिति के बारे में कुछ जानकारी जोड़नी होगी . ऐसा करने के लिए, हम एनकोडर और डिकोडर स्टैक के नीचे इनपुट एम्बेडिंग में "पोजिशनल एन्कोडिंग" जोड़ते हैं। स्थिति एन्कोडिंग और एम्बेडिंग के आयाम समान हैं, जो कि $d_{\text{model}}$ भी है, इसलिए इन दो वैक्टरों को जोड़ा जा सकता है। चुनने के लिए कई स्थितीय एन्कोडिंग हैं, जैसे सीखी गई स्थितिगत एन्कोडिंग और निश्चित स्थितिगत एन्कोडिंग [(उद्धरण)ई)](https://arxiv.org/pdf/1705.03122.pdf)।

&#8195;&#8195;इस कार्य में हम विभिन्न आवृत्तियों के साइन और कोसाइन फ़ंक्शन का उपयोग करते हैं:$$PE_{(pos,2i)} = पाप(pos / 10000^{2i/d_{\text{model}}})$$

$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})$$
&#8195;&#8195;जहां $pos$ स्थिति है और $i$ आयाम है। अर्थात्, स्थितीय एन्कोडिंग का प्रत्येक आयाम एक साइनसॉइड से मेल खाता हैतार। ये तरंग दैर्ध्य $2\pi$ से $10000 \cdot 2\pi$ तक एक सेट श्रृंखला बनाते हैं। हम इस फ़ंक्शन को चुनते हैं क्योंकि हमारा अनुमान है कि इससे मॉडल के लिए सापेक्ष स्थिति पर ध्यान देना सीखना आसान हो जाएगा, क्योंकि किसी भी निर्धारित ऑफसेट $k$ के लिए, $PE_{pos+k}$ को $PE_{pos के रूप में व्यक्त किया जा सकता है }$ रैखिक फ़ंक्शन।

&#8195;&#8195;इसके अलावा, हम एनकोडर और डिकोडर स्टैक में एम्बेडिंग और पोजिशनल एन्कोडिंग के योग में एक ड्रॉपआउट जोड़ देंगे। मूल मॉडल के लिए, हम जो ड्रॉपआउट अनुपात उपयोग करते हैं वह $P_{drop}=0.1$ है।
                                                                                                                                                                                                                                                    



```अजगर
कक्षास्थितिnalEncoding(nn.मॉड्यूल):
    "पीई फ़ंक्शन लागू करें।"
    def __init__(स्वयं, d_model, ड्रॉपआउट, max_len=5000):
        सुपर(पोजीशनल एन्कोडिंग, स्वयं).__init__()
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(पी=ड्रॉपआउट)
        
        # लॉग स्पेस में एक बार स्थितीय एन्कोडिंग की गणना करें।
        पीई = टॉर्च.शून्य (मैक्स_लेन, डी_मॉडल)
        स्थिति = टॉर्च.अरेंज(0, max_len).अनस्क्वीज़(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) *
                             -(गणित.लॉग(10000.0) / d_model))पीई[:, 0::2] = मशाल.पाप(स्थिति * div_term)
        पीई[:, 1::2] = मशाल.कोस(स्थिति * div_term)
        पे = पे.अनस्क्वीज़(0)
        self.register_buffer('पे', पे)
        
    डीईएफ़ फॉरवर्ड(स्वयं, एक्स):
        x = x + वेरिएबल(self.pe[:, :x.size(1)],
                         आवश्यकता_ग्रेड=गलत)
        स्व.ड्रॉपआउट(x) वापस करें
```

> जैसा कि नीचे दिखाया गया है, स्थिति एन्कोडिंग स्थिति के अनुसार साइन तरंगें जोड़ेगी। प्रत्येक आयाम के लिए तरंगों की आवृत्ति और ऑफसेट अलग-अलग होते हैं।


```अजगर
plt.figure(figsize=(15,5))
पीई = पोजिशनल एन्कोडिंग(20, 0)
y = pe.forward(चर(मशाल.शून्य(1, 100, 20)))
plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())
plt.legend(["dim %d"%p for p in [4,5,6,7]])
कोई नहीं
```


    
![svg](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_48_0.svg)
    


हमने निश्चित स्थितिगत एन्कोडिंग के बजाय सीखा स्थितिगत एम्बेडिंग [(उद्धरण)] (https://arxiv.org/pdf/1705.03122.pdf) का उपयोग करने का भी प्रयास किया, और पाया कि दोनों विधियों ने लगभग समान प्रभाव उत्पन्न किया। इसलिए हमने साइनसॉइडल संस्करण को चुना क्योंकि यह मॉडल को प्रशिक्षण के दौरान सामने आए अनुक्रमों की तुलना में लंबे अनुक्रमों में एक्सट्रपलेशन करने की अनुमति दे सकता है।

## पूरा मॉडल

> यहां हम हाइपरपैरामीटर से लेकर संपूर्ण मॉडल तक एक फ़ंक्शन को परिभाषित करते हैं।


```अजगर
def मेक_मॉडल(src_vocab, tgt_vocab, N=6,
               d_model=512, d_ff=2048,h=8, ड्रॉपआउट=0.1):
    "सहायक: हाइपरपैरामीटर से एक मॉडल बनाएं।"
    सी = कॉपी.डीपकॉपी
    ध्यान दें = मल्टीहेडेडअटेंशन(एच, डी_मॉडल)
    एफएफ = पोजिशनवाइजफीडफॉरवर्ड (डी_मॉडल, डी_एफएफ, ड्रॉपआउट)
    स्थिति = पोजिशनल एन्कोडिंग(d_model, ड्रॉपआउट)
    मॉडल = एनकोडरडिकोडर(
        एनकोडर(एनकोडरलेयर(d_model, c(attn), c(ff), ड्रॉपआउट), N),
        डिकोडर(डिकोडरलेयर(d_model, c(attn), c(attn),
                             सी(एफएफ), ड्रॉपआउट), एन),
        nn.Sequential(एंबेडिंग(d_model, src_voकैब), सी(स्थिति)),
        nn.अनुक्रमिक(एम्बेडिंग्स(d_model, tgt_vocab), c(स्थिति)),
        जेनरेटर(d_model, tgt_vocab))
    
    # ये उनके कोड से अहम था.
    # ग्लोरोट / फैन_एवीजी के साथ पैरामीटर आरंभ करें।
    मॉडल.पैरामीटर() में पी के लिए:
        यदि p.dim() > 1:
            nn.init.xavier_uniform(p)
    वापसी मॉडल
```


```अजगर
# छोटा उदाहरण मॉडल.
tmp_model = make_model(10, 10, 2)
कोई नहीं
```

    /var/फ़ोल्डर/2k/x3py0v857kgcwqvvl00xxhxw0000gn/T/ipykernel_27532/2289673833.py:20: उपयोगकर्ता चेतावनी: nn.init.xavier_uniform को अब nn.init.xavier_uniform_ के पक्ष में हटा दिया गया है।
      nn.init.xavier_uniform(p)


# रेलगाड़ी

यह अनुभाग हमारे मॉडल के प्रशिक्षण तंत्र का वर्णन करता है।

> यहां हम मानक एनकोडर-डिकोडर मॉडल के प्रशिक्षण के लिए कुछ उपकरण शीघ्रता से प्रस्तुत करते हैं। सबसे पहले, हम एक बैच ऑब्जेक्ट को परिभाषित करते हैं जिसमें प्रशिक्षण के लिए स्रोत और लक्ष्य वाक्य, साथ ही निर्माण मास्क भी शामिल है।

## बैच प्रोसेसिंग और मास्किंग


```अजगर
क्लास बैच:
    "प्रशिक्षण के दौरान मास्क के साथ डेटा का एक बैच रखने पर आपत्ति।"
    def __init__(स्वयं, src, trg=कोई नहीं, पैड=0):
        self.src = src
        self.src_mask = (src != पैड).अनस्क्वीज़(-2)
        यदि trg कोई नहीं है:self.trg = trg[:, :-1]
            self.trg_y = trg[:, 1:]
            स्वयं.trg_mask = \
                self.make_std_mask(self.trg, पैड)
            self.ntokens = (self.trg_y != पैड).data.sum()
    
    @staticmethod
    def make_std_mask(tgt, पैड):
        "पैडिंग और भविष्य के शब्दों को छिपाने के लिए एक मुखौटा बनाएं।"
        tgt_mask = (tgt != पैड).अनस्क्वीज़(-2)
        tgt_mask = tgt_mask और वेरिएबल(
            अनुवर्ती_मास्क(tgt.आकार(-1)).type_as(tgt_mask.data))
        वापसीtgt_mask
```> इसके बाद हम नुकसान को ट्रैक करने के लिए एक सामान्य प्रशिक्षण और मूल्यांकन फ़ंक्शन बनाते हैं। हम एक सामान्य हानि फ़ंक्शन पास करते हैं और पैरामीटर को अपडेट करने के लिए इसका उपयोग भी करते हैं।

## ट्रेनिंग लूप


```अजगर
def run_epoch(data_iter, मॉडल, loss_compute):
    "मानक प्रशिक्षण और लॉगिंग फ़ंक्शन"
    प्रारंभ = समय.समय()
    कुल_टोकन = 0
    कुल हानि = 0
    टोकन = 0
    i के लिए, enumerate(data_iter) में बैच:
        आउट = मॉडल.फॉरवर्ड(बैच.src, बैच.trg,
                            बैच.src_mask, बैच.trg_mask)
        हानि = हानि_गणना(बाहर, बैच.trg_y, बैच.एनटोकेंस)
        कुल_हानि += हानिtotal_tokens += बैच.ntokens
        टोकन += बैच.एनटोकेंस
        यदि मैं % 50 == 1:
            बीता हुआ = समय.समय()-प्रारंभ
            प्रिंट करें ("युग चरण: %d हानि: %f टोकन प्रति सेकंड: %f" %
                    (i, हानि / बैच.एनटोकेंस, टोकन / बीता हुआ))
            प्रारंभ = समय.समय()
            टोकन = 0
    कुल_नुकसान/कुल_टोकन लौटाएं
```

## प्रशिक्षण डेटा और बैच प्रोसेसिंग
&#8195;&#8195;हमने लगभग 4.5 मिलियन वाक्य जोड़े वाले मानक WMT 2014 अंग्रेजी-जर्मन डेटासेट पर प्रशिक्षण लिया। वाक्यों को बाइट जोड़ी एन्कोडिंग का उपयोग करके एन्कोड किया गया है, और स्रोत और लक्ष्य वाक्य लगभग 37,000 टोकन की शब्दावली साझा करते हैं। अंग्रेजी-फ़्रेंच अनुवाद के लिए हमने काफी बड़े WMT 2014 अंग्रेजी का उपयोग किया- फ़्रेंच डेटा सेट, जिसमें 36 मिलियन वाक्य और टोकन शामिल हैं, 32,000 शब्द-टुकड़े शब्दावली सूचियों में विभाजित हैं। <br>
प्रत्येक प्रशिक्षण बैच में वाक्य जोड़े का एक सेट होता है, जो समान अनुक्रम लंबाई के अनुसार बैच किया जाता है। वाक्य जोड़े के प्रत्येक प्रशिक्षण बैच में स्रोत भाषा के लगभग 25,000 टोकन और लक्ष्य भाषा के 25,000 टोकन होते हैं।

> हम बैच प्रोसेसिंग के लिए टॉर्च टेक्स्ट का उपयोग करेंगे (बाद में अधिक विस्तार से चर्चा की जाएगी)। यहां हम टॉर्चटेक्स्ट फ़ंक्शन में बैच बनाते हैं ताकि यह सुनिश्चित किया जा सके कि जिस बैच का आकार हम अधिकतम भरते हैं वह सीमा (25000 यदि हमारे पास 8 जीपीयू है) से अधिक न हो।


```अजगर
वैश्विक max_src_in_batch, max_tgt_in_batch
डीईएफ़ बैच_आकार_एफएन(नया, गिनती, अब तक):
    "बैच बढ़ाते रहें और टोकन + पैडिंग की कुल संख्या की गणना करें।"
    वैश्विक max_src_in_batch, max_tgt_in_batch
    यदि गिनती == 1:
        max_src_in_baटीच = 0
        max_tgt_in_batch = 0
    max_src_in_batch = max(max_src_in_batch, len(new.src))
    max_tgt_in_batch = max(max_tgt_in_batch, len(new.trg) + 2)
    src_elements = गिनती * max_src_in_batch
    tgt_elements = गिनती * max_tgt_in_batch
    अधिकतम वापसी(src_elements, tgt_elements)
```

## हार्डवेयर और प्रशिक्षण का समय
हम अपने मॉडल को 8 NVIDIA P100 GPU से सुसज्जित मशीन पर प्रशिक्षित करते हैं। पेपर में वर्णित हाइपरपैरामीटर के साथ बेस मॉडल का उपयोग करते हुए, प्रत्येक प्रशिक्षण चरण में लगभग 0.4 सेकंड लगते हैं। हमने बेस मॉडल को कुल 100,000 कदम या 12 घंटे तक प्रशिक्षित किया। बड़े मॉडलों के लिए, प्रत्येक चरण के लिए प्रशिक्षण का समय 1.0 सेकंड है, और बड़े मॉडलों को 300,000 चरणों (3.5 दिन) के लिए प्रशिक्षित किया जाता है।

## अनुकूलक

हमएडम ऑप्टिमाइज़र [(उद्धरण)](https://arxiv.org/abs/1412.6980) का उपयोग करें, जहां $\beta_1=0.9$, $\beta_2=0.98$ और $\epsilon=10^{-9}$। हम प्रशिक्षण के दौरान सीखने की दर को निम्नलिखित सूत्र के अनुसार बदलते हैं:
$$
lrate = d_{\text{model}}^{-0.5} \cdot\min({step\_num}^{-0.5},{step\_num} \cdot {warmup\_steps}^{-1.5})                                                                                                                                                                                                                                                                               
$$यह पहले $warmup\_steps$ चरणों में सीखने की दर को रैखिक रूप से बढ़ाने और बाद में चरणों की संख्या के वर्गमूल के अनुपात में इसे कम करने के अनुरूप है। हम $warmup\_steps=4000$ का उपयोग करते हैं।                            

> नोट: यह भाग बहुत महत्वपूर्ण है। प्रशिक्षण के लिए यह मॉडल सेटअप आवश्यक है.


```अजगर

क्लासनोमऑप्ट:
    "ऑप्टिम रैपर जो दर लागू करता है।"
    def __init__(स्वयं, मॉडल_आकार, कारक, वार्मअप, अनुकूलक):
        self.optimizer = अनुकूलक
        स्व._चरण = 0
        सेल्फ.वार्मअप = वार्मअप
        स्व.कारक = कारक
        एसईएलएफ.मॉडल_आकार = मॉडल_आकार
        स्व._दर = 0
        
    डीईएफ़ चरण(स्वयं):
        "पैरामीटर और दर अपडेट करें"
        स्व._चरण += 1
        दर = स्व.दर()
        self.optimizer.param_groups में p के लिए:
            पी['एलआर'] = दर
        स्व._दर = दर
        self.optimizer.step()
        
    डीईएफ़ दर (स्वयं, चरण = कोई नहीं):
        "ऊपर `lrate` लागू करें"
        यदि चरण कोई नहीं है:
            चरण = स्व._चरण
        स्व.कारक लौटाएँ * \
            (स्वयं.मॉडल_आकार ** (-0.5) *न्यूनतम(चरण ** (-0.5), चरण * स्व.वार्मअप ** (-1.5)))
        
def get_std_opt(मॉडल):
    वापसी NoamOpt(model.src_embed[0].d_model, 2, 4000,
            Torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))
```


> विभिन्न मॉडल आकारों और अनुकूलित हाइपरपैरामीटरों के लिए इस मॉडल के वक्रों के उदाहरण नीचे दिए गए हैं।


```अजगर
# Lrate हाइपरपैरामीटर की तीन सेटिंग्स।
opts = [NoamOpt(512, 1, 4000, कोई नहीं),
        NoamOpt(512, 1, 8000, कोई नहीं),
        NoamOpt(256, 1,4000, कोई नहीं)]
plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] रेंज में i के लिए (1, 20000)])
plt.legend(["512:4000", "512:8000", "256:4000"])
कोई नहीं
```


    
![svg](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_68_0.एसवीजी)
    


## नियमितीकरण
### लेबल स्मूथिंग

प्रशिक्षण प्रक्रिया के दौरान, हम जिस लेबल स्मूथिंग मान का उपयोग करते हैं वह $\epsilon_{ls}=0.1$ [(उद्धरण)](https://arxiv.org/abs/1512.00567) है। हालाँकि लेबल को चिकना करने से मॉडल भ्रमित हो जाएगा, यह सटीकता और BLEU स्कोर में सुधार करता है।

> हम लेबल स्मूथिंग प्राप्त करने के लिए केएल डिव लॉस का उपयोग करते हैं। एक-हॉट वितरण का उपयोग करने के बजाय, हम एक ऐसा वितरण बनाते हैं जो लक्ष्य वितरण को 1-सुचारू करने के लिए सेट करता है और शेष संभावनाओं को शब्दावली में अन्य शब्दों को निर्दिष्ट करता है।


```अजगर
क्लास लेबलस्मूथिंग(एनएन.मॉड्यूल):
    "प्रयोगशाला लागू करेंएल चौरसाई।"
    def __init__(स्वयं, आकार, पैडिंग_idx, स्मूथिंग=0.0):
        सुपर(लेबलस्मूथिंग, सेल्फ).__init__()
        स्व.मानदंड = nn.KLDivLoss(size_average=गलत)
        self.padding_idx = पैडिंग_idx
        आत्मविश्वास = 1.0 - सहजता
        स्व.स्मूथिंग = स्मूथिंग
        स्व.आकार = आकार
        self.true_dist = कोई नहीं
        
    डीईएफ़ फॉरवर्ड (स्वयं, एक्स, लक्ष्य):
        जोर देकर कहें x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothआईएनजी / (स्वयं आकार - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), आत्मविश्वास)
        true_dist[:, self.padding_idx] = 0
        मास्क = टॉर्च.नॉनजीरो(लक्ष्य.डेटा == सेल्फ.पैडिंग_आईडीएक्स)
        यदि मास्क.डिम() > 0:
            true_dist.index_fill_(0, मास्क.स्क्वीज़(), 0.0)
        self.true_dist = true_dist
        स्व.मानदंड लौटाएं(x, वेरिएबल(true_dist, require_grad=गलत))
```

आइए सुचारू वास्तविक संभाव्यता वितरण को देखने के लिए नीचे एक उदाहरण देखें।


```अजगर
#लेबल स्मूथिंग का उदाहरण.
क्रिट = लेबलस्मूथिंग(5, 0, 0.4)
पीरिडिक्ट = टॉर्च.फ्लोटटेन्सर([[0, 0.2, 0.7, 0.1, 0],
                             [0, 0.2, 0.7, 0.1, 0],
                             [0, 0.2, 0.7, 0.1, 0]])
v = क्रिट(वैरिएबल(भविष्यवाणी.लॉग()),
         वेरिएबल(मशाल.लॉन्गटेन्सर([2,1,0])))

# सिस्टम द्वारा अपेक्षित लक्ष्य वितरण दिखाएं।
plt.imshow(crit.true_dist)
कोई नहीं
```

    /Users/niepig/Desktop/zhihu/learn-nlp-with-transformers/venv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: उपयोगकर्ता चेतावनी: size_average और कम args होंगेबहिष्कृत किया जाए, कृपया इसके बजाय कमी = 'योग' का उपयोग करें।
      चेतावनियाँ.चेतावनी(warning.format(ret))



    
![svg](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_73_1.svg)
    



```अजगर
प्रिंट(crit.true_dist)
```

    टेंसर([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],
            [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],
            [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])


लेबल स्मूथिंग के अस्तित्व के कारण, यदि मॉडल किसी निश्चित शब्द के बारे में विशेष रूप से आश्वस्त है और विशेष रूप से उच्च संभावना का आउटपुट देता है, तो उसे दंडित किया जाएगा। जैसा कि निम्नलिखित कोड में दिखाया गया है, जैसे-जैसे इनपुट x बढ़ता है, x/d बड़ा और बड़ा होता जाएगा, और 1/d छोटा और छोटा होता जाएगा, लेकिन नुकसानयह हमेशा कम नहीं होता.


```अजगर
क्रिट = लेबलस्मूथिंग(5, 0, 0.1)
डीईएफ़ हानि(एक्स):
    d=x+3*1
    भविष्यवाणी = मशाल.फ्लोटटेन्सर([[0, एक्स/डी, 1/डी, 1/डी, 1/डी],
                                 ])
    #प्रिंट(भविष्यवाणी)
    रिटर्न क्रिट (वेरिएबल (भविष्यवाणी.लॉग()),
                 वेरिएबल(मशाल.लॉन्गटेन्सर([1]))).आइटम()

y = [सीमा(1, 100) में x के लिए हानि(x)]
x = np.arange(1, 100)
plt.प्लॉट(x, y)

```




    [<matplotlib.lines.Line2D at 0x7f7fad46c970>]




    
![svg](2.2.1-पाइटोरच%E7%BC%96%E5%86%99ट्रांसफॉर्मर_files/2.2.1-Pytorch%E7%BC%96%E5%86%99Transformer_76_1.svg)
    


# उदाहरण

> हम एक सरल प्रतिलिपि कार्य आज़माकर शुरुआत कर सकते हैं। एक छोटी शब्दावली से यादृच्छिक इनपुट प्रतीकों के एक सेट को देखते हुए, लक्ष्य इन्हीं प्रतीकों को उत्पन्न करना है।

## सिंथेटिक डेटा


```अजगर
डीईएफ़ डेटा_जेन (वी, बैच, एनबैच):
    "src-tgt प्रतिलिपि कार्य के लिए यादृच्छिक डेटा उत्पन्न करें।"
    रेंज में i के लिए (nbatches):
        डेटा = torch.from_numpy(np.random.randint(1, V, आकार=(बैच, 10)))
        डेटा[:, 0] = 1
        src = वेरिएबल (डेटा, require_grad=गलत)
        tgt = परिवर्तनीय(डेटा, require_grad=गलत)
        यील्डबैच(src, tgt, 0)
```

## हानि फ़ंक्शन गणना


```अजगर
क्लास SimpleLossCompute:
    "एक साधारण हानि गणना और ट्रेन फ़ंक्शन।"
    def __init__(स्वयं, जनरेटर, मानदंड, ऑप्ट=कोई नहीं):
        स्व.जनरेटर = जनरेटर
        स्व.मानदंड = कसौटी
        self.opt = opt
        
    def __call__(स्वयं, x, y, मानदंड):
        x = स्व.जनरेटर(x)
        हानि = स्व.मानदंड(x.सन्निहित().देखें(-1, x.आकार(-1)),
                              y.contiguous().view(-1)) / मानदंड
        हानि.पिछड़ा()
        यदि self.opt हैकोई नहीं:
            self.opt.step()
            self.opt.optimizer.zero_grad()
        वापसी हानि.आइटम() * मानदंड
```

## लालची डिकोडिंग


```अजगर
# सरल प्रतिलिपि कार्य को प्रशिक्षित करें।
वी=11
मानदंड = लेबल स्मूथिंग (आकार = वी, पैडिंग_आईडीएक्स = 0, स्मूथिंग = 0.0)
मॉडल = मेक_मॉडल(वी, वी, एन=2)
model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,
        Torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))

श्रेणी में युग के लिए (10):
    मॉडल.ट्रेन()
    run_epoch(data_gen(V, 30, 20), मॉडल,SimpleLossCompute(model.generator, मानदंड, model_opt))
    मॉडल.eval()
    प्रिंट(run_epoch(data_gen(V, 30, 5), मॉडल,
                    SimpleLossCompute(मॉडल.जनरेटर, मानदंड, कोई नहीं)))
```

> सरलता के लिए, यह कोड अनुवाद की भविष्यवाणी करने के लिए लालची डिकोडिंग का उपयोग करता है।


```अजगर
डीईएफ़ लालची_डीकोड(मॉडल, स्रोत, स्रोत_मास्क, मैक्स_लेन, प्रारंभ_प्रतीक):
    मेमोरी = मॉडल.एनकोड(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)
    रेंज में i के लिए (max_len-1):
        आउट = मॉडल.डीकोड(मेमोरी, src_mask,चर(ys),
                           वेरिएबल(बाद में_मास्क(ys.size(1))
                                    .type_as(src.data)))
        प्रोब = मॉडल.जेनरेटर(बाहर[:, -1])
        _, अगला_शब्द = टॉर्च.मैक्स(संभावना, मंद = 1)
        अगला_शब्द = अगला_शब्द.डेटा[0]
        ys = मशाल.कैट([ys,
                        Torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
    वापसी ys

मॉडल.eval()
src = वेरिएबल(मशाल.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )
src_mask = वीएरिएबल(मशाल.ओन्स(1,1,10) )
प्रिंट(greedy_decode(मॉडल, src, src_mask, max_len=10, प्रारंभ_प्रतीक=1))
```

    टेंसर([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])


#वास्तविक दृश्य उदाहरण
चूंकि मूल ज्यूपिटर के वास्तविक डेटा परिदृश्य के लिए मल्टी-जीपीयू प्रशिक्षण की आवश्यकता होती है, इसलिए इस ट्यूटोरियल में फिलहाल इसे शामिल नहीं किया जाएगा। इच्छुक पाठक [मूल ट्यूटोरियल](https://nlp.seas.harvard.edu/2018/) पढ़ना जारी रख सकते हैं। 04/03/ध्यान.html). इसके अलावा, क्योंकि वास्तविक डेटा का मूल यूआरएल अमान्य है, मूल ट्यूटोरियल वास्तविक डेटा परिदृश्य के कोड को चलाने में सक्षम नहीं होना चाहिए।

# निष्कर्ष

अब तक, हमने एक संपूर्ण ट्रांसफॉर्मर लाइन को लाइन दर लाइन लागू किया है और सिंथेटिक डेटा का उपयोग करके इसका प्रशिक्षण और भविष्यवाणी की है। मुझे उम्मीद है कि यह ट्यूटोरियल आपके लिए उपयोगी होगा।

# आभार
इस लेख का अनुवाद झांग होंगक्सू द्वारा किया गया था और डुओदुओ द्वारा संकलित किया गया था। मूल ज्यूपिटर हार्वर्ड एनएलपी [एनोटेटेड ट्रांसफार्मर] (https://nlp.seas.harvard.edu/2018/0) से आया है।4/03/ध्यान.html).

<div id='diskus_thread'></div>
<स्क्रिप्ट>
    /**
     * अनुशंसित कॉन्फ़िगरेशन चर: अपने प्लेटफ़ॉर्म या सीएमएस से गतिशील मान सम्मिलित करने के लिए नीचे दिए गए अनुभाग को संपादित करें और टिप्पणी हटाएँ।
     * जानें कि इन वेरिएबल्स को परिभाषित करना क्यों महत्वपूर्ण है: https://diskus.com/admin/universalcode/#configuration-variables
     */
    /*
    var डिस्कस_कॉन्फिग = फ़ंक्शन () {
        this.page.url = PAGE_URL; // PAGE_URL को अपने पेज के कैनोनिकल URL वेरिएबल से बदलें
        यह.पेज.पहचानer = PAGE_IDENTIFIER; // PAGE_IDENTIFIER को अपने पृष्ठ के विशिष्ट पहचानकर्ता वेरिएबल से बदलें
    };
    */
    (फ़ंक्शन() {// आवश्यक कॉन्फ़िगरेशन वैरिएबल: नीचे दिए गए संक्षिप्त नाम को संपादित करें
        var d = दस्तावेज़, s = d.createElement ('स्क्रिप्ट');
        
        s.src = 'https://EXAMPLE.disqus.com/embed.js'; // महत्वपूर्ण: EXAMPLE को अपने फ़ोरम संक्षिप्त नाम से बदलें!
        
        s.setAttribute('डेटा-टाइमस्टैम्प', +नई तिथि());
        (d.head || d.body).appendChild(s);
    })();
</स्क्रिप्ट>
<नोस्क्रीpt>डिस्कस द्वारा संचालित <a href='https://diskus.com/?ref_noscript' rel='nofollow'>टिप्पणियों को देखने के लिए कृपया जावास्क्रिप्ट सक्षम करें।</a></noscript>