## अध्याय प्रश्नोत्तरी
* प्रश्न 1: ट्रांसफार्मर में सॉफ्टमैक्स गणना को $d_k$ से विभाजित करने की आवश्यकता क्यों है?
* प्रश्न 2: ट्रांसफार्मर में ध्यान स्कोर की गणना करते समय पैडिंग स्थिति को कैसे छुपाया जाए?
* प्रश्न 3: ट्रांसफॉर्मर में पोजिशनल एंबेडिंग क्यों जोड़ी जाती है?
* प्रश्न 4: क्या बीईआरटी पूर्व-प्रशिक्षण के दौरान मास्क अनुपात बड़ा हो सकता है?
* प्रश्न 5: बीईआरटी टोकन संचालन कैसे करता है? क्या फायदा है?
* प्रश्न 6: जीपीटी को टोकनाइज़ कैसे करें? BERT से क्या अंतर है?
* प्रश्न 7: बीईआरटी मॉडल को कैसे प्रशिक्षित किया जाए जब यह बहुत बड़ा है और इसे केवल एक जीपीयू पर 1 बैच में प्रशिक्षित किया जा सकता है?
* प्रश्न 8: ट्रांसफार्मर को पोजीशन एंबेडिंग की आवश्यकता क्यों है?
*प्रश्न 9: ट्रांसफार्मर में अवशिष्ट नेटवर्क संरचना की क्या भूमिका है?
* प्रश्न 10: क्या बीईआरटी प्रशिक्षण के दौरान मुखौटा शब्दों का अनुपात विशेष रूप से बड़ा (80% से अधिक) हो सकता है?
* प्रश्न 11: BERT पूर्व-प्रशिक्षण कैसे मास्किंग करता है?
*प्रश्न 11: Word2vec से BERT में क्या सुधार किये गये हैं?