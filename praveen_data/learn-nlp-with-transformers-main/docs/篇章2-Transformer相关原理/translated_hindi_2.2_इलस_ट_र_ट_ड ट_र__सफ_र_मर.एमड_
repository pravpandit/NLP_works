# ग्राफिकल ट्रांसफार्मर

सामग्री संगठन:
- सचित्र ट्रांसफार्मर
    - ट्रांसफार्मर मैक्रो संरचना
    - ट्रांसफार्मर संरचना विवरण
      - इनपुट प्रोसेसिंग
        - शब्द सदिश
        - स्थिति वेक्टर
      - एनकोडर एनकोडर
        -स्व-ध्यान परत
        - मल्टी-हेड ध्यान तंत्र
        - ध्यान कोड उदाहरण
        - अवशिष्ट कनेक्शन
      - डिकोडर
      - रैखिक परतें और सॉफ्टमैक्स
      - लॉस फंकशन
    - अतिरिक्त जानकारी
    - आभार


[इलस्ट्रेटेड अटेंशन](./2.1-इलस्ट्रेटेड अटेंशन.एमडी) का अध्ययन करने के बाद, हम उन लाभों को जानते हैं जो ध्यान आवर्ती तंत्रिका नेटवर्क में लाता है। तो क्या कोई तंत्रिका नेटवर्क संरचना है जो सीधे ध्यान पर आधारित है और अब आरएनएन, एलएसटीएम या सीएनएन नेटवर्क संरचना पर निर्भर नहीं है? उत्तर है: ट्रांसफार्मर. इसलिए, हम इस अनुभाग में ट्रांसफार्मर में शामिल विवरणों पर चर्चा करेंगे।

ट्रांसफार्मर मॉडल 2017 में Google द्वारा प्रस्तावित किया गया था, सीधे स्व-ध्यान संरचना पर आधारित, पिछले एनएलपी कार्यों में आमतौर पर उपयोग की जाने वाली आरएनएन तंत्रिका नेटवर्क संरचना को प्रतिस्थापित किया, और WMT2014 अंग्रेजी-जर्मन और WMT2014 अंग्रेजी-से-फ्रेंच मशीन अनुवाद कार्यों दोनों में तत्कालीन SOTA हासिल किया।

आरएनएन जैसे तंत्रिका नेटवर्क संरचनाओं की तुलना में, ट्रांसफार्मर का एक बड़ा लाभ यह है कि जब ** मॉडल अनुक्रम इनपुट को संसाधित करता है, तो यह पूरे अनुक्रम इनपुट पर समानांतर गणना कर सकता है, और समय चरणों के अनुसार इनपुट अनुक्रम को पुनरावर्ती रूप से संसाधित करने की आवश्यकता नहीं होती है . **अध्याय 2.1 विस्तार से बताता है कि आरएनएन तंत्रिका नेटवर्क इनपुट अनुक्रम को चक्रीय और पुनरावर्ती रूप से कैसे संसाधित करता है। पाठकों का इसकी समीक्षा करने के लिए स्वागत है।

नीचे दिया गया चित्र ट्रांसफार्मर का समग्र संरचना आरेख है। यह अध्याय 2.1 में प्रस्तुत seq2seq मॉडल के समान है। ट्रांसफार्मर मॉडल संरचना का बायां आधा हिस्सा एनकोडर है और दायां आधा हिस्सा डिकोडर है। आइए चरण दर चरण इसे अलग करें ट्रांसफार्मर.

![ट्रांसफार्मर](./pictures/2-transformer.png)
चित्र: ट्रांसफार्मर मॉडल संरचना

नोट्स और उद्धरण: यह आलेख कुल बिंदुओं के माध्यम से ट्रांसफार्मर को विघटित और समझाएगा, जिससे शुरुआती लोगों को ट्रांसफार्मर मॉडल संरचना को समझने में मदद मिलेगी।यह लेख मुख्य रूप से [इलस्ट्रेटेड-ट्रांसफॉर्मर](http://jalammar.github.io/ Illustrator-transformer) को संदर्भित करता है।


## ट्रांसफार्मर मैक्रो संरचना

नोट: इस खंड में स्पष्टीकरण विधि है: कुल-बिंदु, पहले संपूर्ण, फिर भाग।

ट्रांसफार्मर को सबसे पहले मशीनी अनुवाद कार्यों को हल करने के लिए प्रस्तावित किया गया था, इसलिए इसे एक प्रकार का seq2seq मॉडल माना जा सकता है। यह खंड पहले ट्रांसफार्मर मॉडल के विशिष्ट संरचनात्मक विवरणों को अलग रखता है, और पहले seq2seq के परिप्रेक्ष्य से ट्रांसफार्मर की मैक्रोस्ट्रक्चर का अध्ययन करता है। मशीन अनुवाद कार्य को एक उदाहरण के रूप में लेते हुए, पहले ट्रांसफॉर्मर के विशेष seqseq मॉडल को ब्लैक बॉक्स के रूप में मानें, ब्लैक बॉक्स का इनपुट एक फ्रेंच टेक्स्ट अनुक्रम है और आउटपुट एक अंग्रेजी टेक्स्ट अनुक्रम है (अध्याय 2.1 में seq2seq फ्रेमवर्क ज्ञान की तुलना करें)। , हम पा सकते हैं कि ट्रांसफार्मर मैक्रो संरचना seq2seq श्रेणी से संबंधित है, लेकिन पिछले seq2seq में एनकोडर और डिकोडर को RNN मॉडल से ट्रांसफार्मर मॉडल में बदल दिया गया है)।

![इनपुट-आउटपुट](./चित्र/2-इनपुट-आउटपुट.पीएनजी)
चित्र: ट्रांसफार्मर ब्लैक बॉक्स इनपुट और आउटपुट

चित्र के मध्य भाग को "परिवर्तन" के ऊपर रखेंER" को seq2seq मानक संरचना में विघटित किया गया है, और निम्नलिखित चित्र प्राप्त किया गया है: बाईं ओर एनकोडर का एन्कोडिंग भाग है, और दाईं ओर डिकोडर का डिकोडर भाग है।
![एनकोडर-डिकोडर](./चित्र/2-एनकोडर-डिकोडर.png)
चित्र: एनकोडर-डिकोडर

इसके बाद, निम्नलिखित चित्र प्राप्त करने के लिए उपरोक्त चित्र में एनकोडर और डिकोडर का विवरण बनाएं। हम देख सकते हैं कि एन्कोडिंग भाग (एनकोडर) में मल्टी-लेयर एनकोडर होते हैं (ट्रांसफॉर्मर पेपर 6-लेयर एनकोडर का उपयोग करता है। यहां परतों की संख्या निश्चित नहीं है। आप प्रयोगात्मक परिणामों के अनुसार परतों को संशोधित भी कर सकते हैं। संख्या) . उसी तरह, डिकोडिंग भाग (डिकोडर) भी मल्टी-लेयर डिकोडर (डिकोडर) से बना होता है (पेपर में 6-लेयर डिकोडर का भी उपयोग किया जाता है)। प्रत्येक परत की एनकोडर नेटवर्क संरचना समान है, और प्रत्येक परत की डिकोडर नेटवर्क संरचना भी समान है। एनकोडर और डिकोडर नेटवर्क संरचनाओं की विभिन्न परतें पैरामीटर साझा नहीं करती हैं।
![अनुवाद उदाहरण](./pictures/2-2-encoder-detail.png)

चित्र: 6-लेयर एनकोडर और 6-लेयर डिकोडर

आगे, आइए सिंगल-लेयर एनकोडर पर एक नज़र डालें। सिंगल-लेयर एनकोडर में मुख्य रूप से निम्नलिखित दो भाग होते हैं, जैसा कि नीचे दिए गए चित्र में दिखाया गया है
-स्व-ध्यान परत
- फ़ीड फॉरवर्ड न्यूरल नेtwork (फीडफॉरवर्ड न्यूरल नेटवर्क, संक्षिप्त रूप में FFNN)

एनकोडर के इनपुट टेक्स्ट अनुक्रम $w_1, w_2,...,w_n$ को शुरू में प्रत्येक शब्द $x_1, x_2,...,x_n$ का वेक्टर प्रतिनिधित्व प्राप्त करने के लिए एम्बेडिंग रूपांतरण से गुजरना पड़ता है, जहां $x_i \in \ Mathbb {R}^{d}$ आयाम $d$ वाला एक वेक्टर है, और फिर सभी वैक्टर $h_1, h_2,...h_n$ प्राप्त करने के लिए सेल्फ-अटेंशन न्यूरल नेटवर्क परत के माध्यम से परिवर्तन और सूचना इंटरैक्शन से गुजरते हैं, जहां $ h_i \in \mathbb{R}^{d}$ आयाम $d$ वाला एक वेक्टर है। जब आत्म-ध्यान परत किसी शब्द वेक्टर को संसाधित करती है, तो यह न केवल शब्द की जानकारी का उपयोग करेगी, बल्कि वाक्य में अन्य शब्दों की जानकारी का भी उपयोग करेगी (आप इसे इसके अनुरूप कर सकते हैं: जब हम किसी शब्द का अनुवाद करते हैं, तो हम न केवल) वर्तमान शब्द पर ध्यान केंद्रित करें, यह इस शब्द के संदर्भ में अन्य शब्दों की जानकारी पर भी ध्यान देगा)। सेल्फ-अटेंशन लेयर का आउटपुट नए $x_1, x_2,...,x_n$ प्राप्त करने के लिए फीडफॉरवर्ड न्यूरल नेटवर्क के माध्यम से जाएगा, जो अभी भी $d$ आयाम वाले $n$ वेक्टर हैं। उसी ऑपरेशन को जारी रखने के लिए इन वैक्टरों को एनकोडर की अगली परत पर फीड किया जाएगा।

![एनकोडर](./pictures/2-encoder.png)

चित्र: सिंगल लेयर एनकोडर

एनकोडर के अनुरूप, जैसा कि नीचे दिखाया गया है, डिकोडर एन्कोडिंग कर रहा हैडिकोडर के स्व-ध्यान और एफएफएनएन के बीच एक एनकोडर-डिकोडर ध्यान परत डाली जाती है। यह परत डिकोडर को इनपुट अनुक्रम के सबसे प्रासंगिक भाग पर ध्यान केंद्रित करने में मदद करती है (seq2seq मॉडल में ध्यान के समान)।

![डिकोडर](./pictures/2-decoder.webp)

चित्र: सिंगल लेयर डिकोडर

संक्षेप में, हम मूल रूप से समझते हैं कि ट्रांसफार्मर में एक एन्कोडिंग भाग और एक डिकोडिंग भाग होता है, और एन्कोडिंग भाग और डिकोडिंग भाग एक ही नेटवर्क संरचना के साथ कई एन्कोडिंग परतों और डिकोडिंग परतों से बने होते हैं। प्रत्येक एन्कोडिंग परत में आत्म-ध्यान और एफएफएनएन होता है, और प्रत्येक डिकोडिंग परत में आत्म-ध्यान, एफएफएन और एनकोडर-डिकोडर ध्यान होता है।

ऊपर ट्रांसफार्मर की मैक्रो संरचना है। अब हम मैक्रो संरचना में मॉडल विवरण देखना शुरू करते हैं।

## ट्रांसफार्मर संरचना विवरण

ट्रांसफार्मर की स्थूल संरचना को समझने के बाद। आगे, आइए देखें कि ट्रांसफार्मर इनपुट टेक्स्ट अनुक्रम को वेक्टर प्रतिनिधित्व में कैसे परिवर्तित करता है, और यह अंतिम आउटपुट प्राप्त करने के लिए परत दर परत इन वेक्टर प्रतिनिधित्व को कैसे संसाधित करता है।

इसलिए, इस अनुभाग की मुख्य सामग्री में शामिल हैं:
- इनपुट प्रोसेसिंग
  - शब्द सदिश
  - स्थिति वेक्टर
- एनकोडर
- डिकोडर### इनपुट प्रोसेसिंग

#### शब्द वेक्टर
सामान्य एनएलपी कार्यों की तरह, हम पहले इनपुट टेक्स्ट अनुक्रम के प्रत्येक शब्द को एक शब्द वेक्टर में बदलने के लिए एक शब्द एम्बेडिंग एल्गोरिदम का उपयोग करते हैं। व्यावहारिक अनुप्रयोगों में वेक्टर आमतौर पर 256 या 512 आयाम होते हैं। लेकिन सरलता के लिए, हम यहां समझाने के लिए 4-आयामी शब्द वैक्टर का उपयोग करते हैं।

जैसा कि नीचे दिए गए चित्र में दिखाया गया है, यह मानते हुए कि हमारा इनपुट टेक्स्ट 3 शब्दों वाला एक अनुक्रम है, तो प्रत्येक शब्द शब्द एम्बेडिंग एल्गोरिदम के माध्यम से 4-आयामी वेक्टर प्राप्त कर सकता है, और संपूर्ण इनपुट एक वेक्टर अनुक्रम में परिवर्तित हो जाता है। व्यावहारिक अनुप्रयोगों में, हम आमतौर पर एक ही समय में मॉडल में कई वाक्य इनपुट करते हैं, यदि प्रत्येक वाक्य की लंबाई अलग-अलग है, तो हम इनपुट टेक्स्ट अनुक्रम की अधिकतम लंबाई के रूप में एक उचित लंबाई चुनेंगे: यदि कोई वाक्य इस लंबाई तक नहीं पहुंचता है। , फिर पहले एक विशेष "पैडिंग" शब्द भरें; यदि वाक्य इस लंबाई से अधिक है, तो इसे छोटा कर दिया जाता है। अधिकतम अनुक्रम लंबाई एक हाइपरपैरामीटर है। आमतौर पर यह आशा की जाती है कि जितना बड़ा उतना बेहतर, लेकिन लंबे अनुक्रम बड़ी प्रशिक्षण वीडियो मेमोरी/मेमोरी पर कब्जा कर लेते हैं, इसलिए इसे मॉडल प्रशिक्षण के दौरान स्थिति के अनुसार तय करने की आवश्यकता होती है।

![शब्द वैक्टर](./चित्र/2-x.png)
चित्र: 3 शब्द और संबंधित शब्द सदिश

इनपुट अनुक्रम में प्रत्येक शब्द को शब्द वेक्टर प्रतिनिधित्व में परिवर्तित किया जाता है और शब्द का अंतिम वेक्टर प्रतिनिधित्व प्राप्त करने के लिए एक स्थिति वेक्टर जोड़ा जाता है।

#### स्थिति वेक्टर

जैसा कि नीचे दिए गए चित्र में दिखाया गया है, ट्रांसफ़ॉर्म करेंएर मॉडल प्रत्येक इनपुट शब्द वेक्टर में एक स्थिति वेक्टर जोड़ता है। ये वैक्टर प्रत्येक शब्द की स्थितिगत विशेषताओं, या एक वाक्य में विभिन्न शब्दों के बीच की दूरी की विशेषताओं को निर्धारित करने में मदद करते हैं। शब्द वैक्टर और स्थिति वैक्टर को जोड़ने के पीछे अंतर्ज्ञान यह है कि इन स्थिति वैक्टर को शब्द वैक्टर में जोड़कर, परिणामी नए वैक्टर मॉडल को अधिक सार्थक जानकारी प्रदान कर सकते हैं, जैसे शब्द की स्थिति, शब्दों के बीच की दूरी, आदि।

![स्थिति एन्कोडिंग](./चित्र/2-स्थिति.png)
चित्र: स्थिति एन्कोडिंग वेक्टर

फिर भी यह मानते हुए कि शब्द सदिश और स्थिति सदिश के आयाम 4 हैं, हम नीचे दिए गए चित्र में एक संभावित स्थिति सदिश + शब्द सदिश दिखाते हैं:

![स्थिति एन्कोडिंग](./चित्र/2-स्थिति2.png)
चित्र: स्थिति एन्कोडिंग वेक्टर

तो स्थितीय एन्कोडिंग जानकारी वाले वेक्टर किस पैटर्न का अनुसरण करते हैं? मूल पेपर में दी गई डिज़ाइन अभिव्यक्ति है:
$$
PE_{(pos,2i)} = syn(pos / 10000^{2i/d_{\text{model}}}) \\ PE_{(pos,2i+1)} = cos(pos / 10000^{2i/ d_{\text{मॉडल}}})
$$
उपरोक्त तालिकाअभिव्यक्ति में $pos$ शब्द की स्थिति का प्रतिनिधित्व करता है, $d_{model}$ स्थिति वेक्टर के आयाम का प्रतिनिधित्व करता है, $i \in [0, d_{model})$ स्थिति का प्रतिनिधित्व करता है $d_{model}$- आयामी स्थिति वेक्टर $ i$ आयाम। तो उपरोक्त सूत्र के अनुसार, हम $pos$th स्थिति का $d_{model}$-आयामी स्थिति वेक्टर प्राप्त कर सकते हैं। नीचे दिए गए चित्र में, हम 4थे, 5वें, 6वें और 7वें आयामों में विभिन्न स्थितियों पर एक स्थिति वेक्टर का संख्यात्मक परिमाण बनाते हैं। एब्सिस्सा स्थिति सबस्क्रिप्ट का प्रतिनिधित्व करता है, और कोटि संख्यात्मक मान का प्रतिनिधित्व करता है।

![स्थिति एन्कोडिंग चित्र](./pictures/2-2-pos-embedding.png)
चित्र: 0-100 स्थानों पर स्थिति कोडिंग, आयाम 4, 5, 6, और 7 में संख्यात्मक चित्रण

बेशक, उपरोक्त सूत्र स्थिति एन्कोडिंग वैक्टर उत्पन्न करने का एकमात्र तरीका नहीं है। लेकिन इस पद्धति का लाभ यह है कि इसे अज्ञात अनुक्रम लंबाई तक बढ़ाया जा सकता है। उदाहरण के लिए: जब हमारे मॉडल को एक वाक्य का अनुवाद करने की आवश्यकता होती है, और इस वाक्य की लंबाई प्रशिक्षण सेट में सभी वाक्यों की लंबाई से अधिक है, तो यह स्थिति एन्कोडिंग विधि उसी लंबाई की स्थिति एन्कोडिंग वेक्टर भी उत्पन्न कर सकती है।

### एनकोडर एनकोडर

एन्कोडिंग भाग में इनपुट टेक्स्ट अनुक्रम को एक वेक्टर अनुक्रम प्राप्त करने के लिए संसाधित किया जाता है। यह वेक्टर अनुक्रम पहली परत एनकोडर का आउटपुट भी एक वेक्टर अनुक्रम होता है, और फिर एन्कोडिंग की अगली परत पर भेजा जाता है एनकोडर: पहली परत एनकोडर का इनपुट स्थिति वेक्टर के साथ जुड़ा हुआ शब्द वेक्टर है, * ऊपरी परत एनकोडर का इनपुट।इनपुट पिछली परत एनकोडर* का आउटपुट है।

नीचे दिया गया चित्र एकल-परत एनकोडर में वेक्टर अनुक्रमों के प्रवाह को दर्शाता है: स्थिति की जानकारी के साथ जुड़ा हुआ शब्द वेक्टर स्व-ध्यान परत में प्रवेश करता है, और प्रत्येक स्थिति के लिए स्व-ध्यान आउटपुट वेक्टर को प्राप्त करने के लिए एफएफएन तंत्रिका नेटवर्क में इनपुट किया जाता है। प्रत्येक स्थिति के लिए एक नया वेक्टर।

![इनपुट एनकोडर](./pictures/2-x-encoder.png)
चित्र: सिंगल-लेयर एनकोडर का अनुक्रम वेक्टर प्रवाह

आइए 2 शब्दों का एक और उदाहरण देखें:
![एक समय में एक परत पास करें](./pictures/2-multi-encoder.webp)
चित्र: 2 शब्दों का उदाहरण: $x_1, x_2 \to z_1, z_2 \to r_1, r_2$

### आत्म-ध्यान परत

आइए उपरोक्त चित्र में स्व-ध्यान परत के विशिष्ट तंत्र का विश्लेषण करें।

##### आत्म-ध्यान अवलोकन

मान लीजिए जिस वाक्य का हम अनुवाद करना चाहते हैं वह है:
```
जानवर सड़क पार नहीं कर पाया क्योंकि वह बहुत थका हुआ था
```
इस वाक्य में *यह* एक सर्वनाम है, तो *यह* किसको संदर्भित करता है? क्या इसका मतलब *जानवर* या *सड़क* है? यह समस्या लोगों के लिए बहुत सरल है, लेकिन मॉडलों के लिए इतनी आसान नहीं है। हालाँकि, यदि मॉडल *सेल्फ अटेंशन* तंत्र का परिचय देता है, तो मॉडल इसे जानवर के साथ जोड़ सकता है। इसी प्रकार, जब मॉडल वाक्य में अन्य शब्दों को संसाधित करता है, तो *सेल्फ अटेंशन*एन तंत्र भी मॉडल को न केवल वर्तमान स्थिति में शब्द पर ध्यान केंद्रित कर सकता है, बल्कि वाक्य में अन्य स्थितियों में संबंधित शब्दों पर भी ध्यान दे सकता है, जिससे शब्दों की वर्तमान स्थिति को बेहतर ढंग से समझना।

इसकी तुलना अध्याय 2.1 में उल्लिखित आरएनएन से करें: जब आरएनएन किसी शब्द को अनुक्रम में संसाधित करता है, तो यह वाक्य में पिछले शब्द द्वारा पारित *छिपी हुई स्थिति* पर विचार करेगा, और *छिपी हुई स्थिति* में पिछले शब्द की जानकारी शामिल होती है; और *स्व ध्यान* तंत्र का मूल्य यह है कि वर्तमान शब्द सीधे अपने वाक्य में सभी संबंधित शब्दों पर ध्यान देगा, जैसा कि नीचे *यह* उदाहरण में दिखाया गया है:

![एक शब्द और दूसरे शब्द का ध्यान](./pictures/2-attention-word.png)

चित्र: एक शब्द और दूसरे शब्द का ध्यान

ऊपर चित्र में दिखाया गया *यह* एक वास्तविक उदाहरण है। यह वह स्थिति है जब ट्रांसफार्मर पांचवें परत एनकोडर में "इट" को एनकोड करता है, यह दिखाता है कि *इसका* ध्यान "द एनिमल" पर केंद्रित है। और इन दोनों शब्दों की जानकारी को "i" में मिला दियाटी" में.

##### आत्म-ध्यान विवरण

आइए पहले इसे एक सरल उदाहरण के माध्यम से समझें: "आत्म-ध्यान तंत्र" क्या है? मान लीजिए कि एक वाक्य में दो शब्द हैं: सोचने वाली मशीनें। आत्म-ध्यान की एक समझ है: सोच-विचार, सोच-मशीनें, मशीनें-सोच, मशीनें-मशीनें, जोड़ियों में कुल $2^2$ प्रकार का ध्यान। तो इसकी विशेष गणना कैसे करें? मान लें कि दो शब्दों थिंकिंग और मशीन के लिए शब्द वेक्टर एल्गोरिदम का उपयोग करके प्राप्त वेक्टर $X_1, X_2$​ हैं:
$$
1: q_1 = X_1 W^Q, q_2 = X_2 W^Q; }\\
2-3: स्कोर_{11} = \frac{q_1 \cdot q_1}{\sqrt{d_k}} , स्कोर_{12} = \frac{q_1 \cdot q_2}{\sqrt{d_k}};
4: स्कोर_{11} = \frac{e^{score_{11}}}{e^{score_{11}} + e^{score_{12}}},score_{12} = \frac{e^{ स्कोर_{12}}}{e^{स्कोर_{11}} + e^{स्कोर_{12}}}; स्कोर_{21} = \frac{e^{स्कोर_{21}}}{e^{स्कोर_{21 }} + e^{score_{22}}},score_{22} = \frac{e^{score_{22}}}{e^{score_{21} \
5-6: z_1 = v_1 \गुना स्कोर_{11} + v_2 \गुना स्कोर_{12}; z_2 = v_1 \गुना स्कोर_{21} + v_2 \गुना स्कोर_{22}
$$
अंतर्गत,हम आत्म-ध्यान गणना के 6 चरणों की कल्पना करते हैं।

चरण 1: प्राप्त करने के लिए शब्द वेक्टर इनपुट को एनकोडर में रैखिक रूप से बदलें: क्वेरी वेक्टर: $q_1, q_2$, कुंजी वेक्टर: $k_1, k_2$, मान वेक्टर: $v_1, v_2$। ये तीन वैक्टर क्रमशः शब्द वैक्टर और तीन पैरामीटर मैट्रिक्स को गुणा करके प्राप्त किए जाते हैं, और यह मैट्रिक्स मॉडल द्वारा सीखा जाने वाला पैरामीटर भी है।

![Q,K,V](./pictures/2-qkv.png) चित्र: क्वेरी वेक्टर की गणना करें: $q_1, q_2$, मुख्य वेक्टर: $k_1, k_2$, मान वेक्टर: $v_1, v_2$।

क्वेरी वेक्टर, कुंजी वेक्टर और वैल्यू वेक्टर का क्या मतलब है?

वास्तव में, वे तीन वेक्टर हैं। उनमें एक नाम जोड़ने से हम सेल्फ-अटेंशन की गणना प्रक्रिया और तर्क को बेहतर ढंग से समझ सकते हैं। ध्यान गणना के तर्क को अक्सर इस प्रकार वर्णित किया जा सकता है: ** क्वेरी और कुंजी गणना संबंधित हैं या ध्यान स्कोर कहा जाता है, और फिर ध्यान स्कोर के अनुसार मूल्य को भारित और सारांशित किया जाता है। **

चरण 2: ध्यान स्कोर की गणना करें। मान लीजिए कि अब हम पहले शब्द *सोच* के ध्यान स्कोर की गणना करते हैं), *सोच* के अनुरूप शब्द वेक्टर के आधार पर वाक्य में अन्य शब्द वैक्टर के लिए स्कोर की गणना करना आवश्यक है। ये स्कोर निर्धारित करते हैं कि *सोच* शब्द को एन्कोड करते समय हमें वाक्य में कहीं और शब्द वैक्टर को कितना महत्व देने की आवश्यकता है।

ध्यान स्कोर "*सोच*" के अनुरूप क्वेरी वेक्टर और अन्य स्थानों पर प्रत्येक शब्द के मुख्य वेक्टर के आधार पर एक डॉट उत्पाद निष्पादित करके प्राप्त किया जाता है। सोच का पहला ध्यान स्कोर $q_1$ और $k_1$ का आंतरिक उत्पाद है, और दूसरा स्कोर $q_1$ और $k_2$ का डॉट उत्पाद है। यह गणना प्रक्रिया नीचे दिए गए चित्र में दिखाई गई है। नीचे दिए गए चित्र में विशिष्ट स्कोर डेटा को अभिव्यक्ति की सुविधा के लिए अनुकूलित किया गया है।

![थिंकिंग कंप्यूटिंग](./pictures/2-think.png)
चित्र: सोच का ध्यान स्कोर गणना

चरण 3: प्रत्येक स्कोर को $\sqrt{d_k}$ से विभाजित करें, $d_{k}$ कुंजी वेक्टर का आयाम है। आप अन्य संख्याओं से भी विभाजित कर सकते हैं। किसी संख्या से विभाजित करने का उद्देश्य बैकप्रोपेगेशन के दौरान ग्रेडिएंट को अधिक स्थिर बनाना है।

चरण 4: फिर इन अंकों को सॉफ्टमैक्स फ़ंक्शन के माध्यम से पास करें। सॉफ्टमैक्स स्कोर को सामान्य कर सकता है ताकि सभी स्कोर सकारात्मक हों और 1 तक जुड़ जाएं, जैसा कि नीचे दिए गए चित्र में दिखाया गया है।
ये स्कोर अन्य के लिए थिंकिंग शब्द वेक्टर निर्धारित करते हैंसभी पदों पर सदिश शब्द पर कितना ध्यान दिया जाता है।

![थिंकिंग कंप्यूटिंग](./pictures/2-think2.png)
चित्र: सोच का ध्यान स्कोर गणना

चरण 5: प्रत्येक शब्द वेक्टर का स्कोर प्राप्त करने के बाद, स्कोर को संबंधित वैल्यू वेक्टर से गुणा करें। इस दृष्टिकोण के पीछे सहज समझ यह है: उच्च स्कोर वाले पदों के लिए, गुणा मूल्य जितना बड़ा होगा, और कम स्कोर वाले पदों के लिए हम उन पर अधिक ध्यान केंद्रित करते हैं, गुणा मूल्य जितना छोटा होगा, इन पदों में शब्द हो सकते हैं कम प्रासंगिक.

चरण 6: वर्तमान स्थिति में सेल्फ अटेंशन के अनुरूप आउटपुट प्राप्त करने के लिए चरण 5 में प्राप्त वैल्यू वैक्टर जोड़ें (यहां उदाहरण पहली स्थिति है)।

अंत में, नीचे दिया गया चित्र प्रथम स्थिति शब्द वेक्टर के लिए सेल्फ अटेंशन की गणना करने की पूरी प्रक्रिया को दर्शाता है। वर्तमान स्थिति में अंतिम शब्द वेक्टर (इस उदाहरण में पहली स्थिति) फीडफॉरवर्ड न्यूरल नेटवर्क में इनपुट होता रहेगा। नोट: उपरोक्त 6 चरण एक समय में केवल एक स्थिति के आउटपुट वेक्टर की गणना कर सकते हैं, वास्तविक कोड कार्यान्वयन में, सेल्फ अटेंशन की गणना प्रक्रिया को मैट्रिक्स का उपयोग करके जल्दी से गणना की जाती है, और सभी स्थितियों के आउटपुट वेक्टर एक ही बार में प्राप्त किए जाते हैं।

![गणना सोचो](./pictures/2-sum.png)
चित्र: ध्यान के बाद सोच का वेक्टर प्रतिनिधित्व $z_1$

##### स्व-ध्यान मैट्रिक्स गणना

स्व-ध्यान गणना के छह चरणों में वैक्टर को एक साथ रखें, जैसे $X=[x_1;x_2]$​, और आप मैट्रिक्स गणना कर सकते हैं। नीचे, हम अभी भी चरण दर चरण आत्म-ध्यान मैट्रिक्स गणना विधि दिखाते हैं।
$$
एक्स = [X_1;X_2] \\
क्यू = एक्स डब्ल्यू^क्यू, के = एक्स डब्ल्यू^के, वी=एक्स डब्ल्यू^वी \\
Z = Softmax(\frac{QK^T}{\sqrt{d_k}}) V
$$
चरण 1: क्वेरी, कुंजी और मान के मैट्रिक्स की गणना करें। सबसे पहले, हम सभी शब्द वैक्टर को एक मैट्रिक्स में डालते हैं मैट्रिक्स X में प्रत्येक पंक्ति वाक्य में प्रत्येक शब्द के शब्द वेक्टर का प्रतिनिधित्व करती है। Q, K, V मैट्रिक्स में प्रत्येक पंक्ति क्वेरी वेक्टर, कुंजी वेक्टर, वैल्यू वेक्टर का प्रतिनिधित्व करती है, और वेक्टर आयाम $d_k$ है।

![](./pictures/2-qkv-multi.png)चित्र: QKV मैट्रिक्स गुणन

चरण 2: चूँकि हम गणना करने के लिए एक मैट्रिक्स का उपयोग करते हैं, हम उपरोक्त चरण 2 से 6 को एक चरण में संपीड़ित कर सकते हैं और सीधे सेल्फ अटेंशन का आउटपुट प्राप्त कर सकते हैं।

![आउटपुट](./तस्वीरtures/2-ध्यान-आउटपुट.वेब)
चित्र: आउटपुट $Z$ प्राप्त करें

#### मल्टी-हेड ध्यान तंत्र

ट्रांसफॉर्मर का पेपर मल्टी-हेड अटेंशन मैकेनिज्म (ध्यान के समूह को अटेंशन हेड कहा जाता है) जोड़कर सेल्फ-अटेंशन को और बेहतर बनाता है। यह तंत्र निम्नलिखित दो पहलुओं से ध्यान परत की क्षमताओं को बढ़ाता है:

- **यह विभिन्न स्थानों पर ध्यान केंद्रित करने की मॉडल की क्षमता को बढ़ाता है**। उपरोक्त उदाहरण में, पहली स्थिति के आउटपुट $z_1$ में वाक्य में हर दूसरी स्थिति की जानकारी का एक बहुत छोटा सा हिस्सा होता है, लेकिन $z_1$ सिर्फ एक वेक्टर है, इसलिए इसमें केवल जानकारी शामिल हो सकती है पहले स्थान पर रहा. और जब हम इस वाक्य का अनुवाद करते हैं: `जानवर सड़क पार नहीं कर सका क्योंकि वह बहुत थका हुआ था`, तो हम न केवल आशा करते हैं कि मॉडल "इस" पर ध्यान देगा, बल्कि यह भी आशा करते हैं कि मॉडल "इस पर" ध्यान देगा। "और" जानवर ", और यहां तक ​​कि" थके हुए "पर भी ध्यान दें। यहीं पर बहु-प्रमुख ध्यान तंत्र मदद करता है।
- **मल्टी-हेड ध्यान तंत्र ध्यान परत को कई "उप-प्रतिनिधित्व स्थान" देता है**। नीचे हम देखेंगे कि मल्टी-हेड अटेंशन मैकेनिज्म में वेट मैट्रिक्स $W^Q, W^K W^V$​ के कई सेट होंगे (ट्रांसफॉर्मर पेपर में, 8 का उपयोग किया जाता है)समूह ध्यान), इसलिए $X$​ को प्रतिनिधित्व के लिए अधिक बीज स्थानों में परिवर्तित किया जा सकता है। आगे हम ध्यान प्रमुखों के 8 सेटों का भी उपयोग करते हैं)। ध्यान के प्रत्येक समूह का वजन मैट्रिक्स यादृच्छिक रूप से प्रारंभ किया जाता है, लेकिन प्रशिक्षण के बाद, ध्यान के प्रत्येक समूह का वजन $W^Q, W^K W^V$​ इनपुट वेक्टर को संबंधित "उप प्रतिनिधित्व स्थान" पर मैप कर सकता है।

![मल्टी-हेड ध्यान तंत्र](./pictures/2-multi-head.png)
चित्र: बहु-सिर ध्यान तंत्र

मल्टी-हेड ध्यान तंत्र में, हम ध्यान के प्रत्येक समूह के लिए अलग-अलग WQ, WK और WV पैरामीटर मैट्रिक्स सेट करते हैं। Q, K और V मैट्रिक्स के 8 समूह प्राप्त करने के लिए प्रत्येक ध्यान समूह के इनपुट X और WQ, WK और WV को गुणा करें।

इसके बाद, हम K, Q, V के प्रत्येक समूह के Z मैट्रिक्स की गणना करते हैं और 8 Z मैट्रिक्स प्राप्त करते हैं।

![8 Z मैट्रिसेस](./pictures/2-8z.webp)
चित्र: 8 Z मैट्रिक्स

चूंकि फीडफॉरवर्ड न्यूरल नेटवर्क परत को 8 मैट्रिक्स के बजाय 1 मैट्रिक्स (जहां प्रत्येक पंक्ति में वेक्टर एक शब्द का प्रतिनिधित्व करता है) प्राप्त होता है, हम एक बड़े मैट्रिक्स को प्राप्त करने के लिए सीधे 8 उप-मैट्रिसेस को जोड़ते हैं, और फिर इसे दूसरे वेट मैट्रिक्स $W^ के साथ जोड़ते हैं। परिवर्तन करने के लिए O$ को गुणा किया जाता है और फीडफॉरवर्ड न्यूरल नेटवर्क परत के लिए आवश्यक आयामों पर मैप किया जाता है।

![एकीकरण मैट्रिक्स](./चित्र/2-टू1.वेबपी)
चित्र: 8 सबमैट्रिस को विभाजित करना और मानचित्रण परिवर्तन करना

इसको जोड़कर:
1. 8 आव्यूहों को एक साथ जोड़ें {Z0,Z1...,Z7}
2. स्प्लिस्ड मैट्रिक्स और WO वेट मैट्रिक्स को गुणा करें
3. अंतिम मैट्रिक्स Z प्राप्त करें, जिसमें सभी ध्यान प्रमुखों की जानकारी शामिल है। यह मैट्रिक्स एफएफएनएन (फीड फॉरवर्ड न्यूरल नेटवर्क) परत पर इनपुट होगा।

यह सब तेजी से ध्यान आकर्षित करने के लिए है। अंत में सब कुछ एक चित्र में रखें:

![एक साथ रखें](./pictures/2-put-together.webp)
चित्र: मल्टी-हेड ध्यान तंत्र का मैट्रिक्स ऑपरेशन

अब जब हमने मल्टी-हेड ध्यान तंत्र सीख लिया है, तो आइए उस उदाहरण पर एक नज़र डालें जिसका हमने पहले उल्लेख किया था कि "यह" ध्यान विभिन्न ध्यान शीर्षों (ध्यान शीर्षों) से किस सामग्री से मेल खाता है। नीचे दी गई तस्वीर में हरी और नारंगी रेखाएँ क्रमशः ध्यान प्रमुखों के 2 अलग-अलग समूहों का प्रतिनिधित्व करती हैं:

![`इस पर` का ध्यान](./pictures/2-it-attention.webp)
चित्र: `इसका` ध्यान

जब हम "इट" शब्द को एन्कोड करते हैं, तो ध्यान प्रमुखों में से एक (नारंगी ध्यान शीर्ष) "एनिमा" पर सबसे अधिक ध्यान केंद्रित करता है।एल", अन्य हरे रंग का ध्यान सिर "थका हुआ" पर केंद्रित है। इसलिए, एक अर्थ में, मॉडल में "यह" का प्रतिनिधित्व "जानवर" और "टायर" की आंशिक अभिव्यक्तियों को जोड़ता है।

#### ध्यान कोड उदाहरण
निम्नलिखित कोड कार्यान्वयन में, टेंसर का पहला आयाम बैच आकार है, और दूसरा आयाम वाक्य की लंबाई है। कोड पर अच्छी तरह से टिप्पणी की गई है और समझाया गया है।

```
क्लास मल्टीहेडअटेंशन(एनएन.मॉड्यूल):
    # n_heads: मल्टी-हेड ध्यान की संख्या
    #hid_dim: प्रत्येक शब्द आउटपुट का वेक्टर आयाम
    def __init__(स्वयं, hid_dim, n_heads, ड्रॉपआउट):
        सुपर(मल्टीहेडअटेंशन, स्वयं).__init__()
        स्वयं.hid_dim = hid_dim
        self.n_heads = n_heads

        # hid_dim को h से विभाज्य होने के लिए बाध्य करें
        ज़ोर से hid_dim % n_heads == 0
        # W_q मैट्रिक्स को परिभाषित करेंself.w_q = nn.रैखिक(hid_dim, hid_dim)
        # W_k मैट्रिक्स को परिभाषित करें
        self.w_k = nn.रैखिक(hid_dim, hid_dim)
        # W_v मैट्रिक्स को परिभाषित करें
        self.w_v = nn.रैखिक(hid_dim, hid_dim)
        self.fc = nn.रैखिक(hid_dim, hid_dim)
        self.do = nn.ड्रॉपआउट(ड्रॉपआउट)
        # ज़ूम करें
        सेल्फ.स्केल = मशाल.वर्ग(मशाल.फ्लोटटेन्सर([hid_dim // n_heads]))

    डीईएफ़ फॉरवर्ड (स्वयं, क्वेरी, कुंजी, मान, मुखौटा = कोई नहीं):
        # ध्यान दें कि वाक्य लंबाई आयाम में Q, K और V का मान समान या भिन्न हो सकता है।
        # K: [64,10,300], मान लीजिए कि बैच_आकार 64 है, प्रत्येक में 10 शब्द हैंशब्दों का क्वेरी वेक्टर 300 आयाम है
        # वी: [64,10,300], बैच आकार 64 मानते हुए, 10 शब्द हैं, और प्रत्येक शब्द का क्वेरी वेक्टर 300 आयाम है
        # प्रश्न: [64,12,300], बैच आकार 64 मानते हुए, 12 शब्द हैं, और प्रत्येक शब्द का क्वेरी वेक्टर 300 आयाम है
        bsz = query.shape[0]
        Q = self.w_q(क्वेरी)
        के = self.w_k(कुंजी)
        वी = self.w_v(मूल्य)
        # यहां K Q V मैट्रिक्स को ध्यान के कई समूहों में विभाजित किया गया है
        # अंतिम आयाम self.hid_dim // self.n_heads का उपयोग करके प्राप्त किया जाता है, जो ध्यान के प्रत्येक समूह की वेक्टर लंबाई को दर्शाता है: प्रत्येक शीर्ष की वेक्टर लंबाई है: 300/6=50
        # 64 बैच आकार का प्रतिनिधित्व करता है, 6 ध्यान के 6 समूहों का प्रतिनिधित्व करता है, 10 10 शब्दों का प्रतिनिधित्व करता है, और 50 ध्यान के प्रत्येक समूह के शब्दों की वेक्टर लंबाई का प्रतिनिधित्व करता है।
        # K: [64,10,300] ध्यान के कई समूहों को विभाजित करें-> [64,10,6,50] प्राप्त करने के लिए ट्रांसपोज़ किया गया -> [64,6,10,50]
        # वी: [64,10,300] ध्यान के कई समूहों को विभाजित करें -> [64,10,6,50] और स्थानांतरित करें -> [64,6,10,50]
        # प्रश्न: [64,12,300] ध्यान के कई समूहों को विभाजित करें -> [64,12,6,50] प्राप्त करने के लिए स्थानांतरित करें -> [64,6,12,50]
        # ट्रांसपोज़िशन में नीचे दी गई गणना को सुविधाजनक बनाने के लिए ध्यान की संख्या 6 को सामने और 10 और 50 को पीछे रखना है।
        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim //
                   self.n_heads).permute(0, 2, 1, 3)
        K = K.view(bsz, -1, self.n_heads, self.hid_dim //
                   self.n_heads).permute(0, 2, 1, 3)
        वी = वी.व्यू(बीएसजेड, -1, सेल्फ.एन_हेड्स, सेल्फ.एचid_dim //
                   self.n_heads).permute(0, 2, 1, 3)

        # चरण 1: K के स्थानान्तरण को Q गुना, पैमाने से विभाजित किया गया
        # [64,6,12,50] * [64,6,50,10] = [64,6,12,10]
        # ध्यान: [64,6,12,10]
        ध्यान = टार्च.मैटमूल(क्यू, के.परम्यूट(0, 1, 3, 2)) / सेल्फ.स्केल

        # यदि मास्क खाली नहीं है, तो उस स्थिति का ध्यान स्कोर सेट करें जहां मास्क 0 से -1e10 है। यहां, "0" का उपयोग यह इंगित करने के लिए किया जाता है कि किस स्थिति में शब्द वैक्टर पर ध्यान नहीं दिया जा सकता है, जैसे कि पैडिंग स्थिति बेशक, "1" का उपयोग "या अन्य संख्याओं को इंगित करने के लिए भी किया जा सकता है, मुख्य रूप से कोड की निम्नलिखित 2 पंक्तियों में डिज़ाइन परिवर्तन।
        यदि मास्क कोई नहीं है:
            ध्यान = ध्यान.नकाबपोश_भरण(मुखौटा == 0, -1e10)

        #चरण 2: पिछले चरण के परिणाम के सॉफ्टमैक्स की गणना करें, और फिर ध्यान आकर्षित करने के लिए ड्रॉपआउट से गुजरें।
        # ध्यान दें कि सॉफ्टमैक्स यहां अंतिम आयाम पर किया जाता है, यानी सॉफ्टमैक्स इनपुट अनुक्रम के आयाम पर किया जाता है।
        # ध्यान: [64,6,12,10]
        ध्यान = स्व.करें(मशाल.सॉफ्टमैक्स(ध्यान, मंद=-1))

        # तीसरा चरण मल्टी-हेड ध्यान का परिणाम प्राप्त करने के लिए ध्यान परिणाम को V से गुणा करना है।
        # [64,6,12,10] * [64,6,10,50] = [64,6,12,50]
        # एक्स: [64,6,12,50]
        एक्स = टॉर्च.मैटमुल(ध्यान, वी)

        # क्योंकि क्वेरी में 12 शब्द हैं, नीचे परिणामों के कई सेटों को जोड़ने की सुविधा के लिए आगे 12 और पीछे 50 और 6 रखें।
        # x: [64,6,12,50] स्थानांतरण -> [64,12,6,50]
        x = x.permute(0, 2, 1, 3.सन्निहित()
        # यहां मैट्रिक्स परिवर्तन है: ध्यान के कई सेटों के परिणामों को एक साथ जोड़ना
        #अंतिम परिणाम है [64,12,300]
        # x: [64,12,6,50] -> [64,12,300]
        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))
        x = self.fc(x)
        वापसी एक्स


# बैच_आकार 64 है, 12 शब्द हैं, और प्रत्येक शब्द का क्वेरी वेक्टर 300 आयाम है
क्वेरी = टॉर्च.रैंड(64, 12, 300)
#बैच_आकार 64 है, 12 शब्द हैं, और प्रत्येक शब्द का कुंजी वेक्टर 300 आयाम है
कुंजी = टॉर्च.रैंड(64, 10, 300)
# बैच_आकार 64 है, 10 शब्द हैं, और प्रत्येक शब्द का मान वेक्टर 300 आयाम है
मूल्य = टॉर्च.रैंड(64, 10, 300)
ध्यान = मल्टीहेडअटेंशन(hid_dim=300, n_heads=6, ड्रॉपआउट=0.1)
आउटपुट = ध्यान (क्वेरी, कुंजी, मूल्य)
## आउटपुट: टॉर्च.आकार([64,12,300])
प्रिंट(आउटपुट.आकार)

```

#### अवशिष्ट कनेक्शन

अब तक, हमने आत्म-ध्यान के आउटपुट वेक्टर की गणना की है। सिंगल-लेयर एनकोडर में दो महत्वपूर्ण बाद के ऑपरेशन हैं: अवशिष्ट लिंकिंग और मानकीकरण।

एनकोडर की प्रत्येक उप-परत (सेल्फ अटेंशन लेयर और एफएफएनएन) में एक अवशिष्ट कनेक्शन और परत-सामान्यीकरण होता है, जैसा कि नीचे दिए गए चित्र में दिखाया गया है।

![अवशिष्ट कनेक्शन](./pictures/2-resnet.png)
चित्र: अवशिष्ट कनेक्शन

स्व-ध्यान परत की परत-सामान्यीकरण और इसमें शामिल वेक्टर गणना विवरण की कल्पना इस प्रकार करें:

![मानकीकरण](./pictures/2-lyn.png)
चित्र: मानकीकरण विवरण

एनकोडर और डिकोडर की उप-परतों में परत-सामान्यीकरण होता है। मान लीजिए कि एक ट्रांसफार्मर 2 से बना हैयह एक लेयर एनकोडर और दो-लेयर डिकोडर से बना है, और सभी आंतरिक विवरण नीचे दिए गए चित्र में दिखाए गए अनुसार प्रदर्शित होते हैं।

![2-परत आरेख](./चित्र/2-2लेयर.png)
चित्र: 2-लेयर ट्रांसफार्मर का योजनाबद्ध आरेख

## डिकोडर

अब जबकि हमने एनकोडर में अधिकांश अवधारणाओं को पेश कर दिया है, हमें यह भी बुनियादी समझ है कि एनकोडर कैसे काम करते हैं। अब आइए देखें कि एनकोडर और डिकोडर एक साथ कैसे काम करते हैं।

एनकोडर में आम तौर पर कई परतें होती हैं। पहले एनकोडर का इनपुट एक अनुक्रम टेक्स्ट होता है, और अंतिम एनकोडर का आउटपुट अनुक्रम वैक्टर का एक सेट होता है, जहां अनुक्रम वैक्टर का यह सेट डिकोडर के K और V इनपुट के रूप में उपयोग किया जाएगा K=V=प्रोसेसर के आउटपुट का डिकोडिंग अनुक्रम वेक्टर प्रतिनिधित्व। ये ध्यान वैक्टर प्रत्येक डिकोडर की एनकोडर-डिकोडर ध्यान परत पर इनपुट होंगे, जो डिकोडर को इनपुट अनुक्रम की उचित स्थिति पर अपना ध्यान केंद्रित करने में मदद करता है, जैसा कि नीचे दिए गए चित्र में दिखाया गया है।

![](./pictures/transformer_decoding_1.gif)

डिकोडिंग चरण का प्रत्येक बार चरण एक अनुवादित शब्द को आउटपुट करता है (उदाहरण यहां अंग्रेजी अनुवाद है)। डिकोडर के वर्तमान समय चरण का आउटपुट फिर से इनपुट क्यू के रूप में उपयोग किया जाता है और एनकोडर के आउटपुट के और वी का उपयोग किया जाता है। टाइम स्टेप डिकोडर में अगला इनपुट। फिर इस प्रक्रिया को तब तक दोहराएँ जब तक कि टर्मिनेटर आउटपुट न हो जाए। जैसा कि नीचे दिया गया है:

![डिकोडर गतिशील चित्र](./pictures/2-decoder.gif)
डायनेमिक ग्राफ़: डिकोडर डायनेमिक ग्राफ़

डिकोडर में सेल्फ अटेंशन लेयर और एनकोडर में सेल्फ अटेंशन लेयर के बीच अंतर:
1. डिकोडर में, सेल्फ अटेंशन परत केवल उन शब्दों पर ध्यान देने की अनुमति देती है जो आउटपुट अनुक्रम में वर्तमान स्थिति से पहले के हैं। विशिष्ट विधि यह है: सेल्फ अटेंशन स्कोर सॉफ्टमैक्स परत से गुजरने से पहले, वर्तमान स्थिति के बाद की स्थिति अवरुद्ध हो जाती है (ध्यान स्कोर को -inf पर सेट करें)।
2. डिकोडर अटेंशन लेयर क्वेरी मैट्रिक्स के निर्माण के लिए पिछली परत के आउटपुट का उपयोग करता है, और कुंजी मैट्रिक्स और वैल्यू मैट्रिक्स एनकोडर के अंतिम आउटपुट से आते हैं।

## रैखिक परत और सॉफ्टमैक्स

डिकोडर का अंतिम आउटपुट एक वेक्टर है जहां प्रत्येक तत्व एक फ्लोटिंग पॉइंट नंबर है। हम इस वेक्टर को शब्दों में कैसे परिवर्तित करें? यह रैखिक परतों और सॉफ्टमैक्स के साथ किया जाता है।

रैखिक परत एक सामान्य पूरी तरह से जुड़ा हुआ तंत्रिका नेटवर्क है जो डिकोडर द्वारा वेक्टर आउटपुट को एक बड़े वेक्टर में मैप कर सकता है। इस वेक्टर को लॉगिट वेक्टर कहा जाता है: मान लीजिए कि हमारे मॉडल में 10,000 अंग्रेजी शब्द हैं (मॉडल की आउटपुट शब्दावली), यह लॉग वेक्टर करता है। इसमें 10000 संख्याएँ होंगी, प्रत्येक संख्या एक शब्द के स्कोर को दर्शाती है।

फिर, सॉफ़्टमकुल्हाड़ी परत इन अंकों को संभावनाओं में परिवर्तित करती है (सभी अंकों को सकारात्मक संख्याओं में परिवर्तित करती है और 1 तक जोड़ती है)। फिर उच्चतम संभावना वाली संख्या के अनुरूप शब्द का चयन करें, जो इस समय चरण में आउटपुट शब्द है।

![रैखिक परत](./pictures/2-linear.png)
चित्र: रैखिक परत

## लॉस फंकशन

ट्रांसफार्मर को प्रशिक्षित करते समय, नुकसान प्राप्त करने के लिए डिकोडर और लेबल के आउटपुट को एक साथ नुकसान फ़ंक्शन पर भेजने की आवश्यकता होती है। अंतिम मॉडल नुकसान के अनुसार दिशा में प्रसारित होता है। इस अनुभाग में, हम प्रशिक्षण प्रक्रिया की हानि गणना को स्पष्ट करने के लिए एक सरल उदाहरण का उपयोग करते हैं: "दया" का "धन्यवाद" में अनुवाद करना।

हमें उम्मीद है कि मॉडल डिकोडर के अंतिम आउटपुट का संभाव्यता वितरण "धन्यवाद" शब्द को इंगित करेगा ("धन्यवाद" शब्द की संभावना सबसे अधिक है)। हालाँकि, मॉडल को शुरुआत में अच्छी तरह से प्रशिक्षित नहीं किया गया है, और यह जो संभाव्यता वितरण आउटपुट करता है वह उस संभाव्यता वितरण से बहुत दूर हो सकता है जैसा कि नीचे दिए गए चित्र में दिखाया गया है, सही संभाव्यता वितरण उच्चतम के साथ "धन्यवाद" शब्द होना चाहिए संभावना। हालाँकि, चूंकि मॉडल के मापदंडों को यादृच्छिक रूप से आरंभ किया गया है, इसलिए शुरुआत में सभी शब्दों की भविष्यवाणी करने वाले मॉडल की संभावना लगभग यादृच्छिक है।

![संभावना वितरण](./चित्र/2-नुकसान.वेबपी)
चित्र: संभाव्यता वितरण

जब तक ट्रांसफार्मर डिकोडर समूह संभाव्यता की भविष्यवाणी करता है, हम इस समूह संभाव्यता की तुलना सही आउटपुट संभावना के साथ कर सकते हैं, और फिर बैकप्रॉपैगेशन का उपयोग कर सकते हैंमॉडल के वजन को समायोजित करें ताकि आउटपुट की संभाव्यता वितरण पूर्णांक आउटपुट के करीब हो।

तो हम दो संभाव्यता वितरणों की तुलना कैसे करते हैं? : हम संभाव्यता सदिशों के दो सेटों के बीच की स्थानिक दूरी को हानि के रूप में उपयोग कर सकते हैं (वैक्टरों को घटाएं, फिर वर्गों का योग ज्ञात करें, और फिर वर्गमूल लें। बेशक, हम क्रॉस-एन्ट्रॉपी (क्रॉस-) का भी उपयोग कर सकते हैं। एन्ट्रॉपी)] और केएल विचलन (कुल्बैक-लीबलर विचलन)। पाठक आगे पढ़ने से संबंधित ज्ञान की खोज कर सकते हैं, और इस खंड में हानि कार्यों के ज्ञान का विस्तार नहीं किया जाएगा।

चूँकि ऊपर दिया गया एक-शब्द का उदाहरण बहुत सरल है, हम अधिक जटिल वाक्य देख सकते हैं। वाक्य इनपुट है: "je suis étudiant" और आउटपुट है: "मैं एक छात्र हूं"। इसका मतलब है कि हमारा ट्रांसफार्मर मॉडल डिकोडर संभाव्यता वितरण वेक्टर को कई बार आउटपुट करता है:

- प्रत्येक आउटपुट का संभाव्यता वितरण vocab_size की लंबाई वाला एक वेक्टर है (अधिकतम vocab आकार पर पहले सहमति हुई थी, यानी, वेक्टर लंबाई 6 है, लेकिन वास्तविक vocab आकार 30000 या 50000 होने की अधिक संभावना है)
- पहले आउटपुट के संभाव्यता वितरण में, उच्चतम संभावना के अनुरूप शब्द "i" है
- दूसरे आउटपुट के संभाव्यता वितरण में, उच्चतम संभावना के अनुरूप शब्द "am" है
- सादृश्य से, पांचवें संभाव्यता वितरण तक, उच्चतम संभावना के अनुरूप शब्द "\<eos>" है, जिसका अर्थ हैइंगित करता है कि कोई अगला शब्द नहीं है

तो हमारे लक्ष्य का संभाव्यता वितरण इस तरह दिखता है:

![संभावना वितरण](./चित्र/2-target.png)
चित्र: लक्ष्य संभाव्यता वितरण

हम चित्र में दिखाए गए संभाव्यता वितरण का उत्पादन करने की उम्मीद में, उदाहरण में वाक्यों का उपयोग करके मॉडल को प्रशिक्षित करते हैं
काफी लंबे समय तक पर्याप्त बड़े डेटा सेट पर अपने मॉडल को प्रशिक्षित करने के बाद, हम एक संभाव्यता वितरण आउटपुट करने की उम्मीद करते हैं जैसा कि नीचे दिखाया गया है:

![प्रशिक्षण के बाद संभाव्यता वितरण](./pictures/2-trained.webp)
चित्र: मॉडल प्रशिक्षण के बाद एकाधिक संभाव्यता वितरण आउटपुट

हमें उम्मीद है कि प्रशिक्षण के बाद मॉडल जो संभाव्यता वितरण आउटपुट कर सकता है वह भी सही अनुवाद के अनुरूप होगा। बेशक, यदि आप जिस वाक्य का अनुवाद करना चाहते हैं वह प्रशिक्षण सेट का हिस्सा है, तो आउटपुट परिणाम का कोई खास मतलब नहीं है। हमें उम्मीद है कि मॉडल अनदेखे वाक्यों का सटीक अनुवाद कर सकता है।

मैं लालची डिकोडिंग और बीम खोज की अवधारणाओं का भी उल्लेख करना चाहूंगा:
- लालची डिकोडिंग: चूंकि मॉडल प्रति समय चरण केवल एक आउटपुट उत्पन्न करता है, हम इसे इस तरह से देखते हैं: मॉडल संभाव्यता वितरण से उच्चतम संभावना वाले शब्द का चयन करता है और अन्य शब्दों को हटा देता है। इस विधि को लालची डिकोडिंग कहा जाता है।
- बीम खोज: प्रत्येक बार चरण उच्चतम संभावना के साथ k आउटपुट शब्दों को बरकरार रखता है, और फिर अगले समय चरण में, निर्धारित करता है कि पिछले समय चरण में बनाए गए k शब्दों के आधार पर वर्तमान में कौन से k शब्दों को बरकरार रखा जाना चाहिए।. मान लें कि k=2, पहली स्थिति में उच्चतम संभावना वाले दो आउटपुट शब्द "I" और "a" हैं, दोनों शब्द बरकरार रखे गए हैं, और फिर दूसरी स्थिति में शब्द की संभाव्यता वितरण की गणना पहले शब्द के आधार पर की जाती है , फिर दूसरे स्थान पर सबसे अधिक संभावना वाले दो शब्दों को हटा दें। हम इस प्रक्रिया को तीसरे और चौथे स्थान के लिए भी दोहराते हैं। इस विधि को किरण खोज कहा जाता है।

## अतिरिक्त जानकारी

मुझे आशा है कि जो मैंने ऊपर कहा वह आपको ट्रांसफार्मर की मुख्य अवधारणाओं को समझने में मदद कर सकता है। यदि आप इसे और अधिक समझना चाहते हैं, तो मेरा सुझाव है कि आप निम्नलिखित का संदर्भ लें:

- ट्रांसफार्मर पेपर पढ़ें:
"आपको केवल ध्यान देने की आवश्यकता है"
लिंक पता: https://arxiv.org/abs/1706.03762
- ट्रांसफार्मर का ब्लॉग पोस्ट पढ़ें:
"ट्रांसफॉर्मर: भाषा समझ के लिए एक नवीन तंत्रिका नेटवर्क वास्तुकला"
लिंक पता: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
पढ़ें "Tensor2Tensor घोषणा"- लिंक पता: https://ai.googleblog.com/2017/06/accelerating-dep-learning-research.html
- मॉडल और उसके विवरण को समझने के लिए वीडियो देखें [लुकाज़ कैसर की बातचीत]
लिंक पता: https://www.youtube.com/watch?v=rBCqOTEfxvg
इस कोड को चलाएँ: [ज्यूपिटर नोटबुक Tensor2Tensor रेपो के भाग के रूप में प्रदान किया गया]
- लिंक पता: https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb।
- इस प्रोजेक्ट को देखें: [Tensor2Tensor रेपो]
लिंक पता: https://github.com/tensorflow/tensor2tensor

## आभार
इसका मुख्य रूप से अनुवाद और लेखन हार्बिन इंस्टीट्यूट ऑफ टेक्नोलॉजी के झांग जियान द्वारा किया गया था, और डुओडुओ और डेटाव्हेल प्रोजेक्ट के छात्रों द्वारा पुनर्गठित किया गया था। आखिरकार,आपकी पढ़ने की प्रतिक्रिया और स्टार की प्रतीक्षा में हूं, धन्यवाद।