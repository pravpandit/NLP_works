```अजगर
IPython.display से छवि आयात करें
छवि(फ़ाइलनाम='चित्र/ट्रांसफॉर्मर.पीएनजी')
```

![png](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_0_0.पीएनजी)

यह लेख हार्वर्ड एनएलपी [द एनोटेटेड ट्रांसफॉर्मर] (https://nlp.seas.harvard.edu/2018/04/03/attention.html) से अनुवादित है।
यह लेख मुख्य रूप से 2018 की शुरुआत में हार्वर्ड एनएलपी के विद्वानों द्वारा लिखा गया था, जिसमें लाइन-बाय-लाइन कार्यान्वयन के रूप में पेपर का "एनोटेटेड" संस्करण प्रस्तुत किया गया था।मूल पेपर को व्यवस्थित करना और पूरी प्रक्रिया के दौरान टिप्पणियाँ और एनोटेशन जोड़ना। इस लेख की नोटबुक [अध्याय 2](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs) में पाई जा सकती है। /%E7%AF%87%E7%AB%A02-ट्रांसफॉर्मर%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86) डाउनलोड।

सामग्री संगठन:
- पाइटोरच में एक संपूर्ण ट्रांसफार्मर लिखें
-पृष्ठभूमि
- मॉडल वास्तुकला
- एनकोडर भाग और डिकोडर भाग
-एनकोडर
-डिकोडर
-ध्यान
- एम में ध्यान का आवेदनओडेल
- स्थिति-आधारित फ़ीडफ़ॉरवर्ड नेटवर्क
- एंबेडिंग और सॉफ्टमैक्स
-स्थिति एन्कोडिंग
- पूरा मॉडल
- प्रशिक्षण
-बैच प्रोसेसिंग और मास्क
- प्रशिक्षण पाश
- प्रशिक्षण डेटा और बैच प्रोसेसिंग
-हार्डवेयर और प्रशिक्षण का समय
- अनुकूलक
-नियमितीकरण
- लेबल स्मूथिंग
-उदाहरण
-सिंथेटिक डेटा
- हानि समारोह गणना
- लालची डिकोडिंग
- वास्तविक दृश्य उदाहरण
-निष्कर्ष
-स्वीकृति

# प्रारंभिक कार्य

```अजगर
# !पिप इंस्टॉल http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib स्पेसी टॉर्चटेक्स्ट सीबॉर्न
```

```अजगर
एनपी के रूप में सुन्न आयात करें
मशाल आयात करें
टॉर्च.एनएन को एनएन के रूप में आयात करें
टॉर्च.एनएन.फंक्शनल को एफ के रूप में आयात करें
गणित, कॉपी, समय आयात करें
टॉर्च.ऑटोग्राड से वेरिएबल आयात करें
matplotlib.pyplot को plt के रूप में आयात करें
समुद्री जहाज़ आयात करें
Seaborn.set_context(संदर्भ='बातचीत')
%matplotlib इनलाइन
```

विषयसूची

* विषयसूची
{:टोक}

#पृष्ठभूमि

ट्रांसफार्मर पर अधिक पृष्ठभूमि के लिए, पाठक [अध्याय 2.2 इलस्ट्रेटेड ट्रांसफार्मर](https://) पढ़ सकते हैंgithub.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E %9F%E7%90%86/2.2-%E5%9B%BE%E8%A7%A3transformer.md) सीखने के लिए।

#मॉडल वास्तुकला

अधिकांश अनुक्रम-से-अनुक्रम (seq2seq) मॉडल एक एनकोडर-डिकोडर संरचना का उपयोग करते हैं [(संदर्भ)](https://arxiv.org/abs/1409.0473)। एनकोडर एक इनपुट अनुक्रम $(x_{1}, को मैप करता है। .x_{n})$ से निरंतर प्रतिनिधित्व $z=(z_{1},...z_{n})$ डिकोडर एक आउटपुट अनुक्रम $(y_{1},...y_{m} उत्पन्न करता है।)$ z में प्रत्येक तत्व के लिए। डिकोडर प्रत्येक चरण पर एक आउटपुट उत्पन्न करता है [(संदर्भ)](https://arxiv.org/abs/1308.0850), और अगला परिणाम उत्पन्न करते समय। , पहले उत्पन्न परिणाम को भविष्यवाणी के लिए इनपुट अनुक्रम में जोड़ा गया है अब एक seq2seq आर्किटेक्चर बनाने के लिए एक एनकोडर डिकोडर क्लास बनाएं:

```अजगर
क्लास एनकोडरडिकोडर(एनएन.मॉड्यूल):
"""
मूल एनकोडर-डिकोडर संरचना।
इसके लिए एक मानक एनकोडर-डिकोडर आर्किटेक्चर और कई
ओटीउसके मॉडल.
"""
def __init__(स्वयं, एनकोडर, डिकोडर, src_embed, tgt_embed, जनरेटर):
सुपर(एनकोडरडिकोडर, स्वयं).__init__()
स्व.एनकोडर = एनकोडर
सेल्फ.डिकोडर = डिकोडर
self.src_embed = src_embed
self.tgt_embed = tgt_embed
स्व.जनरेटर = जनरेटर

डीईएफ़ फॉरवर्ड(स्वयं, स्रोत, टीजीटी, स्रोत_मास्क, टीजीटी_मास्क):"नकाबपोश स्रोत और लक्ष्य अनुक्रम लें और संसाधित करें।"
self.decode(self.encode(src, src_mask), src_mask लौटाएं,
टीजीटी, टीजीटी_मास्क)

डीईएफ़ एनकोड(स्वयं, स्रोत, स्रोत_मास्क):
self.encoder(self.src_embed(sr.) लौटाएंसी), src_mask)

डीईएफ़ डिकोड(स्वयं, मेमोरी, src_mask, tgt, tgt_mask):
self.decoder(self.tgt_embed(tgt), मेमोरी, src_mask, tgt_mask) वापस करें
```

```अजगर
क्लास जेनरेटर (एनएन.मॉड्यूल):
"जनरेटर को परिभाषित करें, जो रैखिक और सॉफ्टमैक्स से बना है"
"मानक रैखिक + सॉफ्टमैक्स पीढ़ी चरण को परिभाषित करें।"
def __init__(self, d_model, vocab):
सुपर(जनरेटर, स्वयं).__init__()
self.proj = nn.Linear(d_model, vocab)

डीईएफ़ फॉरवर्ड(स्वयं, एक्स):
वापसी F.log_softmax(self.proj(x), dim=-1)
```

TTransformer का एनकोडर और डिकोडरदोनों को आत्म-ध्यान और पूरी तरह से जुड़ी हुई परतों के साथ जोड़ा गया है, जैसा कि नीचे दिए गए चित्र के बाईं और दाईं ओर दिखाया गया है।

```अजगर
छवि(फ़ाइलनाम='./चित्र/2-ट्रांसफॉर्मर.पीएनजी')
```

![png](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_13_0.पीएनजी)

## एनकोडर भाग और डिकोडर भाग

### एनकोडर

एनकोडर में N = 6 समान परतें होती हैं।

```अजगर
डीईएफ़ क्लोन (मॉड्यूल, एन):
"एन समरूप नेटवर्क परतें तैयार करें"
"एन समान परतें तैयार करें।"
वापसी एनn.ModuleList([copy.depcopy(module) for _ inrange(N)])
```

```अजगर
क्लास एनकोडर (एनएन.मॉड्यूल):
"संपूर्ण एनकोडर में एन परतें होती हैं"
def __init__(स्वयं, परत, N):
सुपर(एनकोडर, स्वयं).__init__()
स्व.परतें = क्लोन(परत, एन)
self.norm = LayerNorm(परत.आकार)

डीईएफ़ फॉरवर्ड (स्वयं, एक्स, मास्क):
"प्रत्येक परत का इनपुट x और मास्क है"
self.layers में परत के लिए:
x = परत(x, मास्क)
self.norm(x) वापस करें
```

प्रत्येक एनकोडर परत में एक सेल्फ अटेंशन सबलेयर और एक एफएफएनएन सबलेयर होता है, जिनमें से प्रत्येक एक का उपयोग करता हैअवशिष्ट कनेक्शन [(उद्धरण)](https://arxiv.org/abs/1512.03385) और परत-सामान्यीकरण [(उद्धरण)](https://arxiv.org/abs/1607.06450)।

```अजगर
क्लास लेयरनॉर्म(एनएन.मॉड्यूल):
"एक लेयरनॉर्म मॉड्यूल का निर्माण करें (विवरण के लिए उद्धरण देखें)।"
def __init__(स्वयं, सुविधाएँ, eps=1e-6):
सुपर(लेयरनॉर्म, सेल्फ).__init__()
self.a_2 = nn.Parameter(torch.ones(features))
self.b_2 = nn.Parameter(मशाल.शून्य(विशेषताएं))
स्वयं.ईपीएस = ईपीएस

डीईएफ़ फॉरवर्ड(स्वयं, एक्स):
माध्य = x.माध्य(-1, Keepdim=Truइ)
std = x.std(-1, Keepdim=True)
वापसी self.a_2 * (x - माध्य) / (std + self.eps) + self.b_2
```

हम सबलेयर को कॉल करते हैं: $\mathrm{Sublayer}(x)$, और प्रत्येक सबलेयर का अंतिम आउटपुट $\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$ है। (http://jmlr.org/papers/v15/srivastava14a.html) को सबलेयर में जोड़ा गया है।

अवशिष्ट कनेक्शन की सुविधा के लिए, मॉडल में सभी उप-परतों और एम्बेडिंग परतों के आउटपुट आयाम $d_{\text{model}}=512$ हैं।

निम्नलिखित SThe ublayerConnection class का उपयोग एकल सबलेयर के आउटपुट को संसाधित करने के लिए किया जाता है, जो अगले सबलेयर में इनपुट होता रहेगा:

```अजगर
क्लास सबलेयरकनेक्शन(एनएन.मॉड्यूल):
"""
एक परत मानदंड के बाद एक अवशिष्ट कनेक्शन।
कोड सरलता के लिए नोट करें कि मानक अंतिम के विपरीत प्रथम है।
"""
def __init__(स्वयं, आकार, ड्रॉपआउट):
सुपर(सबलेयरकनेक्शन, सेल्फ).__init__()
self.norm = लेयरनॉर्म(आकार)
स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(ड्रॉपआउट)

डीईएफ़ फॉरवर्ड (स्वयं, एक्स, सबलेयर):
"टी के साथ किसी भी सबलेयर पर अवशिष्ट कनेक्शन लागू करेंवह एक ही आकार का है।"
वापसी x + self.dropout(sublayer(self.norm(x)))
```

प्रत्येक एनकोडर परत में दो उपपरतें होती हैं। पहली परत एक मल्टी-हेड सेल्फ-अटेंशन परत होती है, और दूसरी परत एक सरल पूरी तरह से कनेक्टेड फीडफॉरवर्ड नेटवर्क होती है। दोनों परतों को सबलेयरकनेक्शन क्लास का उपयोग करके संसाधित करने की आवश्यकता होती है।

```अजगर
क्लास एनकोडरलेयर (एनएन.मॉड्यूल):
"एनकोडर सेल्फ-एटीएन और फीड फॉरवर्ड से बना है (नीचे परिभाषित)"
def __init__(स्वयं, आकार, self_attn, फ़ीड_फॉरवर्ड, ड्रॉपआउट):
सुपर(एनकोडरलेयर, स्वयं).__init__()
self.self_attn = self_attn
सेल्फ.फीड_फॉरवर्ड = फीड_फॉरवर्ड
self.sublayer = क्लोन (SublayerConnection(आकार, ड्रॉपआउट), 2)
स्व.आकार = आकार

डीईएफ़ फॉरवर्ड (स्वयं, एक्स, मास्क):
"कनेक्शन के लिए चित्र 1 (बाएं) का अनुसरण करें।"
x = self.sublayer[0](x, Lambda x: self.self_attn(x, x, x, मास्क))
सेल्फ.सबलेयर लौटाएं[1](x, सेल्फ.फीड_फॉरवर्ड)
```

### डिकोडर

डिकोडर भी N = 6 समान डिकोडर परतों से बना है।

```अजगर
क्लास डिकोडर (एनएन.मॉड्यूल):
"मास्किंग के साथ जेनेरिक एन लेयर डिकोडर।"
def __init__(सेएलएफ, परत, एन):
सुपर(डिकोडर, सेल्फ).__init__()self.layers = क्लोन(लेयर, N)
self.norm = LayerNorm(परत.आकार)

डीईएफ़ फॉरवर्ड (स्वयं, एक्स, मेमोरी, src_mask, tgt_mask):
self.layers में परत के लिए:
x = परत(x, मेमोरी, src_mask, tgt_mask)
self.norm(x) वापस करें
```

सिंगल-लेयर एनकोडर की तुलना में, सिंगल-लेयर डिकोडर में एक तीसरी सबलेयर होती है जो एनकोडर के आउटपुट पर ध्यान केंद्रित करती है: एनकोडर-डिकोडर-ध्यान परत, क्यू वेक्टर डिकोड की पिछली परत के आउटपुट से आता हैएर, और के और वी वेक्टर एनकोडर की अंतिम परत के आउटपुट वेक्टर हैं, एनकोडर के समान, हम प्रत्येक उपलेयर में अवशिष्ट कनेक्शन का उपयोग करते हैं और फिर परत सामान्यीकरण करते हैं।

```अजगर
क्लास डिकोडरलेयर(एनएन.मॉड्यूल):
"डिकोडर सेल्फ-एटीएन, एसआरसी-एटीएन और फीड फॉरवर्ड से बना है (नीचे परिभाषित)"
def __init__(स्वयं, आकार, self_attn, src_attn, फ़ीड_फॉरवर्ड, ड्रॉपआउट):
सुपर(डिकोडरलेयर, स्वयं).__init__()
स्व.आकार = आकार
self.self_attn = self_attn
self.src_attn = src_attn
self.feed_forwaआरडी = फ़ीड_फॉरवर्ड
self.sublayer = क्लोन (SublayerConnection(आकार, ड्रॉपआउट), 3)

डीईएफ़ फॉरवर्ड (स्वयं, एक्स, मेमोरी, src_mask, tgt_mask):
"कनेक्शन के लिए चित्र 1 (दाएं) का अनुसरण करें।"
एम = मेमोरीएक्स = सेल्फ.सबलेयर[0](एक्स, लैम्ब्डा एक्स: सेल्फ.सेल्फ_एटीएन(एक्स, एक्स, एक्स, टीजीटी_मास्क))
x = self.sublayer[1](x, Lambda x: self.src_attn(x, m, m, src_mask))
सेल्फ.सबलेयर लौटाएं[2](x, सेल्फ.फीड_फॉरवर्ड)
```

सिंगल-लेयर डिकोडर में सेल्फ-अटेंशन सबलेयर के लिए, हमें वर्तमान स्थिति को रोकने के लिए मास्क तंत्र का उपयोग करने की आवश्यकता हैअगली स्थिति पर ध्यान देना.

```अजगर
डीईएफ़ अनुवर्ती_मास्क(आकार):
"बाद की स्थितियों को छुपाएं।"
attn_shape = (1, आकार, साइज)
अनुवर्ती_मास्क = np.triu(np.ones(attn_shape), k=1).astype('uint8')
वापसी मशाल.from_numpy(subsequent_mask) == 0
```

> नीचे दिया गया ध्यान मास्क दिखाता है कि प्रशिक्षण के दौरान प्रत्येक tgt शब्द (पंक्ति) को (कॉलम) देखने की अनुमति कहां है, इस शब्द को निम्नलिखित शब्दों पर ध्यान देने से रोकने के लिए वर्तमान शब्द की भविष्य की जानकारी को छुपाया जाता है।

```अजगर

पीएलटी.एफआकृति(अंजीरआकार=(5,5))
plt.imshow(बाद में_मास्क(20)[0])
कोई नहीं
```

![svg](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_30_0.एसवीजी)

### ध्यान

ध्यान फ़ंक्शन को आउटपुट के लिए एक क्वेरी और कुंजी-मानों के एक सेट को मैप करने के रूप में वर्णित किया जा सकता है, जहां क्वेरी, कुंजी, मान और आउटपुट सभी वैक्टर हैं, जहां आउटपुट मानों का भारित योग है प्रत्येक मान की गणना क्वेरी और संबंधित कुंजी द्वारा की जाती है।
हम विशेष करेंगेआयन इसे "स्केल्ड डॉट-प्रोडक्ट अटेंशन" कहता है। इसका इनपुट क्वेरी, कुंजी (आयाम $d_k$ है), और मान (आयाम $d_v$ है) है। हम क्वेरी और सभी कुंजियों के डॉट उत्पाद की गणना करते हैं। फिर प्रत्येक को $\sqrt{d_k}$ से विभाजित करें, और अंत में मूल्य का वजन प्राप्त करने के लिए सॉफ्टमैक्स फ़ंक्शन का उपयोग करें।```पायथन
छवि(फ़ाइलनाम='./चित्र/ट्रांसफॉर्मर-स्वयं-ध्यान.png')
```

![png](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_32_0.पीएनजी)

व्यवहार में, हम एटन की गणना करते हैंएक ही समय में प्रश्नों के एक सेट का फ़ंक्शन और उन्हें मैट्रिक्स $Q$ में संयोजित करें कुंजी और मान को मैट्रिक्स $K$ और $V$ में भी संयोजित किया जाता है। आउटपुट मैट्रिक्स की हम गणना करते हैं:

$$
\mathrm{ध्यान दें}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

```अजगर
ध्यान आकर्षित करें (क्वेरी, कुंजी, मूल्य, मुखौटा = कोई नहीं, ड्रॉपआउट = कोई नहीं):
"'स्केल्ड डॉट उत्पाद ध्यान' की गणना करें"
d_k = query.size(-1)
स्कोर = Torch.matmul(क्वेरी, key.transpose(-2, -1)) \
/ गणित.sqrt(d_k)
यदि मास्क कोई नहीं है:
स्कोर =स्कोर.मास्कड_फिल(मास्क == 0, -1e9)
p_attn = F.softmax(स्कोर, मंद = -1)
यदि ड्रॉपआउट कोई नहीं है:
p_attn = ड्रॉपआउट(p_attn)
returntorch.matmul(p_attn, value), p_attn
```

&#8195;&#8195;दो सबसे अधिक उपयोग किए जाने वाले ध्यान कार्य हैं:
- अतिरिक्त ध्यान[(उद्धरण)](https://arxiv.org/abs/1409.0473)
- डॉट उत्पाद (गुणा) ध्यान

डॉट उत्पाद ध्यान हमारे सामान्य डॉट उत्पाद एल्गोरिथ्म के समान है, स्केलिंग कारक $\frac{1}{\sqrt{d_k}}$ को छोड़कर, योगात्मक ध्यान एक फ़ीड-फॉरवार का उपयोग करता हैसमानता की गणना करने के लिए एकल छिपी हुई परत के साथ डी नेटवर्क। हालांकि डॉट उत्पाद ध्यान और योगात्मक ध्यान में सिद्धांत में समान जटिलता है, व्यवहार में, डॉट उत्पाद ध्यान को अत्यधिक अनुकूलित मैट्रिक्स गुणन का उपयोग करके कार्यान्वित किया जा सकता है, इसलिए डॉट उत्पाद ध्यान तेज और अधिक स्थान-कुशल है। गणना करना।
जब $d_k$ छोटा होता है, तो दोनों तंत्रों का प्रदर्शन समान होता है, जब $d_k$ बड़ा होता है, तो योगात्मक ध्यान अनस्केल्ड डॉट उत्पाद ध्यान से बेहतर प्रदर्शन करता है [(उद्धरण)](https://arxiv.org/abs/1703.03906)। हमें संदेह है कि $d_k$ के बड़े मूल्यों के लिए, डॉट उत्पाद नाटकीय रूप से बढ़ता है, सॉफ्टमैक्स फ़ंक्शन को बहुत छोटे ग्रेडिएंट वाले क्षेत्र में धकेलता है (यह देखने के लिए कि डॉट क्यों है उत्पाद बढ़ता है, मान लें कि q और k माध्य 0 और विचरण 1 के साथ स्वतंत्र यादृच्छिक चर हैं। फिर उनके बिंदु उत्पाद $q \cdot k = \sum_{i=1}^{d_k} q_ik_i$ का माध्य 0 और विचरण $d_k$ है ). इस प्रभाव का प्रतिकार करने के लिए, हम डॉट उत्पाद को $\frac{1}{\sqrt{d_k}}$ तक कम करते हैं।

यहाँ सु जे का एक उद्धरण हैIanlin का लेख ["ट्रांसफॉर्मर इनिशियलाइज़ेशन, पैरामीटर और मानकीकरण पर एक संक्षिप्त चर्चा"] (https://zhuanlan.zhihu.com/p/400925524?utm_source=wechat_session&utm_medium=social&utm_oi=140082341713968 विभाजित करना $\sqrt{d}$ ध्यान में?

```अजगर
छवि(फ़ाइलनाम='चित्र/ट्रांसफॉर्मर-रेखीय.पीएनजी')
```![png](2.2.1-पाइटोरच%E7%BC%96%E5%86%99Transformer_files/2.2.1-पाइटोरच%E7%BC%96%E5%86%99ट्रांसफॉर्मर_37_0.png)

मल्टी-हेड ध्यान अलयदि केवल एक ही ध्यान शीर्ष है, तो वेक्टर की प्रतिनिधित्व क्षमता कम हो जाएगी।

$$
\mathrm{मल्टीहेड}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\
\text{where}~\mathrm{head_i} = \mathrm{ध्यान}(QW^Q_i, KW^K_i, VW^V_i)
$$मैपिंग वेट मैट्रिक्स द्वारा की जाती है: $W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{\text{मॉडल}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ और $W^O \in \ Mathbb{R}^{hd_v \times d_{\text{model}}}$.

इस कार्य में, हम $h=8$ समानांतर ध्यान परतों या शीर्षों का उपयोग करते हैं। इनमें से प्रत्येक शीर्ष के लिए, हम कम आयाम के कारण $d_k=d_v=d_{\text{model}}/h=64$ का उपयोग करते हैं प्रत्येक शीर्ष पर, कुल कम्प्यूटेशनल लागत पूर्ण आयामीता के साथ एकल शीर्ष ध्यान के समान है।

```अजगर
क्लास मल्टीहेडेडअटेंशन(एनएन.मॉड्यूल):
def __init__(स्वयं, h, d_model, ड्रॉपआउट=0.1):
"मॉडल का आकार और सिरों की संख्या लें।"
सुपर(मल्टीहेडेडअटेंशन, स्वयं).__init__()
जोर से d_model % h == 0
# हम मानते हैं कि d_v हमेशा d_k के बराबर होता है
self.d_k = d_model // h
स्वयं.एच = एच
self.linears = क्लोन्स(nn.Linear(d_model, d_model), 4)
self.attn = कोई नहीं
स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(पी=ड्रॉपआउट)

डीईएफ़ फॉरवर्ड (स्वयं, क्वेरी, कुंजी, मान, मुखौटा = कोई नहीं):
"चित्रा 2 लागू करें"
यदि मास्क नहीं है तो कोई नहीं: # सभी सिरों पर एक ही मास्क लगाया जाता है।
मुखौटा = मुखौटा.खोलना(1)
nbatches = query.size(0)

#1) सभी लीनियर करेंd_model => h x d_k से बैच में अनुमान
क्वेरी, कुंजी, मान = \
[l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
ज़िप में l, x के लिए (self.linears, (क्वेरी, कुंजी, मान))]

#2) बैच में सभी प्रक्षेपित वैक्टरों पर ध्यान दें।
x, self.attn = ध्यान (क्वेरी, कुंजी, मूल्य, मुखौटा = मुखौटा, ड्रॉपआउट = self.ड्रॉपआउट)

#3) एक दृश्य का उपयोग करके "कॉनकैट" करें और एक अंतिम रैखिक लागू करें।
x = x.transpose(1, 2).contiguous() \
.view(nbatches, -1, self.h * self.d_k)
स्व.रेखीय वापसी[-1](x)
```

### का आवेदनमॉडल में ध्यान दें

ट्रांसफार्मर में मल्टी-हेड अटेंशन का उपयोग तीन अलग-अलग तरीकों से किया जाता है:
- एनकोडर-डिकोडर ध्यान परत में, प्रश्न पिछली डिकोडर परत से आते हैं, और कुंजी और मान एनकोडर के आउटपुट से आते हैं, यह डिकोडर में प्रत्येक स्थिति को इनपुट अनुक्रम में सभी स्थितियों पर ध्यान देने की अनुमति देता है डिकोडर में प्रत्येक स्थिति को इनपुट अनुक्रम में सभी स्थितियों पर ध्यान देने की अनुमति देता है, अनुक्रम मॉडल में विशिष्ट एनकोडर-डिकोडर ध्यान तंत्र, जैसे कि [(उद्धरण)](https://arxiv.org/abs/)। 1609.08144)।

- एनकोडर में एक आत्म-ध्यान परत होती है। आत्म-ध्यान परत में, सभी कुंजियाँ, मान और क्वेरीज़ एक ही स्थान से आती हैं, अर्थात् एनकोडर में पिछली परत का आउटपुट, एनकोडर में प्रत्येक स्थिति भुगतान कर सकती है एनकोडर की पिछली परत की सभी स्थितियों पर ध्यान दें।

- इसी तरह, डिकोडर में स्व-ध्यान परत डिकोडर में प्रत्येक स्थिति को मुख्य करने के लिए डिकोडर परत में वर्तमान स्थिति को शामिल करने से पहले सभी स्थितियों पर ध्यान देने की अनुमति देती हैडिकोडर की ऑटोरेग्रेसिव संपत्ति को नियंत्रित करने के लिए, डिकोडर में जानकारी को बाईं ओर बहने से रोकना आवश्यक है। हम सॉफ्टमैक्स इनपुट ($-\infty$ पर सेट) में सभी अवैध कनेक्शन मानों को मास्क करके इसे प्राप्त करते हैं स्केल्ड डॉट उत्पाद ध्यान।### स्थिति-आधारित फीडफॉरवर्ड नेटवर्क

ध्यान उपलेयर को छोड़कर, हमारे एनकोडर और डिकोडर की प्रत्येक परत में एक पूरी तरह से जुड़ा हुआ फीडफॉरवर्ड नेटवर्क होता है, जो प्रत्येक परत में एक ही स्थिति में होता है (प्रत्येक एनकोडर के अंत में)-लेयर या डिकोडर-लेयर) फीडफॉरवर्ड नेटवर्क में बीच में एक ReLU सक्रियण फ़ंक्शन के साथ दो रैखिक परिवर्तन होते हैं।

$$\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2$$

हालाँकि दोनों परतें रैखिक परिवर्तन हैं, वे परतों के बीच विभिन्न मापदंडों का उपयोग करते हैं। इसका वर्णन करने का दूसरा तरीका कर्नेल आकार 1 के साथ दो कनवल्शन हैं। इनपुट और आउटपुट के आयाम $d_{\text{model}}=512$ हैं। आंतरिक परत का आयाम $d_{ff}=2048$ है (अर्थात, पहली परत का इनपुट 51 है2 आयाम और आउटपुट 2048 आयाम है; दूसरी परत इनपुट 2048 आयाम है और आउटपुट 512 आयाम है)

```अजगर
क्लास पोजिशनवाइजफीडफॉरवर्ड(एनएन.मॉड्यूल):
"एफएफएन समीकरण लागू करता है।"
def __init__(स्वयं, d_model, d_ff, ड्रॉपआउट=0.1):
सुपर(स्थितिवार फ़ीडफॉरवर्ड, स्वयं).__init__()
self.w_1 = nn.रैखिक(d_model, d_ff)
self.w_2 = nn.रैखिक(d_ff, d_model)
स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(ड्रॉपआउट)

डीईएफ़ फॉरवर्ड(स्वयं, एक्स):
self.w_2(self.dropout(F.relu(self.w_1(x)))) लौटाएं
```

## एम्बेडिंगऔर सॉफ्टमैक्स

मॉडल के समान अन्य seq2seq के साथ, हम इनपुट और आउटपुट टोकन को $d_{\text{model}}$ आयामों के वैक्टर में परिवर्तित करने के लिए सीखे गए एम्बेडिंग का उपयोग करते हैं। हम डिकोडर आउटपुट को परिवर्तित करने के लिए एक सामान्य रैखिक परिवर्तन और एक सॉफ्टमैक्स फ़ंक्शन का भी उपयोग करते हैं अगले टोकन की भविष्यवाणी करने की संभावना में, हमारे मॉडल में, दो एम्बेडिंग परतें प्री-सॉफ्टमैक्स रैखिक परिवर्तन के समान वजन मैट्रिक्स साझा करती हैं, [(उद्धरण)](https://arxiv.org/abs/1608.05859) के समान। .एम्बेडिंग ला मेंयार, हम इन वज़न को $\sqrt{d_{\text{model}}}$ से गुणा करते हैं।

```अजगर
क्लास एंबेडिंग्स (एनएन.मॉड्यूल):
def __init__(self, d_model, vocab):
सुपर(एंबेडिंग, सेल्फ).__init__()self.lut = nn.एंबेडिंग(शब्दावली, d_model)
self.d_model = d_model

डीईएफ़ फॉरवर्ड(स्वयं, एक्स):
self.lut(x) * गणित.sqrt(self.d_model) वापस करें
```

## स्थितीय एन्कोडिंग
&#8195;&#8195;चूंकि हमारे मॉडल में लूप और कन्वोल्यूशन शामिल नहीं हैं, मॉडल को अनुक्रम के क्रम का लाभ उठाने के लिए, हमें कुछ जानकारी जोड़नी होगीअनुक्रम में टोकन की सापेक्ष या निरपेक्ष स्थिति के बारे में, ऐसा करने के लिए, हम एनकोडर और डिकोडर स्टैक के नीचे इनपुट एम्बेडिंग में एक "पोजीशनल एन्कोडिंग" जोड़ते हैं। पोजिशनल एन्कोडिंग का आयाम एम्बेडिंग के समान होता है। जो $d_{\text{model}}$ भी है, इसलिए दो वैक्टर जोड़े जा सकते हैं। चुनने के लिए कई स्थितीय एन्कोडिंग हैं, जैसे सीखने से प्राप्त स्थितिगत एन्कोडिंग और निश्चित स्थितिगत एन्कोडिंग [(उद्धरण)]( https://arxiv.org/pdf/1705.03122.pdf).

&#8195;&#8195;इस कार्य में, हम विभिन्न आवृत्तियों के साइन और कोसाइन फ़ंक्शन का उपयोग करते हैं:$$PE_{(pos,2i)} = syn(pos / 10000^{2i/d_{\text{model}} })$$

$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})$$
&#8195;&#8195;जहां $pos$ स्थिति है और $i$ आयाम है। यानी, स्थिति एन्कोडिंग का प्रत्येक आयाम एक साइन तरंगरेखा से मेल खाता है। ये तरंग दैर्ध्य $2\pi$ से एक सामूहिक श्रृंखला बनाते हैं $10000 \cdot 2\pi$ हमने यह फ़ंक्शन चुना क्योंकि हम मानते हैं कि यह बनेगाई मॉडल के लिए सापेक्ष स्थितियों पर ध्यान देना सीखना आसान है, क्योंकि किसी भी निश्चित ऑफसेट $k$ के लिए, $PE_{pos+k}$ को $PE_{pos+k}$ के रैखिक फ़ंक्शन के रूप में व्यक्त किया जा सकता है।

&#8195;&#8195;इसके अलावा, हम एनकोडर और डिकोडर स्टैक में एम्बेडिंग और स्थिति एन्कोडिंग के योग में एक ड्रॉपआउट जोड़ देंगे, मूल मॉडल के लिए, हम $P_{drop}=0.1 के ड्रॉपआउट अनुपात का उपयोग करते हैं $.

```अजगर
क्लास पोजिशनलएन्कोडिंग(एनएन.मॉड्यूल):
"पीई फ़ंक्शन लागू करें।"
def __init__(स्वयं, d_model, ड्रॉपआउट, max_len=5000):सुपर(पोजीशनल एन्कोडिंग, स्वयं).__init__()
स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(पी=ड्रॉपआउट)

# लॉग स्पेस में एक बार स्थितीय एन्कोडिंग की गणना करें।
पीई = टॉर्च.शून्य (मैक्स_लेन, डी_मॉडल)
स्थिति = टॉर्च.अरेंज(0, max_len).अनस्क्वीज़(1)
div_term = torch.exp(torch.arange(0, d_model, 2) *
-(math.log(10000.0) / d_model))pe[:, 0::2] = मशाल.पाप(स्थिति * div_term)
पीई[:, 1::2] = मशाल.कोस(स्थिति * div_term)
पे = पे.अनस्क्वीज़(0)
self.register_buffer('पे', पे)

डीईएफ़ फॉरवर्ड(स्वयं, एक्स):
x = x + वेरिएबल(self.pe[:, :x.sizई(1)],
आवश्यकता_ग्रेड=गलत)
स्व.ड्रॉपआउट(x) वापस करें
```

> जैसा कि नीचे दिखाया गया है, स्थितिगत एन्कोडिंग स्थिति के आधार पर साइन तरंगें जोड़ेगी। प्रत्येक आयाम के लिए तरंग की आवृत्ति और ऑफसेट अलग-अलग हैं।

```अजगर
plt.figure(figsize=(15,5))
पीई = पोजिशनल एन्कोडिंग(20, 0)
y = pe.forward(चर(मशाल.शून्य(1, 100, 20)))
plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())
plt.legend(["dim %d"%p for p in [4,5,6,7]])
कोई नहीं
```

![svg](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइल्स/2.2.1-पाइटोरेक%E7%BC%96%E5%86%99ट्रांसफॉर्मर_48_0.svg)

हमने निश्चित स्थितिगत एन्कोडिंग के बजाय सीखा स्थितिगत एम्बेडिंग [(उद्धरण)] (https://arxiv.org/pdf/1705.03122.pdf) का उपयोग करने का भी प्रयास किया और पाया कि दोनों विधियों ने लगभग समान परिणाम दिए क्योंकि हमने साइनसॉइडल संस्करण को चुना यह मॉडल को प्रशिक्षण के दौरान सामने आए अनुक्रमों की तुलना में लंबे अनुक्रमों को सामान्यीकृत करने की अनुमति दे सकता है।

## पूर्ण मॉडल

> यहां, हम हाइपरपैरामीटर से पूर्ण मॉडल तक एक फ़ंक्शन को परिभाषित करते हैं।

```अजगर
def मेक_मॉडल(src_vocab, tgt_vocab, एन=6,
d_model=512, d_ff=2048,h=8, ड्रॉपआउट=0.1):
"सहायक: हाइपरपैरामीटर से एक मॉडल बनाएं।"
सी = कॉपी.डीपकॉपी
ध्यान दें = मल्टीहेडेडअटेंशन(एच, डी_मॉडल)
एफएफ = पोजिशनवाइजफीडफॉरवर्ड (डी_मॉडल, डी_एफएफ, ड्रॉपआउट)
स्थिति = पोजिशनल एन्कोडिंग(d_model, ड्रॉपआउट)
मॉडल = एनकोडरडिकोडर(
एनकोडर(एनकोडरलेयर(d_model, c(attn), c(ff), ड्रॉपआउट), N),
डिकोडर(डिकोडरलेयर(d_model, c(attn), c(attn),
सी(एफएफ), ड्रॉपआउट), एन),
एनएन.अनुक्रमिक(एम्बेडिंग्स(डी_मॉडल, src_vocab), सी(स्थिति)),
एन.एन.अनुक्रमिक (एम्बेड)डिंग्स(d_model, tgt_vocab), c(स्थिति)),
जेनरेटर(d_model, tgt_vocab))

# ये उनके कोड से अहम था.
# ग्लोरोट / फैन_एवीजी के साथ पैरामीटर आरंभ करें।
मॉडल.पैरामीटर() में पी के लिए:
यदि p.dim() > 1:
nn.init.xavier_uniform(p)
वापसी मॉडल
```

```अजगर
# छोटा उदाहरण मॉडल.
tmp_model = make_model(10, 10, 2)
कोई नहीं
```

/var/folders/2k/x3py0v857kgcwqvvl00xxhxw0000gn/T/ipykernel_27532/2289673833.py:20: उपयोगकर्ता चेतावनी: nn.init.xavier_uniform को अब nn.init.xavier_uniform_ के पक्ष में हटा दिया गया है।
एनn.init.xavier_uniform(p)

#प्रशिक्षण

यह अनुभाग हमारे मॉडल के प्रशिक्षण यांत्रिकी का वर्णन करता है।

> हम यहां जल्दी से कुछ उपकरण पेश करते हैं जिनका उपयोग एक मानक एनकोडर-डिकोडर मॉडल को प्रशिक्षित करने के लिए किया जाता है, सबसे पहले, हम एक बैच ऑब्जेक्ट को परिभाषित करते हैं जिसमें प्रशिक्षण के लिए स्रोत और लक्ष्य वाक्य, साथ ही बिल्डिंग मास्क भी शामिल होते हैं।

## बैचिंग और मास्किंग

```अजगर
क्लास बैच:
"प्रशिक्षण के दौरान मास्क के साथ डेटा का एक बैच रखने पर आपत्ति।"
def __init__(स्वयं, src, trg=कोई नहीं, पैड=0):
self.src = src
self.src_mask = (src != पैड).अनस्क्वीज़(-2)
यदि trg कोई नहीं है: self.trg = trg[:, :-1]
self.trg_y = trg[:, 1:]
स्वयं.trg_mask = \
self.make_std_mask(self.trg, पैड)
self.ntokens = (self.trg_y != पैड).data.sum()

@staticmethod
def make_std_mask(tgt, पैड):
"पैडिंग और भविष्य के शब्दों को छिपाने के लिए एक मुखौटा बनाएं।"
tgt_mask = (tgt != पैड).अनस्क्वीज़(-2)
tgt_mask = tgt_mask और वेरिएबल(
अनुवर्ती_मास्क(tgt.आकार(-1)).type_as(tgt_mask.data))
वापसीtgt_mask
```> इसके बाद हम ओ पर नज़र रखने के लिए एक सामान्य प्रशिक्षण और मूल्यांकन फ़ंक्शन बनाते हैंएफ हानि। हम एक सामान्य हानि फ़ंक्शन पास करते हैं और पैरामीटर अपडेट के लिए भी इसका उपयोग करते हैं।

## ट्रेनिंग लूप

```अजगर
def run_epoch(data_iter, मॉडल, loss_compute):
"मानक प्रशिक्षण और लॉगिंग फ़ंक्शन"
प्रारंभ = समय.समय()
कुल_टोकन = 0
कुल हानि = 0
टोकन = 0
i के लिए, enumerate(data_iter) में बैच:
आउट = मॉडल.फॉरवर्ड(बैच.src, बैच.trg,
बैच.src_mask, बैच.trg_mask)
हानि = हानि_गणना(बाहर, बैच.trg_y, बैच.एनटोकेंस)
टोटल_लॉस += लॉसटोटल_टोकेंस += बैच.एनटोकेंस
टोकन += बैच.एनटोकएन एस
यदि मैं % 50 == 1:
बीता हुआ = समय.समय()-प्रारंभ
प्रिंट करें ("युग चरण: %d हानि: %f टोकन प्रति सेकंड: %f" %
(i, हानि / बैच.एनटोकेंस, टोकन / बीता हुआ))
प्रारंभ = समय.समय()
टोकन = 0
कुल_नुकसान/कुल_टोकन लौटाएं
```

## प्रशिक्षण डेटा और बैचिंग
&#8195;&#8195;हमने मानक WMT 2014 अंग्रेजी-जर्मन डेटासेट पर प्रशिक्षण लिया जिसमें लगभग 4.5 मिलियन वाक्य जोड़े हैं, ये वाक्य बाइट जोड़ी एन्कोडिंग का उपयोग करके एन्कोड किए गए हैं, और स्रोत और लक्ष्य वाक्य लगभग 37,000 टोकन की शब्दावली साझा करते हैंअंग्रेजी-फ़्रेंच अनुवाद, हमने काफी बड़े WMT 2014 अंग्रेजी-फ़्रेंच डेटासेट का उपयोग किया, जिसमें 36 मिलियन वाक्य हैं और टोकन 32,000 शब्द-टुकड़ों में विभाजित हैं
प्रत्येक प्रशिक्षण बैच में वाक्य जोड़े का एक सेट होता है, जो समान अनुक्रम लंबाई के अनुसार बैच किया जाता है। वाक्य जोड़े के प्रत्येक प्रशिक्षण बैच में लगभग 25,000 स्रोत भाषा टोकन और 25,000 लक्ष्य भाषा टोकन होते हैं।

> हम बैचिंग के लिए टॉर्च टेक्स्ट का उपयोग करेंगे (बाद में अधिक विस्तार से चर्चा की जाएगी)।यह सुनिश्चित करने के लिए कि हम जिस बैच का आकार अधिकतम मान तक भरते हैं वह एक सीमा (25,000 यदि हमारे पास 8 जीपीयू है तो 25,000) से अधिक न हो, यह सुनिश्चित करने के लिए टार्चटेक्स्ट फ़ंक्शन में चेक करें।

```अजगर
वैश्विक max_src_in_batch, max_tgt_in_batch
डीईएफ़ बैच_आकार_एफएन(नया, गिनती, अब तक):
"बैच बढ़ाते रहें और टोकन + पैडिंग की कुल संख्या की गणना करें।"
वैश्विक max_src_in_batch, max_tgt_in_batch
यदि गिनती == 1:
max_src_in_batchtch = 0
max_tgt_in_batch = 0
max_src_in_batch = max(max_src_in_batch, len(new.src))
max_tgt_in_batch = अधिकतम(max_tgt_in_बैच, लेन(new.trg) + 2)
src_elements = गिनती * max_src_in_batch
tgt_elements = गिनती * max_tgt_in_batch
अधिकतम वापसी(src_elements, tgt_elements)
```

## हार्डवेयर और प्रशिक्षण का समय
हमने अपने मॉडलों को 8 NVIDIA P100 GPU के साथ एक मशीन पर प्रशिक्षित किया। पेपर में वर्णित हाइपरपैरामीटर का उपयोग करके प्रत्येक प्रशिक्षण चरण में बेस मॉडल के लिए लगभग 0.4 सेकंड का समय लगा। हमने बेस मॉडल को कुल 100,000 चरणों या 12 घंटों के लिए प्रशिक्षित किया मॉडल, प्रत्येक चरण में 1.0 सेकंड लगे और बड़े मॉडलों को प्रशिक्षित किया गयाया 300,000 कदम (3.5 दिन)।

## अनुकूलक

WeThe एडम ऑप्टिमाइज़र का उपयोग [(उद्धरण)](https://arxiv.org/abs/1412.6980) $\beta_1=0.9$, $\beta_2=0.98$ और $\epsilon=10^{-9}$ के साथ किया जाता है। हम प्रशिक्षण के दौरान सीखने की दर को निम्नलिखित सूत्र के अनुसार बदलते हैं:
$$
lrate = d_{\text{model}}^{-0.5} \cdot\min({step\_num}^{-0.5},{step\_num} \cdot {warmup\_steps}^{-1.5})$$यह पहले $warmup\_steps$ चरणों में सीखने की दर को रैखिक रूप से बढ़ाने और बाद में इसे चरणों की संख्या के वर्गमूल के अनुपात में कम करने के अनुरूप है। हम $warmup\_steps=4000$ का उपयोग करते हैं।

> नोट: यह हिस्सा इस मॉडल सेटअप के साथ प्रशिक्षित करना आवश्यक है।

```अजगर

क्लासनोमऑप्ट:
"ऑप्टिम रैपर जो दर लागू करता है।"

डीईएफ़__init__(स्वयं, मॉडल_आकार, कारक, वार्मअप, अनुकूलक):
self.optimizer = अनुकूलक
स्व._चरण = 0
सेल्फ.वार्मअप = वार्मअप
स्व.कारक = कारक
स्वयं.मॉडल_आकार = मॉडल_आकार
स्व._दर = 0

डीईएफ़ चरण(स्वयं):
"पैरामीटर और दर अपडेट करें"
स्व._चरण += 1
दर = स्व.दर()
self.optimizer.param_groups में p के लिए:
पी['एलआर'] = दर
स्व._दर = दर
self.optimizer.step()

डीईएफ़ दर (स्वयं, चरण = कोई नहीं):
"ऊपर `lrate` लागू करें"
यदि चरण कोई नहीं है:
चरण = स्व._चरण
स्व.कारक लौटाएँ * \
(स्वयं.मॉडल_आकार ** (-0.5) *न्यूनतम(चरण ** (-0.5), चरण * स्व.वार्मअप ** (-1.5)))

def get_std_opt(मॉडल):
वापसी NoamOpt(model.src_embed[0].d_model, 2, 4000,
Torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))
```

> विभिन्न मॉडल आकारों और अनुकूलित हाइपरपैरामीटरों के लिए इस मॉडल के कुछ उदाहरण वक्र यहां दिए गए हैं।

```अजगर
# Lrate हाइपरपैरामीटर की तीन सेटिंग्स।
opts = [NoamOpt(512, 1, 4000, कोई नहीं),
NoamOpt(512, 1, 8000, कोई नहीं),
NoamOpt(256, 1,4000, कोई नहीं)]
plt.plot(np.arange(1,20000), [[opt.raटीई(आई) फॉर ऑप्ट इन ऑप्ट्स] फॉर आई इन रेंज(1, 20000)])
plt.legend(["512:4000", "512:8000", "256:4000"])
कोई नहीं
```

![svg](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_68_0.एसवीजी)

## नियमितीकरण
### लेबल स्मूथिंग

प्रशिक्षण के दौरान, हमने $\epsilon_{ls}=0.1$ [(उद्धरण)](https://arxiv.org/abs/1512.00567) के लेबल स्मूथिंग मान का उपयोग किया, हालांकि लेबल को स्मूथिंग करने से मॉडल भ्रमित हो जाता है, लेकिन इससे सटीकता में सुधार होता है BLEU स्कोर.

> हम लेबल लागू करने के लिए केएल डिव लॉस का उपयोग करते हैंस्मूथिंग। एक-हॉट वितरण का उपयोग करने के बजाय, हम एक वितरण बनाते हैं जो लक्ष्य वितरण को 1-स्मूथिंग पर सेट करता है और शेष संभावनाओं को शब्दावली में अन्य शब्दों को निर्दिष्ट करता है।

```अजगर
क्लास लेबलस्मूथिंग(एनएन.मॉड्यूल):
"लेबल स्मूथिंग लागू करें।"
def __init__(स्वयं, आकार, पैडिंग_idx, स्मूथिंग=0.0):
सुपर(लेबलस्मूथिंग, सेल्फ).__init__()
स्व.मानदंड = nn.KLDivLoss(size_average=गलत)
self.padding_idx = पैडिंग_idx
आत्मविश्वास = 1.0 - सहजता
स्व.स्मूथिंग = एसशांत करना
स्व.आकार = आकार
self.true_dist = कोई नहीं

डीईएफ़ फॉरवर्ड (स्वयं, एक्स, लक्ष्य):
जोर देकर कहें x.size(1) == self.size
true_dist = x.data.clone()
true_dist.fill_(स्वयं.स्मूथिंग / (स्वयं.आकार - 2))
true_dist.scatter_(1, target.data.unsqueeze(1), आत्मविश्वास)
true_dist[:, self.padding_idx] = 0
मास्क = टॉर्च.नॉनजीरो(लक्ष्य.डेटा == सेल्फ.पैडिंग_आईडीएक्स)
यदि मास्क.डिम() > 0:
true_dist.index_fill_(0, मास्क.स्क्वीज़(), 0.0)
self.true_dist = true_dist
स्व.मानदंड लौटाएं(x, वेरिएबल(true_dist, require_grad=गलत))
```

आइए स्मूथिंग के बाद वास्तविक संभाव्यता वितरण को देखने के लिए एक उदाहरण देखें।

```अजगर
#लेबल स्मूथिंग का उदाहरण.
क्रिट = लेबलस्मूथिंग(5, 0, 0.4)
भविष्यवाणी = मशाल.फ्लोटटेन्सर([[0, 0.2, 0.7, 0.1, 0],
[0, 0.2, 0.7, 0.1, 0],
[0, 0.2, 0.7, 0.1, 0]])
v = क्रिट(वैरिएबल(भविष्यवाणी.लॉग()),
वेरिएबल(मशाल.लॉन्गटेन्सर([2,1,0])))

# सिस्टम द्वारा अपेक्षित लक्ष्य वितरण दिखाएं।
plt.imshow(crit.true_dist)
कोई नहीं
```

/ उपयोगकर्ता/niepig/डेस्कटॉप/zhihu/learn-nlp-with-ट्रांसफॉर्मर/venv/lib/Python3.8/site-packages/torch/nn/_reduction.py:42: उपयोगकर्ता चेतावनी: आकार_औसत और कम करने वाले तर्कों को हटा दिया जाएगा, कृपया इसके बजाय कमी = 'योग' का उपयोग करें।
चेतावनियाँ.चेतावनी(warning.format(ret))

![svg](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइलें/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_73_1.svg)

```अजगर
प्रिंट(crit.true_dist)
```

टेंसर([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],
[0.0000, 0.6000, 0.1333, 0.1333, 0.1333],
[0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])

एसएमओ लेबल के अस्तित्व के कारणकुछ भी नहीं, यदि मॉडल किसी शब्द के बारे में विशेष रूप से आश्वस्त है और विशेष रूप से बड़ी संभावना को आउटपुट करता है, तो उसे दंडित किया जाएगा, जैसा कि निम्नलिखित कोड में दिखाया गया है, जैसे-जैसे इनपुट x बढ़ता है, x/d बड़ा और बड़ा होता जाएगा, 1/d बन जाएगा छोटा और छोटा, लेकिन नुकसान हमेशा कम नहीं हो रहा है।

```अजगर
क्रिट = लेबलस्मूथिंग(5, 0, 0.1)
डीईएफ़ हानि(एक्स):
d=x+3*1
भविष्यवाणी = मशाल.फ्लोटटेन्सर([[0, एक्स/डी, 1/डी, 1/डी, 1/डी],
])
#प्रिंट(भविष्यवाणी)
रिटर्न क्रिट (वेरिएबल (भविष्यवाणी.लॉग()),
वेरिएबल(मशाल.लोngTensor([1]))).आइटम()

y = [सीमा (1, 100) में x के लिए हानि(x)]
x = np.arange(1, 100)
plt.प्लॉट(x, y)

```

[<matplotlib.lines.Line2D at 0x7f7fad46c970>]

![svg](2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_फाइल्स/2.2.1-पाइटोरच%ई7%बीसी%96%ई5%86%99ट्रांसफॉर्मर_76_1.svg)

#उदाहरण

> हम एक साधारण प्रतिलिपि कार्य का प्रयास करके शुरुआत कर सकते हैं, एक छोटी शब्दावली से यादृच्छिक इनपुट प्रतीकों के एक सेट को देखते हुए, लक्ष्य उन्हीं प्रतीकों को उत्पन्न करना है।

## सिंथेटिक डेटा

```अजगर
डीईएफ़ डेटा_जेन (वी, बैच, एनबैच):
"उत्पन्न भागाsrc-tgt कॉपी कार्य के लिए डोम डेटा।"
रेंज में i के लिए (nbatches):
डेटा = torch.from_numpy(np.random.randint(1, V, आकार=(बैच, 10)))
डेटा[:, 0] = 1
src = वेरिएबल (डेटा, require_grad=गलत)
tgt = परिवर्तनीय(डेटा, require_grad=गलत)
यील्डबैच(src, tgt, 0)
```

## हानि फ़ंक्शन गणना

```अजगर
क्लास SimpleLossCompute:
"एक साधारण हानि गणना और ट्रेन फ़ंक्शन।"
def __init__(स्वयं, जनरेटर, मानदंड, ऑप्ट=कोई नहीं):
स्व.जनरेटर = जनरेटर
स्व.मानदंड = कसौटी
self.opt = opt

def __कॉल__(स्वयं, एक्स, वाई, मानक):
x = स्व.जनरेटर(x)
हानि = स्व.मानदंड(x.सन्निहित().देखें(-1, x.आकार(-1)),
y.contiguous().view(-1)) / मानदंड
हानि.पिछड़ा()
यदि self.opt कोई नहीं है:
self.opt.step()
self.opt.optimizer.zero_grad()
वापसी हानि.आइटम() * मानदंड
```

## लालची डिकोडिंग

```अजगर
# सरल प्रतिलिपि कार्य को प्रशिक्षित करें।
वी=11
मानदंड = लेबल स्मूथिंग (आकार = वी, पैडिंग_आईडीएक्स = 0, स्मूथिंग = 0.0)
मॉडल = मेक_मॉडल(वी, वी, एन=2)
model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,
मशाल.ऑप्टिम.एडम(मॉडल.पैरामेटers(), lr=0, betas=(0.9, 0.98), eps=1e-9))

श्रेणी में युग के लिए (10):
मॉडल.ट्रेन()
run_epoch(data_gen(V, 30, 20), मॉडल, SimpleLossCompute(model.generator, criterion, model_opt))
मॉडल.eval()
प्रिंट(run_epoch(data_gen(V, 30, 5), मॉडल,
SimpleLossCompute(मॉडल.जनरेटर, मानदंड, कोई नहीं)))
```

> सरलता के लिए, यह कोड अनुवाद की भविष्यवाणी करने के लिए लालची डिकोडिंग का उपयोग करता है।

```अजगर
डीईएफ़ लालची_डीकोड(मॉडल, स्रोत, स्रोत_मास्क, मैक्स_लेन, प्रारंभ_प्रतीक):
मेमोरी = मॉडल.एनकोड(src, src_mask)
ys = टॉर्च.ओन्स(1,1).fill_(start_symbol).type_as(src.data)
रेंज में i के लिए (max_len-1):
आउट = मॉडल.डीकोड(मेमोरी, src_mask, वेरिएबल(ys),
वेरिएबल(बाद में_मास्क(ys.size(1))
.type_as(src.data)))
प्रोब = मॉडल.जेनरेटर(बाहर[:, -1])
_, अगला_शब्द = टॉर्च.मैक्स(संभावना, मंद = 1)
अगला_शब्द = अगला_शब्द.डेटा[0]
ys = मशाल.कैट([ys,
Torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
वापसी ys

मॉडल.eval()
src = वेरिएबल(मशाल.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )
src_mask = वेरिएबल(torch.ones(1, 1, 10) )
प्रिंट(जीआरeedy_decode(मॉडल, src, src_mask, max_len=10, प्रारंभ_प्रतीक=1))
```

टेंसर([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])

# वास्तविक दृश्य उदाहरण
चूंकि मूल ज्यूपिटर के वास्तविक डेटा दृश्य के लिए मल्टी-जीपीयू प्रशिक्षण की आवश्यकता होती है, इसलिए इस ट्यूटोरियल में फिलहाल इसे शामिल नहीं किया जाएगा। इच्छुक पाठक [मूल ट्यूटोरियल](https://nlp.seas.harvard.edu/) को पढ़ना जारी रख सकते हैं। 2018/04/03/attention.html)। इसके अलावा, चूंकि वास्तविक डेटा का मूल यूआरएल अमान्य है, इसलिए मूल ट्यूटोरियल चलने में सक्षम नहीं होना चाहिएवास्तविक डेटा दृश्य का कोड.

#निष्कर्ष

अब तक, हमने एक संपूर्ण ट्रांसफार्मर लाइन को लाइन दर लाइन लागू किया है और सिंथेटिक डेटा का उपयोग करके इसका प्रशिक्षण और भविष्यवाणी की है। मुझे उम्मीद है कि यह ट्यूटोरियल आपकी मदद कर सकता है।

#आभार
इस लेख का अनुवाद झांग होंगक्सू द्वारा किया गया था और डुओडुओ द्वारा संपादित किया गया था। मूल ज्यूपिटर हार्वर्ड एनएलपी [एनोटेटेड ट्रांसफार्मर] (https://nlp.seas.harvard.edu/2018/04/03/attention.html) से आया है।

<div id='diskus_thread'></div>
<स्क्रिप्ट>
/**
* अनुशंसित कॉन्फ़िगरेशन चर: ईडीआईटी और अपने प्लेटफ़ॉर्म या सीएमएस से डायनामिक मान सम्मिलित करने के लिए नीचे दिए गए अनुभाग को अनकम्मेंट करें।
* जानें कि इन वेरिएबल्स को परिभाषित करना क्यों महत्वपूर्ण है: https://diskus.com/admin/universalcode/#configuration-variables
*/
/*
var डिस्कस_कॉन्फिग = फ़ंक्शन () {
this.page.url = PAGE_URL; // PAGE_URL को अपने पेज के कैनोनिकल URL वेरिएबल से बदलें
this.page.identifier = PAGE_IDENTIFIER; // PAGE_IDENTIFIER को अपने पृष्ठ के विशिष्ट पहचानकर्ता वेरिएबल से बदलें
};
*/
(फ़ंक्शन() {// आवश्यक कॉन्फ़िगरेशन वैरिएबल: संपादित करेंनीचे संक्षिप्त नाम
var d = दस्तावेज़, s = d.createElement ('स्क्रिप्ट');

s.src = 'https://EXAMPLE.disqus.com/embed.js'; // महत्वपूर्ण: EXAMPLE को अपने फ़ोरम संक्षिप्त नाम से बदलें!

s.setAttribute('डेटा-टाइमस्टैम्प', +नई तिथि());
(d.head || d.body).appendChild(s);
})();
</स्क्रिप्ट>
<noscript>pt>कृपया डिस्कस द्वारा संचालित <a href='https://diskus.com/?ref_noscript' rel='nofollow'>टिप्पणियों को देखने के लिए जावास्क्रिप्ट सक्षम करें।</a></noscript>