# PyTorch में ट्रांसफार्मर स्रोत कोड स्पष्टीकरण
[2.2-इलस्ट्रेटेड ट्रांसफार्मर] (./अध्याय 2-ट्रांसफॉर्मर संबंधित सिद्धांत/2.2-इलस्ट्रेटेड ट्रांसफार्मर.एमडी) को पढ़ने के बाद, मुझे आशा है कि आप इस खंड में ट्रांसफार्मर के प्रत्येक मॉड्यूल के डिजाइन और गणना की स्पष्ट समझ प्राप्त कर सकते हैं आपको इस जटिल मॉडल को और अधिक सीखने में मदद करने के लिए पाइटोरच पर आधारित एक ट्रांसफार्मर लागू करेगा। 2.2.1 के विपरीत, यह लेख आपके संदर्भ के लिए इनपुट-मॉडल-आउटपुट के क्रम में ट्रांसफार्मर को लागू करता है।
**बच्चूटेर**

- [शब्द एम्बेडिंग](#एम्बेड)
- [स्थिति एनकोडिंग](#pos)
- [मल्टी-हेड अटेंशन](#मल्टीहेड)
- [ट्रांसफार्मर बनाएं](#बिल्ड)

![](./pictures/0-1-transformer-arc.png)

चित्र: ट्रांसफार्मर संरचना

## **<div id='embed'>शब्द एम्बेडिंग</div>**

जैसा कि ऊपर दिए गए चित्र में दिखाया गया है, ट्रांसफार्मर आरेख का बाईं ओर एनकोडर है, और दाईं ओर डिकोडर है। एनकोडर स्रोत भाषा अनुक्रम को इनपुट करता है, और डिकोडर अनुवादित किए जाने वाले भाषा पाठ को इनपुट करता है (प्रशिक्षण के दौरान)। टेक्सइसमें अक्सर कई अनुक्रम होते हैं। सामान्य ऑपरेशन अनुक्रम पर कुछ प्रीप्रोसेसिंग (जैसे शब्द विभाजन) करना है ताकि अनुक्रमों की सूची के तत्व आमतौर पर सबसे छोटे शब्द हों जिन्हें शब्दावली में विभाजित नहीं किया जा सकता है संपूर्ण पाठ एक बड़ी सूची है, और तत्व एक-एक करके अनुक्रमों से बनी सूचियाँ हैं, उदाहरण के लिए, विभाजन के बाद, एक अनुक्रम बन जाता है ["am", "##ro", "##zi", "meets"। , "उसका", "पिता"] इसके बाद, वे के अनुसार परिवर्तित हो जाते हैंशब्दावली में उनके संगत अनुक्रमणिका के लिए। मान लें कि परिणाम [23, 94, 13, 41, 27, 96] है। यदि पूरे पाठ में कुल 100 वाक्य हैं, तो लंबाई के रूप में 100 सूचियाँ हैं प्रत्येक अनुक्रम अलग है, अधिकतम लंबाई निर्धारित करने की आवश्यकता है, यहां, इसे 128 पर सेट किया जा सकता है। फिर, पूरे पाठ को एक सरणी में परिवर्तित करने के बाद, आकार 100 x 128 है, जो बैच_आकार और seq_length से मेल खाता है।

इनपुट के बाद, शब्द एम्बेडिंग प्रक्रिया तुरंत की जाती हैly. वर्ड एम्बेडिंग प्रत्येक शब्द को पूर्व-प्रशिक्षित वेक्टर के साथ मैप करना है।

वर्ड एम्बेडिंग को टॉर्च में `torch.nn.Embedding` के आधार पर कार्यान्वित किया जाता है। इंस्टेंटियेटिंग करते समय जिन मापदंडों को सेट करने की आवश्यकता होती है, वे हैं शब्दावली का आकार और मैप किए जा रहे वेक्टर का आयाम, जैसे `embed = nn.Embedding(10) ,8)`। एक वेक्टर का आयाम बस वेक्टर में संख्याओं की संख्या है। ध्यान दें कि पहला पैरामीटर शब्दावली का आकार है। छोटा, यदि आपके पास वर्तमान में अधिकतम 8 शब्द हैं, तो आमतौर पर फाई।10 में 10 (अनक और पैड के लिए एक और स्थिति), यदि आप बाद में इन 8 शब्दों से भिन्न शब्द दर्ज करते हैं, तो इसे अनक में मैप किया जाएगा, और अनुक्रम पैडिंग भाग को पैड में मैप किया जाएगा।

यदि हम 8 आयामों (num_features या embed_dim) पर मैप करने की योजना बनाते हैं, तो पूरे पाठ का आकार 100 x 128 x 8 हो जाता है। इसके बाद, समझाने के लिए एक छोटा सा उदाहरण लेते हैं: मान लीजिए कि हमारी शब्दावली में कुल 10 शब्द हैं (unk सहित) और पैड), पाठ में 2 वाक्य हैं, प्रत्येक वाक्य में 4 शब्द हैं, और हम मानचित्र बनाना चाहते हैंप्रत्येक शब्द 8-आयामी वेक्टर के लिए है, इसलिए 2, 4, और 8 बैच_आकार, seq_length, और एम्बेड_डिम के अनुरूप हैं (यदि बैच पहले आयाम में है)।

इसके अलावा, सामान्य गहन शिक्षण कार्य केवल num_features को बदलते हैं, इसलिए आयाम आम तौर पर उस आयाम के लिए होता है जहां अंतिम सुविधा स्थित होती है।

प्रोग्रामिंग प्रारंभ करें:

सभी आवश्यक पैकेज आयात करें:

```अजगर
मशाल आयात करें
टॉर्च.एनएन को एनएन के रूप में आयात करें
Torch.nn.parameter से पैरामीटर आयात करें
Torch.nn.init से xavier_uniform_ आयात करें
Torch.nn.init छोटा सा भूत सेऑर्ट स्थिरांक_
Torch.nn.init से xavier_सामान्य_ आयात करें
टॉर्च.एनएन.फंक्शनल को एफ के रूप में आयात करें
टाइपिंग से आयात वैकल्पिक, टपल, कोई भी
आयात सूची, वैकल्पिक, टपल टाइप करने से
गणित आयात करें
आयात चेतावनियाँ
```

```अजगर
एक्स = मशाल.शून्य((2,4),dtype=मशाल.लंबा)
एम्बेड = एनएन.एंबेडिंग(10,8)
प्रिंट(एम्बेड(X).आकार)
```

टॉर्च.आकार([2,4,8])

## **<div id='pos'>स्थिति एन्कोडिंग</div>**

शब्द एम्बेडिंग के बाद स्थिति एन्कोडिंग होती है, जिसका उपयोग विभिन्न शब्दों और संबंध शर्त के बीच अंतर करने के लिए किया जाता हैएक ही शब्द की विभिन्न विशेषताएं। कोड में ध्यान दें: अपरिवर्तित ही रहेंगे।

```अजगर
टेंसर = टॉर्च.टेन्सर
डीईएफ़ स्थिति_एन्कोडिंग(एक्स, संख्या_फीचर, ड्रॉपआउट_पी=0.1, अधिकतम_लेन=512) -> टेंसर:
आर'''
इनपुट में स्थितीय एन्कोडिंग जोड़ें
पैरामीटर:
- num_features: इनपुट का आयाम
- ड्रॉपआउट_पी: संभावनाड्रॉपआउट का, जब यह गैर-शून्य हो, ड्रॉपआउट करें
- max_len: वाक्य की अधिकतम लंबाई, डिफ़ॉल्ट 512 है

आकार:
- इनपुट: [बैच_आकार, seq_length, num_features]
- आउटपुट: [बैच_आकार, seq_length, num_features]

उदाहरण:
>>> एक्स = टॉर्च.रैंडन((2,4,10))
>>> एक्स = पोजिशनल_एन्कोडिंग(एक्स, 10)
>>> प्रिंट(एक्स.आकार)
>>> टॉर्च.आकार([2,4,10])
'''

ड्रॉपआउट = एनएन.ड्रॉपआउट(ड्रॉपआउट_पी)
पी = टॉर्च.शून्य((1,मैक्स_लेन,न्यू_फीचर्स))
X_ = torch.arange(max_len,dtype=torch.float32).reshape(-1,1) / torch.pow(
10000,
कोrc.arange(0,num_features,2,dtype=torch.float32) /num_features)
पी[:,:,0::2] = मशाल.पाप(एक्स_)
पी[:,:,1::2] = torch.cos(X_)
एक्स = एक्स + पी[:,:एक्स.आकार[1],:].से(एक्स.डिवाइस)
वापसी ड्रॉपआउट(एक्स)
```

```अजगर
#स्थितीय एन्कोडिंग उदाहरण
एक्स = टॉर्च.रैंडएन((2,4,10))
एक्स = पोजिशनल_एन्कोडिंग(एक्स, 10)
प्रिंट(एक्स.आकार)
```

टॉर्च.आकार([2,4,10])

## **<div id='multihead'>मल्टी-हेड ध्यान</div>**

### मल्टी-हेड ध्यान तंत्र पर एक नज़र डालें
**मल्टी-हेड अटेंशन एमईसी के पूर्ण संस्करण की कक्षाहैनिज़म पीछे है। आइए संपूर्ण पर एक नज़र डालें: मल्टी-हेड अटेंशन मैकेनिज्म-मल्टीहेड अटेंशन अनुभाग और फिर एक-एक करके निम्नलिखित स्पष्टीकरणों को पढ़ने के लिए वापस आएं।

मल्टी-हेड अटेंशन क्लास के मुख्य घटक हैं: पैरामीटर आरंभीकरण, मल्टी_हेड_अटेंशन_फॉरवर्ड

#### आरंभीकरण पैरामीटर
```अजगर
यदि self._qkv_same_embed_dim ग़लत है:
# आरंभीकरण से पहले और बाद में आकार अपरिवर्तित रहता है
# (seq_length x एम्बेड_डिम) x (एम्बेड_डिम x एम्बेड_डिम) ==> (seq_length x एंबेड_डिम)
self.q_proj_weight = पैरामीटर(मशाल.खाली((एम्बेड_डिम, एंबेड_डिम)))
self.k_proj_weight = पैरामीटर(मशाल.खाली((embed_dim, self.kdim)))
self.v_proj_weight = पैरामीटर(मशाल.खाली((एम्बेड_डिम, सेल्फ.vdim)))
self.register_parameter('in_proj_weight', कोई नहीं)
अन्य:
self.in_proj_weight = पैरामीटर(मशाल.खाली((3 * एम्बेड_डिम, एम्बेड_डिम)))
self.register_parameter('q_proj_weight', कोई नहीं)
self.register_parameter('k_proj_weight', कोई नहीं)
self.register_parameter('v_proj_weight', कोई नहीं)

अगरपक्षपात:
self.in_proj_bias = पैरामीटर(मशाल.खाली(3 * एंबेड_डिम))
अन्य:
self.register_parameter('in_proj_bias', कोई नहीं)
# सभी प्रमुखों का ध्यान एक साथ एकत्रित किया जाएगा और फिर वेट मैट्रिक्स आउटपुट से गुणा किया जाएगा
#आउट_प्रोज बाद की तैयारी के लिए है
self.out_proj = nn.रैखिक (एम्बेड_डिम, एंबेड_डिम, पूर्वाग्रह = पूर्वाग्रह)
self._reset_parameters()
```

मशाल.खाली दिए गए आकार के अनुसार संबंधित टेंसर बनाता है, और विशेषता यह है कि भरे हुए मान को आरंभ नहीं किया गया है, के समानTorch.randn (मानक सामान्य वितरण), जो आरंभीकरण का एक तरीका है। PyTorch में, यदि चर प्रकार टेंसर है, तो मान को संशोधित नहीं किया जा सकता है, और पैरामीटर () फ़ंक्शन को एक प्रकार रूपांतरण फ़ंक्शन के रूप में माना जा सकता है, जो परिवर्तित करता है। एक प्रशिक्षित और संशोधित मॉडल पैरामीटर में अपरिवर्तनीय टेंसर, यानी, यह मॉडल.पैरामीटर से जुड़ा हुआ है। रजिस्टर_पैरामीटर का मतलब है कि इस पैरामीटर को मॉडल.पैरामीटर में रखना है या नहीं, और कोई नहीं का मतलब है कि ऐसा कोई पैरामीटर नहीं है।

एक if j हैयह निर्धारित करने के लिए यहां निर्णय लें कि क्या q, k, और v का अंतिम आयाम सुसंगत है। यदि वे सुसंगत हैं, तो एक बड़े वजन मैट्रिक्स को गुणा किया जाता है और फिर विभाजित किया जाता है, वास्तव में, आरंभीकरण किया जाएगा मूल आकार न बदलें (जैसे![](http://latex.codecogs.com/svg.latex?q=qW_q+b_q), टिप्पणियाँ देखें)।

आप पा सकते हैं कि अंत में एक _reset_parameters() फ़ंक्शन है, जिसका उपयोग पैरामीटर मानों को प्रारंभ करने के लिए किया जाता है, जिसका अर्थ है यादृच्छिक रूप से va का नमूना लेना[निरंतर समान वितरण] से संकेत(https://zh.wikipedia.org/wiki/%E9%80%A3%E7%BA%8C%E5%9E%8B%E5%9D%87%E5%8B %BB %E5%88%86%E5%B8%83) आरंभीकरण मूल्य के रूप में, और xavier_normal_ नमूनाकरण का वितरण सामान्य वितरण है क्योंकि तंत्रिका नेटवर्क को प्रशिक्षित करते समय आरंभीकरण मूल्य बहुत महत्वपूर्ण है, इन दो कार्यों की आवश्यकता होती है।

स्थिरांक_ का अर्थ इनपुट वेक्टर को दिए गए मान से भरना है।

इसके अलावा, PyTorch के स्रोत कोड में, ऐसा लगता है कि प्रक्षेपण प्रतिनिधित्व करता हैएक रैखिक परिवर्तन, और in_proj_bias का अर्थ प्रारंभिक रैखिक परिवर्तन का पूर्वाग्रह है

```अजगर
def _reset_parameters(स्वयं):
यदि self._qkv_same_embed_dim:
xavier_uniform_(self.in_proj_weight)
अन्य:
xavier_uniform_(self.q_proj_weight)
xavier_uniform_(self.k_proj_weight)
xavier_uniform_(self.v_proj_weight)
यदि self.in_proj_bias कोई नहीं है:
स्थिर_(self.in_proj_bias, 0.)
स्थिर_(self.out_proj.bias, 0.)

```

#### मल्टी_हेड_अटेंशन_फॉरवर्ड
यह फ़ंक्शन निम्नलिखित कोड में दिखाया गया हैजिसे मुख्यतः 3 भागों में बाँटा गया है:
- क्वेरी, कुंजी, मान को _in_projection_packed के माध्यम से q, k, v में बदल दिया जाता है
- रोड़ा तंत्र
- डॉट उत्पाद ध्यान

```अजगर
मशाल आयात करें
टेंसर = टॉर्च.टेन्सर
डीईएफ़ मल्टी_हेड_अटेंशन_फॉरवर्ड(
क्वेरी: टेन्सर, कुंजी: टेन्सर,
मूल्य: टेंसर,
num_heads: int,
in_proj_weight: टेंसर,
in_proj_bias: वैकल्पिक[टेंसर],
ड्रॉपआउट_पी: फ्लोट,
आउट_प्रोज_वेट: टेंसर,
out_proj_bias: वैकल्पिक[टेंसर],
प्रशिक्षण: बूल = सत्य,
key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं,नीड_वेट्स: बूल = सत्य,
attn_mask: वैकल्पिक[टेंसर] = कोई नहीं,
उपयोग_अलग_प्रोज_वेट = कोई नहीं,
q_proj_weight: वैकल्पिक[टेंसर] = कोई नहीं,
k_proj_weight: वैकल्पिक[टेंसर] = कोई नहीं,
v_proj_weight: वैकल्पिक[टेंसर] = कोई नहीं,
) -> टुपल[टेंसर, वैकल्पिक[टेंसर]]:
आर'''
आकार:
इनपुट:
- क्वेरी: `(एल, एन, ई)`
- कुंजी: `(एस, एन, ई)`
- मान: `(एस, एन, ई)`
- key_padding_mask: `(एन, एस)`
- attn_mask: `(L, S)` या `(N * num_heads, L, S)`
आउटपुट:
- attn_output:`(एल, एन, ई)`
- attn_output_weights:`(एन, एल, एस)`
'''
tgt_len, bsz, ईmbed_dim = query.shape
src_len, _, _ = key.shape
हेड_डिम = एम्बेड_डिम // num_heads
q, k, v = _in_projection_packed(क्वेरी, कुंजी, मान, in_proj_weight, in_proj_bias)

यदि attn_mask कोई नहीं है:
यदि attn_mask.dtype == torch.uint8:
चेतावनियां.चेतावनी('nn.MultiheadAttention में attn_mask के लिए बाइट टेंसर अप्रचलित है। इसके बजाय बूल टेंसर का उपयोग करें।')
attn_mask = attn_mask.to(torch.bool)
अन्य:
attn_mask.is_floating_point() या attn_mask.dtype == torch.bool, \ पर जोर दें
f"केवल फ्लोट, बाइट और बूल प्रकार समर्थित हैंया attn_mask, नहीं {attn_mask.dtype}"यदि attn_mask.dim() == 2:
सही_2d_आकार = (tgt_len, src_len)
यदि attn_mask.shape != सही_2d_आकार:
raise RuntimeError(f"2D attn_mask का आकार {attn_mask.shape} है, लेकिन {correct_2d_size} होना चाहिए।")
attn_mask = attn_mask.unsqueeze(0)
एलिफ़ attn_mask.dim() == 3:
सही_3डी_आकार = (बीएसजेड * num_heads, tgt_len, src_len)
यदि attn_mask.shape != सही_3d_आकार:
raise RuntimeError(f"2D attn_mask का आकार {attn_mask.shape} है, लेकिन {correct_2d_s होना चाहिए)ize}.")3D attn_mask का पे {attn_mask.shape} है, लेकिन {correct_3d_size} होना चाहिए।")
अन्य:
raise RuntimeError(f"attn_mask का आयाम {attn_mask.dim()} समर्थित नहीं है")

यदि key_padding_mask कोई नहीं है और key_padding_mask.dtype == torch.uint8:
चेतावनियाँ.चेतावनी('nn.MultiheadAttention में key_padding_mask के लिए बाइट टेंसर अप्रचलित है। इसके बजाय बूल टेंसर का उपयोग करें।')
key_padding_mask = key_padding_mask.to(torch.bool)

# डॉट उत्पाद का ध्यान आकर्षित करने के लिए बैच को पहले आयाम में रखने के लिए q,k,v को दोबारा आकार दें
# एएक ही समय में मल्टी-हेड तंत्र के लिए, एक परत बनाने के लिए अलग-अलग हेड्स को एक साथ रखा जाता है
q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
यदि key_padding_mask कोई नहीं है:
ज़ोर key_padding_mask.shape == (bsz, src_len), \
f"की_पैडिंग_मास्क आकार {(bsz, src_len)} की उम्मीद कर रहा था, लेकिन मिला {key_padding_mask.shape}"
key_padding_mask =key_padding_mask.view(bsz, 1, 1, src_len).
विस्तार(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)
यदि attn_mask कोई नहीं है:
attn_mask = key_padding_mask
एलिफ़ attn_mask.dtype == मशाल.बूल:
attn_mask = attn_mask.ological_or(key_padding_mask)
अन्य:
attn_mask = attn_mask.masked_fill(key_padding_mask, फ्लोट("-inf"))
# यदि attn_mask मान एक बूलियन मान है, तो मास्क को फ़्लोट में बदलें
यदि attn_mask कोई नहीं है और attn_mask.dtype == torch.bool:
new_attn_mask = मशाल.शून्य_लाइक(attn_mask, dtype=torच.फ्लोट)
new_attn_mask.masked_fill_(attn_mask, फ्लोट("-inf"))
attn_mask = new_attn_mask

# प्रशिक्षण सही होने पर ही ड्रॉपआउट लागू करें
यदि प्रशिक्षण नहीं है:
ड्रॉपआउट_पी = 0.0
attn_output, attn_output_weights = _scale_dot_product_attention(q, k, v, attn_mask, ड्रॉपआउट_p)
attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
attn_output = nn.functional.linear(attn_output, out_proj_weight, out_proj_bias)
यदि आवश्यकता हो तो वज़न:
# सिर पर औसत ध्यान भार
attn_output_weights= attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
वापसी attn_output, attn_output_weights.sum(dim=1) / num_heads
अन्य:
वापसी attn_output, कोई नहीं
```

##### क्वेरी, कुंजी, मान को q, k, v प्राप्त करने के लिए _in_projection_packed द्वारा रूपांतरित किया जाता है
```
q, k, v = _in_projection_packed(क्वेरी, कुंजी, मान, in_proj_weight, in_proj_bias)
```

`nn.functional.linear` फ़ंक्शन के लिए, यह वास्तव में एक रैखिक परिवर्तन है। `nn.Linear` के विपरीत, पहला वजन मैट्रिक्स और पूर्वाग्रह प्रदान कर सकता है, और निष्पादित कर सकता है![](http://latex.codecogs.com/svg.latex?y=xW^T+b), और बाद वाला स्वतंत्र रूप से आउटपुट आयाम निर्धारित कर सकता है।

```अजगर
def _in_projection_packed(
क्यू: टेंसर,
के: टेंसर,
वी: टेंसर,
डब्ल्यू: टेंसर,
बी: वैकल्पिक[टेंसर] = कोई नहीं,
) -> सूची[टेंसर]:
आर"""
बड़े वजन पैरामीटर मैट्रिक्स के साथ रैखिक परिवर्तन

पैरामीटर:
q, k, v: आत्म-ध्यान के लिए, तीनों src हैं; seq2seq मॉडल के लिए, k और v एक ही टेंसर हैं।
लेकिन उनका अंतिम आयाम (num_features या embed_dim) सुसंगत होना चाहिए।
w: लिन के लिए एक बड़ा मैट्रिक्सकान परिवर्तन, क्यू, के, वी के क्रम में एक टेंसर में संपीड़ित।
बी: रैखिक परिवर्तन के लिए एक पूर्वाग्रह, क्यू, के, वी के क्रम में एक टेंसर में संपीड़ित।

आकार:
इनपुट:
- q: आकार:`(..., E)`, E शब्द एम्बेडिंग का आयाम है (E इस उद्देश्य के लिए नीचे दिखाई देता है)।
- k: आकृति:`(..., E)`
- वी: आकार:`(..., ई)`
- डब्ल्यू: आकार:`(ई * 3, ई)`
- बी: आकार:`ई * 3`

आउटपुट:
- आउटपुट सूची:`[q', k', v']`, q, k, v का आकार रैखिक परिवर्तन से पहले और बाद में समान है।
"""
ई = क्यू.आकार(-1)
# यदि यह स्व-अटेट हैउल्लेख करें, तो q = k = v = src, इसलिए उनके संदर्भ चर सभी src हैं
# अर्थात, k, v है और q, k, दोनों सत्य हैं
# यदि यह seq2seq है, k = v, तो k क्या v सत्य है
यदि k, v है:
यदि q, k है:
वापसी F.रैखिक(q, w, b).खंड(3, dim=-1)
अन्य:
# seq2seq मॉडल
w_q, w_kv = w.स्प्लिट([ई, ई * 2])
यदि b कोई नहीं है:
b_q = b_kv = कोई नहीं
अन्य:
b_q, b_kv = b.स्प्लिट([ई, ई * 2])
रिटर्न (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).chunk(2, dim=-1)
अन्य:
w_q, w_k, w_v = w.chunk(3)
यदि b कोई नहीं है:
b_q = b_k = b_v = कोई नहीं
अन्य:
बी_क्यू, बी_के, बी_वी = बी.चंक(3)
वापसी F.रेखीय(q, w_q, b_q), F.रेखीय(k, w_k, b_k), F.रेखीय(v, w_v, b_v)

# q, k, v = _in_projection_packed(क्वेरी, कुंजी, मान, in_proj_weight, in_proj_bias)
```

***

##### मास्किंग तंत्र

Attn_mask के लिए, यदि यह 2D है, तो आकार `(L, S)` जैसा है, L और S क्रमशः लक्ष्य भाषा और स्रोत भाषा अनुक्रम लंबाई का प्रतिनिधित्व करते हैं; यदि यह 3D है, तो आकार `(N * num_heads, L) जैसा है , S)`, N बैच_आकार का प्रतिनिधित्व करता है, और num_heads ध्यान शीर्षों की संख्या का प्रतिनिधित्व करता है।यदि attn_mask का dtype ByteTensor है, तो गैर-शून्य स्थिति पर ध्यान नहीं दिया जाएगा और यदि यह BoolTensor है, तो True से संबंधित स्थिति को अनदेखा कर दिया जाएगा, यदि यह एक संख्यात्मक मान है, तो इसे सीधे attn_weights में जोड़ दिया जाएगा .

क्योंकि जब डिकोडर डिकोड कर रहा होता है, तो वह केवल स्थिति और उससे पहले वाली स्थिति को देख सकता है। यदि वह पीछे देखता है, तो यह उल्लंघन होगा, इसलिए attn_mask को अवरुद्ध करने की आवश्यकता है।

निम्नलिखित फ़ंक्शन सीधे PyTorch की प्रतिलिपि बनाता है, जिसका अर्थ है सह सुनिश्चित करनाविभिन्न आयामों और विभिन्न प्रकार के रूपांतरणों का सही मुखौटा आकार

```अजगर
यदि attn_mask कोई नहीं है:
यदि attn_mask.dtype == torch.uint8:
चेतावनियां.चेतावनी('nn.MultiheadAttention में attn_mask के लिए बाइट टेंसर अप्रचलित है। इसके बजाय बूल टेंसर का उपयोग करें।')
attn_mask = attn_mask.to(torch.bool)
अन्य:
attn_mask.is_floating_point() या attn_mask.dtype == torch.bool, \ पर जोर दें
f"केवल फ्लोट, बाइट और बूल प्रकार attn_mask के लिए समर्थित हैं, {attn_mask.dtype} के लिए नहीं"
# विभिन्न आयामों के लिए आकार निर्धारणवजहें
यदि attn_mask.dim() == 2:
सही_2d_आकार = (tgt_len, src_len)
यदि attn_mask.shape != सही_2d_आकार:
raise RuntimeError(f"2D attn_mask का आकार {attn_mask.shape} है, लेकिन {correct_2d_size} होना चाहिए।")
attn_mask = attn_mask.unsqueeze(0)
एलिफ़ attn_mask.dim() == 3:
सही_3डी_आकार = (बीएसजेड * num_heads, tgt_len, src_len)
यदि attn_mask.shape != सही_3d_आकार:
raise RuntimeError(f"3D attn_mask का आकार {attn_mask.shape} है, लेकिन {correct_3d_size} होना चाहिए।")
अन्य:
रनटाइम त्रुटि बढ़ाएँr(f"attn_mask का आयाम {attn_mask.dim()} समर्थित नहीं है")

```
`attn_mask` के साथ अंतर यह है कि `key_padding_mask` का उपयोग कुंजी में मान को छिपाने के लिए किया जाता है, जिसे विस्तार से `<PAD>` होना चाहिए। अनदेखा किए जाने की स्थिति attn_mask के समान है।

```अजगर
# key_padding_mask मान को बूलियन मान में बदलें
यदि key_padding_mask कोई नहीं है और key_padding_mask.dtype == torch.uint8:
चेतावनियां.चेतावनी(''nn.MultiheadAttention में key_padding_mask के लिए बाइट टेंसर अप्रचलित है। बूल टेंसर का उपयोग करेंबजाय।")
key_padding_mask = key_padding_mask.to(torch.bool)
```

पहले दो छोटे फ़ंक्शन, `लॉजिकल_ऑर` पेश करें, दो टेंसर इनपुट करें, और दो टेनसर में मानों पर `लॉजिकल या` ऑपरेशन करें, यह केवल तभी `गलत` होता है जब दोनों मान 0 होते हैं, और अन्यथा `सही` है। दूसरा `मास्कड_फिल` है, जो एक मास्क और भरे जाने वाले मान को इनपुट करता है। मास्क में 1 और 0 होते हैं, स्थिति 0 का मान अपरिवर्तित रहता है, और 1 की स्थिति नए मान से भरी जाती है।
```अजगर
ए = टॉर्च.टेंसर([0,1,10,0],dtype=torch.int8)
बी = टॉर्च.टेंसर([4,0,1,0],डीटाइप=टॉर्च.इंट8)
प्रिंट(मशाल.लॉजिकल_या(ए,बी))
# टेंसर([सच्चा, सच्चा, सच्चा, गलत])
```

```अजगर
आर = टॉर्च.टेंसर([[0,0,0,0],[0,0,0,0]])
मास्क = टॉर्च.टेंसर([[1,1,1,1],[0,0,0,0]])
प्रिंट(आर.मास्कड_फिल(मास्क,1))
# टेंसर([[1, 1, 1, 1],
# [0, 0, 0, 0]])
```
वास्तव में, attn_mask और key_padding_mask कभी-कभी एक ही ऑब्जेक्ट होते हैं, इसलिए कभी-कभी आप उन्हें एक साथ देख सकते हैं। सॉफ्टमैक्स के बाद `-inf` 0 होगा, यानी अनदेखा कर दिया जाएगा।
```अजगर
यदि key_padding_maएसके कोई नहीं है:
ज़ोर key_padding_mask.shape == (bsz, src_len), \
f"की_पैडिंग_मास्क आकार {(bsz, src_len)} की उम्मीद कर रहा था, लेकिन मिला {key_padding_mask.shape}"
key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).\
विस्तार(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)
# यदि attn_mask खाली है, तो सीधे key_padding_mask का उपयोग करें
यदि attn_mask कोई नहीं है:
attn_mask = key_padding_mask
एलिफ़ attn_mask.dtype == मशाल.बूल:
attn_mask = attn_mask.ological_or(key_padding_mask)
अन्यथा:attn_mask = attn_mask.masked_fill(key_padding_mask, फ्लोट("-inf"))

# यदि attn_mask मान एक बूलियन मान है, तो मास्क को फ़्लोट में बदलें
यदि attn_mask कोई नहीं है और attn_mask.dtype == torch.bool:
new_attn_mask = मशाल.शून्य_लाइक(attn_mask, dtype=torch.float)
new_attn_mask.masked_fill_(attn_mask, फ्लोट("-inf"))
attn_mask = new_attn_mask

```

***
##### डॉट उत्पाद ध्यान

```अजगर
टाइपिंग से आयात वैकल्पिक, टपल, कोई भी
def _स्केल्ड_डॉट_प्रोडक्ट_अटेंशन(
क्यू: टेंसर,
के: टेंसर,
वी: टेंसर,
attn_mask: वैकल्पिक[दसियोंया] = कोई नहीं,
ड्रॉपआउट_पी: फ्लोट = 0.0,
) -> टुपल[टेंसर, टेन्सर]:
आर'''
क्वेरी, कुंजी, मान पर डॉट उत्पाद ध्यान की गणना करें, यदि कोई हो तो ध्यान मास्क का उपयोग करें, और संभावना ड्रॉपआउट_पी के साथ ड्रॉपआउट लागू करें

पैरामीटर:
- क्यू: आकार:`(बी, एनटी, ई)` बी बैच आकार का प्रतिनिधित्व करता है, एनटी लक्ष्य भाषा अनुक्रम लंबाई है, ई एम्बेडिंग के बाद फीचर आयाम है
- कुंजी: आकार:`(बी, एनएस, ई)` एनएस स्रोत भाषा अनुक्रम लंबाई है
- मान: आकार:`(बी, एनएस, ई)` कुंजी के समान आकार है
- attn_mask: या तो एक 3D टेंसर के साथआकार:`(बी, एनटी, एनएस)` या एक 2डी टेंसर जिसका आकार इस प्रकार है:`(एनटी, एनएस)`

- आउटपुट: ध्यान मान: आकार:`(बी, एनटी, ई)`, क्यू के समान आकार; ध्यान भार: आकार:`(बी, एनटी, एनएस)` उदाहरण:
>>> क्यू = टॉर्च.रैंडन((2,3,6))
>>> के = टॉर्च.रैंडन((2,4,6))
>>> वी = टॉर्च.रैंडन((2,4,6))
>>> आउट = स्केल्ड_डॉट_प्रोडक्ट_अटेंशन(क्यू, के, वी)
>>> बाहर[0].आकार, बाहर[1].आकार
>>> टॉर्च.आकार([2,3,6]) टॉर्च.आकार([2,3,4])
'''
बी, एनटी, ई = क्यू.आकार
क्यू = क्यू / गणित.वर्ग(ई)
# (बी, एनटी, ई) एक्स (बी, ई, एनएस) -> (बी, एनटी, एनएस)
ध्यान दें = टॉर्च.बीएमएम(क्यू, के.टीआरउत्तर(-2,-1))
यदि attn_mask कोई नहीं है:
attn += attn_mask
# attn का अर्थ है कि लक्ष्य अनुक्रम में प्रत्येक शब्द भाषा अनुक्रम के लिए ध्यान है
attn = F.softmax(attn, dim=-1)
यदि ड्रॉपआउट_पी:
attn = F.ड्रॉपआउट(attn, p=dropout_p)
# (बी, एनटी, एनएस) एक्स (बी, एनएस, ई) -> (बी, एनटी, ई)
आउटपुट = टॉर्च.बीएमएम(ध्यान, वी)
रिटर्न आउटपुट, ध्यान दें

```

### संपूर्ण मल्टी-हेड अटेंशन मैकेनिज्म-मल्टीहेडअटेंशन

```अजगर
क्लास मल्टीहेडअटेंशन(एनएन.मॉड्यूल):
आर'''
पैरामीटर:
एम्बेड_डिम: शब्द एम्बेडिंग आयाम
num_heads:nuसमानांतर शीर्षों का सदस्य
बैच_फर्स्ट: यदि `सही` है, तो यह (बैच, seq, फीचर) है, यदि यह `गलत` है, तो यह (seq, बैच, फीचर) है

उदाहरण:
>>> मल्टीहेड_एटीएन = मल्टीहेडअटेंशन (एम्बेड_डिम, नंबर_हेड्स)
>>> attn_output, attn_output_weights = मल्टीहेड_attn(क्वेरी, कुंजी, मान)
'''
def __init__(स्वयं, एम्बेड_डिम, num_heads, ड्रॉपआउट=0., पूर्वाग्रह=सत्य,
kdim=कोई नहीं, vdim=कोई नहीं, बैच_फर्स्ट=गलत) -> कोई नहीं:
# फ़ैक्टरी_क्वार्ग्स = {'डिवाइस': डिवाइस, 'डीटाइप': डीटाइप}
सुपर(मल्टीहेडअटेंशन, स्वयं).__init__()
self.embed_dim = ईmbed_dim
self.kdim = kdim यदि kdim कोई नहीं है और कोई नहीं embed_dim है
self.vdim = vdim यदि vdimis नहीं और कोई नहीं एम्बेड_dim
self._qkv_same_embed_dim = self.kdim == एम्बेड_डिम और self.vdim == एम्बेड_डिम

self.num_heads = num_heads
स्व.ड्रॉपआउट = ड्रॉपआउट
स्वयं.बैच_प्रथम = बैच_प्रथम
self.head_dim = एम्बेड_डिम // num_heads
assert self.head_dim * num_heads == self.embed_dim, "एम्बेड_डिम को num_heads से विभाज्य होना चाहिए"

यदि self._qkv_same_embed_dim ग़लत है:
self.q_proj_weight = पैरामीटर(मशाल.खाली((एम्बेड_डिम, ईmbed_dim)))self.k_proj_weight = पैरामीटर(मशाल.खाली((एम्बेड_डिम, self.kdim)))
self.v_proj_weight = पैरामीटर(मशाल.खाली((एम्बेड_डिम, सेल्फ.vdim)))
self.register_parameter('in_proj_weight', कोई नहीं)
अन्य:
self.in_proj_weight = पैरामीटर(मशाल.खाली((3 * एम्बेड_डिम, एम्बेड_डिम)))
self.register_parameter('q_proj_weight', कोई नहीं)
self.register_parameter('k_proj_weight', कोई नहीं)
self.register_parameter('v_proj_weight', कोई नहीं)यदि पूर्वाग्रह:
self.in_proj_bias = पैरामीटर(मशाल.खाली(3 * एंबेड_डिम))
अन्य:
स्व.रजिस्टर_पैरामीटर('in_proj_bias', कोई नहीं)
self.out_proj = nn.रैखिक (एम्बेड_डिम, एंबेड_डिम, पूर्वाग्रह = पूर्वाग्रह)

self._reset_parameters()

def _reset_parameters(स्वयं):
यदि self._qkv_same_embed_dim:
xavier_uniform_(self.in_proj_weight)
अन्य:
xavier_uniform_(self.q_proj_weight)
xavier_uniform_(self.k_proj_weight)
xavier_uniform_(self.v_proj_weight)

यदि self.in_proj_bias कोई नहीं है:
स्थिर_(self.in_proj_bias, 0.)
स्थिर_(self.out_proj.bias, 0.)

डीईएफ़ फॉरवर्ड (स्वयं, क्वेरी: टेन्सर, कुंजी: टेन्सर, मान: टेन्सर, की_पैडing_mask: वैकल्पिक[टेंसर] = कोई नहीं,
आवश्यकता_भार: बूल = सत्य, attn_mask: वैकल्पिक[टेंसर] = कोई नहीं) -> टुपल[टेंसर, वैकल्पिक[टेंसर]]:
यदि self.batch_first:
क्वेरी, कुंजी, मान = [x.transpose(1, 0) for x in (क्वेरी, कुंजी, मान)]यदि self._qkv_same_embed_dim नहीं है:
attn_output, attn_output_weights = मल्टी_हेड_अटेंशन_फॉरवर्ड(
क्वेरी, कुंजी, मान, self.num_heads,
self.in_proj_weight, self.in_proj_bias,
सेल्फ.ड्रॉपआउट, सेल्फ.आउट_प्रोज.वेट, सेल्फ.आउट_प्रोज.बायस,
प्रशिक्षण=स्वयं प्रशिक्षण,
key_padding_mask=key_padडिंग_मास्क, आवश्यकता_वजन=आवश्यकता_वजन,
attn_mask=attn_mask, use_separate_proj_weight=True,q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,
v_proj_weight=self.v_proj_weight)
अन्य:
attn_output, attn_output_weights = मल्टी_हेड_अटेंशन_फॉरवर्ड(
क्वेरी, कुंजी, मान, self.num_heads,
self.in_proj_weight, self.in_proj_bias,
सेल्फ.ड्रॉपआउट, सेल्फ.आउट_प्रोज.वेट, सेल्फ.आउट_प्रोज.बायस,
प्रशिक्षण=स्वयं प्रशिक्षण,
key_padding_mask=key_padding_mask, जरुरत_वजन=आवश्यकता_वजन,
attn_mask=attn_mask)यदि self.batch_first:
attn_output.transpose(1, 0), attn_output_weights लौटाएँ
अन्य:
attn_output, attn_output_weights लौटाएँ
```

इसके बाद, आप अभ्यास कर सकते हैं और स्थिति एन्कोडिंग जोड़ सकते हैं। आप पा सकते हैं कि स्थिति एन्कोडिंग और मल्टी-हेड ध्यान जोड़ने से पहले और बाद का आकार नहीं बदलेगा।

```अजगर
# क्योंकि बैच_फर्स्ट गलत है, src का आकार है: `(seq, बैच, एम्बेड_डिम)`
स्रोत = मशाल.रैंडन((2,4,100))
स्रोत = स्थितिगत_एन्कोडिंग(src,100,0.1)
प्रिंट(src.shape)
मल्टीहेड_अट्टन = मल्टीहेडएटनटियोन(100, 4, 0.1)
attn_output, attn_output_weights = मल्टीहेड_attn(src,src,src)
प्रिंट(attn_output.shape, attn_output_weights.shape)

#मशाल.आकार([2,4,100])
# टॉर्च.आकार([2,4,100]) टॉर्च.आकार([4,2,2])
```

टॉर्च.आकार([2,4,100])
मशाल.आकार([2, 4,100]) मशाल.आकार([4,2,2])

***
## **<div id='build'>बिल्ड ट्रांसफार्मर</div>**
-एनकोडर परत

![](./pictures/2-2-1-encoder.png)

```अजगर
क्लास ट्रांसफार्मरएनकोडरलेयर(एनएन.मॉड्यूल):
आर'''
पैरामीटर:
d_model: शब्द एम्बेडिंग का आयाम (आवश्यक)।ईडी)
एनहेड: मल्टी-हेड अटेंशन में समानांतर हेड की संख्या (आवश्यक)
dim_feedforward: पूरी तरह से जुड़ा हुआ परत में न्यूरॉन्स की संख्या, जिसे इस परत के माध्यम से इनपुट के आयाम के रूप में भी जाना जाता है (डिफ़ॉल्ट = 2048)
ड्रॉपआउट: ड्रॉपआउट की संभावना (डिफ़ॉल्ट = 0.1)
सक्रियण: दो रैखिक परतों, डिफ़ॉल्ट रिले या गेलु के बीच सक्रियण फ़ंक्शन
lay_norm_eps: हर को 0 होने से रोकने के लिए परत सामान्यीकरण में एक छोटी राशि (डिफ़ॉल्ट = 1e-5)
बैच_प्रथम: यदि `सही` है, तो यह (बैच, एसeq, फ़ीचर), यदि यह `गलत` है, तो यह (seq, बैच, फ़ीचर) है (डिफ़ॉल्ट: ग़लत)

उदाहरण:
>>> एनकोडर_लेयर = ट्रांसफार्मरएनकोडरलेयर(d_model=512, nhead=8)
>>> स्रोत = टॉर्च.रैंडन((32, 10, 512))
>>> आउट = एनकोडर_लेयर(src)
'''

def __init__(स्वयं, d_model, nhead, dim_feedforward=2048, ड्रॉपआउट=0.1, सक्रियण=F.relu,
लेयर_नॉर्म_ईपीएस=1ई-5, बैच_फर्स्ट=गलत) -> कोई नहीं:
सुपर(ट्रांसफॉर्मरएनकोडरलेयर, स्वयं).__init__()
self.self_attn = मल्टीहेडअटेंशन(d_model, nhead, ड्रॉपआउट=ड्रॉपआउट, बैच_फर्स्ट=बैच_फ़िरअनुसूचित जनजाति)
self.linear1 = nn.Linear(d_model, dim_feedforward)
स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(ड्रॉपआउट)
self.linear2 = nn.Linear(dim_feedforward, d_model)

self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)
self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)
स्व.ड्रॉपआउट1 = एनएन.ड्रॉपआउट(ड्रॉपआउट)
स्व.ड्रॉपआउट2 = एनएन.ड्रॉपआउट(ड्रॉपआउट)
स्व.सक्रियण = सक्रियण

डीईएफ़ फॉरवर्ड (स्वयं, स्रोत: टेंसर, स्रोत_मास्क: वैकल्पिक [टेंसर] = कोई नहीं, स्रोत_की_पैडिंग_मास्क: वैकल्पिक [टेंसर] = कोई नहीं) -> टेंसर:
स्रोत = स्थिति_एनकोडआईएनजी(src, src.shape[-1])
src2 = self.self_attn(src, src, src, attn_mask=src_mask,
key_padding_mask=src_key_padding_mask)[0]
src = src + self.dropout1(src2)
src = self.norm1(src)
src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
src = src + self.dropout(src2)
src = self.norm2(src)
वापसी स्रोत

```

```अजगर
# एक छोटा सा उदाहरण देखिये
एनकोडर_लेयर = ट्रांसफार्मरएनकोडरलेयर(d_model=512, nhead=8)
स्रोत = मशाल.रैंडन((32, 10, 512))
आउट = एनकोडर_लेयर(src)
प्रिंट(आउट.आकार)
#टॉर्च.आकार([32,10,512])
```

टॉर्च.आकार([32,10,512])

### एनकोडर से बनी ट्रांसफार्मर परत

```अजगर
क्लास ट्रांसफार्मरएनकोडर(एनएन.मॉड्यूल):
आर'''
पैरामीटर:
एनकोडर_लेयर (आवश्यक)
num_layers: एनकोडर_लेयर परतों की संख्या (आवश्यक)
मानक: सामान्यीकरण विकल्प (वैकल्पिक)

उदाहरण:
>>> एनकोडर_लेयर = ट्रांसफार्मरएनकोडरलेयर(d_model=512, nhead=8)
>>> ट्रांसफार्मर_एनकोडर = ट्रांसफार्मरएनकोडर(एनकोडर_लेयर, संख्या_लेयर=6)
>>> स्रोत = मशाल.रैंडन((10, 32, 512))
>>> आउट = ट्रांसफार्मर_एनकोडरआरसी)
'''

def __init__(स्वयं, एनकोडर_लेयर, num_layers, मानदंड=कोई नहीं):
सुपर(ट्रांसफॉर्मरएनकोडर, स्वयं).__init__()
सेल्फ.लेयर = एनकोडर_लेयर
self.num_layers = num_layers
स्व.मानदंड = आदर्श

डीईएफ़ फॉरवर्ड (स्वयं, स्रोत: टेंसर, मास्क: वैकल्पिक [टेंसर] = कोई नहीं, src_key_padding_mask: वैकल्पिक [टेंसर] = कोई नहीं) -> टेंसर:
आउटपुट = पोजिशनल_एन्कोडिंग(src, src.shape[-1])
रेंज में _ के लिए (self.num_layers):
आउटपुट = सेल्फ.लेयर(आउटपुट, src_mask=mask, src_key_padding_mask=src_key_padding_mask)

यदि self.norm कोई नहीं है:
कहांtput = self.norm(आउटपुट)

वापसी आउटपुट
```

```अजगर
#उदाहरण
एनकोडर_लेयर = ट्रांसफार्मरएनकोडरलेयर(d_model=512, nhead=8)
ट्रांसफार्मर_एनकोडर = ट्रांसफार्मरएनकोडर(एनकोडर_लेयर, संख्या_लेयर=6)
स्रोत = मशाल.रैंडन((10, 32, 512))
आउट = ट्रांसफार्मर_एनकोडर(src)
प्रिंट(आउट.आकार)
#मशाल.आकार([10,32,512])
```

टॉर्च.आकार([10,32,512])

***
## डिकोडर परत:

```अजगर
वर्ग ट्रांसफार्मरडिकोडरलेयर(एनएन.मॉड्यूल):
आर'''
पैरामीटर:
d_model: शब्द एम्बेडिंग आयाम (आवश्यक)nhead: पैराल की संख्यामल्टी-हेड अटेंशन में एलएल हेड्स (आवश्यक)
dim_feedforward: पूरी तरह से जुड़ी हुई परत में न्यूरॉन्स की संख्या, जिसे इस परत के माध्यम से इनपुट के आयाम के रूप में भी जाना जाता है (डिफ़ॉल्ट = 2048)
ड्रॉपआउट: ड्रॉपआउट की संभावना (डिफ़ॉल्ट = 0.1)
सक्रियण: दो रैखिक परतों, डिफ़ॉल्ट रिले या गेलु के बीच सक्रियण फ़ंक्शन
lay_norm_eps: हर को 0 होने से रोकने के लिए परत सामान्यीकरण में एक छोटी राशि (डिफ़ॉल्ट = 1e-5)
बैच_फर्स्ट: यदि `सही` है, तो यह (बैच, सीक, फीचर) है, यदि `गलत` है,यह (seq, बैच, फीचर) है (डिफ़ॉल्ट: ग़लत)

उदाहरण:
>>> डिकोडर_लेयर = ट्रांसफार्मरडिकोडरलेयर(d_model=512, nhead=8)
>>> मेमोरी = टॉर्च.रैंडन((10, 32, 512))
>>>टीजीटी = टॉर्च.रैंडन((20, 32, 512))
>>> आउट = डिकोडर_लेयर(टीजीटी, मेमोरी)
'''
def __init__(स्वयं, d_model, nhead, dim_feedforward=2048, ड्रॉपआउट=0.1, सक्रियण=F.relu,
लेयर_नॉर्म_ईपीएस=1ई-5, बैच_फर्स्ट=गलत) -> कोई नहीं:
सुपर(ट्रांसफॉर्मरडिकोडरलेयर, स्वयं).__init__()
self.self_attn = मल्टीहेडअटेंशन(d_model, nhead, ड्रॉपआउट=ड्रॉपआउट, बैच_पहला=बैच_पहला)
self.multihead_attn = मल्टीहेडअटेंशन(d_model, nhead, ड्रॉपआउट=ड्रॉपआउट, बैच_फर्स्ट=बैच_फर्स्ट)

self.linear1 = nn.Linear(d_model, dim_feedforward)
स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(ड्रॉपआउट)
self.linear2 = nn.Linear(dim_feedforward, d_model)

self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)
self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)
self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)
स्व.ड्रॉपआउट1 = एनएन.ड्रॉपआउट(ड्रॉपआउट)
स्व.ड्रॉपआउट2 = एनएन.ड्रॉपआउट(ड्रॉपआउट)
स्वयं.डीरोपआउट3 = एनएन.ड्रॉपआउट(ड्रॉपआउट)स्वयं.सक्रियण = सक्रियण

डीईएफ़ फॉरवर्ड(स्वयं, टीजीटी: टेंसर, मेमोरी: टेंसर, टीजीटी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं,
मेमोरी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं, tgt_key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं, मेमोरी_की_पैडिंग_मास्क: वैकल्पिक[टेंसर] = कोई नहीं) -> टेंसर:
आर'''
पैरामीटर:
tgt: लक्ष्य भाषा अनुक्रम (आवश्यक)
मेमोरी: अंतिम एन्कोडर_लेयर से वाक्य (आवश्यक)
tgt_mask: लक्ष्य भाषा अनुक्रम का मुखौटा (वैकल्पिक)
मेमोरी_मास्क (वैकल्पिक)
tgt_key_padding_mask(वैकल्पिक)
मेमोरी_की_पैडिंग_मास्क (वैकल्पिक)
'''
tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,
key_padding_mask=tgt_key_padding_mask)[0]
टीजीटी = टीजीटी + सेल्फ.ड्रॉपआउट1(टीजीटी2)
टीजीटी = self.norm1(टीजीटी)
tgt2 = self.multihead_attn(tgt, मेमोरी, मेमोरी, attn_mask=memory_mask,
key_padding_mask=memory_key_padding_mask)[0]
टीजीटी = टीजीटी + सेल्फ.ड्रॉपआउट2(टीजीटी2)
टीजीटी = self.norm2(टीजीटी)
tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
टीजीटी = टीजीटी + सेल्फ.ड्रॉपआउट3(टीजीटी2)
टीजीटी = self.norm3(टीजीटी)
गीला करनाकलश टीजीटी
```

```अजगर
# प्यारा सा उदाहरण
डिकोडर_लेयर = nn.TransformerDecoderLayer(d_model=512, nhead=8)
मेमोरी = टॉर्च.रैंडन((10, 32, 512))
टीजीटी = टॉर्च.रैंडन((20, 32, 512))
आउट = डिकोडर_लेयर(टीजीटी, मेमोरी)
प्रिंट(आउट.आकार)
#मशाल.आकार([20,32,512])
```

टॉर्च.आकार([20,32,512])

```अजगर
##डिकोडर
```

```अजगर
क्लास ट्रांसफार्मर डिकोडर (एनएन.मॉड्यूल):
आर'''पैरामीटर:
डिकोडर_लेयर (आवश्यक)
num_layers: डिकोडर_लेयर की परतों की संख्या (आवश्यक)
मानक: सामान्यीकरण चयन

उदाहरण:>>> डिकोडर_लेयर = ट्रांसफार्मरडिकोडरलेयर(d_model=512, nhead=8)
>>> ट्रांसफार्मर_डिकोडर = ट्रांसफार्मर डिकोडर(डिकोडर_लेयर, संख्या_लेयर=6)
>>> मेमोरी = टॉर्च.रैंड(10, 32, 512)
>>> टीजीटी = टॉर्च.रैंड(20, 32, 512)
>>>आउट = ट्रांसफार्मर_डिकोडर(टीजीटी, मेमोरी)
'''
def __init__(स्वयं, डिकोडर_लेयर, num_layers, मानदंड=कोई नहीं):
सुपर(ट्रांसफॉर्मरडिकोडर, स्वयं).__init__()
सेल्फ.लेयर = डिकोडर_लेयर
self.num_layers = num_layers
स्व.मानदंड = आदर्श

डीईएफ़ फ़ॉरवर्ड(स्वयं, टीजीटी: टेंसर, मेमोरी: टेंसर, टीजीटी_मास्क: वैकल्पिक[टीसेंसर] = कोई नहीं,
मेमोरी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं, tgt_key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं,
मेमोरी_की_पैडिंग_मास्क: वैकल्पिक[टेंसर] = कोई नहीं) -> टेंसर:
आउटपुट=tgt
रेंज में _ के लिए (self.num_layers):
आउटपुट = सेल्फ.लेयर(आउटपुट, मेमोरी, tgt_mask=tgt_mask,
मेमोरी_मास्क=मेमोरी_मास्क,
tgt_key_padding_mask=tgt_key_padding_mask,
मेमोरी_की_पैडिंग_मास्क=मेमोरी_की_पैडिंग_मास्क)
यदि self.norm कोई नहीं है:
आउटपुट = self.norm(आउटपुट)

वापसी आउटपुट
```

```अजगर
# प्यारा सा उदाहरण
डिकोडर_लेयर =ट्रांसफार्मरडिकोडरलेयर(d_model=512, nhead=8)
ट्रांसफार्मर_डिकोडर = ट्रांसफार्मर डिकोडर(डिकोडर_लेयर, संख्या_लेयर=6)
मेमोरी = टॉर्च.रैंड(10, 32, 512)
टीजीटी = टॉर्च.रैंड(20, 32, 512)
आउट = ट्रांसफार्मर_डिकोडर(टीजीटी, मेमोरी)
प्रिंट(आउट.आकार)
#मशाल.आकार([20,32,512])
```

टॉर्च.आकार([20,32,512])

संक्षेप में कहें तो, स्थिति एन्कोडिंग और मल्टी-हेड ध्यान के बाद, एनकोडर लेयर और डिकोडर लेयर के आकार नहीं बदलेंगे, जबकि एनकोडर और डिकोडर क्रमशः src और tgt के आकार के अनुरूप हैं।सक्रिय रूप से

## ट्रांसफार्मर

```अजगर
क्लास ट्रांसफार्मर (एनएन.मॉड्यूल):
आर'''
पैरामीटर:
d_model: शब्द एम्बेडिंग आयाम (आवश्यक) (डिफ़ॉल्ट=512)
एनहेड: मल्टी-हेड अटेंशन में समानांतर हेड की संख्या (आवश्यक) (डिफ़ॉल्ट=8)
num_encoder_layers: एन्कोडिंग परतों की संख्या (डिफ़ॉल्ट=8)
num_decoder_layers: डिकोडिंग परतों की संख्या (डिफ़ॉल्ट=8)
dim_feedforward: पूरी तरह से जुड़ी हुई परत में न्यूरॉन्स की संख्या, जिसे इस परत के माध्यम से इनपुट के आयाम के रूप में भी जाना जाता है (डिफ़ॉल्ट = 2048)
ड्रॉपआउट: संभावनाड्रॉपआउट की मात्रा (डिफ़ॉल्ट = 0.1)
सक्रियण: दो रैखिक परतों, डिफ़ॉल्ट रिले या गेलु के बीच सक्रियण फ़ंक्शन
कस्टम_एनकोडर: कस्टम एनकोडर (डिफ़ॉल्ट=कोई नहीं)
कस्टम_डिकोडर: कस्टम डिकोडर (डिफ़ॉल्ट=कोई नहीं)
lay_norm_eps: हर को 0 होने से रोकने के लिए परत सामान्यीकरण में एक छोटी राशि (डिफ़ॉल्ट = 1e-5)
बैच_फर्स्ट: यदि `सही` है, तो यह (बैच, seq, फीचर) है, यदि `गलत` है, तो यह (seq, बैच, फीचर) है (डिफ़ॉल्ट: गलत)

उदाहरण:
>>> ट्रांसफार्मर_मॉडल = ट्रांसफार्मर (एनहेड = 16, संख्या_एनकोडर_लेयरएस=12)
>>> स्रोत = टॉर्च.रैंड((10, 32, 512))
>>> टीजीटी = टॉर्च.रैंड((20, 32, 512))
>>> आउट = ट्रांसफार्मर_मॉडल(src, tgt)
'''
def __init__(स्वयं, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,
num_decoder_layers: int = 6, dim_feedforward: int = 2048, ड्रॉपआउट: फ्लोट = 0.1,
सक्रियण = F.relu, कस्टम_एनकोडर: वैकल्पिक[कोई भी] = कोई नहीं, कस्टम_डिकोडर: वैकल्पिक[कोई भी] = कोई नहीं,
लेयर_नॉर्म_ईपीएस: फ्लोट = 1ई-5, बैच_फर्स्ट: बूल = गलत) -> कोई नहीं:
सुपर(ट्रांसफॉर्मर, स्वयं).__init__()
यदि कस्टम_एन्कोडरकोई नहीं है:
स्व.एनकोडर = कस्टम_एनकोडर
अन्य:
एनकोडर_लेयर = ट्रांसफार्मरएनकोडरलेयर(डी_मॉडल, एनहेड, डिम_फीडफॉरवर्ड, ड्रॉपआउट,
सक्रियण, लेयर_नॉर्म_ईपीएस, बैच_फर्स्ट)
एनकोडर_नॉर्म = एनएन.लेयरनॉर्म(डी_मॉडल, ईपीएस=लेयर_नॉर्म_ईपीएस)
self.encoder = ट्रांसफार्मरएनकोडर(एनकोडर_लेयर, num_encoder_layers)

यदि ccustom_decoder कोई नहीं है:
सेल्फ.डिकोडर = कस्टम_डिकोडर
अन्य:
डिकोडर_लेयर = ट्रांसफार्मरडिकोडरलेयर(डी_मॉडल, एनहेड, डिम_फीडफॉरवर्ड, ड्रॉपआउट,
सक्रियण, लेयर_नॉर्म_ईपीएस, बैच_फर्स्ट)
डिकोडर_एनओआरएम = एनएन.लेयरनॉर्म(डी_मॉडल, ईपीएस=लेयर_नॉर्म_ईपीएस)
self.decoder = ट्रांसफार्मरडिकोडर(डिकोडर_लेयर, num_decoder_layers, डिकोडर_नॉर्म)

self._reset_parameters()

self.d_model = d_modelself.nhead = nhead

स्वयं.बैच_प्रथम = बैच_प्रथम

डीईएफ़ फ़ॉरवर्ड(स्वयं, स्रोत: टेंसर, टीजीटी: टेंसर, स्रोत_मास्क: वैकल्पिक[टेंसर] = कोई नहीं, टीजीटी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं,
मेमोरी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं, src_key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं,
tgt_key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं, मेमोरी_की_पैडिंग_मासk: वैकल्पिक[टेंसर] = कोई नहीं) -> टेंसर:
आर'''
पैरामीटर:
स्रोत: स्रोत भाषा अनुक्रम (एनकोडर को भेजा गया) (आवश्यक)
tgt: लक्ष्य भाषा अनुक्रम (डिकोडर को भेजा गया) (आवश्यक)
src_mask: (वैकल्पिक)
tgt_mask: (वैकल्पिक)
मेमोरी_मास्क: (वैकल्पिक)
src_key_padding_mask: (वैकल्पिक)
tgt_key_padding_mask: (वैकल्पिक)
मेमोरी_की_पैडिंग_मास्क: (वैकल्पिक)

आकार:
- स्रोत: आकार:`(एस, एन, ई)`, `(एन, एस, ई)` यदि बैच_फर्स्ट।
- tgt: आकार:`(T, N, E)`, `(N, T, E)` यदि बैच_फर्स्ट।
- src_mask: आकार:`(एस, एस)`।
- tgt_mask: शापे:`(टी, टी)`।
- मेमोरी_मास्क: आकार:`(टी, एस)`.- src_key_padding_mask: आकार:`(एन, एस)`।
- tgt_key_padding_mask: आकार:`(एन, टी)`।
- मेमोरी_की_पैडिंग_मास्क: आकार:`(एन, एस)`।

[src/tgt/memory]_mask यह सुनिश्चित करता है कि कुछ स्थितियाँ दिखाई न दें। उदाहरण के लिए, डिकोडिंग करते समय, केवल स्थिति और पिछली स्थिति देखी जा सकती है, लेकिन बाद की स्थिति नहीं।
यदि यह बाइटटेन्सर है, तो गैर-शून्य स्थितियों को नजरअंदाज कर दिया जाएगा और यदि यह बूलटेन्सर है, तो ट्रू के अनुरूप स्थितियों को नजरअंदाज कर दिया जाएगा;यदि यह एक संख्यात्मक मान है, तो इसे सीधे attn_weights में जोड़ा जाएगा

[src/tgt/memory]_key_padding_mask कुंजी में कुछ तत्वों को ध्यान गणना में भाग नहीं देता है। तीन मामले ऊपर के समान हैं

- आउटपुट: आकार:`(टी, एन, ई)`, `(एन, टी, ई)` यदि बैच_फर्स्ट।

नोट: src और tgt का अंतिम आयाम d_model के बराबर होना चाहिए, और बैच का आयाम बराबर होना चाहिए

उदाहरण:
>>> आउटपुट = ट्रांसफार्मर_मॉडल(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)
'''
स्मृति = स्व.एनकोडर(src, मास्क=src_mask, src_key_padding_mask=src_key_padding_mask)
आउटपुट = सेल्फ.डिकोडर(टीजीटी, मेमोरी, टीजीटी_मास्क=टीजीटी_मास्क, मेमोरी_मास्क=मेमोरी_मास्क,
tgt_key_padding_mask=tgt_key_padding_mask,
मेमोरी_की_पैडिंग_मास्क=मेमोरी_की_पैडिंग_मास्क)
वापसी आउटपुट

def generate_square_subsequent_mask(self, sz: int) -> Tensor:
r'''अनुक्रम के बारे में एक मुखौटा बनाएं, नकाबपोश क्षेत्र को `-inf` को सौंपा गया है, और नकाबपोश क्षेत्र को `0`''' को सौंपा गया है
मुखौटा = (मशाल.triu(मशाल.ones(sz, sz)) == 1).स्थानांतरण(0,1)
मास्क = मास्क.फ्लोट().मास्कड_फिल(मास्क == 0, फ्लोट('-inf')).मास्कड_फिल(मास्क == 1, फ्लोट(0.0))
वापसी मुखौटा

def _reset_parameters(स्वयं):
r'''सामान्य वितरण के साथ पैरामीटर प्रारंभ करें''
self.parameters() में p के लिए:
यदि p.dim() > 1:
जेवियर_यूनिफ़ॉर्म_(पी)
```

```अजगर
# छोटा सा उदाहरण
ट्रांसफार्मर_मॉडल = ट्रांसफार्मर(nhead=16, num_encoder_layers=12)
स्रोत = टॉर्च.रैंड((10, 32, 512))
टीजीटी = टॉर्च.रैंड((20, 32, 512))
आउट = ट्रांसफार्मर_मॉडल(src, tgt)
प्रिंट(आउट.आकार)
#मशाल.आकार([20,32,512])
```टॉर्च.आकार([20,32,512])

अब तक, हमने PyTorch की ट्रांसफॉर्मर लाइब्रेरी को पूरी तरह से कार्यान्वित कर दिया है, आधिकारिक संस्करण की तुलना में, इस हस्तलिखित में कम निर्णय कथन हैं।
## आभार
यह लेख ताई युनपेंग द्वारा लिखा गया था और परियोजना के सदस्यों द्वारा पुनर्गठित किया गया था, अंत में, मैं आपकी पढ़ने की प्रतिक्रिया और स्टार की प्रतीक्षा कर रहा हूं, धन्यवाद।