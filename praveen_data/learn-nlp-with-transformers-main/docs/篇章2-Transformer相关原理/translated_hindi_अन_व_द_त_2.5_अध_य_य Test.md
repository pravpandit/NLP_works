## अध्याय प्रश्नोत्तरी
* प्रश्न 1: ट्रांसफार्मर में सॉफ्टमैक्स गणना को $d_k$ से विभाजित करने की आवश्यकता क्यों है?
* प्रश्न 2: ट्रांसफार्मर में ध्यान स्कोर की गणना करते समय पैडिंग स्थिति को कैसे छुपाया जाए?
* प्रश्न 3: ट्रांसफॉर्मर में पोजिशनल एंबेडिंग क्यों जोड़ी जाती है?
* प्रश्न 4: बीईआरटी पूर्व-प्रशिक्षण के दौरान मास्क का अनुपात, क्या बड़े अनुपात को मास्क किया जा सकता है?
*प्रश्न 5: BERT टोकनाइज़ेशन कैसे होता है? इसके क्या लाभ हैं?
* प्रश्न 6: जीपीटी टोकनाइज़ेशन कैसे करता है और बीईआरटी में क्या अंतर है??
* प्रश्न 7: बीईआरटी मॉडल बहुत बड़ा है, और एक एकल जीपीयू प्रशिक्षण केवल 1 बैच में लगाया जा सकता है, प्रशिक्षण कैसे दिया जाए?
* प्रश्न 8: ट्रांसफार्मर को पोजीशन एंबेडिंग की आवश्यकता क्यों है?
*प्रश्न 9: ट्रांसफार्मर में अवशिष्ट नेटवर्क संरचना की क्या भूमिका है?
* प्रश्न 10: क्या बीईआरटी प्रशिक्षण के दौरान नकाबपोश शब्दों का अनुपात बहुत बड़ा (80% से अधिक) हो सकता है?
* प्रश्न 11: BERT प्री-ट्रेनिंग मास्क कैसे बनाता है?
*प्रश्न 11: Word2vec से BERT में क्या सुधार किये गये हैं?