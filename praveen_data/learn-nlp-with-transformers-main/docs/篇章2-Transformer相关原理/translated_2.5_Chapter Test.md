## Chapter Quiz
* Question 1: Why does the softmax calculation in Transformer need to be divided by $d_k$?
* Question 2: How to mask the padding position when calculating the attention score in Transformer?
* Question 3: Why is positional embedding added to Transformer?
* Question 4: The proportion of masks during BERT pre-training, can a larger proportion be masked?
* Question 5: How does BERT tokenize? What are the benefits?
* Question 6: How does GPT tokenize? What is the difference with BERT?
* Question 7: The BERT model is very large, and a single GPU training can only put in 1 batch, how to train?
* Question 8: Why does Transformer need a position embedding?
* Question 9: What is the role of the residual network structure in Transformer?
* Question 10: Can the proportion of masked words be very large (greater than 80%) during BERT training?
* Question 11: How does BERT pre-training mask?
* Question 11: What improvements have been made from word2vec to BERT?