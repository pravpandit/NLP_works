## ग्राफिक ध्यान
सामग्री संगठन:
- सचित्र ध्यान
  - seq2seq ढांचा
  - seq2seq विवरण
  -ध्यान
  - आभार

अध्याय 1 में हमने एनएलपी में ट्रांसफॉर्मर्स के उदय का एक सिंहावलोकन दिया। इस ट्यूटोरियल का सीखने का मार्ग है: ध्यान दें->ट्रांसफॉर्मर->बीईआरटी->एनएलपी एप्लिकेशन। इसलिए, यह अध्याय ध्यान से शुरू होगा और धीरे-धीरे ट्रांसफार्मर संरचना में शामिल ज्ञान को गहराई से समझाएगा, जिससे पाठकों को एक विशद विवरण मिलेगा।

प्रश्नः अटेंशन का कारण क्या है?
संभावित उत्तर: आवर्ती तंत्रिका नेटवर्क (आरएनएन) पर आधारित seq2seq मॉडल लंबे पाठों को संसाधित करते समय चुनौतियों का सामना करते हैं, और लंबे पाठों में विभिन्न स्थानों पर जानकारी पर ध्यान देने से आरएनएन के मॉडल प्रभाव को बेहतर बनाने में मदद मिल सकती है।

तो सीखने की समस्या को इस प्रकार विभाजित किया गया है: 1. seq2seq मॉडल क्या है? 2. RNN-आधारित seq2seq मॉडल टेक्स्ट/लंबे टेक्स्ट अनुक्रमों को कैसे संभालता है? 3. लंबे पाठ अनुक्रमों को संसाधित करते समय seq2seq मॉडल को किन समस्याओं का सामना करना पड़ता है? 4. RNN पर आधारित seq2seq मॉडल मॉडल प्रभाव को बेहतर बनाने के लिए ध्यान के साथ कैसे जुड़ता है?

## seq2seq ढांचा

seq2seq एक सामान्य एनएलपी मॉडल संरचना है, इसका पूरा नाम हैयह है: अनुक्रम से अनुक्रम, जिसका अनुवाद "अनुक्रम से अनुक्रम" के रूप में किया गया है। जैसा कि नाम से पता चलता है: टेक्स्ट अनुक्रम से एक नया टेक्स्ट अनुक्रम प्राप्त करें। विशिष्ट कार्यों में शामिल हैं: मशीनी अनुवाद कार्य और पाठ सारांशीकरण कार्य। Google अनुवाद ने 2016 के अंत में seq2seq मॉडल का उपयोग करना शुरू किया और 2 अभूतपूर्व पेपर प्रकाशित किए: [सीक्वेंस टू सीक्वेंस लर्निंग, 2014 में सुतस्केवर एट अल द्वारा प्रकाशित
न्यूरल नेटवर्क्स के साथ](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) और [चो एट अल द्वारा प्रकाशित आरएनएन एनकोडर का उपयोग करके वाक्यांश प्रतिनिधित्व सीखना 2014- डिकोडर
सांख्यिकीय मशीनी अनुवाद के लिए](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf), इच्छुक पाठक सीखने के लिए मूल पाठ पढ़ सकते हैं।

भले ही पाठकों ने उपरोक्त दो Google लेख पढ़े हों, एनएलपी शुरुआती लोगों के लिए seq2seq मॉडल को पूरी तरह से समझना और लागू करना आसान नहीं है।. क्योंकि हमें संबंधित एनएलपी अवधारणाओं की एक श्रृंखला को खत्म करने की आवश्यकता है, और ये एनएलपी अवधारणाएं परत दर परत प्रगतिशील हैं, इसलिए seq2seq मॉडल की स्पष्ट समझ होना आसान नहीं है। हालाँकि, अगर इन जटिल और अपरिचित एनएलपी अवधारणाओं की कल्पना की जा सकती है, तो इसे समझना वास्तव में आसान हो जाएगा। इसलिए, यह लेख एनएलपी के शुरुआती लोगों को चित्रों और गतिशील ग्राफिक्स की एक श्रृंखला के माध्यम से seq2seq और ध्यान से संबंधित अवधारणाओं और ज्ञान को सीखने में मदद करने की उम्मीद करता है।

सबसे पहले, आइए देखें कि seq2seq क्या करता है? Seq2seq मॉडल का इनपुट एक अनुक्रम (शब्द, अक्षर, या छवि विशेषताएँ) हो सकता है, और आउटपुट एक अन्य अनुक्रम (शब्द, अक्षर, या छवि विशेषताएँ) हो सकता है। एक प्रशिक्षित seq2seq मॉडल नीचे दिए गए चित्र में दिखाया गया है (नोट: माउस को चित्र पर रखें और चित्र हिल जाएगा):


![seq2seq](./pictures/1-seq2seq.gif) गतिशील चित्र: seq2seq

जैसा कि नीचे दिए गए चित्र में दिखाया गया है, एनएलपी में मशीनी अनुवाद कार्य को एक उदाहरण के रूप में लेते हुए, अनुक्रम शब्दों की एक श्रृंखला को संदर्भित करता है, और आउटपुट भी शब्दों की एक श्रृंखला है।
![अनुवाद](./pictures/1-2-translation.gif) गतिशील चित्र: अनुवाद

## seq2seq विवरण
उपरोक्त चित्र में नीले seq2seq मॉडल को अलग करें, जैसा कि नीचे दिए गए चित्र में दिखाया गया है: seq2seq मॉडल में एक एनकोडर (एनकोडर) और एक डिकोडर होता है(डिकोडर) रचना। हरा एनकोडर इनपुट अनुक्रम में प्रत्येक तत्व को संसाधित करता है और इनपुट जानकारी प्राप्त करता है, जिसे पीले वेक्टर (संदर्भ वेक्टर कहा जाता है) में परिवर्तित किया जाता है। जब हम संपूर्ण इनपुट अनुक्रम को संसाधित करते हैं, तो एनकोडर संदर्भ वेक्टर को बैंगनी डिकोडर को भेजता है। डिकोडर तत्व द्वारा एक नए अनुक्रम तत्व को आउटपुट करने के लिए संदर्भ वेक्टर में जानकारी का उपयोग करता है।

![encoder-decode](./pictures/1-3-encoder-decoder.gif) गतिशील चित्र: seq2seq में एनकोडर-डिकोडर

चूँकि seq2seq मॉडल का उपयोग मशीनी अनुवाद कार्यों को हल करने के लिए किया जा सकता है, मशीनी अनुवाद कार्य का अध्ययन seq2seq मॉडल के एक विशिष्ट उदाहरण के रूप में किया जाता है जैसा कि नीचे दिए गए चित्र में दिखाया गया है।

![एनकोडर-डिकोडर](./pictures/1-3-mt.gif) गतिशील चित्र: seq2seq में एनकोडर-डिकोडर, मशीनी अनुवाद का उदाहरण

मशीनी अनुवाद कार्यों में seq2seq मॉडल को गहराई से सीखें, जैसा कि नीचे दिए गए चित्र में दिखाया गया है। Seq2seq मॉडल में एनकोडर और डिकोडर आम तौर पर आवर्तक तंत्रिका नेटवर्क आरएनएन (ट्रांसफॉर्मर मॉडल के प्रकट होने से पहले का युग) का उपयोग करते हैं। एनकोडर इनपुट फ्रेंच शब्द अनुक्रम को एक संदर्भ वेक्टर (हरे रंग में एन्कोड) में एन्कोड करता हैr बैंगनी डिकोडर के बीच दिखाई देता है), और फिर डिकोडर संदर्भ वेक्टर के आधार पर अंग्रेजी शब्द अनुक्रम को डिकोड करता है। * आवर्तक तंत्रिका नेटवर्क के संबंध में, यह लेख पढ़ने की अनुशंसा करता है [लुइस सेरानो द्वारा लिखित आवर्तक तंत्रिका नेटवर्क का उत्कृष्ट परिचय](https://www.youtube.com/watch?v=UNmqTiOnRfg)।*

![संदर्भ वेक्टर चित्र के मध्य में एक फ़्लोटिंग पॉइंट वेक्टर से मेल खाता है। नीचे हम उच्च मूल्यों को दर्शाने के लिए चमकीले रंगों का उपयोग करके इन वैक्टरों की कल्पना करेंगे, जैसा कि ऊपर की छवि में दाईं ओर दिखाया गया है](./pictures/1-4-context-example.png)

चित्र: संदर्भ वेक्टर उपरोक्त चित्र के मध्य में फ़्लोटिंग पॉइंट वेक्टर से मेल खाता है। नीचे हम उच्च मानों को दर्शाने के लिए चमकीले रंगों का उपयोग करते हुए इन संख्यात्मक वैक्टरों की कल्पना करेंगे, जैसा कि ऊपर की छवि में दाईं ओर दिखाया गया है

जैसा कि ऊपर चित्र में दिखाया गया है, आइए एक नजर डालते हैं कि पीला संदर्भ वेक्टर क्या है? मूलतः फ़्लोटिंग पॉइंट संख्याओं का एक सेट। इस संदर्भ की सरणी लंबाई एनकोडर आरएनएन के छिपे हुए परत न्यूरॉन्स की संख्या पर आधारित है। उपरोक्त आंकड़ा 4 की लंबाई के साथ एक संदर्भ वेक्टर दिखाता है, लेकिन वास्तविक अनुप्रयोगों में, संदर्भ वेक्टर की लंबाई अनुकूलित की जाती है, जैसे 256, 512 या 1024।

तो आरएनएन विशेष रूप से इनपुट अनुक्रम को कैसे संसाधित करता है?

1. मान लें कि अनुक्रम इनपुट एक वाक्य है, जिसे $n$ शब्दों द्वारा दर्शाया जा सकता है: $senटेंस = \{w_1, w_2,...,w_n\}$.
2. आरएनएन पहले एक वेक्टर अनुक्रम प्राप्त करने के लिए वाक्य में प्रत्येक शब्द को एक वेक्टर में मैप करता है: $X = \{x_1, x_2,...,x_n\}$। प्रत्येक शब्द के लिए मैप किए गए वेक्टर को आमतौर पर शब्द एम्बेडिंग भी कहा जाता है .
3. फिर $t\in के अनुक्रम इनपुट $x_t$ को [1,n]$वें समय चरण में संसाधित करते समय, RNN नेटवर्क के इनपुट और आउटपुट को इस प्रकार व्यक्त किया जा सकता है: $h_{t} = RNN(x_t, h_{t -1})$

    - इनपुट: समय चरण $t$ पर RNN के इनपुट में से एक वेक्टर $x_t$ है जो शब्द $w_t$ को मैप करके प्राप्त किया जाता है।
    - इनपुट: आरएनएन का अन्य इनपुट छिपा हुआ राज्य वेक्टर $h_{t-1}$ है जो पिछले समय चरण $t-1$ पर प्राप्त हुआ था, जो एक वेक्टर भी है।
    - आउटपुट: समय चरण $t$ पर RNN का आउटपुट $h_t$ छिपा हुआ राज्य वेक्टर है।



![शब्दों को संसाधित करने से पहले, हमें उन्हें वेक्टर में परिवर्तित करना होगा। यह रूपांतरण शब्द एम्बेडिंग एल्गोरिदम का उपयोग करके किया जाता है। हम पूर्व-प्रशिक्षित एम्बेडिंग का उपयोग कर सकते हैं, या अपने डेटासेट पर अपनी स्वयं की एम्बेडिंग को प्रशिक्षित कर सकते हैं। आमतौर पर एम्बेडिंग वेक्टर का आकार 200 या 30 होता है0. सरलता के लिए, यहां दिखाए गए वेक्टर की लंबाई 4 है](./pictures/1-5-word-vector.png) चित्र: शब्द एम्बेडिंग उदाहरण। शब्दों को संसाधित करने से पहले, हमें शब्दों को वैक्टर में मैप करने की आवश्यकता होती है, जो आमतौर पर शब्द एम्बेडिंग एल्गोरिदम का उपयोग करके किया जाता है। सामान्यतया, हम पूर्व-प्रशिक्षित शब्द एम्बेडिंग का उपयोग कर सकते हैं, या अपने स्वयं के डेटा सेट पर शब्द एम्बेडिंग को प्रशिक्षित कर सकते हैं। सरलता के लिए, उपरोक्त चित्र में दिखाया गया शब्द एम्बेडिंग आयाम 4 है। उपरोक्त चित्र के बाईं ओर प्रत्येक शब्द शब्द एम्बेडिंग एल्गोरिदम से गुजरने के बाद बीच में एक संबंधित 4-आयामी वेक्टर प्राप्त करता है।


आइए आगे देखें कि RNN-आधारित seq2seq मॉडल में एनकोडर पहले चरण में कैसे काम करता है:

![rnn](./pictures/1-6-rnn.gif) गतिशील आरेख: जैसा कि चित्र में दिखाया गया है, RNN छिपी हुई स्थिति #10 (छिपी हुई परत स्थिति) और इनपुट वेक्टर इनपुट#1 प्राप्त करने के लिए पहली बार चरण का उपयोग करता है दूसरी बार के चरण का उपयोग नई आउटपुट छिपी हुई स्थिति#1 प्राप्त करने के लिए किया जाता है।

नीचे दिए गए गतिशील आरेख को देखते हुए, आइए विस्तार से देखें कि एनकोडर प्रत्येक समय चरण में छिपी हुई स्थिति को कैसे प्राप्त करता है और अंतिम छिपी हुई स्थिति को डिकोडर तक कैसे पहुंचाता है।, डिकोडर एनकोडर द्वारा दी गई अंतिम छिपी हुई स्थिति की जानकारी के आधार पर आउटपुट अनुक्रम को डिकोड करता है। ध्यान दें कि अंतिम छिपी हुई स्थिति वास्तव में संदर्भ वेक्टर है जिसका हमने ऊपर उल्लेख किया है।
![](./pictures/1-6-seq2seq.gif) गतिशील आरेख: एनकोडर धीरे-धीरे छिपी हुई स्थिति प्राप्त करता है और अंतिम छिपी हुई स्थिति को डिकोडर तक पहुंचाता है।

इसके बाद, इनपुट अनुक्रम को संसाधित करने वाले एनकोडर के साथ मिलकर, आइए देखें कि डिकोडर चरण दर चरण आउटपुट अनुक्रम का एल कैसे प्राप्त करता है। एनकोडर के समान, डिकोडर को भी प्रत्येक समय चरण पर छिपी हुई स्थिति (छिपी हुई परत की स्थिति) मिलती है, और उसे एक समय के चरण से अगली बार के चरण तक छिपी हुई स्थिति (छिपी हुई परत की स्थिति) को पारित करने की भी आवश्यकता होती है।

![](./pictures/1-6-seq2seq-decoder.gif) गतिशील आरेख: एनकोडर पहले समय चरणों के अनुसार क्रम में प्रत्येक फ्रांसीसी शब्द को एनकोड करता है, और अंत में अंतिम छिपी स्थिति, जो कि संदर्भ वेक्टर है, को पास करता है डिकोडर, और डिकोड डिवाइस अंग्रेजी आउटपुट प्राप्त करने के लिए संदर्भ वेक्टर के अनुसार धीरे-धीरे डिकोड करता है।

अब तक, मुझे आशा है कि आप इस लेख की शुरुआत में पूछे गए पहले दो प्रश्नों को समझ गए होंगे: 1. seq2seq मॉडल क्या है? 2. seq2seq मॉडल टेक्स्ट/लंबे टेक्स्ट अनुक्रमों को कैसे संभालता है? तो फिर कृपया प्रश्न 3 और 4:3 के बारे में सोचेंपाठ अनुक्रमों (विशेषकर लंबे पाठ अनुक्रमों) को संसाधित करते समय 2seq मॉडल को किन समस्याओं का सामना करना पड़ता है? 4. RNN पर आधारित seq2seq मॉडल समस्या 3 को हल करने और मॉडल प्रभाव में सुधार करने के लिए ध्यान के साथ कैसे जुड़ता है?

## ध्यान
RNN-आधारित seq2seq मॉडल एनकोडर की सभी जानकारी एक संदर्भ वेक्टर में एन्कोड की गई है, जो इस प्रकार के मॉडल की अड़चन है। एक ओर, एक वेक्टर के लिए सभी पाठ अनुक्रमों की जानकारी शामिल करना मुश्किल है, दूसरी ओर, आरएनएन पाठ अनुक्रमों को पुनरावर्ती रूप से एन्कोड करता है, जिससे लंबे पाठों को संसाधित करते समय मॉडल को बहुत बड़ी चुनौतियों का सामना करना पड़ता है (उदाहरण के लिए, जब आरएनएन)। 500वें शब्द को संसाधित करता है, इससे अधिक जानकारी को 1-499 शब्दों में समाहित करना कठिन है।

उपरोक्त समस्याओं का सामना करते हुए, 2014 में बहदानौ एट अल द्वारा जारी [ज्वाइंटली लर्निंग टू अलाइन एंड ट्रांसलेशन द्वारा न्यूरल मशीन ट्रांसलेशन] (https://arxiv.org/abs/1409.0473) और [ध्यान-आधारित न्यूरल लर्निंग के लिए प्रभावी दृष्टिकोण 2015 मशीनी अनुवाद में लुओंग एट अल
](https://arxiv.org/abs/1508.04025) दो पेपरों में, एक विधि कहा जाता हैध्यान की तकनीक **ध्यान**। ध्यान प्रौद्योगिकी के माध्यम से, seq2seq मॉडल मशीन अनुवाद की गुणवत्ता में काफी सुधार करता है। इसका कारण यह है: ध्यान तंत्र seq2seq मॉडल को भेद और फोकस के साथ इनपुट अनुक्रम पर ध्यान देने की अनुमति देता है।

नीचे दिया गया चित्र अभी भी मशीनी अनुवाद का एक उदाहरण है:

![समय चरण 7 पर, ध्यान तंत्र अंग्रेजी अनुवाद तैयार करने से पहले डिकोडर को शब्द "छात्र" (जिसका फ्रेंच में अर्थ है "छात्र") पर ध्यान केंद्रित करने की अनुमति देता है। इनपुट अनुक्रम से प्रासंगिक संकेतों को बढ़ाने की यह क्षमता ध्यान मॉडल को बिना ध्यान दिए मॉडल की तुलना में बेहतर परिणाम देने की अनुमति देती है। ](./pictures/1-7-attetion.png) चित्र: 7वें समय चरण में, ध्यान तंत्र अंग्रेजी अनुवाद छात्र अंग्रेजी अनुवाद: étudiant का उत्पादन करने से पहले डिकोडर को फ्रेंच इनपुट अनुक्रम पर ध्यान केंद्रित करने की अनुमति देता है। इस प्रकार का भेदभावपूर्ण ध्यान इनपुट अनुक्रम की महत्वपूर्ण जानकारी पर ध्यान दे सकता है, जिससे मॉडल के बेहतर परिणाम हो सकते हैं।

आइए हम seq2seq मॉडल को ध्यान से समझना जारी रखें: ध्यान मॉडल और क्लासिक seq2seq मॉडल के बीच दो मुख्य अंतर हैं:


- ए. सबसे पहले, एनकोडर डिकोडर को अधिक डेटा पास करेगा। एनकोडर सभी समय चरणों की छिपी हुई स्थिति (छिपी परत स्थिति) को डिकोडर तक पहुंचाता है, औरकेवल अंतिम छिपी हुई स्थिति (छिपी हुई परत की स्थिति) को पार करने के बजाय, जैसा कि नीचे दिए गए गतिशील चित्र में दिखाया गया है:
![](./pictures/1-6-mt-1.gif) गतिशील चित्र: अधिक जानकारी डिकोडर को भेज दी जाती है

- बी. ध्यान मॉडल का डिकोडर आउटपुट उत्पन्न करने से पहले एक अतिरिक्त ध्यान प्रक्रिया करता है। जैसा कि नीचे चित्र में दिखाया गया है, विवरण इस प्रकार हैं:

  - 1. चूंकि एनकोडर में प्रत्येक छिपी हुई स्थिति (छिपी हुई परत की स्थिति) इनपुट वाक्य में एक शब्द से मेल खाती है, इसलिए डिकोडर को एनकोडर की सभी प्राप्त छिपी हुई स्थिति (छिपी हुई परत की स्थिति) को देखने की जरूरत है।
  - 2. प्रत्येक छिपे हुए राज्य के लिए एक स्कोर की गणना करें (फिलहाल हम इस स्कोर की गणना प्रक्रिया को अनदेखा करते हैं)।
  - 3. सभी छिपे हुए राज्य (छिपे हुए परत राज्य) स्कोर को सॉफ्टमैक्स द्वारा सामान्यीकृत किया जाता है।
  - 4. प्रत्येक छिपी हुई स्थिति (छिपी हुई परत की स्थिति) को संबंधित स्कोर से गुणा करें, ताकि उच्च स्कोर के अनुरूप छिपी हुई स्थिति (छिपी हुई परत की स्थिति) को बढ़ाया जा सके, और कम स्कोर के अनुरूप छिपी हुई स्थिति (छिपी हुई परत की स्थिति) को बढ़ाया जा सके। बढ़ाया जाएगा घटाया जाएगा.
  - 5. संबंधित समय चरण के संदर्भ वेक्टर को प्राप्त करने के लिए संबंधित स्कोर के अनुसार सभी छिपी हुई स्थितियों का भारित योग निष्पादित करें।![](./pictures/1-7-attention-dec.gif) गतिशील आरेख: चौथे समय चरण पर, एनकोडर संदर्भ वेक्टर के 5 चरणों को प्राप्त करने के लिए ध्यान जोड़ता है।

इसलिए, ध्यान को बस इस प्रकार समझा जा सकता है: एक प्रभावी भारित योग तकनीक, जिसकी कला वजन प्राप्त करने के तरीके में निहित है।

अब, आइए सब कुछ नीचे दिए गए चित्र में एकीकृत करें और ध्यान के साथ seq2seq मॉडल डिकोडर की पूरी प्रक्रिया पर एक नज़र डालें। गतिशील चित्र चौथा समय चरण दिखाता है:

1. ध्यान मॉडल के डिकोडर आरएनएन के इनपुट में शामिल हैं: एक शब्द एम्बेडिंग वेक्टर, और एक प्रारंभिक डिकोडर छिपी हुई स्थिति, जो चित्र में $h_{init}$ है।
2. आरएनएन उपरोक्त दो इनपुट को संसाधित करता है और एक आउटपुट और एक नई छिपी हुई स्थिति, चित्र में h4 उत्पन्न करता है।
3. ध्यान देने योग्य चरण: हम इस समय चरण के लिए संदर्भ वेक्टर (C4) की गणना करने के लिए एनकोडर के सभी छिपे हुए राज्य वैक्टर और h4 वैक्टर का उपयोग करते हैं।
4. हम नारंगी वेक्टर प्राप्त करने के लिए h4 और C4 को जोड़ते हैं।
5. हम इस नारंगी वेक्टर को एक फीडफॉरवर्ड न्यूरल नेटवर्क में फीड करते हैं (इस नेटवर्क को पूरे मॉडल के साथ प्रशिक्षित किया जाता है)।
6. फीडफॉरवर्ड न्यूरल नेटवर्क के आउटपुट वेक्टर के अनुसार आउटपुट शब्द प्राप्त करें: यह मानते हुए कि आउटपुट अनुक्रम में एन संभावित शब्द हैं,फिर इस फीडफॉरवर्ड न्यूरल नेटवर्क का आउटपुट वेक्टर आमतौर पर एन-आयामी होता है। प्रत्येक आयाम का सबस्क्रिप्ट एक आउटपुट शब्द से मेल खाता है, और प्रत्येक आयाम का मान शब्द की आउटपुट संभावना से मेल खाता है।
7. अगली बार चरण में चरण 1-6 दोहराएँ।
![](./pictures/1-7-attention-pro.gif) गतिशील चित्र: डिकोडर की पूरी प्रक्रिया ध्यान के साथ संयुक्त

अब तक, मुझे आशा है कि आप इस लेख की शुरुआत में उठाए गए प्रश्न 3 और 4 के उत्तर पहले से ही जानते होंगे: 3. लंबे पाठ अनुक्रमों को संसाधित करने में seq2seq की चुनौतियाँ क्या हैं? 4. प्रश्न 3 में चुनौती को हल करने के लिए seq2seq ध्यान के साथ कैसे जुड़ता है?

अंत में, हम यह देखने के लिए ध्यान तंत्र की कल्पना करते हैं कि डिकोडर प्रत्येक समय चरण में इनपुट अनुक्रम के किन हिस्सों पर ध्यान दे रहा है:
![](./pictures/1-7-attention.gif) गतिशील चित्र: वह शब्द जिस पर ध्यान डिकोडिंग चरण के दौरान केंद्रित होता है

यह ध्यान दिया जाना चाहिए कि ध्यान मॉडल अनजाने में आउटपुट के पहले शब्द को इनपुट के पहले शब्द से मैप नहीं करता है, यह सीखता है कि प्रशिक्षण चरण के दौरान दो भाषाओं में शब्दों के अनुरूप कैसे होना चाहिए (हमारे उदाहरण में, फ्रेंच हैं)। और अंग्रेजी)।

निम्नलिखित आंकड़ा यह भी दर्शाता है कि ध्यान तंत्र कितना सटीक है (ऊपर उल्लिखित पेपर से छवि):
![जब मॉडल "यूरोपीय आर्थिक क्षेत्र" आउटपुट करता है तो आप उसका ध्यान वितरण देख सकते हैं। फ़्रेंच में, ये शब्दअंग्रेजी के सापेक्ष शब्दों का क्रम उलटा है ("यूरोपियन इकोनोमिक जोन")। अन्य शब्दों का क्रम भी इसी प्रकार है। ](./pictures/1-8-attention-vis.png) चित्र: जब मॉडल "यूरोपीय आर्थिक क्षेत्र" आउटपुट करता है तो आप उसका ध्यान वितरण देख सकते हैं। फ़्रेंच में, इन शब्दों का क्रम अंग्रेजी ("यूरोपिएने इकोनोमिक ज़ोन") के सापेक्ष उलटा है। अन्य शब्दों का क्रम भी इसी प्रकार है।

ध्यान और seq2seq मॉडल को समझने के बाद, github पर जाएं और चेक इन करने के लिए **स्टार** पर क्लिक करें~~


## आभार
मुख्य रूप से हार्बिन इंस्टीट्यूट ऑफ टेक्नोलॉजी के छात्र झांग जियान द्वारा अनुवादित और लिखित (मूल लेखक [@JayAlammmar](https://twitter.com/JayAlammar) द्वारा अधिकृत), और डुओडुओ छात्रों और डेटाव्हेल शिक्षार्थियों द्वारा पुनर्गठित और व्यवस्थित किया गया है। अंत में, मैं आपकी पढ़ने की प्रतिक्रिया और सितारों की प्रतीक्षा कर रहा हूं।