# PyTorch के लिए ट्रांसफार्मर स्रोत कोड स्पष्टीकरण
[2.2-इलस्ट्रेटेड ट्रांसफार्मर] (./अध्याय 2-ट्रांसफॉर्मर संबंधित सिद्धांत/2.2-इलस्ट्रेटेड ट्रांसफार्मर.एमडी) को पढ़ने के बाद, मुझे आशा है कि आप इस खंड में ट्रांसफार्मर के प्रत्येक मॉड्यूल के डिजाइन और गणना की स्पष्ट समझ प्राप्त कर सकते हैं पाइटोरच पर आधारित हैं आइए इस जटिल मॉडल को और सीखने में सभी की मदद करने के लिए एक ट्रांसफार्मर लागू करें। 2.2.1 से भिन्न, जब यह आलेख ट्रांसफार्मर को लागू करता है, तो इसे इनपुट-मॉडल-आउटपुट के क्रम में लागू किया जाता है। संदर्भ के लिए।
**अध्याय**

- [शब्द एम्बेडिंग](#एम्बेड)
- [स्थिति एन्कोडिंग](#pos)
- [मल्टीहेड अटेंशन](#मल्टीहेड)
- [ट्रांसफार्मर बनाएं](#बिल्ड)

![](./pictures/0-1-transformer-arc.png)

चित्र: ट्रांसफार्मर संरचना आरेख

## **<div id='embed'>शब्द एम्बेडिंग</div>**

जैसा कि ऊपर चित्र में दिखाया गया है, ट्रांसफार्मर आरेख का बाईं ओर एनकोडर है, और दाईं ओर डिकोडर भाग है। एनकोडर स्रोत भाषा अनुक्रम को इनपुट करता है, और डिकोडर उस भाषा पाठ को इनपुट करता है जिसका अनुवाद करने की आवश्यकता होती है (प्रशिक्षण के दौरान)। एक पाठ अक्सर कई अनुक्रमों से बना होता है। एक सामान्य ऑपरेशन अनुक्रम पर कुछ प्रीप्रोसेसिंग करना है (जैसे शब्द विभाजन, आदि) और इसे एक सूची में बदलना है। अनुक्रम सूची के तत्व आमतौर पर शब्दावली में सबसे छोटे शब्द होते हैं जिसे खंडित नहीं किया जा सकता है। संपूर्ण पाठ एक बड़ी सूची है जिसके तत्व अनुक्रमों से बनी सूचियाँ हैं। उदाहरण के लिए, एक अनुक्रम को खंडित करने के बाद, यह ["am", "##ro", "##zi", "meets", "his", "father"] बन जाता है, और फिर उनके संबंधित अनुक्रमणिका के अनुसार शब्दावली रूपांतरण करें और मान लें कि परिणाम [23, 94, 13, 41, 27, 96] है। यदि पूरे पाठ में कुल 100 वाक्य हैं, तो उसके तत्वों के रूप में 100 सूचियाँ होंगी। क्योंकि प्रत्येक अनुक्रम की लंबाई अलग-अलग है, इसलिए अधिकतम लंबाई निर्धारित करने की आवश्यकता है संपूर्ण पाठ को एक सरणी में, आकार 100 x 128 है, जो बैच_आकार और seq_length से मेल खाता है।

इनपुट के बाद, शब्द एम्बेडिंग प्रसंस्करण किया जाता है। शब्द एम्बेडिंग प्रत्येक शब्द को पूर्व-प्रशिक्षित वेक्टर के साथ मैप करना है।

वर्ड एम्बेडिंग को टॉर्च में `torch.nn.Embedding` के आधार पर कार्यान्वित किया जाता है। इंस्टेंटेशन के दौरान जिन मापदंडों को सेट करने की आवश्यकता होती है, वे हैं शब्दावली का आकार और मैप किए गए वेक्टर का आयाम, जैसे कि `embed = nn.Embedding(10, 8)`. आम आदमी के शब्दों में, एक वेक्टर का आयाम यह है कि वेक्टर में कितनी संख्याएँ हैं। ध्यान दें कि पहला पैरामीटर शब्दावली का आकार हैछोटा, यदि आपके पास वर्तमान में 8 शब्द तक हैं, तो आमतौर पर 10 भरें (एक और स्थान अनक और पैड के लिए आरक्षित है)। यदि आप बाद में इन 8 शब्दों से भिन्न शब्द दर्ज करते हैं, तो इसे अनक और पैडिंग भाग में मैप किया जाएगा अनुक्रम को पैड पर मैप किया जाएगा.

यदि हम 8 आयामों (num_features या embed_dim) पर मैप करने की योजना बनाते हैं, तो संपूर्ण पाठ का आकार 100 x 128 x 8 हो जाता है। आइए समझाने के लिए एक छोटा सा उदाहरण दें: मान लें कि हमारी शब्दावली सूची में कुल 10 शब्द हैं (अनक और पैड को गिनते हुए), और पाठ में 2 वाक्य हैं, प्रत्येक वाक्य में 4 शब्द हैं हम प्रत्येक शब्द को 8- तक मैप करना चाहते हैं। आयामी वेक्टर. तो 2, 4, और 8 बैच_साइज़, seq_length, एम्बेड_डिम के अनुरूप हैं (यदि बैच पहले आयाम में है)।

इसके अलावा, आम तौर पर गहन शिक्षण कार्य केवल num_features को बदलते हैं, इसलिए आयाम आम तौर पर उस आयाम पर आधारित होता है जहां अंतिम सुविधा स्थित होती है।

प्रोग्रामिंग प्रारंभ करें:

सभी आवश्यक पैकेजों का आयात:


```अजगर
मशाल आयात करें
टॉर्च.एनएन को एनएन के रूप में आयात करें
Torch.nn.parameter से पैरामीटर आयात करें
Torch.nn.init से xavier_uniform_ आयात करें
Torch.nn.init im सेपोर्ट स्थिरांक_
Torch.nn.init से xavier_सामान्य_ आयात करें
टॉर्च.एनएन.फंक्शनल को एफ के रूप में आयात करें
टाइपिंग से आयात वैकल्पिक, टपल, कोई भी
आयात सूची, वैकल्पिक, टपल टाइप करने से
गणित आयात करें
आयात चेतावनियाँ
```


```अजगर
एक्स = मशाल.शून्य((2,4),dtype=मशाल.लंबा)
एम्बेड = एनएन.एंबेडिंग(10,8)
प्रिंट(एम्बेड(X).आकार)
```

    टॉर्च.आकार([2,4,8])


## **<div id='pos'>स्थिति एन्कोडिंग</div>**

शब्द एम्बेडिंग के बाद स्थिति कोडिंग होती है, जिसका उपयोग विभिन्न शब्दों और एक ही शब्द की विभिन्न विशेषताओं के बीच संबंध को अलग करने के लिए किया जाता है। कोड में ध्यान दें: X_ केवल एक प्रारंभिक मैट्रिक्स है, कोई इनपुट नहीं; स्थिति एन्कोडिंग पूरी होने के बाद एक ड्रॉपआउट जोड़ा जाएगा। इसके अलावा, स्थिति एन्कोडिंग को सबसे अंत में जोड़ा जाता है, इसलिए इनपुट और आउटपुट आकार अपरिवर्तित रहते हैं।



```अजगर
दसियोंया = टॉर्च.टेन्सर
डीईएफ़ स्थिति_एन्कोडिंग(एक्स, संख्या_फीचर, ड्रॉपआउट_पी=0.1, अधिकतम_लेन=512) -> टेंसर:
    आर'''
        इनपुट में स्थिति एन्कोडिंग जोड़ें
    पैरामीटर:
        - num_features: इनपुट आने वाले आयाम
        - ड्रॉपआउट_पी: ड्रॉपआउट की संभावना, गैर-शून्य होने पर ड्रॉपआउट करें
        - max_len: वाक्य की अधिकतम लंबाई, डिफ़ॉल्ट 512
    
    आकार:
        - इनपुट: [बैच_आकार, seq_length, num_features]
        - आउटपुट: [बैच_आकार, seq_length, num_features]

    उदाहरण:
        >>> एक्स = टॉर्च.रैंडन((2,4,10))
        >>> एक्स = पोजिशनल_एन्कोडिंग(एक्स, 10)
        >>> प्रिंट(एक्स.आकार)
        >>>मशाल.आकार([2,4,10])
    '''

    ड्रॉपआउट = एनएन.ड्रॉपआउट(ड्रॉपआउट_पी)
    पी = टॉर्च.शून्य((1,मैक्स_लेन,न्यू_फीचर्स))
    X_ = torch.arange(max_len,dtype=torch.float32).reshape(-1,1) / torch.pow(
        10000,
        टॉर्च.अरेंज(0,num_features,2,dtype=torch.float32) /num_features)
    पी[:,:,0::2] = मशाल.पाप(एक्स_)
    पी[:,:,1::2] = torch.cos(X_)
    एक्स = एक्स + पी[:,:एक्स.आकार[1],:].से(एक्स.डिवाइस)
    वापसी ड्रॉपआउट(एक्स)
```


```अजगर
#स्थितीय एन्कोडिंग उदाहरण
एक्स = टॉर्च.रैंडएन((2,4,10))
एक्स = पोजिशनल_एन्कोडिंग(एक्स, 10)
प्रिंट(एक्स.आकार)
```

    टॉर्च.आकार([2,4,10])


## **<div id='multihead'>मल्टी-हेड ध्यान</div>**

### अलग करें और बैल के ध्यान तंत्र को देखें
**मल्टीहेड अटेंशन मैकेनिज्म का वर्ग जिसे पूर्ण संस्करण में चलाया जा सकता है वह पीछे है। आइए पहले पूर्ण संस्करण पर एक नजर डालें: मल्टीहेड अटेंशन मैकेनिज्म-मल्टीहेड अटेंशन अनुभाग और फिर निम्नलिखित स्पष्टीकरण पर वापस आएं। **

मल्टी-हेड अटेंशन क्लास के मुख्य घटक हैं: पैरामीटर आरंभीकरण, मल्टी_हेड_अटेंशन_फॉरवर्ड

#### आरंभीकरण पैरामीटर
```अजगर
यदि self._qkv_same_embed_dim ग़लत है:
    #प्रारंभिकरण से पहले और बाद में आकार अपरिवर्तित रहता है
    # (seq_length x एम्बेड_डिम) x (एम्बेड_डिम x एम्बेड_डिम) ==> (seq_length x एम्बेड_डिम)
    self.q_proj_weight = पैरामीटर(मशाल.खाली((एम्बेड_डिम, एंबेड_डिम)))
    self.k_proj_weight = पैरामीटर(मशाल.खालीy((एम्बेड_डिम, सेल्फ.केडिम)))
    self.v_proj_weight = पैरामीटर(मशाल.खाली((एम्बेड_डिम, सेल्फ.vdim)))
    self.register_parameter('in_proj_weight', कोई नहीं)
अन्य:
    self.in_proj_weight = पैरामीटर(मशाल.खाली((3 * एम्बेड_डिम, एम्बेड_डिम)))
    self.register_parameter('q_proj_weight', कोई नहीं)
    self.register_parameter('k_proj_weight', कोई नहीं)
    self.register_parameter('v_proj_weight', कोई नहीं)

यदि पूर्वाग्रह:
    self.in_proj_bias = पैरामीटर(मशाल.खाली(3 * एंबेड_डिम))
अन्य:
    self.register_parameter('in_proj_bias', कोई नहीं)
# बाद के चरण में, सभी प्रमुखों का ध्यान एक साथ लगाया जाएगा और फिर वेट मैट्रिक्स से गुणा करके आउटपुट दिया जाएगा
#आउट_प्रोज बाद की तैयारी के लिए है
self.out_proj = nn.रैखिक (एम्बेड_डिम, एंबेड_डिम, पूर्वाग्रह = पूर्वाग्रह)
self._reset_parameters()
```

मशाल.खाली दिए गए आकार के अनुसार संबंधित टेंसर बनाता है। विशेषता यह है कि भरे हुए मान को मशाल.रैंडन (मानक सामान्य वितरण) के अनुरूप प्रारंभ नहीं किया गया है, यह एक आरंभीकरण विधि है। PyTorch में, यदि चर प्रकार टेंसर है, तो मान को संशोधित नहीं किया जा सकता है, और पैरामीटर() फ़ंक्शन को एक प्रकार रूपांतरण फ़ंक्शन के रूप में माना जा सकता है, जो अपरिवर्तनीय टेंसर को प्रशिक्षित और संशोधित मॉडल पैरामीटर में परिवर्तित करता है, यानी मॉडल के समान। पैरामीटर एक साथ बंधे हैं, रजिस्टर_पैरामीटर का मतलब है कि इस पैरामीटर को मॉडल.पैरामीटर में रखा जाए या नहीं, और कोई नहीं का मतलब है कि ऐसा कोई पैरामीटर नहीं है।

यह निर्धारित करने के लिए यहां एक निर्णय है कि क्या q, k, v का अंतिम आयाम सुसंगत है, यदि वे सुसंगत हैं, तो एक बड़े वजन मैट्रिक्स को गुणा किया जाएगा और फिर विभाजित किया जाएगा, वास्तव में, आरंभीकरण मूल आकार नहीं बदलेगा (जैसे![](http://latex.codecogs.com/svg.latex?q=qW_q+b_q), नोट्स देखें)।

आप पा सकते हैं कि अंत में एक _reset_parameters() फ़ंक्शन है, जिसका उपयोग पैरामीटर मानों को प्रारंभ करने के लिए किया जाता है। xavier_uniform का अर्थ है [निरंतर समान वितरण](https://zh.wikipedia.org/wiki/%E9%80%A3%E7%BA%8C%E5%9E%8B%E5%9D%87%E5% 8B% BB%E5%88%86%E5%B8%83) आरंभीकरण मान के रूप में यादृच्छिक रूप से नमूना मान, और xavier_normal_ नमूनाकरण का वितरण सामान्य वितरण है। सटीक रूप से क्योंकि तंत्रिका नेटवर्क को प्रशिक्षित करते समय आरंभीकरण मूल्य बहुत महत्वपूर्ण है, इन दो कार्यों की आवश्यकता होती है।

स्थिरांक_ का अर्थ है इनपुट वेक्टर को दिए गए मान से भरना।

इसके अलावा, PyTorch के स्रोत कोड में, ऐसा लगता है कि प्रक्षेपण एक रैखिक परिवर्तन का प्रतिनिधित्व करता है, और in_proj_bias का अर्थ प्रारंभिक रैखिक परिवर्तन का पूर्वाग्रह है।

```अजगर
def _reset_parameters(स्वयं):
    यदि self._qkv_same_embed_dim:
        xavier_uniform_(self.in_proj_weighटी)
    अन्य:
        xavier_uniform_(self.q_proj_weight)
        xavier_uniform_(self.k_proj_weight)
        xavier_uniform_(self.v_proj_weight)
    यदि self.in_proj_bias कोई नहीं है:
        स्थिर_(self.in_proj_bias, 0.)
        स्थिर_(self.out_proj.bias, 0.)

```



#### मल्टी_हेड_अटेंशन_फॉरवर्ड
यह फ़ंक्शन निम्नलिखित कोड में दिखाया गया है, जिसे मुख्य रूप से 3 भागों में विभाजित किया गया है:
- क्यू, के, वी प्राप्त करने के लिए क्वेरी, कुंजी, मान को _in_projection_packed के माध्यम से रूपांतरित किया जाता है
- अवरोधन तंत्र
- बिंदु एकाग्रता


```अजगर
मशाल आयात करें
टेंसर = टॉर्च.टेन्सर
डीईएफ़ मल्टी_हेड_अटेंशन_फॉरवर्ड(
    प्रश्न: टेंसर,कुंजी: टेंसर,
    मूल्य: टेंसर,
    num_heads: int,
    in_proj_weight: टेंसर,
    in_proj_bias: वैकल्पिक[टेंसर],
    ड्रॉपआउट_पी: फ्लोट,
    आउट_प्रोज_वेट: टेंसर,
    out_proj_bias: वैकल्पिक[टेंसर],
    प्रशिक्षण: बूल = सत्य,
    key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं,
    नीड_वेट्स: बूल = सत्य,
    attn_mask: वैकल्पिक[टेंसर] = कोई नहीं,
    उपयोग_अलग_प्रोज_वेट = कोई नहीं,
    q_proj_weight: वैकल्पिक[टेंसर] = कोई नहीं,
    k_proj_weight: वैकल्पिक[टेंसर] = कोई नहीं,
    v_proj_weight: विकल्पएल[टेंसर] = कोई नहीं,
) -> टुपल[टेंसर, वैकल्पिक[टेंसर]]:
    आर'''
    आकार:
        प्रवेश करना:
        - क्वेरी: `(एल, एन, ई)`
        - कुंजी: `(एस, एन, ई)`
        - मान: `(एस, एन, ई)`
        - key_padding_mask: `(एन, एस)`
        - attn_mask: `(L, S)` या `(N * num_heads, L, S)`
        आउटपुट:
        - attn_output:`(एल, एन, ई)`
        - attn_output_weights:`(एन, एल, एस)`
    '''
    tgt_len, bsz, embed_dim = query.shape
    src_len, _, _ = key.shape
    हेड_डिम = एम्बेड_डिम // num_heads
    क्यू, के, वी = _in_projection_packed(क्वेरी, कुंजी, मान, in_proj_weight, in_proj_bias)

    यदि attn_mask कोई नहीं है:
        यदि attn_mask.dtype == torch.uint8:
            चेतावनियां.चेतावनी('nn.MultiheadAttention में attn_mask के लिए बाइट टेंसर अप्रचलित है। इसके बजाय बूल टेंसर का उपयोग करें।')
            attn_mask = attn_mask.to(torch.bool)
        अन्य:
            attn_mask.is_floating_point() या attn_mask.dtype == torch.bool, \ पर जोर दें
                f"केवल फ्लोट, बाइट और बूल प्रकार attn_mask के लिए समर्थित हैं, {attn_mask.dtype} के लिए नहीं"यदि attn_mask.dim() == 2:
            सही_2d_आकार = (tgt_len, src_len)
            यदि attn_mask.shape != सही_2d_आकार:
                raise RuntimeError(f"2D attn_mask का आकार {attn_mask.shape} है, लेकिन {correct_2d_size} होना चाहिए।")
            attn_mask = attn_mask.unsqueeze(0)
        एलिफ़ attn_mask.dim() == 3:
            सही_3डी_आकार = (बीएसजेड * num_heads, tgt_len, src_len)
            यदि attn_mask.shape != सही_3d_आकार:
                RuntimeError बढ़ाएँ(f'sha3D attn_mask का pe {attn_mask.shape} है, लेकिन {correct_3d_size} होना चाहिए।")
        अन्य:
            raise RuntimeError(f"attn_mask का आयाम {attn_mask.dim()} समर्थित नहीं है")

    यदि key_padding_mask कोई नहीं है और key_padding_mask.dtype == torch.uint8:
        चेतावनियाँ.चेतावनी('nn.MultiheadAttention में key_padding_mask के लिए बाइट टेंसर अप्रचलित है। इसके बजाय बूल टेंसर का उपयोग करें।')
        key_padding_mask = key_padding_mask.to(torch.bool)
    
    # क्यू, के, वी को पुनः आकार दें, डॉट उत्पाद के ध्यान को फिट करने के लिए बैच को पहले आयाम में रखें
    # एक ही समय परमल्टी-हेड तंत्र के लिए, एक परत बनाने के लिए अलग-अलग शीर्षों को एक साथ रखा जाता है
    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
    k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
    v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
    यदि key_padding_mask कोई नहीं है:
        ज़ोर key_padding_mask.shape == (bsz, src_len), \
            f"की_पैडिंग_मास्क आकार {(bsz, src_len)} की उम्मीद कर रहा था, लेकिन मिला {key_padding_mask.shape}"
        key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).
            विस्तार(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)
        यदि attn_mask कोई नहीं है:
            attn_mask = key_padding_mask
        एलिफ़ attn_mask.dtype == मशाल.बूल:
            attn_mask = attn_mask.ological_or(key_padding_mask)
        अन्य:
            attn_mask = attn_mask.masked_fill(key_padding_mask, फ्लोट("-inf"))
    # यदि attn_mask मान एक बूलियन मान है, तो मास्क को फ़्लोट में बदलें
    यदि attn_mask कोई नहीं है और attn_mask.dtype == torch.bool:
        new_attn_mask = torch.zeros_लाइक(attn_mask, dtype=torch.float)
        new_attn_mask.masked_fill_(attn_mask, फ्लोट("-inf"))
        attn_mask = new_attn_mask

    # प्रशिक्षण सही होने पर ही ड्रॉपआउट लागू करें
    यदि प्रशिक्षण नहीं है:
        ड्रॉपआउट_पी = 0.0
    attn_output, attn_output_weights = _scale_dot_product_attention(q, k, v, attn_mask, ड्रॉपआउट_p)
    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
    attn_output = nn.functional.linear(attn_output, out_proj_weight, out_proj_bias)
    यदि नहींd_वजन:
        # सिर पर औसत ध्यान भार
        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
        वापसी attn_output, attn_output_weights.sum(dim=1) / num_heads
    अन्य:
        वापसी attn_output, कोई नहीं
```

##### क्वेरी, कुंजी, मान को q, k, v प्राप्त करने के लिए _in_projection_packed के माध्यम से रूपांतरित किया जाता है
```
q, k, v = _in_projection_packed(क्वेरी, कुंजी, मान, in_proj_weight, in_proj_bias)
```

`nn.functional.linear` फ़ंक्शन के लिए, यह वास्तव में एक रैखिक परिवर्तन है। `nn.Linear` से भिन्न, पूर्व एक भार मैट्रिक्स और पूर्वाग्रह प्रदान कर सकता है, और निष्पादित कर सकता है![](http://latex.Codecogs.com/svg.latex?y=xW^T+b), और बाद वाला आउटपुट का आयाम है जिसे स्वतंत्र रूप से निर्धारित किया जा सकता है।


```अजगर
def _in_projection_packed(
    क्यू: टेंसर,
    के: टेंसर,
    वी: टेंसर,
    डब्ल्यू: टेंसर,
    बी: वैकल्पिक[टेंसर] = कोई नहीं,
) -> सूची[टेंसर]:
    आर"""
    वजन मापदंडों के एक बड़े मैट्रिक्स के साथ रैखिक परिवर्तन

    पैरामीटर:
        q, k, v: आत्म-ध्यान के लिए, तीनों src हैं; seq2seq मॉडल के लिए, k और v सुसंगत टेंसर हैं।
                 लेकिन उनका अंतिम आयाम (num_features या embed_dim) सुसंगत होना चाहिए।
        w: रैखिक परिवर्तन के लिए उपयोग किया जाने वाला एक बड़ा मैट्रिक्स, q, k, v के क्रम में एक टेंसर में पैक किया गया।
        बी: रैखिक परिवर्तन के लिए उपयोग किए जाने वाले ऑफसेट को क्यू, के, वी के क्रम में एक टेंसर में दबाया जाता है।

    आकार:
        प्रवेश करना:
        - क्यू: आकार:`(..., ई)`, ई शब्द एम्बेडिंग का आयाम है (सभी के नीचे दिखने वाले ई का यही अर्थ है)।
        - k: आकृति:`(..., E)`
        - वी: आकार:`(..., ई)`
        - डब्ल्यू: आकार:`(ई * 3, ई)`
        - बी: आकार:`ई * 3`

        आउटपुट:
        - आउटपुट सूची: `[q', k', v']`, q, k, v का आकार रैखिक परिवर्तन से पहले और बाद में समान है।
    """
    ई = क्यू.आकार(-1)
    # यदि यह आत्म-ध्यान है, तो q = k = v = src, इसलिए उनके संदर्भ चर सभी src हैं
    # अर्थात्, k का परिणाम v है और q का k दोनों सत्य हैं
    # यदि यह seq2seq, k = v है, तो k का परिणाम v सत्य है
    यदि k, v है:
        यदि q, k है:
            वापसी F.रैखिक(q, w, b).खंड(3, dim=-1)
        अन्य:
            # seq2seq मॉडल
            w_q, w_kv = w.विभाजन([ई, ई * 2])
            यदि b कोई नहीं है:
                b_q = b_kv = कोई नहीं
            अन्य:
                b_q, b_kv = b.स्प्लिट([ई, ई * 2])
            रिटर्न (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).chunk(2, dim=-1)
    अन्य:
        w_q, w_k, w_v = w.chunk(3)
        यदि b कोई नहीं है:
            b_q = b_k = b_v = कोई नहीं
        अन्य:
            b_q, b_k, b_v = b.chunk(3)
        वापसी F.रेखीय(q, w_q, b_q), F.रेखीय(k, w_k, b_k), F.रेखीय(v, w_v, b_v)

# q, k, v = _in_projection_pacकेड(क्वेरी, कुंजी, मान, in_proj_weight, in_proj_bias)
```

***

##### रोड़ा तंत्र

Attn_mask के लिए, यदि यह 2D है, तो आकार `(L, S)` जैसा है, L और S क्रमशः लक्ष्य भाषा और स्रोत भाषा अनुक्रम लंबाई का प्रतिनिधित्व करते हैं, यदि यह 3D है, तो आकार `(N * num_heads, L) जैसा है , S)` , N बैच_आकार का प्रतिनिधित्व करता है, num_heads ध्यान शीर्षों की संख्या का प्रतिनिधित्व करता है। यदि attn_mask का dtype ByteTensor है, तो गैर-0 स्थितियों पर ध्यान नहीं दिया जाएगा; यदि यह BoolTensor है, तो True से संबंधित स्थितियों पर ध्यान नहीं दिया जाएगा, यदि यह एक संख्यात्मक मान है, तो इसे सीधे attn_weights में जोड़ा जाएगा .

क्योंकि जब डिकोडर डिकोड कर रहा होता है, तो वह केवल स्थिति और उससे पहले वाली स्थिति को देख सकता है। यदि आप पीछे की ओर देखते हैं, तो यह उल्लंघन होगा, इसलिए इसे attn_mask द्वारा अवरुद्ध करने की आवश्यकता है।

निम्नलिखित फ़ंक्शन सीधे PyTorch की प्रतिलिपि बनाता है, जिसका अर्थ है कि यह सुनिश्चित करना कि विभिन्न आयामों के मुखौटा आकार सही हैं और विभिन्न प्रकार के रूपांतरण हैं


```अजगर
यदि attn_mask कोई नहीं है:
    यदि attn_mask.dtype == torch.uint8:
        चेतावनियाँ.चेतावनी('दस बाइटnn.MultiheadAttention में attn_mask के लिए sor को हटा दिया गया है। इसके बजाय बूल टेंसर का उपयोग करें।")
        attn_mask = attn_mask.to(torch.bool)
    अन्य:
        attn_mask.is_floating_point() या attn_mask.dtype == torch.bool, \ पर जोर दें
            f"केवल फ्लोट, बाइट और बूल प्रकार attn_mask के लिए समर्थित हैं, {attn_mask.dtype} के लिए नहीं"
    # विभिन्न आयामों में आकृतियों का निर्धारण
    यदि attn_mask.dim() == 2:
        सही_2d_आकार = (tgt_len, src_len)
        यदि attn_mask.shape != सही_2d_आकार:
            RuntimeError बढ़ाएं(f"आकार2D attn_mask का {attn_mask.shape} है, लेकिन {सही_2d_size} होना चाहिए।")
            attn_mask = attn_mask.unsqueeze(0)
    एलिफ़ attn_mask.dim() == 3:
        सही_3डी_आकार = (बीएसजेड * num_heads, tgt_len, src_len)
        यदि attn_mask.shape != सही_3d_आकार:
            raise RuntimeError(f"3D attn_mask का आकार {attn_mask.shape} है, लेकिन {correct_3d_size} होना चाहिए।")
    अन्य:
        raise RuntimeError(f"attn_mask का आयाम {attn_mask.dim()} समर्थित नहीं है")

```
`attn_mask` के साथअंतर यह है कि `key_padding_mask` का उपयोग कुंजी में मान को अवरुद्ध करने के लिए किया जाता है, विशेष रूप से, यह `<PAD>` होना चाहिए। अनदेखा स्थिति attn_mask के अनुरूप है।

```अजगर
# key_padding_mask मान को बूलियन मान में बदलें
यदि key_padding_mask कोई नहीं है और key_padding_mask.dtype == torch.uint8:
    चेतावनियाँ.चेतावनी('nn.MultiheadAttention में key_padding_mask के लिए बाइट टेंसर अप्रचलित है। इसके बजाय बूल टेंसर का उपयोग करें।')
    key_padding_mask = key_padding_mask.to(torch.bool)
```

सबसे पहले दो छोटे फ़ंक्शन, `लॉजिकल_ऑर` का परिचय दें, जो दो टेंसरों को इनपुट करता है और दो टेंसरों में मानों पर `लॉजिकल OR` ऑपरेशन करता है। यह केवल तभी `गलत` होता है जब दोनों मान 0 होते हैं। और अन्य बार दोनों `True` हैं, और दूसरा `masked_fill` है। इनपुट एक मास्क है और भरने के लिए उपयोग किया जाने वाला मान है। मास्क में 1,0 होते हैं, 0 का स्थिति मान अपरिवर्तित रहता है, और 1 की स्थिति नए मान से भर जाती है।
```अजगर
ए = टॉर्च.टेंसर([0,1,10,0],dtype=torch.int8)
बी = टॉर्च.टेंसर([4,0,1,0],डीटाइप=टॉर्च.इंट8)
प्रिंट(मशाल.लॉजिकल_या(ए,बी))
# टेंसर([सच्चा, सच्चा, सच्चा, गलत])
```

```अजगर
आर = टॉर्च.टेंसर([[0,0,0,0],[0,0,0,0]])
मास्क = टॉर्च.टेंसर([[1,1,1,1],[0,0,0,0]])
प्रिंट(आर.मास्कड_फिल(मास्क,1))
# टेंसर([[1, 1, 1, 1],
# [0, 0, 0, 0]])
```
वास्तव में, attn_mask और key_padding_mask में कभी-कभी समान ऑब्जेक्ट होते हैं, इसलिए कभी-कभी उन्हें एक साथ देखा जा सकता है। सॉफ्टमैक्स के लिए `-inf` का उपयोग करने के बाद, मान 0 है, जिसे अनदेखा कर दिया जाता है।
```अजगर
यदि key_padding_mask कोई नहीं है:
    ज़ोरt key_padding_mask.shape == (bsz, src_len), \
        f"की_पैडिंग_मास्क आकार {(bsz, src_len)} की उम्मीद कर रहा था, लेकिन मिला {key_padding_mask.shape}"
    key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).\
        विस्तार(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)
    # यदि attn_mask खाली है, तो सीधे key_padding_mask का उपयोग करें
    यदि attn_mask कोई नहीं है:
        attn_mask = key_padding_mask
    एलिफ़ attn_mask.dtype == मशाल.बूल:
        attn_mask = attn_mask.ological_or(key_padding_mask)
    अन्य:attn_mask = attn_mask.masked_fill(key_padding_mask, फ्लोट("-inf"))

# यदि attn_mask मान एक बूलियन मान है, तो मास्क को फ़्लोट में बदलें
यदि attn_mask कोई नहीं है और attn_mask.dtype == torch.bool:
    new_attn_mask = मशाल.शून्य_लाइक(attn_mask, dtype=torch.float)
    new_attn_mask.masked_fill_(attn_mask, फ्लोट("-inf"))
    attn_mask = new_attn_mask

```

***
##### डॉट उत्पाद ध्यान


```अजगर
टाइपिंग से आयात वैकल्पिक, टपल, कोई भी
def _स्केल्ड_डॉट_प्रोडक्ट_अटेंशन(
    क्यू: टेंसर,
    के: टेंसर,
    वी: टेंसर,
    attn_mask: वैकल्पिक[टेंसर] = कोई नहीं,
    ड्रॉपआउट_पी: फ्लोट = 0.0,
) -> टुपल[टेंसर, टेन्सर]:
    आर'''
    क्वेरी, कुंजी, मान पर डॉट उत्पाद ध्यान की गणना करें, यदि ध्यान मास्क है तो इसका उपयोग करें, और संभावना ड्रॉपआउट_पी के साथ ड्रॉपआउट लागू करें

    पैरामीटर:
        - क्यू: आकार:`(बी, एनटी, ई)` बी बैच आकार का प्रतिनिधित्व करता है, एनटी लक्ष्य भाषा अनुक्रम की लंबाई है, और ई एम्बेडिंग के बाद फीचर आयाम है
        - कुंजी: आकार:`(बी, एनएस, ई)` एनएस स्रोत भाषा अनुक्रम लंबाई है
        - मान: आकार:`(बी, एनएस, ई)` कुंजी आकार के समान है
        - attn_mask: या तो एक 3D टेंसर जिसका आकार है: `(B, Nt, Ns)` या एक 2D टेंसर जिसका आकार है: `(Nt, Ns)`

        - आउटपुट: ध्यान मान: आकार:`(बी, एनटी, ई)`, क्यू के आकार के अनुरूप; ध्यान भार: आकार:`(बी, एनटी, एनएस)`उदाहरण:
        >>> क्यू = टॉर्च.रैंडन((2,3,6))
        >>> के = टॉर्च.रैंडन((2,4,6))
        >>> वी = टॉर्च.रैंडन((2,4,6))
        >>> आउट = स्केल्ड_डॉट_प्रोडक्ट_अटेंशन(क्यू, के, वी)
        >>> बाहर[0].आकार, बाहर[1].आकार
        >>> टॉर्च.आकार([2,3,6]) टॉर्च.आकार([2,3,4])
    '''
    बी, एनटी, ई = क्यू.आकार
    क्यू = क्यू / गणित.वर्ग(ई)
    # (बी, एनटी, ई) एक्स (बी, ई, एनएस) -> (बी, एनटी, एनएस)
    ध्यान दें = टॉर्च.बीएमएम(क्यू, के.ट्रांसपोज़(-2,-1))
    यदि attn_mask कोई नहीं है:
        attn += attn_mask
    # attn का अर्थ है लक्ष्य अनुक्रम का प्रत्येक शब्द युग्म स्रोतध्यान के रूप में भाषा क्रम
    attn = F.softmax(attn, dim=-1)
    यदि ड्रॉपआउट_पी:
        attn = F.ड्रॉपआउट(attn, p=dropout_p)
    # (बी, एनटी, एनएस) एक्स (बी, एनएस, ई) -> (बी, एनटी, ई)
    आउटपुट = टॉर्च.बीएमएम(ध्यान, वी)
    रिटर्न आउटपुट, ध्यान दें

```

### संपूर्ण मल्टी-हेड अटेंशन मैकेनिज्म-मल्टीहेडअटेंशन


```अजगर
क्लास मल्टीहेडअटेंशन(एनएन.मॉड्यूल):
    आर'''
    पैरामीटर:
        एंबेड_डिम: शब्द एम्बेडिंग का आयाम
        num_heads: समानांतर शीर्षों की संख्या
        बैच_प्रथम: यदि `सही` है, तो यह (बैच, सीक, फ़ेचर) है, यदि यह `गलत` है, तो यह (सीक, बैच, फ़ीचर) है
    
    उदाहरण:
        >>> मल्टीहेड_एटीएन = मल्टीहeadAttention(एम्बेड_डिम, num_heads)
        >>> attn_output, attn_output_weights = मल्टीहेड_attn(क्वेरी, कुंजी, मान)
    '''
    def __init__(स्वयं, एम्बेड_डिम, num_heads, ड्रॉपआउट=0., पूर्वाग्रह=सत्य,
                 kdim=कोई नहीं, vdim=कोई नहीं, बैच_फर्स्ट=गलत) -> कोई नहीं:
        # फ़ैक्टरी_क्वार्ग्स = {'डिवाइस': डिवाइस, 'डीटाइप': डीटाइप}
        सुपर(मल्टीहेडअटेंशन, स्वयं).__init__()
        self.embed_dim = एम्बेड_डिम
        self.kdim = kdim यदि kdim कोई नहीं है और कोई नहीं embed_dim है
        self.vdim = vdim यदि vdimकोई और नहीं embed_dim है
        self._qkv_same_embed_dim = self.kdim == एम्बेड_डिम और self.vdim == एम्बेड_डिम

        self.num_heads = num_heads
        स्व.ड्रॉपआउट = ड्रॉपआउट
        स्वयं.बैच_प्रथम = बैच_प्रथम
        self.head_dim = एम्बेड_डिम // num_heads
        assert self.head_dim * num_heads == self.embed_dim, "एम्बेड_डिम को num_heads से विभाज्य होना चाहिए"

        यदि self._qkv_same_embed_dim ग़लत है:
            self.q_proj_weight = पैरामीटर(मशाल.खाली((एम्बेड_डिम, एंबेड_डिम)))self.k_proj_weight = पैरामीटर(मशाल.खाली((embed_dim, self.kdim)))
            self.v_proj_weight = पैरामीटर(मशाल.खाली((एम्बेड_डिम, सेल्फ.vdim)))
            self.register_parameter('in_proj_weight', कोई नहीं)
        अन्य:
            self.in_proj_weight = पैरामीटर(मशाल.खाली((3 * एम्बेड_डिम, एम्बेड_डिम)))
            self.register_parameter('q_proj_weight', कोई नहीं)
            self.register_parameter('k_proj_weight', कोई नहीं)
            self.register_parameter('v_proj_weight', कोई नहीं)यदि पूर्वाग्रह:
            self.in_proj_bias = पैरामीटर(मशाल.खाली(3 * एंबेड_डिम))
        अन्य:
            self.register_parameter('in_proj_bias', कोई नहीं)
        self.out_proj = nn.रैखिक (एम्बेड_डिम, एंबेड_डिम, पूर्वाग्रह = पूर्वाग्रह)

        self._reset_parameters()

    def _reset_parameters(स्वयं):
        यदि self._qkv_same_embed_dim:
            xavier_uniform_(self.in_proj_weight)
        अन्य:
            xavier_uniform_(self.q_proj_weight)
            xavier_uniform_(self.k_proj_weight)
            जेवियर्स_वर्दी_(स्वयं.v_प्रोज_वजन)

        यदि self.in_proj_bias कोई नहीं है:
            स्थिर_(self.in_proj_bias, 0.)
            स्थिर_(self.out_proj.bias, 0.)



    डीईएफ़ फ़ॉरवर्ड(स्वयं, क्वेरी: टेंसर, कुंजी: टेंसर, मान: टेंसर, key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं,
                आवश्यकता_भार: बूल = सत्य, attn_mask: वैकल्पिक[टेंसर] = कोई नहीं) -> टुपल[टेंसर, वैकल्पिक[टेंसर]]:
        यदि self.batch_first:
            क्वेरी, कुंजी, मान = [x.transpose(1, 0) for x in (क्वेरी, कुंजी, मान)]यदि स्वयं नहीं._qkv_same_embed_dim:
            attn_output, attn_output_weights = मल्टी_हेड_अटेंशन_फॉरवर्ड(
                क्वेरी, कुंजी, मान, self.num_heads,
                self.in_proj_weight, self.in_proj_bias,
                सेल्फ.ड्रॉपआउट, सेल्फ.आउट_प्रोज.वेट, सेल्फ.आउट_प्रोज.बायस,
                प्रशिक्षण=स्वयं प्रशिक्षण,
                key_padding_mask=key_padding_mask, जरुरत_वजन=आवश्यकता_वजन,
                attn_mask=attn_mask, use_separate_proj_weight=True,q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,
                v_proj_weight=self.v_proj_weight)
        अन्य:
            attn_output, attn_output_weights = मल्टी_हेड_अटेंशन_फॉरवर्ड(
                क्वेरी, कुंजी, मान, self.num_heads,
                self.in_proj_weight, self.in_proj_bias,
                सेल्फ.ड्रॉपआउट, सेल्फ.आउट_प्रोज.वेट, सेल्फ.आउट_प्रोज.बायस,
                प्रशिक्षण=स्वयं प्रशिक्षण,
                की_पैडिंग_मास्क=की_पैडिंग_मास्क, नीड_वेट्स=नीड_वेईघाट,
                attn_mask=attn_mask)
        यदि self.batch_first:
            attn_output.transpose(1, 0), attn_output_weights लौटाएँ
        अन्य:
            attn_output, attn_output_weights लौटाएँ
```


इसके बाद, आप इसका अभ्यास कर सकते हैं और स्थिति कोडिंग जोड़ सकते हैं। आप पा सकते हैं कि स्थिति कोडिंग और मल्टी-हेड ध्यान जोड़ने से पहले और बाद का आकार नहीं बदलेगा।


```अजगर
# क्योंकि बैच_फर्स्ट गलत है, src का आकार: `(seq, बैच, एम्बेड_डिम)`
स्रोत = मशाल.रैंडन((2,4,100))
स्रोत = स्थितिगत_एन्कोडिंग(src,100,0.1)
प्रिंट(src.shape)
मल्टीहेड_एटीएन = मल्टीहेडअटेंशन(100, 4, 0.1)
attn_output, attn_output_weightएस = मल्टीहेड_एटीएन(src,src,src)
प्रिंट(attn_output.shape, attn_output_weights.shape)

#मशाल.आकार([2,4,100])
# टॉर्च.आकार([2,4,100]) टॉर्च.आकार([4,2,2])
```

    टॉर्च.आकार([2,4,100])
    मशाल.आकार([2, 4,100]) मशाल.आकार([4,2,2])


***
## **<div id='build'>बिल्ड ट्रांसफार्मर</div>**
-एनकोडर परत

![](./pictures/2-2-1-encoder.png)


```अजगर
क्लास ट्रांसफार्मरएनकोडरलेयर(एनएन.मॉड्यूल):
    आर'''
    पैरामीटर:
        d_model: शब्द एम्बेडिंग का आयाम (आवश्यक)
        एनहेड: मल्टी-हेड अटेंशन में समानांतर हेड की संख्या (आवश्यक)
        dim_feedforward: पूर्ण कनेक्शनकनेक्टेड परत में न्यूरॉन्स की संख्या, जिसे इस परत के माध्यम से इनपुट के आयाम के रूप में भी जाना जाता है (डिफ़ॉल्ट = 2048)
        ड्रॉपआउट: ड्रॉपआउट की संभावना (डिफ़ॉल्ट = 0.1)
        सक्रियण: दो रैखिक परतों के बीच सक्रियण फ़ंक्शन, डिफ़ॉल्ट रिले या गेलु
        lay_norm_eps: हर को 0 होने से रोकने के लिए परत सामान्यीकरण में छोटी मात्रा (डिफ़ॉल्ट = 1e-5)
        बैच_फर्स्ट: यदि `सही` है, तो यह (बैच, seq, feture) है, यदि यह `गलत` है, तो यह (seq, बैच, फीचर) है (डिफ़ॉल्ट: गलत)

    उदाहरण:
        >>> एनकोडर_लेयर = ट्रांसफार्मरएनकोडरलेयर(d_model=512, nhead=8)
        >>> स्रोत = टॉर्च.रैंडन((32, 10, 512))
        >>> आउट = एनकोडर_लेयर(src)
    '''

    def __init__(स्वयं, d_modeएल, एनहेड, डिम_फीडफॉरवर्ड=2048, ड्रॉपआउट=0.1, सक्रियण=एफ.रेलू,
                 लेयर_नॉर्म_ईपीएस=1ई-5, बैच_फर्स्ट=गलत) -> कोई नहीं:
        सुपर(ट्रांसफॉर्मरएनकोडरलेयर, स्वयं).__init__()
        self.self_attn = मल्टीहेडअटेंशन(d_model, nhead, ड्रॉपआउट=ड्रॉपआउट, बैच_फर्स्ट=बैच_फर्स्ट)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(ड्रॉपआउट)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)
        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)
        स्व.ड्रॉपआउट1 = एनएन.ड्रॉपआउट(ड्रॉपआउट)
        स्व.ड्रॉपआउट2 = एनएन.ड्रॉपआउट(ड्रॉपआउट)
        स्व.सक्रियण = सक्रियण


    डीईएफ़ फॉरवर्ड (स्वयं, स्रोत: टेंसर, स्रोत_मास्क: वैकल्पिक [टेंसर] = कोई नहीं, स्रोत_की_पैडिंग_मास्क: वैकल्पिक [टेंसर] = कोई नहीं) -> टेंसर:
        src = पोजीशनल_एन्कोडिंग(src, src.shape[-1])
        src2 = self.self_attn(src, src, src, attn_mask=src_mask,
        key_padding_mask=src_key_padding_मुखौटा)[0]
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout(src2)
        src = self.norm2(src)
        वापसी स्रोत

```


```अजगर
# आइए इसे एक छोटे से उदाहरण से देखें
एनकोडर_लेयर = ट्रांसफार्मरएनकोडरलेयर(d_model=512, nhead=8)
स्रोत = मशाल.रैंडन((32, 10, 512))
आउट = एनकोडर_लेयर(src)
प्रिंट(आउट.आकार)
#मशाल.आकार([32,10,512])
```

    टॉर्च.आकार([32,10,512])


### ट्रांसफार्मर परत Enc बनाती हैआदेश


```अजगर
क्लास ट्रांसफार्मरएनकोडर(एनएन.मॉड्यूल):
    आर'''
    पैरामीटर:
        एनकोडर_लेयर (आवश्यक)
        num_layers: एनकोडर_लेयर की परतों की संख्या (आवश्यक)
        मानक: सामान्यीकरण का विकल्प (वैकल्पिक)
    
    उदाहरण:
        >>> एनकोडर_लेयर = ट्रांसफार्मरएनकोडरलेयर(d_model=512, nhead=8)
        >>> ट्रांसफार्मर_एनकोडर = ट्रांसफार्मरएनकोडर(एनकोडर_लेयर, संख्या_लेयर=6)
        >>> स्रोत = मशाल.रैंडन((10, 32, 512))
        >>> आउट = ट्रांसफार्मर_एनकोडर(src)
    '''

    def __init__(स्वयं, एनकोडर_लेयर, num_layers, मानदंड=कोई नहीं):
        एसअपर(ट्रांसफॉर्मरएनकोडर, स्वयं).__init__()
        सेल्फ.लेयर = एनकोडर_लेयर
        self.num_layers = num_layers
        स्व.मानदंड = आदर्श
    
    डीईएफ़ फॉरवर्ड (स्वयं, स्रोत: टेंसर, मास्क: वैकल्पिक [टेंसर] = कोई नहीं, src_key_padding_mask: वैकल्पिक [टेंसर] = कोई नहीं) -> टेंसर:
        आउटपुट = पोजिशनल_एन्कोडिंग(src, src.shape[-1])
        रेंज में _ के लिए (self.num_layers):
            आउटपुट = सेल्फ.लेयर(आउटपुट, src_mask=mask, src_key_padding_mask=src_key_padding_mask)
        
        यदि self.norm नहीं हैकोई नहीं:
            आउटपुट = self.norm(आउटपुट)
        
        वापसी आउटपुट
```


```अजगर
# उदाहरण
एनकोडर_लेयर = ट्रांसफार्मरएनकोडरलेयर(d_model=512, nhead=8)
ट्रांसफार्मर_एनकोडर = ट्रांसफार्मरएनकोडर(एनकोडर_लेयर, संख्या_लेयर=6)
स्रोत = मशाल.रैंडन((10, 32, 512))
आउट = ट्रांसफार्मर_एनकोडर(src)
प्रिंट(आउट.आकार)
#मशाल.आकार([10,32,512])
```

    टॉर्च.आकार([10,32,512])


***
## डिकोडर परत:


```अजगर
वर्ग ट्रांसफार्मरडिकोडरलेयर(एनएन.मॉड्यूल):
    आर'''
    पैरामीटर:
        d_model: शब्द एम्बेडिंग का आयाम (आवश्यक)एनहेड: मल्टी-हेड अटेंशन में समानांतर हेड की संख्या (आवश्यक)
        dim_feedforward: पूरी तरह से जुड़ी हुई परत में न्यूरॉन्स की संख्या, जिसे इस परत के माध्यम से इनपुट के आयाम के रूप में भी जाना जाता है (डिफ़ॉल्ट = 2048)
        ड्रॉपआउट: ड्रॉपआउट की संभावना (डिफ़ॉल्ट = 0.1)
        सक्रियण: दो रैखिक परतों के बीच सक्रियण फ़ंक्शन, डिफ़ॉल्ट रिले या गेलु
        lay_norm_eps: हर को 0 होने से रोकने के लिए परत सामान्यीकरण में छोटी मात्रा (डिफ़ॉल्ट = 1e-5)
        बैच_फर्स्ट: यदि `सही` है, तो यह (बैच, seq, feture) है, यदि यह `गलत` है, तो यह (seq, बैच, फीचर) है (डिफ़ॉल्ट: गलत)
    
    उदाहरण:
        >>> डिकोडर_लेयर = ट्रांसफार्मरडिकोडरलेयर(d_model=512, nhead=8)
        >>> मेमोरी = टॉर्च.रैंडन((10, 32, 512))
        >>>टीजीटी = टॉर्च.रैंडन((20, 32, 512))
        >>> आउट = डिकोडर_लेयर(टीजीटी, मेमोरी)
    '''
    def __init__(स्वयं, d_model, nhead, dim_feedforward=2048, ड्रॉपआउट=0.1, सक्रियण=F.relu,
                 लेयर_नॉर्म_ईपीएस=1ई-5, बैच_फर्स्ट=गलत) -> कोई नहीं:
        सुपर(ट्रांसफॉर्मरडिकोडरलेयर, स्वयं).__init__()
        self.self_attn = मल्टीहेडअटेंशन(d_model, nhead, ड्रॉपआउट=ड्रॉपआउट, बैच_फर्स्ट=बैच_फर्स्ट)
        self.multihead_attn = मल्टीहेडअटेंशन(d_model, nhead, ड्रॉपआउट=ड्रॉपआउट, बैच_फर्स्ट=बैटch_first)

        self.linear1 = nn.Linear(d_model, dim_feedforward)
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(ड्रॉपआउट)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)
        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)
        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)
        स्व.ड्रॉपआउट1 = एनएन.ड्रॉपआउट(ड्रॉपआउट)
        स्व.ड्रॉपआउट2 = एनएन.ड्रॉपआउट(ड्रॉपआउट)
        स्व.ड्रॉपआउट3 = एनएन.ड्रॉपआउट(ड्रॉपआउट)स्व.सक्रियण = सक्रियण

    डीईएफ़ फॉरवर्ड(स्वयं, टीजीटी: टेंसर, मेमोरी: टेंसर, टीजीटी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं,
                मेमोरी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं,tgt_key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं, मेमोरी_की_पैडिंग_मास्क: वैकल्पिक[टेंसर] = कोई नहीं) -> टेंसर:
        आर'''
        पैरामीटर:
            tgt: लक्ष्य भाषा अनुक्रम (आवश्यक)
            मेमोरी: वाक्य अंतिम एन्कोडर_लेयर से चलते हैं (आवश्यक)
            tgt_mask: लक्ष्य भाषा अनुक्रम का मुखौटा (वैकल्पिक)
            मेमोरी_मास्क (वैकल्पिक)
            tgt_key_padding_mask (वैकल्पिक)
            मेमओरी_की_पैडिंग_मास्क (वैकल्पिक)
        '''
        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,
                              key_padding_mask=tgt_key_padding_mask)[0]
        टीजीटी = टीजीटी + सेल्फ.ड्रॉपआउट1(टीजीटी2)
        टीजीटी = self.norm1(टीजीटी)
        tgt2 = self.multihead_attn(tgt, मेमोरी, मेमोरी, attn_mask=memory_mask,
                                   key_padding_mask=memory_key_padding_mask)[0]
        टीजीटी = टीजीटी + सेल्फ.ड्रॉपआउट2(टीजीटी2)
        टीजीटी = self.norm2(टीजीटी)
        tgt2 = self.linear2(स्वयं.ड्रॉपआउट(स्वयं.सक्रियण(स्व.रेखीय1(tgt))))
        टीजीटी = टीजीटी + सेल्फ.ड्रॉपआउट3(टीजीटी2)
        टीजीटी = self.norm3(टीजीटी)
        वापसी टीजीटी
```


```अजगर
# प्यारा सा उदाहरण
डिकोडर_लेयर = nn.TransformerDecoderLayer(d_model=512, nhead=8)
मेमोरी = टॉर्च.रैंडन((10, 32, 512))
टीजीटी = टॉर्च.रैंडन((20, 32, 512))
आउट = डिकोडर_लेयर(टीजीटी, मेमोरी)
प्रिंट(आउट.आकार)
#मशाल.आकार([20,32,512])
```

    टॉर्च.आकार([20,32,512])



```अजगर
##डिकोडर
```


```अजगर
क्लास ट्रांसफार्मर डिकोडर (एनएन.मॉड्यूल):
    आर'''पैरामीटर:
        डिकोडर_लेयर (आवश्यक)
        num_layers: डिकोडर_लेयर की परतों की संख्या (आवश्यक)
        मानक: सामान्यीकृत चयन
    
    उदाहरण:
        >>> डिकोडर_लेयर = ट्रांसफार्मर डिकोडर लेयर (d_model = 512, nhead = 8)
        >>> ट्रांसफार्मर_डिकोडर = ट्रांसफार्मर डिकोडर(डिकोडर_लेयर, संख्या_लेयर्स=6)
        >>> मेमोरी = टॉर्च.रैंड(10, 32, 512)
        >>> टीजीटी = टॉर्च.रैंड(20, 32, 512)
        >>>आउट = ट्रांसफार्मर_डिकोडर(टीजीटी, मेमोरी)
    '''
    def __init__(स्वयं, डिकोडर_लेयर, num_layers, मानदंड=कोई नहीं):
        सुपर(ट्रांसफॉर्मरडिकोडआर, स्वयं).__init__()
        सेल्फ.लेयर = डिकोडर_लेयर
        self.num_layers = num_layers
        स्व.मानदंड = आदर्श
    
    डीईएफ़ फॉरवर्ड(स्वयं, टीजीटी: टेंसर, मेमोरी: टेंसर, टीजीटी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं,
                मेमोरी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं, tgt_key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं,
                मेमोरी_की_पैडिंग_मास्क: वैकल्पिक[टेंसर] = कोई नहीं) -> टेंसर:
        आउटपुट=tgt
        रेंज में _ के लिए (self.num_layers):
            आउटपुट = सेल्फ.लेयर(आउटपुट, मेमोरी, tgt_मास्क = tgt_mask,
                         मेमोरी_मास्क=मेमोरी_मास्क,
                         tgt_key_padding_mask=tgt_key_padding_mask,
                         मेमोरी_की_पैडिंग_मास्क=मेमोरी_की_पैडिंग_मास्क)
        यदि self.norm कोई नहीं है:
            आउटपुट = self.norm(आउटपुट)

        वापसी आउटपुट
```


```अजगर
# प्यारा सा उदाहरण
डिकोडर_लेयर =ट्रांसफॉर्मरडिकोडरलेयर(d_model=512, nhead=8)
ट्रांसफार्मर_डिकोडर = ट्रांसफार्मर डिकोडर(डिकोडर_लेयर, संख्या_लेयर=6)
मेमोरी = टॉर्च.रैंड(10, 32, 512)
टीजीटी = कोrc.rand(20, 32, 512)
आउट = ट्रांसफार्मर_डिकोडर(टीजीटी, मेमोरी)
प्रिंट(आउट.आकार)
#मशाल.आकार([20,32,512])
```

    टॉर्च.आकार([20,32,512])


संक्षेप में कहें तो, वास्तव में, स्थिति एन्कोडिंग और मल्टी-हेड ध्यान के बाद, एनकोडर परत और डिकोडर परत का आकार नहीं बदलेगा, और एनकोडर और डिकोडर क्रमशः src और tgt के आकार के अनुरूप हैं।

## ट्रांसफार्मर


```अजगर
क्लास ट्रांसफार्मर (एनएन.मॉड्यूल):
    आर'''
    पैरामीटर:
        d_model: शब्द एम्बेडिंग का आयाम (आवश्यक) (डिफ़ॉल्ट=512)
        एनहेड: मल्टी-हेड अटेंशन में समानांतर हेड्स की संख्या (आवश्यक) (डिफ़ॉल्ट=8)
        num_encoder_layers: एन्कोडिंग परतों की संख्या (डिफ़ॉल्ट=8)
        num_decoder_layers: डिकोडिंग परतों की संख्या (डिफ़ॉल्ट=8)
        मंद_फ़ीडफ़ॉरवर्ड:पूरी तरह से जुड़ी हुई परत में न्यूरॉन्स की संख्या, जिसे इस परत के माध्यम से इनपुट के आयाम के रूप में भी जाना जाता है (डिफ़ॉल्ट = 2048)
        ड्रॉपआउट: ड्रॉपआउट की संभावना (डिफ़ॉल्ट = 0.1)
        सक्रियण: दो रैखिक परतों के बीच सक्रियण फ़ंक्शन, डिफ़ॉल्ट रिले या गेलु
        कस्टम_एनकोडर: कस्टम एनकोडर (डिफ़ॉल्ट=कोई नहीं)
        कस्टम_डिकोडर: कस्टम डिकोडर (डिफ़ॉल्ट=कोई नहीं)
        lay_norm_eps: हर को 0 होने से रोकने के लिए परत सामान्यीकरण में छोटी मात्रा (डिफ़ॉल्ट = 1e-5)
        बैच_फर्स्ट: यदि `सही` है, तो यह (बैच, seq, feture) है, यदि यह `गलत` है, तो यह (seq, बैच, फीचर) है (डिफ़ॉल्ट: गलत)
    
    उदाहरण:
        >>> ट्रांसफार्मर_मॉडल = ट्रांसफार्मर(nhead=16, num_encoder_layers=12)
        >>>एसआरसी = टॉर्च.रैंड((10, 32, 512))
        >>> टीजीटी = टॉर्च.रैंड((20, 32, 512))
        >>> आउट = ट्रांसफार्मर_मॉडल(src, tgt)
    '''
    def __init__(स्वयं, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,
                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, ड्रॉपआउट: फ्लोट = 0.1,
                 सक्रियण = F.relu, कस्टम_एनकोडर: वैकल्पिक[कोई भी] = कोई नहीं, कस्टम_डिकोडर: वैकल्पिक[कोई भी] = कोई नहीं,
                 लेयर_नॉर्म_ईपीएस: फ्लोट = 1ई-5, बैच_फर्स्ट: बूल = गलत) -> कोई नहीं:
        सुपर(ट्रांसफॉर्मर, स्वयं).__init__()
        यदि कस्टम_एनकोडर कोई नहीं है:
            स्व.एनकोडर = कस्टम_एनकोडर
        अन्य:
            एनकोडर_लेयर = ट्रांसफार्मरएनकोडरलेयर(डी_मॉडल, एनहेड, डिम_फीडफॉरवर्ड, ड्रॉपआउट,
                                                    सक्रियण, लेयर_नॉर्म_ईपीएस, बैच_फर्स्ट)
            एनकोडर_नॉर्म = एनएन.लेयरनॉर्म(डी_मॉडल, ईपीएस=लेयर_नॉर्म_ईपीएस)
            self.encoder = ट्रांसफार्मरएनकोडर(एनकोडर_लेयर, num_encoder_layers)

        यदि सीustom_decoder कोई नहीं है:
            सेल्फ.डिकोडर = कस्टम_डिकोडर
        अन्य:
            डिकोडर_लेयर = ट्रांसफार्मरडिकोडरलेयर(डी_मॉडल, एनहेड, डिम_फीडफॉरवर्ड, ड्रॉपआउट,
                                                    सक्रियण, लेयर_नॉर्म_ईपीएस, बैच_फर्स्ट)
            डिकोडर_नॉर्म = एनएन.लेयरनॉर्म(डी_मॉडल, ईपीएस=लेयर_नॉर्म_ईपीएस)
            self.decoder = ट्रांसफार्मरडिकोडर(डिकोडर_लेयर, num_decoder_layers, डिकोडर_नॉर्म)

        self._reset_parameters()

        self.d_model = d_modelस्वयं.nhead = nhead

        स्वयं.बैच_प्रथम = बैच_प्रथम

    डीईएफ़ फ़ॉरवर्ड(स्वयं, स्रोत: टेंसर, टीजीटी: टेंसर, स्रोत_मास्क: वैकल्पिक[टेंसर] = कोई नहीं, टीजीटी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं,
                मेमोरी_मास्क: वैकल्पिक[टेंसर] = कोई नहीं, src_key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं,
                tgt_key_padding_mask: वैकल्पिक[टेंसर] = कोई नहीं, मेमोरी_की_पैडिंग_मास्क: वैकल्पिक[टेंसर] = कोई नहीं) -> टेंसर:
        आर'''
        पैरामीटर:
            स्रोत: स्रोत भाषा अनुक्रम (एनकोडर को भेजा गया) (आवश्यक)
            टीजीटी: लक्ष्य भाषा अनुक्रम (इनपुटडिकोडर) (आवश्यक)
            src_mask: (वैकल्पिक)
            tgt_mask: (वैकल्पिक)
            मेमोरी_मास्क: (वैकल्पिक)
            src_key_padding_mask: (वैकल्पिक)
            tgt_key_padding_mask: (वैकल्पिक)
            मेमोरी_की_पैडिंग_मास्क: (वैकल्पिक)
        
        आकार:
            - स्रोत: आकार:`(एस, एन, ई)`, `(एन, एस, ई)` यदि बैच_फर्स्ट।
            - tgt: आकार:`(T, N, E)`, `(N, T, E)` यदि बैच_फर्स्ट।
            - src_mask: आकार:`(एस, एस)`।
            - tgt_mask: आकार:`(T, T)`।
            - मेमोरी_मास्क: आकार:`(टी, एस)`।- src_key_padding_mask: आकार:`(एन, एस)`।
            - tgt_key_padding_mask: आकार:`(एन, टी)`।
            - मेमोरी_की_पैडिंग_मास्क: आकार:`(एन, एस)`।

            [src/tgt/memory]_mask यह सुनिश्चित करता है कि कुछ स्थितियाँ दिखाई न दें। उदाहरण के लिए, डिकोड करते समय, आप केवल स्थिति और उसकी पिछली स्थिति देख सकते हैं, लेकिन बाद की स्थिति नहीं।
            यदि यह एक बाइटटेन्सर है, तो गैर-0 पदों को नजरअंदाज कर दिया जाएगा और यदि यह एक बूलटेन्सर है, तो ट्रू के अनुरूप पदों को नजरअंदाज कर दिया जाएगा;
            यदि यह एक संख्यात्मक मान है, तो इसे सीधे attn_weights में जोड़ा जाएगा

            [src/tgt/memory]_key_padding_mask कुंजी में कुछ तत्वों को ध्यान गणना में भाग नहीं लेने देता है। तीनों स्थितियाँ उपरोक्त के समान हैं।

            - आउटपुट: आकार:`(टी, एन, ई)`, `(एन, टी, ई)` यदि बैच_फर्स्ट।

        सूचना:Src और tgt का अंतिम आयाम d_model के बराबर होना चाहिए, और बैच का आयाम बराबर होना चाहिए।
            
        उदाहरण:
            >>> आउटपुट = ट्रांसफार्मर_मॉडल(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)
        '''
        मेमोरी = सेल्फ.एनकोडर(src, मास्क=src_mask, src_key_padding_mask=src_key_padding_mask)
        आउटपुट = सेल्फ.डिकोडर(टीजीटी, मेमोरी, टीजीटी_मास्क=टीजीटी_मास्क, मेमोरी_मास्क=मेमोरी_मास्क,
                              tgt_key_padding_mask=tgt_key_padding_mask,
                              मेमोरी_की_पैडिंग_मास्क=मेमोरी_की_पैडिनजी_मास्क)
        वापसी आउटपुट
        
    def generate_square_subsequent_mask(self, sz: int) -> Tensor:
        r''' अनुक्रम के बारे में एक मुखौटा उत्पन्न करता है, अस्पष्ट क्षेत्र को `-inf` असाइन किया गया है, और अस्पष्ट क्षेत्र को `0`''' असाइन किया गया है
        मुखौटा = (मशाल.triu(मशाल.ones(sz, sz)) == 1).स्थानांतरण(0, 1)
        मास्क = मास्क.फ्लोट().मास्कड_फिल(मास्क == 0, फ्लोट('-inf')).मास्कड_फिल(मास्क == 1, फ्लोट(0.0))
        वापसी मुखौटा

    def _reset_parameters(स्वयं):
        r'''सामान्य वितरण के साथ पैरामीटर प्रारंभ करें''
        self.parameters() में p के लिए:
            यदि p.dim() > 1:
                xavier_uniform_(p)
```


```अजगर
# छोटा सा उदाहरण
ट्रांसफार्मर_मॉडल = ट्रांसफार्मर(nhead=16, num_encoder_layers=12)
स्रोत = टॉर्च.रैंड((10, 32, 512))
टीजीटी = टॉर्च.रैंड((20, 32, 512))
आउट = ट्रांसफार्मर_मॉडल(src, tgt)
प्रिंट(आउट.आकार)
#मशाल.आकार([20,32,512])
```

    टॉर्च.आकार([20,32,512])


अब तक, हमने PyTorch की ट्रांसफॉर्मर लाइब्रेरी को पूरी तरह से कार्यान्वित किया है, आधिकारिक संस्करण की तुलना में, हस्तलिखित संस्करण में कम निर्णय कथन हैं।
## आभार
यह लेख ताई युनपेंग द्वारा लिखा गया था और इस परियोजना के सदस्यों द्वारा पुनर्गठित और संगठित किया गया था। अंत में, मैं आपकी पढ़ने की प्रतिक्रिया और सितारों की प्रतीक्षा कर रहा हूं, धन्यवाद।