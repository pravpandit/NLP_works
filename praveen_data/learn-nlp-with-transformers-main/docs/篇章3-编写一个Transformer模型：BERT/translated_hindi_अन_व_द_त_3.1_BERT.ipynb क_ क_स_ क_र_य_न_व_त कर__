{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## प्रस्तावना\n",
    "इस लेख में बहुत सारे स्रोत कोड और स्पष्टीकरण हैं। प्रत्येक मॉड्यूल को पैराग्राफ और क्षैतिज रेखाओं द्वारा विभाजित किया गया है, साथ ही, वेबसाइट प्रत्येक अनुभाग के बीच तेजी से कूदने में मदद करने के लिए एक साइडबार से सुसज्जित है इसे पढ़ने के बाद BERT की समझ। साथ ही, बर्ट स्रोत कोड को चरण दर चरण डीबग करने, संबंधित मॉड्यूल को डीबग करने और फिर स्पष्टीकरण की तुलना करने के लिए pycharm और vscode जैसे टूल का उपयोग करने की अनुशंसा की जाती है।इस अध्याय का tion.\n",
    "\n",
    "इसमें शामिल ज्यूपिटर को [कोड बेस: अध्याय 3-एक ट्रांसफार्मर मॉडल लिखें: बीईआरटी, डाउनलोड] (https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7) में पाया जा सकता है। %AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AAट्रांसफॉर्मर%E6%A8%A1%E5%9E%8B%EF% बीसी%9एबर्ट)\n",
    "\n",
    "सीखने के लिए यह अध्याय H[HuggingFace/Transformers, 48.9k Star](https://github.com/huggingface/transformers) पर आधारित होगा। इस अध्याय के सभी कोड [huggingface bert, कृपया ध्यान देंतीव्र संस्करण अपडेट के कारण, इसमें अंतर हो सकता है, कृपया संस्करण 4.4.2 देखें](https://github.com/huggingface/transformers/tree/master/src/transformers/models/bert)HuggingFace एक है चैटबॉट स्टार्टअप सेवा प्रदाता का मुख्यालय न्यूयॉर्क में है। इसने BERT प्रवृत्ति के संकेत को बहुत पहले ही पकड़ लिया और pytorch पर आधारित BERT मॉडल को लागू करना शुरू कर दिया। मूल प्रभाव को पुन: प्रस्तुत करते हुए इस परियोजना को मूल रूप से pytorch-pretrained-bert नाम दिया गया था -उपयोग के तरीकेइस शक्तिशाली मॉडल के आधार पर विभिन्न खेल और अनुसंधान को सुविधाजनक बनाने के लिए।\n",
    "\n",
    "जैसे-जैसे उपयोगकर्ताओं की संख्या में वृद्धि हुई, परियोजना भी एक बड़े ओपन सोर्स समुदाय में विकसित हुई, जिसमें विभिन्न पूर्व-प्रशिक्षित भाषा मॉडलों को विलय किया गया और टेन्सरफ़्लो कार्यान्वयन जोड़ा गया, और 2019 की दूसरी छमाही में इसका नाम बदलकर ट्रांसफॉर्मर कर दिया गया। इस लेख को लिखने के समय तक ( 30 मार्च, 2021), परियोजना में 43k+ सितारे हैं, यह कहा जा सकता है कि ट्रांसफॉर्मर एक वास्तविक बुनियादी एनएलपी उपकरण बन गया है।\n",
    "\n",
    "## इस अनुभाग की मुख्य सामग्री\n",
    "![चित्र: BERT संरचना](./pictures/3-6-bert.png) चित्र: BERT संरचना, स्रोत IrEne: ट्रांसफार्मर के लिए व्याख्या योग्य ऊर्जा भविष्यवाणी\n",
    "\n",
    "यह लेख ट्रांसफॉर्मर्स संस्करण 4.4.2 (19 मार्च, 2021 को जारी) परियोजना के पाइटोरच संस्करण के बीईआरटी-संबंधित कोड पर आधारित है, और कोड संरचना, विशिष्ट कार्यान्वयन और सिद्धांत और उपयोग के परिप्रेक्ष्य से इसका विश्लेषण करता है।\n",
    "मुख्य सामग्री:\n",
    "\n",
    "1. BERT टोकनाइजेशन शब्द विभाजन मॉडल (BertTokenizer)\n",
    "\n",
    "2. BERT मॉडल ऑन्टोलॉजी मॉडल (BertModएल)\n",
    "\n",
    "- बर्टएम्बेडिंग्स\n",
    "\n",
    "- बर्टएनकोडर\n",
    "\n",
    "- बर्टलेयर\n",
    "\n",
    "- बर्टअटेंशन\n",
    "\n",
    "- बर्टइंटरमीडिएट\n",
    "\n",
    "- बर्टआउटपुट\n",
    "\n",
    "- बर्टपूलर"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1-टोकनाइजेशन-बर्टटोकनाइजर\n",
    "BERT से संबंधित टोकनाइज़र मुख्य रूप से [`models/bert/tokenization_bert.py`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py) में लिखे गए हैं।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import unicodedata\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"bert-base-uncased\": \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\",\n",
    "    }\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"bert-base-uncased\": 512,\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"bert-base-uncased\": {\"do_lower_case\": True},\n",
    "}\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "\"\"\"एक शब्दावली फ़ाइल को एक शब्दकोश में लोड करता है।\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\")\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "\"\"\"पाठ के एक टुकड़े पर मूल रिक्त स्थान की सफाई और विभाजन चलाता है।\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class BertTokenizer(PreTrainedTokenizer):\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        do_lower_case=True,\n",
    "        do_basic_tokenize=True,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        tokenize_chinese_chars=True,\n",
    "        strip_accents=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            do_lower_case=do_lower_case,\n",
    "            do_basic_tokenize=do_basic_tokenize,\n",
    "            never_split=never_split,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            tokenize_chinese_chars=tokenize_chinese_chars,\n",
    "            strip_accents=strip_accents,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained \"\n",
    "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n",
    "            )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.do_basic_tokenize = do_basic_tokenize\n",
    "        if do_basic_tokenize:\n",
    "            self.basic_tokenizer = BasicTokenizer(\n",
    "                do_lower_case=do_lower_case,\n",
    "                never_split=never_split,\n",
    "                tokenize_chinese_chars=tokenize_chinese_chars,\n",
    "                strip_accents=strip_accents,\n",
    "            )\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n",
    "\n",
    "    @property\n",
    "    def do_lower_case(self):\n",
    "        return self.basic_tokenizer.do_lower_case\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.vocab, **self.added_tokens_encoder)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        if self.do_basic_tokenize:\n",
    "            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
    "\n",
    "# यदि टोकन नेवर_स्प्लिट सेट का हिस्सा है\n",
    "                if token in self.basic_tokenizer.never_split:\n",
    "                    split_tokens.append(token)\n",
    "                else:\n",
    "                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n",
    "        else:\n",
    "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
    "        return split_tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "\"\"\"वोकैब का उपयोग करके एक टोकन (str) को एक आईडी में परिवर्तित करता है।\"\"\"\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "\"\"\"शब्दावली का उपयोग करके एक सूचकांक (पूर्णांक) को एक टोकन (str) में परिवर्तित करता है।\"\"\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "\"\"\"टोकन (स्ट्रिंग) के अनुक्रम को एक स्ट्रिंग में परिवर्तित करता है।\"\"\"\n",
    "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
    "        adding special tokens. A BERT sequence has the following format:\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs to which the special tokens will be added.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "        Returns:\n",
    "            :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` method.\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "        Returns:\n",
    "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
    "            )\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n",
    "        pair mask has the following format:\n",
    "        ::\n",
    "            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "            | first sequence    | second sequence |\n",
    "        If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "        Returns:\n",
    "            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
    "            sequence(s).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
    "        index = 0\n",
    "        if os.path.isdir(save_directory):\n",
    "            vocab_file = os.path.join(\n",
    "                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
    "            )\n",
    "        else:\n",
    "            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\"\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "        return (vocab_file,)\n",
    "\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "\n",
    "    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True, strip_accents=None):\n",
    "        if never_split is None:\n",
    "            never_split = []\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.never_split = set(never_split)\n",
    "        self.tokenize_chinese_chars = tokenize_chinese_chars\n",
    "        self.strip_accents = strip_accents\n",
    "\n",
    "    def tokenize(self, text, never_split=None):\n",
    "        \"\"\"\n",
    "        Basic Tokenization of a piece of text. Split on \"white spaces\" only, for sub-word tokenization, see\n",
    "        WordPieceTokenizer.\n",
    "        Args:\n",
    "            **never_split**: (`optional`) list of str\n",
    "                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n",
    "                :func:`PreTrainedTokenizer.tokenize`) List of token not to split.\n",
    "        \"\"\"\n",
    "# यूनियन() दो सेटों को जोड़कर एक नया सेट लौटाता है।\n",
    "        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n",
    "        text = self._clean_text(text)\n",
    "\n",
    "# इसे बहुभाषी और चीनी के लिए 1 नवंबर, 2018 को जोड़ा गया था\n",
    "# मॉडल। यह अब अंग्रेजी मॉडल पर भी लागू होता है, लेकिन ऐसा नहीं है\n",
    "# मामला यह है कि अंग्रेजी मॉडलों को किसी भी चीनी डेटा पर प्रशिक्षित नहीं किया गया था\n",
    "# और आम तौर पर उनमें कोई चीनी डेटा नहीं होता (चीनी होते हैं)।\n",
    "शब्दावली में # वर्ण क्योंकि विकिपीडिया में कुछ चीनी भाषाएँ हैं\n",
    "अंग्रेजी विकिपीडिया में # शब्द।)\n",
    "        if self.tokenize_chinese_chars:\n",
    "            text = self._tokenize_chinese_chars(text)\n",
    "        orig_tokens = whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if token not in never_split:\n",
    "                if self.do_lower_case:\n",
    "                    token = token.lower()\n",
    "                    if self.strip_accents is not False:\n",
    "                        token = self._run_strip_accents(token)\n",
    "                elif self.strip_accents:\n",
    "                    token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_punc(token, never_split))\n",
    "\n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "\"\"\"पाठ के एक टुकड़े से उच्चारण को अलग करता है।\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text, never_split=None):\n",
    "\"\"\"विराम चिह्न को पाठ के एक टुकड़े पर विभाजित करता है।\"\"\"\n",
    "        if never_split is not None and text in never_split:\n",
    "            return [text]\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    def _tokenize_chinese_chars(self, text):\n",
    "\"\"\"किसी भी सीजेके चरित्र के आसपास रिक्त स्थान जोड़ता है।\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if self._is_chinese_char(cp):\n",
    "                output.append(\" \")\n",
    "                output.append(char)\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _is_chinese_char(self, cp):\n",
    "\"\"\"जांचता है कि क्या सीपी सीजेके वर्ण का कोडपॉइंट है।\"\"\n",
    "# यह CJK यूनिकोड ब्लॉक में किसी भी चीज़ के रूप में \"चीनी वर्ण\" को परिभाषित करता है:\n",
    "# https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "        #\n",
    "# ध्यान दें कि सीजेके यूनिकोड ब्लॉक में सभी जापानी और कोरियाई अक्षर नहीं हैं,\n",
    "# अपने नाम के बावजूद आधुनिक कोरियाई हंगुल वर्णमाला एक अलग ब्लॉक है,\n",
    "# जैसे जापानी हिरागाना और कटकाना उन अक्षरों का उपयोग किया जाता है\n",
    "#शब्दों को स्थान-विभाजित किया गया है, इसलिए उनका विशेष रूप से इलाज और प्रबंधन नहीं किया जाता है\n",
    "# अन्य सभी भाषाओं की तरह।\n",
    "        if (\n",
    "            (cp >= 0x4E00 and cp <= 0x9FFF)\n",
    "            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n",
    "            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n",
    "            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
    "            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n",
    "            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n",
    "            or (cp >= 0xF900 and cp <= 0xFAFF)\n",
    "            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n",
    "        ):  #\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "\"\"\"पाठ पर अमान्य वर्ण हटाने और रिक्त स्थान साफ़ करने का कार्य करता है।\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xFFFD or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "\n",
    "class WordpieceTokenizer(object):\n",
    "\"\"\"वर्डपीस टोकनाइजेशन चलाता है।\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n",
    "        tokenization using the given vocabulary.\n",
    "        For example, :obj:`input = \"unaffable\"` wil return as output :obj:`[\"un\", \"##aff\", \"##able\"]`.\n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer`.\n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "क्लास बर्टटोकनाइज़र(प्रीट्रेन्डटोकनाइज़र):\n",
    "\"\"\"\n",
    "वर्डपीस के आधार पर एक BERT टोकनाइज़र का निर्माण करें।\n",
    "\n",
    "यह टोकननाइज़र :class:`~transformers.PreTrainedTokenizer` से विरासत में मिला है जिसमें अधिकांश मुख्य विधियाँ शामिल हैं।\n",
    "उन तरीकों के बारे में अधिक जानकारी के लिए उपयोगकर्ताओं को इस सुपरक्लास को देखना चाहिए।\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "'बर्टटोकनाइज़र' 'बेसिकटोकनाइज़र' और 'वर्डपीसटोकनाइज़र' पर आधारित एक टोकननाइज़र है:\n",
    "- बेसिकटोकनाइज़र प्रसंस्करण के पहले चरण के लिए ज़िम्मेदार है - विराम चिह्न, स्थान द्वारा वाक्यों को विभाजित करनाएस, आदि, और यह संभालना कि लोअरकेस को एकीकृत करना है या नहीं, और अवैध अक्षरों को साफ़ करना।\n",
    "- चीनी अक्षरों के लिए, प्रीप्रोसेसिंग (रिक्त स्थान जोड़कर) के माध्यम से शब्द द्वारा विभाजित;\n",
    "- उसी समय, आप नेवर_स्प्लिट का उपयोग कर सकते हैं जो निर्दिष्ट करता है कि कुछ शब्दों को खंडित नहीं किया जाना है;\n",
    "- यह चरण वैकल्पिक है (डिफ़ॉल्ट रूप से निष्पादित)।\n",
    "- WordPieceTokenizer शब्दों के आधार पर शब्दों को उपशब्दों में विघटित करता है।\n",
    "- उपशब्द चार और शब्द के बीच होता है, यह न केवल कुछ हद तक शब्दों के अर्थ को बरकरार रखता है, बल्कि एसी में भी ले जाता हैअंग्रेजी में अपंजीकृत शब्दों की एकवचन और बहुवचन, काल और OOV (शब्दावली से बाहर) समस्याओं के कारण होने वाले शब्द सूची विस्फोट की गणना करें, और मूल और काल प्रत्यय को अलग करें, जिससे शब्द सूची और प्रशिक्षण की कठिनाई कम हो जाए;\n",
    "- उदाहरण के लिए, टोकननाइज़र शब्द को दो भागों में विघटित किया जा सकता है: \"टोकन\" और \"##ाइज़र\"। ध्यान दें कि बाद वाले शब्द के \"##\" का अर्थ है कि यह पिछले शब्द से जुड़ा है।\n",
    "BertTokenizer में निम्नलिखित सामान्य विधियाँ हैं:\n",
    "- from_pretrained: आरंभ करें aशब्दावली फ़ाइल (vocab.txt) वाली निर्देशिका से टोकननाइज़र;\n",
    "- टोकननाइज़: किसी पाठ (शब्द या वाक्य) को उपशब्दों की सूची में विघटित करें;\n",
    "- Convert_tokens_to_ids: उपशब्दों की सूची को उपशब्द अनुक्रमणिका की सूची में बदलें;\n",
    "- Convert_ids_to_tokens: पिछले वाले के विपरीत;\n",
    "- Convert_tokens_to_string: s को रूपांतरित करें, ubword सूची को \"##\" द्वारा शब्दों या वाक्यों में वापस संयोजित किया जाता है;\n",
    "- एन्कोड: एकल वाक्य इनपुट के लिए, शब्दों को विघटित करें और \"[सीएलएस]\" की संरचना बनाने के लिए विशेष शब्द जोड़ें।x, [SEP]\" और इसे शब्द सूची के अनुरूप सबस्क्रिप्ट की एक सूची में परिवर्तित करें (केवल कई वाक्यों के लिए पहले दो), शब्दों को विघटित करें और \"[CLS] की संरचना बनाने के लिए विशेष शब्द जोड़ें। , X1, [SEP], x2, [SEP]” और इसे सबस्क्रिप्ट की सूची में परिवर्तित करें;\n",
    "- डिकोड: एनकोड विधि के आउटपुट को पूर्ण वाक्य में बदला जा सकता है।\n",
    "और कक्षा की अपनी विधियाँ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 698kB/s]\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 11.1kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 863kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bt('I like natural language progressing!')\n",
    "# {'इनपुट_आईडी': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'टोकन_टाइप_आईडी': [0, 0, 0, 0, 0, 0, 0, 0], 'ध्यान_मास्क': [1, 1, 1, 1, 1, 1, 1, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2-मॉडल-बर्टमॉडल\n",
    "BERT मॉडल से संबंधित कोड मुख्य रूप से [`/models/bert/modeling_bert.py`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.) में लिखा जाता है। py), जिसमें कोड की एक हजार से अधिक लाइनें हैं, जिसमें BERT मॉडल की मूल संरचना और उस पर आधारित फाइन-ट्यूनिंग मॉडल शामिल है।\n",
    "\n",
    "आइए BERT मॉडल के विश्लेषण से शुरुआत करें:\n",
    "```\n",
    "क्लास बर्टमॉडल(बर्टप्रीट्रेन्डमॉडल):\n",
    "\"\"\"\n",
    "\n",
    "मॉडल एक एनकोडर के रूप में व्यवहार कर सकता है (केवल स्वयं के साथ)।-ध्यान) साथ ही एक डिकोडर, जिस स्थिति में एक परत\n",
    "'ध्यान है' में वर्णित वास्तुकला का पालन करते हुए, आत्म-ध्यान परतों के बीच क्रॉस-अटेंशन जोड़ा जाता है\n",
    "आपको बस इतना ही चाहिए <https://arxiv.org/abs/1706.03762>`__ आशीष वासवानी, नोम शज़ीर, निकी परमार, जैकब उस्ज़कोरिट द्वारा,\n",
    "लिलियन जोन्स, एडन एन. गोमेज़, लुकाज़ कैसर और इलिया पोलोसुखिन।\n",
    "\n",
    "डिकोडर के रूप में व्यवहार करने के लिए मॉडल को कॉन्फ़िगरेशन के :obj:`is_decoder` तर्क के साथ प्रारंभ करने की आवश्यकता है\n",
    ":obj:`True` पर सेट करें ISeq2Seq मॉडल में, मॉडल को :obj:`is_decoder` दोनों के साथ प्रारंभ करने की आवश्यकता है\n",
    "तर्क और :obj:`add_cross_attention` को :obj:`True` पर सेट किया गया है; एक :obj:`encoder_hidden_states` को फिर एक के रूप में अपेक्षित किया जाता है\n",
    "फॉरवर्ड पास के लिए इनपुट।\n",
    "\"\"\"\n",
    "```\n",
    "बर्टमॉडल मुख्य रूप से एक ट्रांसफार्मर एनकोडर संरचना है, जिसमें तीन भाग होते हैं:\n",
    "1. एम्बेडिंग, अर्थात्, BertEmbeddings वर्ग की इकाइयाँ, जो शब्द प्रतीक के अनुसार संबंधित वेक्टर प्रतिनिधित्व प्राप्त करती हैं;\n",
    "2. एनकोडर, अर्थात्, BertE की इकाइयाँएनकोडर वर्ग;\n",
    "3. पूलर, अर्थात्, बर्टपूलर वर्ग की इकाइयाँ, जो वैकल्पिक है।\n",
    "\n",
    "**ध्यान दें कि बर्टमॉडल को डिकोडर के रूप में भी कॉन्फ़िगर किया जा सकता है, लेकिन इस भाग पर नीचे चर्चा नहीं की गई है।**\n",
    "\n",
    "निम्नलिखित BertModel के आगे प्रसार के दौरान प्रत्येक पैरामीटर और रिटर्न मान का अर्थ प्रस्तुत करेगा:\n",
    "```\n",
    "def आगे(\n",
    "खुद,\n",
    "इनपुट_आईडी=कोई नहीं,\n",
    "ध्यान_मास्क=कोई नहीं,\n",
    "टोकन_टाइप_आईडी=कोई नहीं,\n",
    "स्थिति_आईडी=कोई नहीं, हेड_मास्क=कोई नहीं,\n",
    "इनपुट्स_एम्बेड्स=कोई नहीं,\n",
    "एनकोडर_हिडन_स्टेट्स=कोई नहीं,\n",
    "एनकोडर_अटेंशन_मास्क=कोई नहीं,Past_key_values=कोई नहीं,\n",
    "उपयोग_कैश=कोई नहीं,\n",
    "आउटपुट_ध्यान=कोई नहीं,\n",
    "आउटपुट_हिडन_स्टेट्स=कोई नहीं,\n",
    "return_dict=कोई नहीं,\n",
    "): ...\n",
    "```\n",
    "- इनपुट_आईडी: टोकननाइज़र सेगमेंटेशन के बाद सबवर्ड से संबंधित सबस्क्रिप्ट सूची;\n",
    "- ध्यान_मास्क: आत्म-ध्यान की प्रक्रिया में, इस मास्क का उपयोग उपशब्द के वाक्य और पैडिंग के बीच अंतर को चिह्नित करने और पैडिंग भाग को 0 से भरने के लिए किया जाता है;\n",
    "- टोकन_टाइप_आईडी: उस वाक्य को चिह्नित करें जिसमें उपशब्द वर्तमान में स्थित है (पहला वाक्य/दूसरा वाक्य/पैड)।आईएनजी);\n",
    "- स्थिति आईडी: उपशब्द की वर्तमान स्थिति को चिह्नित करें। वाक्य की स्थिति सूचकांक जहां पिछला शब्द स्थित है;\n",
    "- हेड_मास्क: कुछ परतों की कुछ ध्यान गणनाओं को अमान्य करने के लिए उपयोग किया जाता है;\n",
    "- इनपुट्स_एम्बेड्स: यदि प्रदान किया गया है, तो इनपुट_आईड्स की आवश्यकता नहीं है, और एम्बेडिंग लुकअप प्रक्रिया को छोड़ दिया जाता है और सीधे एन्कोडर गणना में एम्बेडिंग के रूप में दर्ज किया जाता है;\n",
    "- एनकोडर_हिडन_स्टेट्स: यह भाग तब काम करता है जब बर्टमॉडल को डिकोडर के रूप में कॉन्फ़िगर किया जाता है, और क्रॉस-अटेंशन i किया जाएगाआत्म-ध्यान के बजाय;\n",
    "- एनकोडर_अटेंशन_मास्क: जैसा कि ऊपर बताया गया है, क्रॉस-अटेंशन में एनकोडर इनपुट की पैडिंग को चिह्नित करने के लिए उपयोग किया जाता है;\n",
    "- Past_key_values: क्रॉस-अटेंशन की लागत को कम करने के लिए यह पैरामीटर पूर्व-गणना किए गए K-V उत्पाद में पास होता प्रतीत होता है (क्योंकि यह भाग मूल रूप से दोहराई गई गणना है);\n",
    "- उपयोग_कैश: पिछले पैरामीटर को सहेजें और डिकोडिंग को तेज करने के लिए इसे वापस पास करें;\n",
    "- आउटपुट_अटेंशन: प्रत्येक मध्यवर्ती परत का ध्यान आउटपुट लौटाना है या नहीं;\n",
    "- आउटपुट_हिडन_स्टेट्स: कप्रत्येक मध्यवर्ती परत के आउटपुट को वापस करने के लिए ईथर;\n",
    "- रिटर्न_डिक्ट: आउटपुट को कुंजी-मूल्य जोड़े (मॉडलआउटपुट क्लास, जिसे टपल के रूप में भी इस्तेमाल किया जा सकता है) के रूप में वापस करना है या नहीं, डिफ़ॉल्ट सत्य है।\n",
    "\n",
    "**ध्यान दें कि यहां हेड_मास्क ध्यान गणना को अमान्य कर देता है, जो नीचे उल्लिखित ध्यान हेड प्रूनिंग से अलग है, और केवल इस गुणांक द्वारा कुछ ध्यान के गणना परिणामों को गुणा करता है।\n",
    "\n",
    "आउटपुट भाग इस प्रकार है:\n",
    "```\n",
    "# BertModel आगे प्रसार पुनःभाग को मोड़ो\n",
    "यदि नहीं तो return_dict:\n",
    "वापसी (अनुक्रम_आउटपुट, पूल्ड_आउटपुट) + एनकोडर_आउटपुट[1:]\n",
    "\n",
    "वापसी BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "अंतिम_छिपी_स्थिति=अनुक्रम_आउटपुट,\n",
    "पूलर_आउटपुट=पूल_आउटपुट,\n",
    "Past_key_values=encoder_outputs.past_key_values,\n",
    "छुपी_स्थिति=अनुक्रम_आउटपुट,\n",
    "पूलर_आउटपुट=पूल_आउटपुट,\n",
    "Past_key_values=encoder_outputs.past_key_values,n_states=encoder_outputs.hidden_states,\n",
    "ध्यान=एनकोडर_आउटपुट.ध्यान,\n",
    "क्रॉस_अटेंशन=एनकोडर_आउटपुट.क्रॉस_अटेंशन,\n",
    ")\n",
    "```\n",
    "यह देखा जा सकता है कि रिटर्न वैल्यू में न केवल एनकोडर और पूलर का आउटपुट शामिल है, बल्कि अन्य निर्दिष्ट आउटपुट भाग (hidden_states और ध्यान इत्यादि भी शामिल हैं, जिन्हें एनकोडर_आउटपुट [1:] में आसानी से एक्सेस किया जाता है):\n",
    "\n",
    "```\n",
    "# BertEncoder का अग्रवर्ती प्रसार रिटर्न भाग, यानी उपरोक्त एनकोडर_आउटपुट\n",
    "यदि नहीं तो return_dict:\n",
    "वापसी टपल(\n",
    "वी\n",
    "v के लिए [\n",
    "छुपे_राज्य,\n",
    "अगला_डिकोडर_कैश,\n",
    "सभी_छिपे हुए_राज्य,\n",
    "सबका_स्वयं_ध्यान,\n",
    "सभी का ध्यान,\n",
    "]\n",
    "यदि v कोई नहीं है\n",
    ")\n",
    "वापसी बासeModelOutputWithPastAndCrossAttentions(\n",
    "अंतिम_हिडन_स्टेट=हिडन_स्टेट्स,\n",
    "Past_key_values=next_decoder_cache,\n",
    "छुपे हुए राज्य=सभी_छिपे हुए राज्य,\n",
    "ध्यान=सभी_स्वयं_ध्यान,\n",
    "क्रॉस_अटेंशन=सभी_क्रॉस_अटेंशन,\n",
    ")\n",
    "```\n",
    "\n",
    "इसके अलावा, BertModel के पास BERT खिलाड़ियों को विभिन्न ऑपरेशन करने में सुविधा प्रदान करने के लिए निम्नलिखित विधियाँ भी हैं:\n",
    "\n",
    "- get_input_embeddings: एम्बेडिंग में Word_embeddings निकालें, यानी, शब्द वेक्टर भाग;\n",
    "\n",
    "- set_input_embeddings: एम्बेडिंग में Word_embeddings को मान निर्दिष्ट करें;- _prune_heads: ध्यान शीर्षों की छंटाई के लिए एक फ़ंक्शन प्रदान करता है, जिसमें {layer_num: इस परत में छंटाई करने वाले प्रमुखों की सूची} के शब्दकोश के रूप में इनपुट होता है, जो निर्दिष्ट परत के कुछ ध्यान शीर्षों की छंटाई कर सकता है।\n",
    "\n",
    "** प्रूनिंग एक जटिल ऑपरेशन है, जिसमें बनाए गए ध्यान वाले हेड भाग Wq, Kq, Vq और पूरी तरह से जुड़े हिस्से के वजन को एक नए छोटे वजन मैट्रिक्स में जोड़ने के बाद कॉपी करने की आवश्यकता होती है (ध्यान दें कि कॉपी करने से पहले ग्रेड को अक्षम किया जाना चाहिए), और रिकॉर्डिंग वास्तविक समय में काटे गए सिरसबस्क्रिप्ट त्रुटियों को रोकने के लिए, विवरण के लिए, BertAttention भाग में prune_heads विधि देखें।**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import *\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
    "    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
    "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
    "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
    "    input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            batch_size, seq_length = input_shape\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size, seq_length = input_shape\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "# अतीत_कुंजी_मान_लंबाई\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "# हम आयामों का एक आत्म-ध्यान मास्क प्रदान कर सकते हैं [बैच_आकार, from_seq_length, to_seq_length]\n",
    "#स्वयं जिस स्थिति में हमें इसे सभी प्रमुखों के लिए प्रसारित करने योग्य बनाने की आवश्यकता है।\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "# यदि क्रॉस-अटेंशन के लिए 2डी या 3डी अटेंशन मास्क प्रदान किया जाता है\n",
    "# हमें [बैच_आकार, num_heads, seq_length, seq_length] को प्रसारण योग्य बनाने की आवश्यकता है\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "# यदि आवश्यक हो तो हेड मास्क तैयार करें\n",
    "हेड_मास्क में # 1.0 इंगित करता है कि हम सिर रखते हैं\n",
    "# ध्यान_प्रॉब्स का आकार bsz x n_heads x N x N है\n",
    "# इनपुट हेड_मास्क का आकार [num_heads] या [num_hidden_layers x num_heads] है\n",
    "# और हेड_मास्क को आकार में बदल दिया गया है [num_hidden_layers x बैच x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.1-बर्टएम्बेडिंग्स\n",
    "इसमें संक्षेप में तीन भाग शामिल हैं:\n",
    "![बर्ट-एम्बेडिंग](./चित्र/3-0-एम्बेडिंग.पीएनजी) चित्र: बर्ट-एम्बेडिंग\n",
    "\n",
    "1. वर्ड_एम्बेडिंग्स, उपरोक्त पाठ में उपशब्द के अनुरूप एम्बेडिंग।\n",
    "2. टोकन_टाइप_एम्बेडिंग, वाक्य को इंगित करने के लिए उपयोग किया जाता है जहां वर्तमान शब्द स्थित है, वाक्य और पैडिंग और वाक्य जोड़े के बीच अंतर को अलग करने में मदद करने के लिए।\n",
    "3. पोजीशन_एम्बेडिंग, वाक्य में प्रत्येक शब्द की स्थिति एम्बेडिंग, शब्द को अलग करने के लिए उपयोग की जाती हैशब्दों का एर। ट्रांसफार्मर पेपर में डिज़ाइन के विपरीत, इस भाग को साइनसॉइडल फ़ंक्शन द्वारा गणना की गई निश्चित एम्बेडिंग के बजाय प्रशिक्षित किया जाता है। यह आमतौर पर माना जाता है कि यह कार्यान्वयन स्केलेबिलिटी के लिए अनुकूल नहीं है (सीधे लंबे वाक्यों में स्थानांतरित करना मुश्किल है)।\n",
    "\n",
    "लेयरनॉर्म+ड्रॉपआउट की एक परत से गुजरने के बाद तीन एम्बेडिंग को वजन और आउटपुट के बिना जोड़ा जाता है, और इसका आकार (बैच_आकार, अनुक्रम_लंबाई, छिपा हुआ_आकार) होता है।\n",
    "\n",
    "** [यहां लेयरनॉर्म+ड्रॉपआउट का उपयोग क्यों करें?BatchNorm?rm के बजाय ayerNorm? आप एक अच्छे उत्तर का उल्लेख कर सकते हैं: ट्रांसफार्मर अन्य सामान्यीकरण विधियों के बजाय परत सामान्यीकरण का उपयोग क्यों करता है?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "\"\"\"शब्द, स्थिति और टोकन_प्रकार एम्बेडिंग से एम्बेडिंग का निर्माण करें।\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "# self.LayerNorm TensorFlow मॉडल वेरिएबल नाम के साथ चिपकने और लोड करने में सक्षम होने के लिए साँप-आवरण वाला नहीं है\n",
    "# कोई भी TensorFlow चेकपॉइंट फ़ाइल\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "# पोजीशन_आईडी (1, लेन पोजीशन एंब) मेमोरी में सन्निहित है और क्रमबद्ध होने पर निर्यात किया जाता है\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
    "            self.register_buffer(\n",
    "                \"token_type_ids\",\n",
    "                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n",
    "                persistent=False,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "# टोकन_टाइप_आईडी को कंस्ट्रक्टर में पंजीकृत बफर पर सेट करना जहां यह सभी शून्य है, जो आमतौर पर होता है\n",
    "# जब इसका स्वतः-जनरेट होता है, तो पंजीकृत बफ़र टोकन_टाइप_आईडी पास किए बिना मॉडल का पता लगाने में उपयोगकर्ताओं की मदद करता है, हल करता है\n",
    "#अंक #5664\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.2-बर्टएनकोडर\n",
    "\n",
    "बर्टलेयर की कई परतें शामिल हैं। इस भाग के बारे में समझाने के लिए कुछ खास नहीं है, लेकिन इसमें उल्लेख करने लायक एक विवरण है: प्रशिक्षण के दौरान मेमोरी के उपयोग को कम करने के लिए ग्रेडिएंट चेकपॉइंटिंग तकनीक का उपयोग करना।\n",
    "\n",
    "**ग्रेडिएंट चेकपॉइंटिंग एक ग्रेडिएंट चेकपॉइंट है। यह सहेजे गए कम्प्यूटेशनल ग्राफ नोड्स को कम करके मॉडल स्पेस को संपीड़ित करता है, लेकिन ग्रेडिएंट की गणना करते समय अनस्टोर किए गए मानों की पुनर्गणना करना आवश्यक है। पेपर \"ट्रेनिंग डीप नेट\" देखेंसबलाइनियर मेमोरी कॉस्ट के साथ।'' प्रक्रिया इस प्रकार है**\n",
    "![ग्रेडिएंट-चेकपॉइंटिंग](./चित्र/3-1-ग्रेडिएंट-चेकपॉइंटिंग.gif) चित्र: ग्रेडिएंट-चेकपॉइंटिंग\n",
    "\n",
    "BertEncoder में, ग्रेडिएंट चेकपॉइंट को torch.utils.checkpoint.checkpoint के माध्यम से कार्यान्वित किया जाता है, जो उपयोग करने के लिए सुविधाजनक है। आप इस तंत्र के विशिष्ट कार्यान्वयन के लिए दस्तावेज़ का उल्लेख कर सकते हैं: torch.utils.checkpoint - PyTorch 1.8.1 दस्तावेज़। काफी जटिल है, इसलिए मैं यहां इसका विस्तार नहीं करूंगा।\n",
    "\n",
    "गहराई में जाने पर, हम एक परत में प्रवेश करते हैंएनकोडर का एर:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
    "                        \"`use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### 2.2.1.1 बर्टअटेंशन\n",
    "\n",
    "मैंने सोचा था कि ध्यान का कार्यान्वयन यहाँ था, लेकिन मुझे अगले स्तर पर जाने की उम्मीद नहीं थी... उनमें से, स्वयं सदस्य बहु-प्रमुख ध्यान का कार्यान्वयन है, और आउटपुट सदस्य ध्यान के बाद संचालन की एक श्रृंखला लागू करता है , जिसमें पूर्ण कनेक्शन + ड्रॉपआउट + अवशिष्ट + लेयरनॉर्म शामिल है।\n",
    "\n",
    "```\n",
    "क्लास बर्टअटेंशन(एनएन.मॉड्यूल):\n",
    "def __init__(स्वयं, कॉन्फिग):\n",
    "सुपर().__init__()\n",
    "self.self = BertSelfAttention(config)\n",
    "self.output = BertSelfOutput(config)\n",
    "एसelf.pruned_heads = सेट()\n",
    "```\n",
    "सबसे पहले, आइए इस परत पर वापस जाएं। ऊपर उल्लिखित प्रूनिंग ऑपरेशन यहां दिखाई देता है, अर्थात् prune_heads विधि:\n",
    "```\n",
    "def prune_heads(स्वयं, प्रमुख):\n",
    "यदि लेन(शीर्ष) == 0:\n",
    "वापस करना\n",
    "शीर्ष, सूचकांक = find_pruneable_heads_and_indices(\n",
    "शीर्ष, स्व.स्व.संख्या_ध्यान_प्रमुख, स्व.स्व.ध्यान_शीर्ष_आकार, स्व.प्रून्ड_सिर\n",
    ")\n",
    "\n",
    "#प्रून रैखिक परतें\n",
    "self.self.query = prune_linear_layer(self.self.query, अनुक्रमणिका)\n",
    "self.self.key = prune_linear_layer(self.self.key, अनुक्रमणिका)\n",
    "स्व.स्व.विalue = prune_linear_layer(self.self.value, सूचकांक)\n",
    "self.output.dense = prune_linear_layer(self.output.dense, Index, dim=1)\n",
    "\n",
    "#हाइपर पैरामीटर्स को अपडेट करें और काटे गए हेड्स को स्टोर करें\n",
    "self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "स्वयं.स्वयं.सभी_सिर_आकार = स्वयं.स्व.ध्यान_सिर_आकार * स्वयं.स्वयं.संख्या_ध्यान_शीर्ष\n",
    "सेल्फ.प्रूनड_हेड्स = सेल्फ.प्रूनड_हेड्स.यूनियन(हेड्स)\n",
    "```\n",
    "यहां विशिष्ट कार्यान्वयन को इस प्रकार संक्षेप में प्रस्तुत किया गया है:\n",
    "- `find_pruneable_heads_and_indices` हेड टी का पता लगाता हैo काट-छांट की जाए और आयाम सूचकांक को बरकरार रखा जाए;\n",
    "\n",
    "- `prune_linear_layer` उन आयामों को नए मैट्रिक्स में स्थानांतरित करने के लिए जिम्मेदार है जिन्हें सूचकांक के अनुसार Wk/Wq/Wv वजन मैट्रिक्स (पूर्वाग्रह के साथ) में काटा नहीं गया है।\n",
    "इसके बाद, आइये मुख्य बिंदु पर आते हैं - आत्म-ध्यान का विशिष्ट कार्यान्वयन।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "#प्रून रैखिक परतें\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "# हाइपर पैराम्स को अपडेट करें और काटे गए हेड्स को स्टोर करें\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### 2.2.1.1.1 बर्टसेल्फअटेंशन\n",
    "\n",
    "**चेतावनी: यह मॉडल का मुख्य क्षेत्र है और एकमात्र भाग है जिसमें सूत्र शामिल हैं, इसलिए बहुत सारे कोड पोस्ट किए जाएंगे।\n",
    "\n",
    "आरंभीकरण भाग:\n",
    "```\n",
    "क्लास बर्टसेल्फअटेंशन(एनएन.मॉड्यूल):\n",
    "def __init__(स्वयं, कॉन्फिग):\n",
    "सुपर().__init__()\n",
    "यदि config.hidden_size % config.num_attention_heads != 0 और hasattr नहीं है(config, \"embedding_size\"):\n",
    "वैल्यूएरर बढ़ाएं(\n",
    "\"छिपा हुआ आकार (%d) ध्यान की संख्या का गुणक नहीं है\"\n",
    "\"हेड्स (%d)\" % (config.hidden_size, conअंजीर.संख्या_ध्यान_शीर्ष))\n",
    "\n",
    "self.num_attention_heads = config.num_attention_heads\n",
    "self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "स्व.मान = nn.रैखिक(config.hidden_size, self.all_head_size)\n",
    "\n",
    "self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "स्वयं.स्थिति_एम्बेडिंग_प्रकार = getattr(कॉन्फिग, \"पोजीशन_एम्बेडिंग_टाइप\", \"एब्सोल्यूट\")\n",
    "यदि self.position_embedding_type == \"relative_key\" या self.position_embedding_type == \"relative_key_query\":\n",
    "self.max_position_embeddings = config.max_position_embeddings\n",
    "self.distance_embedding = nn.embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "self.is_decoder = config.is_decoder\n",
    "```\n",
    "- परिचित क्वेरी, कुंजी, मान थ्रीवेट और एक ड्रॉपआउट को हटा दें, एक रहस्यमय स्थिति_एम्बेडिंग भी है_प्रकार, और डिकोडर टैग;\n",
    "- ध्यान दें किhidden_size और all_head_size शुरुआत में समान हैं। इस चर को सेट करना अनावश्यक क्यों लगता है, इसका कारण ऊपर दिए गए प्रूनिंग फ़ंक्शन है, कुछ ध्यान शीर्षों को काटने के बाद, all_head_size स्वाभाविक रूप से छोटा हो जाएगा;\n",
    "\n",
    "- हिडन_साइज num_attention_heads का पूर्णांक गुणज होना चाहिए। उदाहरण के तौर पर बर्ट-बेस को लेते हुए, प्रत्येक अटेंशन में 12 हेड होते हैं, और हिडन_साइज 768 है, इसलिए प्रत्येक हेड का आकार अटेंशन_हेड_साइज=768/12= है।64;\n",
    "\n",
    "- यदि आप पढ़ना जारी रखेंगे तो आपको पता चल जाएगा कि पोजीशन_एम्बेडिंग_टाइप क्या है।\n",
    "\n",
    "फिर मुख्य बिंदु आगे की प्रसार प्रक्रिया है।\n",
    "\n",
    "सबसे पहले, आइए बहु-सिर आत्म-ध्यान के मूल सूत्र की समीक्षा करें:\n",
    "\n",
    "$$MHA(Q, K, V) = Concat(head_1, ..., head_h)W^O$$\n",
    "$$head_i = SDPA(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "$$SDPA(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt(d_k)})V$$\n",
    "\n",
    "जैसा कि हम सभी जानते हैं, इन ध्यान शीर्षों की गणना समानांतर में की जाती है, इसलिए उपरोक्त क्वेरी, कुंजी और मान के तीन भार अद्वितीय हैं - ऐसा नहीं हैइसका मतलब यह है कि सभी प्रमुखों का वजन साझा होता है, लेकिन वे एक साथ \"जुड़े\" होते हैं।\n",
    "\n",
    "**[मूल पेपर में मल्टी-हेड का कारण यह है कि मल्टी-हेड ध्यान मॉडल को एक ही ध्यान हेड के साथ विभिन्न प्रतिनिधित्व उप-स्थानों से संयुक्त रूप से जानकारी प्राप्त करने की अनुमति देता है, औसत इसे रोकता है : ट्रांसफार्मर को मल्टी-हेड अटेंशन की आवश्यकता क्यों है? ](https://www.zhihu.com/question/341222779/answer/814111138)**\n",
    "\n",
    "आगे की विधि देखें:\n",
    "```\n",
    "डीईएफ़ ट्रांसपोज़e_for_scores(स्वयं, x):\n",
    "new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "x = x.view(*new_x_shape)\n",
    "वापसी x.permute(0, 2, 1, 3)\n",
    "\n",
    "def आगे(\n",
    "खुद,\n",
    "छुपे_राज्य,\n",
    "ध्यान_मास्क=कोई नहीं,\n",
    "हेड_मास्क=कोई नहीं,\n",
    "एनकोडर_हिडन_स्टेट्स=कोई नहीं,\n",
    "एनकोडर_अटेंशन_मास्क=कोई नहीं,\n",
    "Past_key_value=कोई नहीं,\n",
    "आउटपुट_ध्यान=गलत,\n",
    "):\n",
    "मिश्रित_क्वेरी_लेयर = सेल्फ.क्वेरी(हिडन_स्टेट्स, हेड_मास्क=कोई नहीं, एनकोडर_अटेंशन_मास्क=कोई नहीं, पास्ट_की_वैल्यू=कोई नहीं, आउटपुट_अटेंशन=गलत,\n",
    "):\n",
    "मिश्रित_क्वेरी_लेयर = सेlf.query(hidden_states, हेड_मास्क=कोई नहीं, encoder_hidden_states=कोई नहीं, encoder_attention_mask=कोई नहीं, Past_key_value=कोई नहीं, आउटपुट_attentions=गलत,_states)\n",
    "\n",
    "# कुछ क्रॉस-अटेंशन गणनाओं को छोड़ दें\n",
    "key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "वैल्यू_लेयर = self.transpose_for_scores(self.value(hidden_states))\n",
    "query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "# मूल ध्यान स्कोर प्राप्त करने के लिए \"क्वेरी\" और \"कुंजी\" के बीच डॉट उत्पाद लें।\n",
    "ध्यान_स्कोर = मशाल.मैटमुल(क्यूry_layer, key_layer.transpose(-1, -2))\n",
    "#...\n",
    "```\n",
    "यहां `transpose_for_scores` का उपयोग `hidden_size` को कई हेड आउटपुट में विभाजित करने और मैट्रिक्स गुणन के लिए मध्य दो आयामों को स्थानांतरित करने के लिए किया जाता है;\n",
    "\n",
    "यहां `key_layer/value_layer/query_layer`r का आकार है: (बैच_आकार, num_attention_heads, अनुक्रम_लंबाई, ध्यान_head_size);\n",
    "यहां 'ध्यान_स्कोर' का आकार है: (बैच_आकार, संख्या_ध्यान_शीर्ष, अनुक्रम_लंबाई, अनुक्रम_लंबाई), जो प्राप्त ध्यान मानचित्र के आकार के अनुरूप हैकई शीर्षों की अलग-अलग गणना करके गणना की जाती है।\n",
    "\n",
    "यहां हमने कच्चे ध्यान स्कोर प्राप्त करने के लिए K और Q के गुणन को लागू किया है, सूत्र के अनुसार, अगला कदम $d_k$ द्वारा स्केल करना और सॉफ्टमैक्स ऑपरेशन करना है, हालांकि, जो पहली बार हमारे सामने आता है वह अजीब है पोज़िशनल_एम्बेडिंग, और आइंस्टीन के योगों का एक समूह:\n",
    "\n",
    "```\n",
    "#...\n",
    "यदि self.position_embedding_type == \"relative_key\" या self.position_embedding_ttype == \"relative_key_query\":\n",
    "seq_length =hidden_states.size()[1]\n",
    "पोजीशन_आईडी_एल = टॉर्च.अरेंज(seq_length, dtype=torch.long, डिवाइस=hidden_states.device).view(-1, 1)\n",
    "पोजीशन_आईडी_आर = टॉर्च.अरेंज(seq_length, dtype=torch.long, डिवाइस=hidden_states.device).view(1, -1)\n",
    "दूरी = स्थिति_आईडी_एल - स्थिति_आईडी_आर\n",
    "पोजिशनल_एम्बेडिंग = सेल्फ.डिस्टेंस_एम्बेडिंग(दूरी + सेल्फ.मैक्स_पोजीशन_एम्बेडिंग - 1)\n",
    "पोजिशनल_एम्बेडिंग = पोजिशनल_एम्बेडिंग.टू(dtype=query_layer.dtype) # fp16 अनुकूलता\n",
    "\n",
    "यदि self.position_embedding_type == \"relative_key\":\n",
    "रिश्तेदार_स्थिति_स्कोर = मशाल.ईंसम(\"बीएचएलडी,एलआरडी->बीएचएलआर\", क्वेरी_लेयर, पोजिशनल_एम्बेडिंग)\n",
    "ध्यान_स्कोर = ध्यान_स्कोर + सापेक्ष_स्थिति_स्कोर\n",
    "elif self.position_embedding_type == \"relative_key_query\":\n",
    "रिलेटिव_पोजीशन_स्कोर्स_क्वेरी = टॉर्च.ईंसम(\"बीएचएलडी,एलआरडी->बीएचएलआर\", क्वेरी_लेयर, पोजिशनल_एम्बेडिंग)डिंग)\n",
    "रिलेटिव_पोजिशन_स्कोर्स_की = टॉर्च.ईंसम(\"बीएचआरडी,एलआरडी->बीएचएलआर\", की_लेयर, पोजिशनल_एम्बेडिंग)\n",
    "ध्यान_स्कोर = ध्यान_स्कोर + सापेक्ष_स्थिति_स्कोर_क्वेरी + सापेक्ष_स्थिति_स्कोरes_key\n",
    "#...\n",
    "```\n",
    "**[आइंस्टीन सारांश सम्मेलन के लिए, निम्नलिखित दस्तावेज़ देखें: torch.einsum - PyTorch 1.8.1 दस्तावेज़](https://pytorch.org/docs/stable/generated/torch.einsum.html)**\n",
    "\n",
    "अलग-अलग पोजीशनल_एम्बेडिंग_प्रकार के लिए, तीन ऑपरेशन हैं:\n",
    "\n",
    "- निरपेक्ष: डिफ़ॉल्ट मान, इस भाग को संसाधित करने की आवश्यकता नहीं है;\n",
    "- रिलेटिव_की: की_लेयर को प्रोसेस करें और इसकी तुलना पोजिशनल_एम्बेड से करें और यहां की-मैट्रिसेस को की-संबंधित पोजिशन एन्कोडिंग के रूप में गुणा किया जाता है;\n",
    "- सापेक्ष_कुंजी_क्वेरy: कुंजी और मान दोनों को स्थिति एन्कोडिंग के रूप में गुणा करें।\n",
    "\n",
    "सामान्य ध्यान प्रक्रिया पर वापस जाएँ:\n",
    "```\n",
    "#...\n",
    "ध्यान_स्कोर = ध्यान_स्कोर / गणित.वर्ग(स्वयं.ध्यान_शीर्ष_आकार)\n",
    "यदि ध्यान_मास्क कोई नहीं है:\n",
    "# अटेंशन मास्क लागू करें (बर्टमॉडल फॉरवर्ड() फ़ंक्शन में सभी परतों के लिए पूर्व-गणना की गई)\n",
    "ध्यान_स्कोर = ध्यान_स्कोर + ध्यान_मास्क # यहाँ * के स्थान पर + क्यों है?\n",
    "\n",
    "# संभावनाओं पर ध्यान स्कोर को सामान्य करें।\n",
    "ध्यान_प्रोब्स =एनएन.सॉफ्टमैक्स(मंद=-1)(ध्यान_स्कोर)\n",
    "\n",
    "#मुझे यहवास्तव में इसमें भाग लेने के लिए पूरे टोकन छोड़े जा रहे हैं, जो हो सकता है\n",
    "# थोड़ा असामान्य लगता है, लेकिन मूल ट्रांसफार्मर पेपर से लिया गया है।\n",
    "ध्यान_समस्याएँ = स्व.ड्रॉपआउट(ध्यान_समस्याएँ)\n",
    "\n",
    "#चाहें तो सिर पर मास्क लगाएं\n",
    "यदि हेड_मास्क कोई नहीं है:\n",
    "ध्यान_प्रॉब्स = ध्यान_प्रॉब्स * हेड_मास्क\n",
    "\n",
    "सन्दर्भ_परत = मशाल.मैटमूल(ध्यान_प्रोब्स, मूल्य_परत)\n",
    "\n",
    "context_layer = context_layer.permute(0,2, 1, 3).contiguous()\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "सन्दर्भ_लेयर = सीontext_layer.view(*new_context_layer_shape)\n",
    "आउटपुट = (संदर्भ_लेयर, ध्यान_प्रॉब्स) यदि आउटपुट_अटेंशन अन्य (संदर्भ_लेयर,)\n",
    "# डिकोडर रिटर्न वैल्यू भाग को छोड़ दें...\n",
    "वापसी आउटपुट\n",
    "```\n",
    "\n",
    "प्रमुख प्रश्न: अटेंशन_स्कोर्स = अटेंशन_स्कोर्स + अटेंशन_मास्क यहां क्या कर रहा है? क्या इसे मास्क से गुणा नहीं किया जाना चाहिए?\n",
    "- क्योंकि यहां अटेंशन_मास्क के साथ [छेड़छाड़] की गई है, जो भाग मूल रूप से 1 था वह 0 हो जाता है, और जो भाग मूल रूप से 0 था (अर्थात पैडिंग) एक बड़ी ऋणात्मक संख्या बन जाता है,इसलिए जोड़ने से बड़ा नकारात्मक मान प्राप्त होता है:\n",
    "- बड़ी ऋणात्मक संख्या का उपयोग क्यों करें? क्योंकि सॉफ्टमैक्स ऑपरेशन के बाद, यह आइटम 0 के करीब दशमलव बन जाएगा।\n",
    "\n",
    "```\n",
    "(पीडीबी) ध्यान_मास्क\n",
    "टेंसर([[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],\n",
    "[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],\n",
    "[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],\n",
    "...,\n",
    "[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],\n",
    "[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],\n",
    "[[[ -0., -0.,-0., ...,-10000., -10000., -10000.]]]],\n",
    "डिवाइस='क्यूडा:0')\n",
    "```\n",
    "\n",
    "तो, यह चरण कहाँ क्रियान्वित किया गया है?\n",
    "मुझे modeling_bert.py में उत्तर नहीं मिला, लेकिन मुझे modeling_utils.py में एक विशेष वर्ग मिला: class ModuleUtilsMixin, और इसकी get_extensed_attention_mask विधि में एक सुराग मिला:\n",
    "\n",
    "```\n",
    "डीईएफ़ गेट_एक्सटेंडेड_अटेंशन_मास्क(स्वयं, अटेंशन_मास्क: टेंसर, इनपुट_शेप: ट्यूपल[इंट], डिवाइस: डिवाइस) -> टेंसर:\n",
    "\"\"\"\n",
    "प्रसारण योग्य ध्यान और कारणात्मक मुखौटे बनाता है ताकि भविष्य और नकाबपोश टोकन को नजरअंदाज कर दिया जाए।\n",
    "\n",
    "तर्क:\n",
    "दस मेंtion_mask (:obj:`torch.Tensor`):\n",
    "ध्यान देने के लिए टोकन का संकेत देने वाले मास्क, अनदेखा करने के लिए टोकन के लिए शून्य।\n",
    "इनपुट_शेप (:obj:`Tuple[int]`):\n",
    "मॉडल में इनपुट का आकार.\n",
    "डिवाइस: (:obj:`torch.device`):\n",
    "मॉडल में इनपुट का उपकरण.\n",
    "\n",
    "रिटर्न:\n",
    ":obj:`torch.Tensor` विस्तारित ध्यान मास्क, :obj:`attention_mask.dtype` के समान dtype के साथ।\n",
    "\"\"\"\n",
    "#भाग छोड़ें...\n",
    "\n",
    "# चूँकि अटेंशन_मास्क उन पदों के लिए 1.0 है जिनमें हम भाग लेना चाहते हैं और उनके लिए 0.0 है\n",
    "#नकाबपोश स्थिति, यह ऑपरेशन बनाएगाएक टेंसर जो 0.0 के लिए है\n",
    "# वे पद जिनमें हम भाग लेना चाहते हैं और नकाबपोश पदों के लिए -10000.0।\n",
    "# चूंकि हम इसे सॉफ्टमैक्स से पहले कच्चे स्कोर में जोड़ रहे हैं, यह है\n",
    "# प्रभावी रूप से इन्हें पूरी तरह हटाने जैसा ही है।\n",
    "एक्सटेंडेड_अटेंशन_मास्क = एक्सटेंडेड_अटेंशन_मास्क.टू(dtype=self.dtype) # fp16 अनुकूलता\n",
    "विस्तारित_ध्यान_मास्क = (1.0 - विस्तारित_ध्यान_मास्क) * -10000.0\n",
    "विस्तारित_अटेंशन_मास्क लौटाएं\n",
    "```\n",
    "\n",
    "तो, इस फ़ंक्शन को कब कहा जाता है? बर्टमॉडल के साथ इसका क्या संबंध है?\n",
    "\n",
    "ठीक है, ये रहे टीवह 'बर्टमॉडल' का वंशानुक्रम विवरण: 'बर्टमॉडल' 'बर्टप्रीट्रेन्डमॉडल' से विरासत में मिला है, जो 'प्रीट्रेन्डमॉडल' से विरासत में मिला है, और 'प्रीट्रेन्डमॉडल' तीन आधार वर्गों [एनएन.मॉड्यूल, मॉड्यूलयूटिल्समिक्सिन, जेनरेशनमिक्सिन] से विरासत में मिला है - क्या जटिल एनकैप्सुलेशन है!\n",
    "\n",
    "इसका मतलब यह है कि बर्टमॉडल ने किसी मध्यवर्ती चरण में मूल ध्यान_मास्क पर get_extensed_attention_mask को कॉल किया होगा, जिससे ध्यान_मास्क मूल [1, 0] से [0, -1e4] में बदल गया होगा।\n",
    "\n",
    "आख़िरकार, यह कॉल मिल गईबर्टमॉडल की आगे की प्रसार प्रक्रिया (पंक्ति 944):\n",
    "\n",
    "```\n",
    "# हम आयामों का एक आत्म-ध्यान मास्क प्रदान कर सकते हैं [बैच_आकार, from_seq_length, to_seq_length]\n",
    "#स्वयं जिस स्थिति में हमें इसे सभी प्रमुखों के लिए प्रसारित करने योग्य बनाने की आवश्यकता है।\n",
    "एक्सटेंडेड_अटेंशन_मास्क: टॉर्च.टेन्सर = सेल्फ.गेट_एक्सटेंडेड_अटेंशन_मास्क(अटेंशन_मास्क, इनपुट_शेप, डिवाइस)\n",
    "\n",
    "```\n",
    "समस्या हल हो गई: यह विधि न केवल मुखौटे के मूल्य को बदलती है, बल्कि इसे एक ऐसे आकार में भी प्रसारित करती है जिसे सीधे ध्यान में जोड़ा जा सकता हैनक्शा।\n",
    "तुम तुम्हारे योग्य हो, हगिंगफेस।\n",
    "\n",
    "इसके अलावा, ध्यान देने योग्य विवरण हैं:\n",
    "\n",
    "- प्रत्येक शीर्ष के आयाम के अनुसार स्केलिंग, बर्ट-बेस के लिए यह 64 का वर्गमूल है, यानी 8;\n",
    "- ध्यान_प्रॉब्स न केवल सॉफ्टमैक्स करता है, बल्कि एक बार ड्रॉपआउट का भी उपयोग करता है क्योंकि यह चिंतित है कि ध्यान मैट्रिक्स बहुत सघन है... यहां यह भी उल्लेख किया गया है कि यह बहुत असामान्य है, लेकिन मूल ट्रांसफार्मर पेपर ऐसा करता है;\n",
    "- हेड_मास्क पहले बताए गए मल्टी-हेड गणना के लिए मास्क हैयदि यह सेट नहीं है, तो यह डिफ़ॉल्ट रूप से 1 होगा और यहां काम नहीं करेगा;\n",
    "- context_layer ध्यान मैट्रिक्स और मान मैट्रिक्स का उत्पाद है: मूल आकार है: (बैच_आकार, num_attention_heads, अनुक्रम_लंबाई, ध्यान_head_size);\n",
    "- context_layer को स्थानांतरित करने और देखने के बाद, आकार को (बैच_आकार, अनुक्रम_लंबाई, छुपे_आकार) में पुनर्स्थापित किया जाता है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "# यदि इसे क्रॉस-अटेंशन मॉड्यूल के रूप में त्वरित किया जाता है, तो कुंजियाँ\n",
    "# और मान एक एनकोडर से आते हैं; ध्यान मास्क होना चाहिए\n",
    "# ऐसा कि एनकोडर के पैडिंग टोकन पर ध्यान नहीं दिया जाता है।\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "# k,v, क्रॉस_अटेंशन का पुन: उपयोग करें\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        if self.is_decoder:\n",
    "# यदि क्रॉस_अटेंशन सभी क्रॉस अटेंशन कुंजी/वैल्यू_स्टेट्स के टुपल (टॉर्च.टेन्सर, टॉर्च.टेन्सर) को सेव करें।\n",
    "# क्रॉस_अटेंशन लेयर पर आगे कॉल करने पर सभी क्रॉस-अटेंशन का पुन: उपयोग किया जा सकता है\n",
    "# key/value_states (पहला \"यदि\" केस)\n",
    "# यदि यूनिडायरेक्शनल सेल्फ-अटेंशन (डिकोडर) टुपल (टॉर्च.टेन्सर, टॉर्च.टेन्सर) को सेव करता है\n",
    "# सभी पिछले डिकोडर कुंजी/वैल्यू_स्टेट्स। एकदिशात्मक आत्म-ध्यान के लिए आगे की कॉल\n",
    "# पिछले डिकोडर कुंजी/वैल्यू_स्टेट्स को वर्तमान अनुमानित कुंजी/वैल्यू_स्टेट्स (तीसरा \"एलिफ़\" केस) से जोड़ सकता है\n",
    "# यदि एनकोडर द्वि-दिशात्मक आत्म-ध्यान `past_key_value` हमेशा `None` है\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "# मूल ध्यान स्कोर प्राप्त करने के लिए \"क्वेरी\" और \"कुंजी\" के बीच डॉट उत्पाद लें।\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "# अटेंशन मास्क लागू करें (बर्टमॉडल फॉरवर्ड() फ़ंक्शन में सभी परतों के लिए पूर्व-गणना की गई)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "# संभावनाओं पर ध्यान स्कोर को सामान्य करें।\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "# यह वास्तव में प्रतीक्षा करने के लिए संपूर्ण टोकन खो रहा है, जो हो सकता है\n",
    "# थोड़ा असामान्य लगता है, लेकिन मूल ट्रांसफार्मर पेपर से लिया गया है।\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "#चाहें तो सिर पर मास्क लगाएं\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### 2.2.1.1.2 बर्टसेल्फआउटपुट\n",
    "```\n",
    "क्लास BertSelfOutput(nn.Module):\n",
    "def __init__(स्वयं, कॉन्फिग):\n",
    "सुपर().__init__()\n",
    "self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)\n",
    "\n",
    "डीईएफ़ फ़ॉरवर्ड(स्वयं, छुपे_स्टेट्स, इनपुट_टेंसर):\n",
    "hidden_states = self.dense(hidden_states)\n",
    "छुपे हुए राज्य = स्व.ड्रॉपआउट(छिपे हुए राज्य)टेस)\n",
    "hidden_states = self.LayerNorm(hidden_states +इनपुट_टेंसर)\n",
    "छुपे हुए_स्टेट्स वापस करें\n",
    "```\n",
    "\n",
    "**यहां हमारे पास लेयरनॉर्म और ड्रॉपआउट का संयोजन है, लेकिन यहां हम पहले ड्रॉपआउट का उपयोग करते हैं, फिर अवशिष्ट कनेक्शन करते हैं और फिर लेयरनॉर्म। जहां तक ​​​​हमें अवशिष्ट कनेक्शन करने की आवश्यकता है, तो इसका सबसे सीधा उद्देश्य प्रशिक्षण के कारण होने वाली कठिनाई को कम करना है बहुत गहरी नेटवर्क परतें, और मूल इनपुट के प्रति अधिक संवेदनशील होना~**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### 2.2.1.2 बर्टइंटरमीडिएट\n",
    "\n",
    "बर्टअटेंशन को पढ़ने के बाद, अटेंशन के बाद एक पूरी तरह से कनेक्टेड + सक्रिय ऑपरेशन होता है:\n",
    "```\n",
    "क्लास बर्टइंटरमीडिएट(एनएन.मॉड्यूल):\n",
    "def __init__(स्वयं, कॉन्फिग):\n",
    "सुपर().__init__()\n",
    "self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "यदि isinstance(config.hidden_act, str):\n",
    "self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "अन्य:\n",
    "self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):\n",
    "hidden_states = self.dense(hidden_stखाया)\n",
    "hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "छुपे हुए_स्टेट्स वापस करें\n",
    "```\n",
    "\n",
    "- यहां पूरा कनेक्शन विस्तारित है, उदाहरण के तौर पर बर्ट-बेस को लेते हुए, विस्तारित आयाम 3072 है, जो 768 के मूल आयाम से 4 गुना है;\n",
    "- यहां सक्रियण फ़ंक्शन को डिफ़ॉल्ट रूप से गेलु (गाऊसियन एरर लाइनर यूनिट्स (जीईएलयूएस)) के रूप में कार्यान्वित किया जाता है, बेशक, इसकी गणना सीधे नहीं की जा सकती है, लेकिन टैन (छोड़े गए) वाले एक अभिव्यक्ति द्वारा इसका अनुमान लगाया जा सकता है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### 2.2.1.3 बर्टआउटपुट\n",
    "\n",
    "यहां एक और पूर्ण कनेक्शन + ड्रॉपआउट + लेयरनॉर्म और एक अवशिष्ट कनेक्शन अवशिष्ट कनेक्ट है:\n",
    "```\n",
    "क्लास बर्टआउटपुट(एनएन.मॉड्यूल):\n",
    "def __init__(स्वयं, कॉन्फिग):\n",
    "सुपर().__init__()\n",
    "self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)\n",
    "\n",
    "डीईएफ़ फ़ॉरवर्ड(स्वयं, छुपे_स्टेट्स, इनपुट_टेंसर):\n",
    "hidden_states = self.dense(hidden_states)छुपे हुए राज्य = स्व.ड्रॉपआउट(छिपे हुए राज्य)\n",
    "hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "छुपे हुए_स्टेट्स वापस करें\n",
    "```\n",
    "\n",
    "यहां ऑपरेशन BertSelfOutput से असंबंधित नहीं है, लेकिन यह बिल्कुल वैसा ही है... दो घटक जिन्हें भ्रमित करना बहुत आसान है।\n",
    "निम्नलिखित सामग्री में BERT-आधारित एप्लिकेशन मॉडल, साथ ही BERT-संबंधित ऑप्टिमाइज़र और उपयोग भी शामिल हैं, जिन्हें अगले लेख में विस्तार से पेश किया जाएगा।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.2.3 बर्टपूलर\n",
    "यह परत बस वाक्य का पहला टोकन लेती है, `[सीएलएस]` से संबंधित वेक्टर, और फिर इसे पूरी तरह से कनेक्टेड परत और एक सक्रियण फ़ंक्शन के माध्यम से आउटपुट में पास करती है: (यह हिस्सा वैकल्पिक है क्योंकि पूलिंग में कई अलग-अलग ऑपरेशन होते हैं)\n",
    "\n",
    "```\n",
    "क्लास बर्टपूलर (एनएन.मॉड्यूल):\n",
    "def __init__(स्वयं, कॉन्फिग):\n",
    "सुपर().__init__()\n",
    "self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "स्व.सक्रियण = nn.Tanh()\n",
    "\n",
    "डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):\n",
    "# हम \"पूल\" वेंई मॉडल केवल छिपी हुई स्थिति को संगत बनाकर\n",
    "# पहले टोकन के लिए.\n",
    "फर्स्ट_टोकन_टेंसर = छुपे हुए_स्टेट्स[:, 0]\n",
    "पूल्ड_आउटपुट = self.dense(first_token_tensor)\n",
    "पूल्ड_आउटपुट = स्व.सक्रियण(पूलेड_आउटपुट)\n",
    "पूल्ड_आउटपुट लौटाएँ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to bert pooler size: 768\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "# हम केवल छिपी हुई स्थिति को लेकर मॉडल को \"पूल\" करते हैं\n",
    "# पहले टोकन के लिए.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "from transformers.models.bert.configuration_bert import *\n",
    "import torch\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "bert_pooler = BertPooler(config=config)\n",
    "print(\"input to bert pooler size: {}\".format(config.hidden_size))\n",
    "batch_size = 1\n",
    "seq_len = 2\n",
    "hidden_size = 768\n",
    "x = torch.rand(batch_size, seq_len, hidden_size)\n",
    "y = bert_pooler(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## सारांश\n",
    "यह अनुभाग बर्ट मॉडल के कार्यान्वयन का विश्लेषण और अध्ययन करता है, मुझे आशा है कि पाठकों को बर्ट कार्यान्वयन की अधिक विस्तृत समझ हो सकती है।\n",
    "\n",
    "यह ध्यान देने योग्य है कि हगिंगफेस द्वारा कार्यान्वित बर्ट मॉडल में, विभिन्न प्रकार की मेमोरी-सेविंग तकनीकों का उपयोग किया जाता है:\n",
    "\n",
    "- ग्रेडिएंट चेकपॉइंट, जो आगे के प्रसार नोड्स को बरकरार नहीं रखता है और केवल जरूरत पड़ने पर ही उनकी गणना करता है; apply_chunking_to_forward, जो कई छोटे बैचों के अनुसार एफएफएन भाग की गणना करता हैघ निम्न आयाम\n",
    "- बर्टमॉडल में जटिल एनकैप्सुलेशन और कई घटक शामिल हैं, उदाहरण के तौर पर बर्ट-बेस को लेते हुए, मुख्य घटक इस प्रकार हैं:\n",
    "- ड्रॉपआउट की कुल संख्या 1+(1+1+1)x12=37 बार दिखाई देती है;\n",
    "- कुल लेयरनॉर्म 1+(1+1)x12=25 बार प्रकट होता है;\n",
    "BertModel में बहुत बड़ी संख्या में पैरामीटर हैं। उदाहरण के तौर पर bert-base को लेते हुए, इसका पैरामीटर वॉल्यूम 109M है।\n",
    "\n",
    "## आभार\n",
    "यह लेख मुख्य रूप से झेजियांग विश्वविद्यालय के ली लुओकिउ द्वारा लिखा गया था, और इस परियोजना के छात्र उत्तरदायी थेछँटाई और संक्षेपण के लिए सक्षम।"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bfce0b4c492a35815b5705a19fe374a7eea0baaa08b34d90450caf1fe9ce20b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
