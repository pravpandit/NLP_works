{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## प्रस्तावना\n",
    "पिछले अनुभाग के बाद, हमने हगिंगफेस ओपन सोर्स कोड लाइब्रेरी में बर्ट मॉडल का गहन अध्ययन किया। इस अनुभाग में, हम विस्तार से बताएंगे कि BERT कैसे लागू करें।\n",
    "\n",
    "इसमें शामिल ज्यूपिटर को [कोड लाइब्रेरी: अध्याय 3-एक ट्रांसफार्मर मॉडल लिखें: बीईआरटी, डाउनलोड] (https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7) में पाया जा सकता है। %AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AAट्रांसफॉर्मर%E6%A8%A1%E5%9E%8B%EF% बीसी%9एबर्ट)\n",
    "\n",
    "यह लेख बीट्रांसफॉर्मर्स संस्करण 4.4.2 (19 मार्च, 2021 को जारी) परियोजना के पाइटोरच संस्करण के बीईआरटी-संबंधित कोड पर आधारित, और निम्नलिखित सहित कोड संरचना, विशिष्ट कार्यान्वयन और सिद्धांत और उपयोग के परिप्रेक्ष्य से इसका विश्लेषण करें:\n",
    "\n",
    "3. BERT-आधारित मॉडल अनुप्रयोग मॉडल\n",
    "4. BERT प्रशिक्षण और अनुकूलन\n",
    "5. बर्ट एनएलपी कार्यों को हल करता है\n",
    "- बर्टफॉरसीक्वेंसक्लासिफिकेशन\n",
    "-बर्टफॉरमल्टीचॉइस\n",
    "- बर्टफॉरटोकनक्लासिफिकेशन\n",
    "- बर्टफॉरक्वेश्चनआंसरिंग\n",
    "6. BERT प्रशिक्षण और अनुकूलन\n",
    "7.प्रई-प्रशिक्षण\n",
    "-फ़ाइन ट्यूनिंग\n",
    "-एडमडब्ल्यू\n",
    "- जोश में आना\n",
    "\n",
    "## 3-बीईआरटी-आधारित मॉडल\n",
    "BERT-आधारित मॉडल /models/bert/modeling_bert.py में लिखे गए हैं, जिनमें BERT पूर्व-प्रशिक्षण मॉडल और BERT वर्गीकरण मॉडल शामिल हैं।\n",
    "\n",
    "सबसे पहले, निम्नलिखित सभी मॉडल अमूर्त बेस क्लास 'बर्टप्रीट्रेन्डमॉडल' पर आधारित हैं, जबकि बाद वाला एक बड़े बेस क्लास 'प्रीट्रेन्डमॉडल' पर आधारित है। यहां हम 'बर्टप्रीट्रेन्डमॉडल' के कार्यों पर ध्यान केंद्रित करते हैं।\n",
    "\n",
    "इसका उपयोग मॉडल वज़न आरंभ करने और कुछ टैग पहचान बनाए रखने के लिए किया जाता हैमॉडल लोड करते समय `PreTrainedModel` या क्लास वेरिएबल से प्राप्त किया गया।\n",
    "नीचे, हम पूर्व-प्रशिक्षित मॉडल विश्लेषण से शुरुआत करते हैं।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.1 बर्टफॉरप्रीट्रेनिंग\n",
    "\n",
    "जैसा कि हम सभी जानते हैं, BERT पूर्व-प्रशिक्षण कार्यों में दो शामिल हैं:\n",
    "\n",
    "- मास्क्ड लैंग्वेज मॉडल (एमएलएम): एक वाक्य में बेतरतीब ढंग से कुछ शब्दों को `[MASK]` से बदलें, फिर प्रत्येक शब्द की जानकारी को एन्कोड करने के लिए वाक्य को BERT में पास करें, और अंत में भविष्यवाणी करने के लिए `[MASK]` की एन्कोडेड जानकारी का उपयोग करें उस स्थिति में सही शब्द। इस कार्य का उद्देश्य संदर्भ के आधार पर शब्दों के अर्थ को समझने के लिए मॉडल को प्रशिक्षित करना है;\n",
    "\n",
    "- अगला वाक्य पूर्वानुमान (एनएसपी): इनपुट वाक्य जोड़े ए और बी पूर्णांकओ BERT, और `[CLS]` की एन्कोडेड जानकारी का उपयोग यह अनुमान लगाने के लिए करें कि क्या B, A का अगला वाक्य है। इस कार्य का उद्देश्य पूर्वानुमानित वाक्यों के बीच संबंध को समझने के लिए मॉडल को प्रशिक्षित करना है।\n",
    "\n",
    "![फिगर बर्ट प्री-ट्रेनिंग](./pictures/3-3-bert-lm.png) फिगर बर्ट प्री-ट्रेनिंग\n",
    "\n",
    "कोड के अनुरूप, दो कार्यों को संयोजित करने वाला मॉडल BertForPreTraining है, जिसमें दो घटक होते हैं:\n",
    "```\n",
    "क्लास बर्टफॉरप्रीट्रेनिंग(बर्टप्रीट्रेन्डमॉडल):\n",
    "def __init__(स्वयं, कॉन्फिग):\n",
    "सुपर().__init__(configfig)\n",
    "\n",
    "self.bert = BertModel(config)\n",
    "self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "self.init_weights()\n",
    "#...\n",
    "```\n",
    "यहां BertModel को पिछले अध्याय में विस्तार से पेश किया गया है (ध्यान दें कि डिफ़ॉल्ट `add_pooling_layer=True` यहां सेट है, यानी, `[CLS]` के अनुरूप आउटपुट NSP कार्य के लिए निकाला जाएगा), और `BertPreTrainingHeads 'भविष्यवाणी मॉड्यूल दो कार्यों के लिए जिम्मेदार है:\n",
    "```\n",
    "क्लास बर्टप्रीट्रेनिंगहेड्स(एनएन.मॉड्यूल):\n",
    "def __init__(स्वयं, कॉन्फिग):\n",
    "सुपर().__init__()\n",
    "स्व.प्रसंस्करण = BertLMPredictionHead(config)\n",
    "self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "डीईएफ़ फ़ॉरवर्ड(स्वयं, अनुक्रम_आउटपुट, पूल्ड_आउटपुट):\n",
    "भविष्यवाणी_स्कोर = स्व.भविष्यवाणियां(अनुक्रम_आउटपुट)\n",
    "seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "वापसी पूर्वानुमान_स्कोर, seq_relationship_score\n",
    "```\n",
    "एनकैप्सुलेशन की एक और परत: `BertPreTrainingHeads`` BertLMPredictionHead` को एनकैप्सुलेट करती है और NSP कार्य का प्रतिनिधित्व करने वाली एक रैखिक परत यहां, NSP के अनुरूप कार्य नहीं है`BertXXXPredictionHead` में संपुटित।\n",
    "\n",
    "**दरअसल, यह क्लास इनकैप्सुलेटेड है, लेकिन इसे BertOnlyNSPHead कहा जाता है, जिसका उपयोग यहां नहीं किया गया है**\n",
    "\n",
    "`बर्टप्रीट्रेनिंगहेड्स` का अन्वेषण जारी रखें:\n",
    "```\n",
    "क्लास BertLMPredictionHead(nn.मॉड्यूल):\n",
    "def __init__(self, config):super().__init__()\n",
    "self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "# आउटपुट भार इनपुट एम्बेडिंग के समान हैं, लेकिन हैं\n",
    "# प्रत्येक टोकन के लिए केवल-आउटपुट पूर्वाग्रह।\n",
    "self.decoder = nn.Linear(config.hidden_size, config.vocab_sआकार, पूर्वाग्रह = गलत)\n",
    "\n",
    "self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "# दो वेरिएबल्स के बीच एक लिंक की आवश्यकता है ताकि पूर्वाग्रह को 'resize_token_embeddings' के साथ सही ढंग से आकार दिया जा सके\n",
    "self.decoder.bias = self.bias\n",
    "\n",
    "डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):\n",
    "hidden_states = self.transform(hidden_states)\n",
    "hidden_states = self.decoder(hidden_states)\n",
    "छुपे हुए_स्टेट्स वापस करें\n",
    "```\n",
    "\n",
    "इस वर्ग का उपयोग एक श्रेणी के रूप में प्रत्येक शब्द के वर्गीकरण आउटपुट के रूप में `[MASK]` स्थिति के आउटपुट की भविष्यवाणी करने के लिए किया जाता है।\n",
    "\n",
    "-वांक्या वर्ग भविष्यवाणी भार के पूर्वाग्रह के रूप में पूर्ण 0 वेक्टर को पुन: प्रारंभ करता है;\n",
    "\n",
    "- इस वर्ग का आउटपुट आकार [बैच_आकार, seq_length, vocab_size] है, अर्थात, प्रत्येक वाक्य में प्रत्येक शब्द की प्रत्येक श्रेणी की संभाव्यता मान की भविष्यवाणी करना (ध्यान दें कि सॉफ्टमैक्स यहां नहीं किया गया है);\n",
    "- एक अन्य संक्षिप्त वर्ग: BertPredictionHeadTransform, जिसका उपयोग कुछ रैखिक परिवर्तनों को पूरा करने के लिए किया जाता है:\n",
    "```\n",
    "क्लास BertPredictionHeadTransform(nn.Module):\n",
    "def __init__(स्वयं, कॉन्फिग):\n",
    "सुपर().__init__()\n",
    "स्व.घना = एनn.रैखिक(config.hidden_size, config.hidden_size)\n",
    "यदि isinstance(config.hidden_act, str):\n",
    "self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "अन्य:\n",
    "self.transform_act_fn = config.hidden_act\n",
    "self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):\n",
    "hidden_states = self.dense(hidden_states)\n",
    "hidden_states = self.transform_act_fn(hidden_states)hidden_states = self.LayerNorm(hidden_states)\n",
    "छुपे हुए_स्टेट्स वापस करें\n",
    "```\n",
    "\n",
    "`बर्टफॉरप्रीट्रेनिंग` पर वापस जाएं औरयह देखना जारी रखें कि दो `नुकसान` ब्लॉकों को कैसे संसाधित किया जाता है, इसका आगे का प्रसार बर्टमॉडल से अलग है, दो और इनपुट, `लेबल` और `नेक्स्ट_सेन्टेंस_लेबल` के साथ:\n",
    "\n",
    "- लेबल: आकार [बैच_आकार, seq_length] है, जो एमएलएम कार्य के लेबल का प्रतिनिधित्व करता है। ध्यान दें कि यहां, मूल अनमास्क किए गए शब्दों को -100 पर सेट किया गया है, और नकाबपोश शब्दों में उनकी संबंधित आईडी होंगी, जो इसके विपरीत है। कार्य सेटिंग.\n",
    "\n",
    "- उदाहरण के लिए, मूल वाक्य है मैं एक सेब को [मास्क] करना चाहता हूं, और वहपुनः मैं ईट शब्द को मास्क करता हूं और मॉडल को इनपुट करता हूं, और संबंधित लेबल को [-100, -100, -100, [ईट संबंधित आईडी], -100, -100] पर सेट किया जाता है;\n",
    "\n",
    "- इसे अन्य संख्याओं के बजाय -100 पर क्यों सेट किया गया है? क्योंकि torch.nn.CrossEntropyLoss डिफ़ॉल्ट रूप सेign_index=-100 है, जिसका अर्थ है कि लेबल 100 के साथ श्रेणी के इनपुट की गणना नहीं की जाएगी।\n",
    "\n",
    "- अगला_सेन्टेंस_लेबल: यह इनपुट बहुत सरल है, यह 0 और 1 का बाइनरी लेबल है।\n",
    "\n",
    "```\n",
    "#...\n",
    "def आगे(\n",
    "खुद,\n",
    "इनपुट_आईडी=कोई नहीं,\n",
    "ध्यान_मास्क=कोई नहीं,\n",
    "टोकन_टीpe_ids=कोई नहीं,\n",
    "स्थिति_आईडी=कोई नहीं,\n",
    "हेड_मास्क=कोई नहीं,\n",
    "इनपुट्स_एम्बेड्स=कोई नहीं,\n",
    "लेबल=कोई नहीं,\n",
    "अगला_वाक्य_लेबल=कोई नहीं,\n",
    "आउटपुट_ध्यान=कोई नहीं,\n",
    "आउटपुट_हिडन_स्टेट्स=कोई नहीं,\n",
    "return_dict=कोई नहीं,\n",
    "): ...\n",
    "```\n",
    "\n",
    "हानि के अगले दो भागों का संयोजन:\n",
    "```\n",
    "#...\n",
    "कुल हानि = कोई नहीं\n",
    "यदि लेबल कोई नहीं है और अगला_सेन्टेंस_लेबल कोई नहीं है:\n",
    "हानि_एफसीटी = क्रॉसएंट्रॉपीलॉस()\n",
    "नकाबपोश_एलएम_लॉस = लॉस_एफसीटी(भविष्यवाणी_स्कोर्स.व्यू(-1, सेल्फ.कॉन्फिग.वोकैब_साइज), लेबल्स.व्यू(-1))\n",
    "अगला_वाक्य_हानि = हानि_एफसीटी(seq_relationship_score.देखें(-1, 2), अगला_सेन्टेंस_लेबल.व्यू(-1))\n",
    "कुल_नुकसान = नकाबपोश_एलएम_नुकसान + अगला_वाक्य_नुकसान\n",
    "#...\n",
    "```\n",
    "\n",
    "सीधा जोड़, यह बहुत सरल रणनीति है।\n",
    "बेशक, इस कोड में केवल एक ही लक्ष्य के लिए पूर्व-प्रशिक्षण के लिए BERT मॉडल भी शामिल है (विशिष्ट विवरण विस्तारित नहीं हैं):\n",
    "- BertForMaskedLM: एमएलएम कार्यों के लिए केवल पूर्व-प्रशिक्षण;\n",
    "- BertOnlyMLMHead पर आधारित, जो BertLMPredictionHead के एनकैप्सुलेशन की एक और परत भी है;\n",
    "- BertLMHeadModel: इसमें और पिछले वाले के बीच अंतर हैयह मॉडल एक ऐसा संस्करण है जो डिकोडर के रूप में चलता है;\n",
    "- BertOnlyMLMHead पर भी आधारित;\n",
    "- BertForNextSentencePrediction: केवल एनएसपी कार्यों के लिए पूर्व-प्रशिक्षण।\n",
    "- BertOnlyNSPHead पर आधारित, सामग्री एक रैखिक परत है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "_CHECKPOINT_FOR_DOC = \"bert-base-uncased\"\n",
    "_CONFIG_FOR_DOC = \"BertConfig\"\n",
    "_TOKENIZER_FOR_DOC = \"BertTokenizer\"\n",
    "from transformers.models.bert.modeling_bert import *\n",
    "from transformers.models.bert.configuration_bert import *\n",
    "class BertForPreTraining(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        next_sentence_label=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n",
    "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
    "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
    "        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n",
    "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
    "            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n",
    "            - 0 indicates sequence B is a continuation of sequence A,\n",
    "            - 1 indicates sequence B is a random sequence.\n",
    "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
    "            Used to hide legacy arguments that have been deprecated.\n",
    "        Returns:\n",
    "        Example::\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "prediction_logits = outputs.prediction_logits\n",
    "seq_relationship_logits = outputs.seq_relationship_logits\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return BertForPreTrainingOutput(\n",
    "            loss=total_loss,\n",
    "            prediction_logits=prediction_scores,\n",
    "            seq_relationship_logits=seq_relationship_score,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "prediction_logits = outputs.prediction_logits\n",
    "seq_relationship_logits = outputs.seq_relationship_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "@add_start_docstrings(\n",
    "\"\"\"सीएलएम फाइन-ट्यूनिंग के लिए `भाषा मॉडलिंग` के साथ बर्ट मॉडल शीर्ष पर है। \"\"\", BERT_START_DOCSTRING\n",
    ")\n",
    "class BertLMHeadModel(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if not config.is_decoder:\n",
    "            logger.warning(\"If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
    "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
    "            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        Returns:\n",
    "        Example::\n",
    "            from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
    "            import torch\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "            config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "            config.is_decoder = True\n",
    "            model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n",
    "            inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            prediction_logits = outputs.logits\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        if labels is not None:\n",
    "            use_cache = False\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        lm_loss = None\n",
    "        if labels is not None:\n",
    "# हम अगली-टोकन भविष्यवाणी कर रहे हैं; भविष्यवाणी स्कोर और इनपुट आईडी को एक-एक करके बदलें\n",
    "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "# यदि मॉडल को एनकोडर-डिकोडर मॉडल में डिकोडर के रूप में उपयोग किया जाता है, तो डिकोडर ध्यान मास्क तुरंत बनाया जाता है\n",
    "        if attention_mask is None:\n",
    "            attention_mask = input_ids.new_ones(input_shape)\n",
    "\n",
    "# यदि अतीत का उपयोग किया जाता है तो डिकोडर_इनपुट_आईडी काटें\n",
    "        if past is not None:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
    "        return reordered_past\n",
    "\n",
    "from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.is_decoder = True\n",
    "model = BertLMHeadModel.from_pretrained('bert-base-uncased', config=config)\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "prediction_logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 440M/440M [00:30<00:00, 14.5MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class BertForNextSentencePrediction(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertOnlyNSPHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
    "            (see ``input_ids`` docstring). Indices should be in ``[0, 1]``:\n",
    "            - 0 indicates sequence B is a continuation of sequence A,\n",
    "            - 1 indicates sequence B is a random sequence.\n",
    "        Returns:\n",
    "        Example::\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "logits = outputs.logits\n",
    "assert logits[0, 0] < logits[0, 1] # next sentence was random\n",
    "        \"\"\"\n",
    "\n",
    "        if \"next_sentence_label\" in kwargs:\n",
    "            warnings.warn(\n",
    "                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            labels = kwargs.pop(\"next_sentence_label\")\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        seq_relationship_scores = self.cls(pooled_output)\n",
    "\n",
    "        next_sentence_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (seq_relationship_scores,) + outputs[2:]\n",
    "            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n",
    "\n",
    "        return NextSentencePredictorOutput(\n",
    "            loss=next_sentence_loss,\n",
    "            logits=seq_relationship_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "logits = outputs.logits\n",
    "assert logits[0, 0] < logits[0, 1] # next sentence was random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "इसके बाद, हम विभिन्न फाइन-ट्यून मॉडल पेश करेंगे, जो मूल रूप से वर्गीकरण कार्य हैं:\n",
    "\n",
    "![बर्ट:फाइनट्यून](./चित्र/3-4-बर्ट-फीट.पीएनजी) चित्र: बर्ट:फाइनट्यून"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.2 BertForSequenceClassification\n",
    "इस मॉडल का उपयोग वाक्य वर्गीकरण (प्रतिगमन भी हो सकता है) कार्यों के लिए किया जाता है, जैसे GLUE बेंचमार्क के विभिन्न कार्य।\n",
    "- वाक्य वर्गीकरण का इनपुट एक वाक्य (जोड़ी) है, और आउटपुट एक एकल वर्गीकरण लेबल है।\n",
    "\n",
    "संरचना बहुत सरल है, अर्थात्, ड्रॉपआउट के बाद `बर्टमॉडल` (पूलिंग के साथ) वर्गीकरण को आउटपुट करने के लिए एक रैखिक परत के बाद:\n",
    "```\n",
    "वर्ग BertForSequenceClassification(BertPreTrainedModel):\n",
    "def __init__(स्वयं, कॉन्फ़िगरेशन):\n",
    "सुपर().__init__(config)\n",
    "self.num_labels = config.num_labels\n",
    "\n",
    "self.bert = BertModel(config)\n",
    "स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)\n",
    "self.classifier = nn.Linear(config.hidden_size,config.num_labels)\n",
    "\n",
    "self.init_weights()\n",
    "#...\n",
    "```\n",
    "\n",
    "आगे प्रसार के दौरान, उपरोक्त पूर्व-प्रशिक्षित मॉडल की तरह लेबल को पारित करने की आवश्यकता होती है।\n",
    "\n",
    "- यदि आरंभिक num_labels=1 है, तो यह डिफ़ॉल्ट रूप से एक प्रतिगमन कार्य है और MSELoss का उपयोग करता है;\n",
    "\n",
    "- अन्यथा, इसे वर्गीकरण कार्य माना जाता है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n",
    "    output) e.g. for GLUE tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=SequenceClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 596kB/s]\n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 12.4kB/s]\n",
      "Downloading: 100%|██████████| 436k/436k [00:00<00:00, 808kB/s]\n",
      "Downloading: 100%|██████████| 433/433 [00:00<00:00, 166kB/s]\n",
      "Downloading: 100%|██████████| 433M/433M [00:29<00:00, 14.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n",
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# टोकनाइज़र स्वचालित रूप से किसी भी मॉडल विशिष्ट विभाजक (यानी <सीएलएस> और <एसईपी>) और टोकन को अनुक्रम में जोड़ देगा, साथ ही ध्यान मास्क की गणना भी करेगा।\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "#शब्दांश होना चाहिए\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "\n",
    "# व्याख्या नहीं होनी चाहिए\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.3 BertForMultipleChoice\n",
    "\n",
    "इस मॉडल का उपयोग कई विकल्पों के लिए किया जाता है, जैसे कि RocStories/SWAG कार्य।\n",
    "- बहुविकल्पीय कार्य का इनपुट बैचों में इनपुट वाक्यों का एक समूह है, और आउटपुट एक निश्चित वाक्य का चयन करने के लिए एक एकल लेबल है।\n",
    "संरचना वाक्य वर्गीकरण के समान है, सिवाय इसके कि रैखिक परत का आउटपुट आयाम 1 है, अर्थात, हर बार प्रत्येक नमूने के कई वाक्यों के आउटपुट को प्रत्येक नमूने के पूर्वानुमान स्कोर के रूप में संयोजित करने की आवश्यकता होती है।- वास्तव में, विशिष्ट ऑपरेशन प्रत्येक बैच के कई वाक्यों को एक साथ रखना है, इसलिए एक समय में संसाधित इनपुट [बैच_आकार, संख्या_चॉइस] वाक्य है, इसलिए, जब बैच का आकार समान होता है, तो ऐसे कार्यों की तुलना में अधिक वीडियो मेमोरी की आवश्यकता होती है वाक्य वर्गीकरण के रूप में, और प्रशिक्षण के दौरान सावधानी बरतने की आवश्यकता है।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.4 बर्टफॉरटोकनक्लासिफिकेशन\n",
    "इस मॉडल का उपयोग अनुक्रम लेबलिंग (शब्द वर्गीकरण) के लिए किया जाता है, जैसे एनईआर कार्य।\n",
    "- अनुक्रम लेबलिंग कार्य का इनपुट एक एकल वाक्य पाठ है, और आउटपुट प्रत्येक टोकन के अनुरूप श्रेणी लेबल है।\n",
    "चूँकि केवल कुछ के बजाय प्रत्येक टोकन के अनुरूप आउटपुट की आवश्यकता होती है, यहाँ BertModel को पूलिंग परत जोड़ने की आवश्यकता नहीं है;\n",
    "- उसी समय, क्लास पैरामीटर `_keys_to_ignore_on_load_unexpected` को `[r\"pooler\"]` पर सेट किया जाता है, जो कियानी, मॉडल लोड करते समय अनावश्यक भार के लिए कोई त्रुटि रिपोर्ट नहीं की जाएगी।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultipleChoice(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MultipleChoiceModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
    "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
    "            :obj:`input_ids` above)\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
    "\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
    "        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
    "        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
    "        inputs_embeds = (\n",
    "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
    "            if inputs_embeds is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        reshaped_logits = logits.view(-1, num_choices)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(reshaped_logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (reshaped_logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return MultipleChoiceModelOutput(\n",
    "            loss=loss,\n",
    "            logits=reshaped_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
    "    Named-Entity-Recognition (NER) tasks.\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TokenClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "# हानि के केवल सक्रिय भाग ही रखें\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 998/998 [00:00<00:00, 382kB/s]\n",
      "Downloading: 100%|██████████| 1.33G/1.33G [01:30<00:00, 14.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "label_list = [\n",
    "\"O\",       # Outside of a named entity\n",
    "\"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
    "\"I-MISC\",  # Miscellaneous entity\n",
    "\"B-PER\",   # Beginning of a person's name right after another person's name\n",
    "\"I-PER\",   # Person's name\n",
    "\"B-ORG\",   # Beginning of an organisation right after another organisation\n",
    "\"I-ORG\",   # Organisation\n",
    "\"B-LOC\",   # Beginning of a location right after another location\n",
    "\"I-LOC\"    # Location\n",
    "]\n",
    "\n",
    "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\"\n",
    "\n",
    "# विशेष टोकन के साथ टोकन प्राप्त करने के लिए थोड़ा सा हैक\n",
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(inputs).logits\n",
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 'O')\n",
      "('Hu', 'I-ORG')\n",
      "('##gging', 'I-ORG')\n",
      "('Face', 'I-ORG')\n",
      "('Inc', 'I-ORG')\n",
      "('.', 'O')\n",
      "('is', 'O')\n",
      "('a', 'O')\n",
      "('company', 'O')\n",
      "('based', 'O')\n",
      "('in', 'O')\n",
      "('New', 'I-LOC')\n",
      "('York', 'I-LOC')\n",
      "('City', 'I-LOC')\n",
      "('.', 'O')\n",
      "('Its', 'O')\n",
      "('headquarters', 'O')\n",
      "('are', 'O')\n",
      "('in', 'O')\n",
      "('D', 'I-LOC')\n",
      "('##UM', 'I-LOC')\n",
      "('##BO', 'I-LOC')\n",
      "(',', 'O')\n",
      "('therefore', 'O')\n",
      "('very', 'O')\n",
      "('close', 'O')\n",
      "('to', 'O')\n",
      "('the', 'O')\n",
      "('Manhattan', 'I-LOC')\n",
      "('Bridge', 'I-LOC')\n",
      "('.', 'O')\n",
      "('[SEP]', 'O')\n"
     ]
    }
   ],
   "source": [
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.5 BertForQuestionAnswering\n",
    "इस मॉडल का उपयोग प्रश्न-उत्तर कार्यों, जैसे SQuAD कार्यों को हल करने के लिए किया जाता है।\n",
    "- प्रश्न-उत्तर कार्य का इनपुट एक वाक्य युग्म है जिसमें एक प्रश्न + (बीईआरटी के लिए केवल एक) उत्तर होता है, और आउटपुट प्रारंभ स्थिति और अंतिम स्थिति होती है जिसका उपयोग उत्तर में विशिष्ट पाठ को चिह्नित करने के लिए किया जाता है।\n",
    "यहां हमें दो आउटपुट की आवश्यकता है, अर्थात् प्रारंभिक स्थिति की भविष्यवाणी और अंतिम स्थिति की भविष्यवाणी। दोनों आउटपुट की लंबाई वाक्य के समान हैई लंबाई। सबसे बड़े पूर्वानुमान मान के अनुरूप सबस्क्रिप्ट को पूर्वानुमानित स्थिति के रूप में चुना जाता है।\n",
    "- वाक्य की लंबाई से अधिक अवैध लेबल के लिए, उन्हें उचित सीमा तक संपीड़ित (torch.clamp_) किया जाएगा।\n",
    "\n",
    "उपरोक्त BERT स्रोत कोड का परिचय है। निम्नलिखित BERT मॉडल के बारे में कुछ व्यावहारिक प्रशिक्षण विवरण हैं।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
    "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
    "    \"\"\",\n",
    "    BERT_START_DOCSTRING,\n",
    ")\n",
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=QuestionAnsweringModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "# यदि हम मल्टी-जीपीयू पर हैं, तो स्प्लिट एक आयाम जोड़ता है\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "# कभी-कभी प्रारंभ/अंत स्थिति हमारे मॉडल इनपुट के बाहर होती है, हम इन शर्तों को अनदेखा कर देते हैं\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 443/443 [00:00<00:00, 186kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 438kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 845kB/s]\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 10.5kB/s]\n",
      "Downloading: 100%|██████████| 1.34G/1.34G [01:28<00:00, 15.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many pretrained models are available in 🤗 Transformers?\n",
      "Answer: over 32 +\n",
      "Question: What does 🤗 Transformers provide?\n",
      "Answer: general - purpose architectures\n",
      "Question: 🤗 Transformers provides interoperability between which frameworks?\n",
      "Answer: tensorflow 2. 0 and pytorch\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\"\n",
    "\n",
    "questions = [\n",
    "\"How many pretrained models are available in 🤗 Transformers?\",\n",
    "\"What does 🤗 Transformers provide?\",\n",
    "\"🤗 Transformers provides interoperability between which frameworks?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    answer_start = torch.argmax(\n",
    "        answer_start_scores\n",
    "    )  # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## BERT प्रशिक्षण और अनुकूलन\n",
    "### 4.1 पूर्व प्रशिक्षण\n",
    "पूर्व-प्रशिक्षण चरण में, प्रसिद्ध 15% और 80% मुखौटा अनुपात के अलावा, ध्यान देने योग्य एक बात पैरामीटर साझाकरण है।\n",
    "न केवल BERT, बल्कि हगिंगफेस द्वारा कार्यान्वित PLM शब्द एम्बेडिंग और मास्क्ड भाषा मॉडल के सभी पूर्वानुमान भार आरंभीकरण प्रक्रिया के दौरान साझा किए जाते हैं:\n",
    "```\n",
    "क्लास प्रीट्रेंडमॉडल(एनएन.मॉड्यूल, मॉड्यूलयूटिल्समिक्सिन, जेनरेशनमिक्सिन):\n",
    "#...\n",
    "डीईएफ़ टाई_वेट्स(स्वयं):\n",
    "\"\"\"\n",
    "इनपुट एम के बीच वजन बांधेंबेडिंग और आउटपुट एम्बेडिंग।\n",
    "\n",
    "यदि :obj:`torchscript` ध्वज कॉन्फ़िगरेशन में सेट है, तो पैरामीटर साझाकरण को संभाल नहीं सकता है इसलिए हम इसके बजाय वज़न की क्लोनिंग कर रहे हैं।\n",
    "\"\"\"\n",
    "आउटपुट_एम्बेडिंग = self.get_output_embeddings()\n",
    "यदि आउटपुट_एम्बेडिंग कोई नहीं है और self.config.tie_word_embeddings:\n",
    "self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
    "\n",
    "यदि self.config.is_encoder_decoder और self.config.tie_encoder_decoder:\n",
    "यदि hasattr(स्वयं, self.base_model_prefix):\n",
    "स्वयं = गेटएटीआर (स्वयं, एसelf.base_model_prefix)\n",
    "self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)\n",
    "#...\n",
    "```\n",
    "\n",
    "ऐसा क्यों होना चाहिए क्योंकि शब्द_एम्बेडिंग और भविष्यवाणी भार बहुत बड़े हैं। उदाहरण के तौर पर बर्ट-बेस को लेते हुए, इसका आकार (30522, 768) है, जो प्रशिक्षण की कठिनाई को कम करता है।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4.2 फ़ाइन-ट्यूनिंग\n",
    "फ़ाइन-ट्यूनिंग भी डाउनस्ट्रीम कार्य चरण है, और ध्यान देने योग्य दो बिंदु भी हैं।\n",
    "#### 4.2.1 एडमडब्ल्यू\n",
    "सबसे पहले, मैं BERT के ऑप्टिमाइज़र का परिचय देता हूँ: एडमडब्ल्यू (एडमवेटडेकेऑप्टिमाइज़र)।\n",
    "\n",
    "यह ऑप्टिमाइज़र ICLR 2017 के सर्वश्रेष्ठ पेपर से आता है: \"एडम में वज़न क्षय नियमितीकरण को ठीक करना\", जो एडम के वज़न क्षय त्रुटि को ठीक करने के लिए एक नई विधि का प्रस्ताव करता है। पेपर बताता है कि L2 नियमितीकरण और वज़न क्षय ज्यादातर मामलों में समतुल्य नहीं हैं, लेकिन समतुल्य हैं केवल केस मेंएसजीडी अनुकूलन का ई; और अधिकांश ढांचे एडम+एल2 नियमितीकरण के लिए वजन घटाने का उपयोग करते हैं, और दोनों को भ्रमित नहीं किया जा सकता है।\n",
    "\n",
    "एडमडब्ल्यू एडम+एल2 नियमितीकरण पर आधारित एक बेहतर एल्गोरिदम है। सामान्य एडम+एल2 से अंतर इस प्रकार है:\n",
    "\n",
    "![छवि: एडमडब्ल्यू](./चित्र/3-5-एडमडब्ल्यू.पीएनजी) छवि: एडमडब्ल्यू\n",
    "\n",
    "एडमडब्ल्यू के विश्लेषण के लिए कृपया देखें:\n",
    "\n",
    "- एडमडब्ल्यू और सुपर-कन्वर्जेंस अब तंत्रिका जाल को प्रशिक्षित करने का सबसे तेज़ तरीका है [1]\n",
    "- पेपरप्लेनेट: यह 2019 है, अब एडम + एल2 नियमितीकरण का उपयोग न करें [2]\n",
    "\n",
    "आमतौर पर, हमक्षय प्रक्रिया में भाग लेने के लिए मॉडल का वजन भाग चुनें, जबकि दूसरा भाग (लेयरनॉर्म के वजन सहित) भाग नहीं लेता है (कोड का मूल स्रोत हगिंगफेस का उदाहरण होना चाहिए)\n",
    "अतिरिक्त: ऐसा करने के कारण के संबंध में, मुझे अभी तक कोई उचित उत्तर नहीं मिला है।\n",
    "\n",
    "```\n",
    "# मॉडल: एक बर्ट-आधारित-मॉडल ऑब्जेक्ट\n",
    "# सीखने की दर: पाठ वर्गीकरण के लिए डिफ़ॉल्ट 2e-5\n",
    "परम_ऑप्टिमाइज़र = सूची (मॉडल.नामांकित_पैरामीटर())\n",
    "no_decay = ['पूर्वाग्रह', 'LayerNorm.bias', 'LayerNoआरएम.वेट']\n",
    "ऑप्टिमाइज़र_ग्रुपेड_पैरामीटर = [\n",
    "{'पैराम्स': [एन के लिए पी, पैरा_ऑप्टिमाइज़र में पी यदि कोई नहीं है (एन में एन के लिए एन डी में नो_डेके)], 'वेट_डेके': 0.01},\n",
    "{'पैराम्स': [एन के लिए पी, पैरा_ऑप्टिमाइज़र में पी यदि कोई हो(\n",
    "nd में n के लिए nd में no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "ऑप्टिमाइज़र = एडमडब्ल्यू (ऑप्टिमाइज़र_ग्रुप्ड_पैरामीटर,\n",
    "एलआर=लर्निंग_रेट)\n",
    "#...\n",
    "```\n",
    "\n",
    "#### 4.2.2 वार्मअप\n",
    "\n",
    "BERT प्रशिक्षण की एक अन्य विशेषता वार्मअप है, जिसका अर्थ है:\n",
    "\n",
    "प्रशिक्षण की शुरुआत में और धीरे-धीरे कम सीखने की दर (0 से शुरू) का उपयोग करेंमॉडल को स्थानीय इष्टतम में बहुत जल्दी प्रवेश करने और ओवरफिटिंग से बचने के लिए इसे एक निश्चित संख्या में चरणों (जैसे 1000 कदम) के भीतर सामान्य आकार (जैसे ऊपर 2e-5) में क्रीज करें;\n",
    "- बाद के प्रशिक्षण में बड़े पैरामीटर परिवर्तनों से बचने के लिए प्रशिक्षण के बाद के चरण में सीखने की दर को धीरे-धीरे 0 तक कम करें।\n",
    "- हगिंगफेस के कार्यान्वयन में, कई वार्मअप रणनीतियों का उपयोग किया जा सकता है:\n",
    "```\n",
    "TYPE_TO_SCHEDULER_FUNCTION = {\n",
    "शेड्यूलर प्रकार.LINEAR: get_linear_schedule_with_warmup,\n",
    "शेड्यूलर प्रकार.COSINE: get_cosine_schedule_with_warmup,\n",
    "शेड्यूलर प्रकार.COSINE_WITH_RESARTTS: get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "शेड्यूलर प्रकार.बहुपद: get_बहुपद_decay_schedule_with_warmup,\n",
    "शेड्यूलर प्रकार.CONSTANT: get_constant_schedule,\n",
    "शेड्यूलर प्रकार.CONSTANT_WITH_WARMUP: get_constant_schedule_with_warmup,\n",
    "}\n",
    "```\n",
    "विशेष रूप से:\n",
    "- स्थिर: निश्चित सीखने की दर को अपरिवर्तित रखें;\n",
    "- CONSTANT_WITH_WARMUP: प्रत्येक चरण में सीखने की दर को रैखिक रूप से समायोजित करें;\n",
    "- रैखिक: ऊपर उल्लिखित दो-चरणीय समायोजन;\n",
    "-सीओसाइन: दो-चरण समायोजन के समान, लेकिन त्रिकोणमितीय वक्र समायोजन का उपयोग करता है;\n",
    "- COSINE_WITH_RESARTTS: प्रशिक्षण के दौरान COSINE समायोजन को n बार दोहराएं;\n",
    "- बहुपद: एक घातीय वक्र के अनुसार दो-चरणीय समायोजन करें।\n",
    "\n",
    "विशिष्ट उपयोग के लिए, ट्रांसफार्मर/ऑप्टिमाइज़ेशन.py देखें:\n",
    "\n",
    "सबसे अधिक उपयोग किया जाने वाला get_linear_scheduler_with_warmup है, जो सीखने की दर का एक रैखिक दो-चरण समायोजन है।\n",
    "\n",
    "```\n",
    "def get_scheduler(\n",
    "नाम: यूनियन[str, शेड्यूलरटाइप],\n",
    "अनुकूलक: ऑप्टिमिज़ेर,\n",
    "num_warmup_steps: वैकल्पिक[int] = कोई नहीं,\n",
    "num_training_steps: वैकल्पिक[int] = कोई नहीं,\n",
    "): ...\n",
    "\n",
    "```\n",
    "\n",
    "उपरोक्त ट्रांसफार्मर लाइब्रेरी (संस्करण 4.4.2) में बीईआरटी एप्लिकेशन के प्रासंगिक कोड का विशिष्ट कार्यान्वयन विश्लेषण है। पाठकों के साथ चर्चा करने के लिए आपका स्वागत है।## आभार\n",
    "यह लेख मुख्य रूप से झेजियांग विश्वविद्यालय के ली लुओकिउ द्वारा लिखा गया था, और इस परियोजना के छात्र इसे एकत्र करने और सारांशित करने के लिए जिम्मेदार थे।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bfce0b4c492a35815b5705a19fe374a7eea0baaa08b34d90450caf1fe9ce20b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
