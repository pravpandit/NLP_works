## प्रस्तावना
इस लेख में बड़ी मात्रा में स्रोत कोड और स्पष्टीकरण शामिल हैं। प्रत्येक मॉड्यूल को पैराग्राफ और क्षैतिज रेखाओं द्वारा विभाजित किया गया है। साथ ही, वेबसाइट आपको प्रत्येक अनुभाग में तेजी से आगे बढ़ने में मदद करने के लिए साइडबार से सुसज्जित है इसे पढ़ने के बाद BERT का. साथ ही, बर्ट स्रोत कोड को सिंगल-स्टेप डीबग करने, संबंधित मॉड्यूल को डीबग करने और फिर इस अध्याय में स्पष्टीकरण के साथ इसकी तुलना करने के लिए pycharm, vscode और अन्य टूल का उपयोग करने की अनुशंसा की जाती है।

इसमें शामिल ज्यूपिटर को [कोड लाइब्रेरी: अध्याय 3-ट्रांसफॉर्मर मॉडल लिखना: बीईआरटी, डाउनलोड] (https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7) में पाया जा सकता है। % AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AAट्रांसफॉर्मर%E6%A8%A1%E5%9E%8B%EF% बीसी %9एबर्ट)

सीखने के लिए यह अध्याय H[HuggingFace/Transformers, 48.9k Star](https://github.com/huggingface/transformers) पर आधारित होगा। इस अध्याय का सारा कोड [हगिंगफेस बर्ट में है। ध्यान दें कि तीव्र संस्करण अद्यतन के कारण, ऐसा हो सकता हैयदि कोई अंतर है, तो कृपया संस्करण 4.4.2 देखें](https://github.com/huggingface/transformers/tree/master/src/transformers/models/bert) हगिंगफेस एक स्टार्ट-अप चैट रोबोट सेवा प्रदाता है जिसका मुख्यालय कहां है न्यूयॉर्क। मैंने BERT प्रवृत्ति के संकेत को बहुत पहले ही पकड़ लिया था और पाइटोरच पर आधारित BERT मॉडल को लागू करना शुरू कर दिया था। मूल रूप से पाइटोरच-प्रीट्रेन्ड-बर्ट नाम की यह परियोजना इस शक्तिशाली मॉडल पर खेलने और शोध करने के लिए उपयोग में आसान विधि प्रदान करते हुए मूल प्रभाव को दोहराती है।

जैसे-जैसे उपयोगकर्ताओं की संख्या बढ़ी, यह प्रोजेक्ट एक बड़े ओपन सोर्स समुदाय के रूप में विकसित हुआ, जिसमें विभिन्न पूर्व-प्रशिक्षित भाषा मॉडल शामिल किए गए और टेन्सरफ़्लो कार्यान्वयन जोड़ा गया, और 2019 की दूसरी छमाही में इसका नाम बदलकर ट्रांसफॉर्मर कर दिया गया। लेखन के समय (मार्च 30, 2021) तक, इस परियोजना में पहले से ही 43k+ सितारे हैं। यह कहा जा सकता है कि ट्रांसफॉर्मर एक वास्तविक बुनियादी एनएलपी उपकरण बन गया है।

## इस अनुभाग की मुख्य सामग्री
![चित्र: BERT संरचना](./pictures/3-6-bert.png) चित्र: BERT संरचना, स्रोत IrEne: Intट्रांसफार्मर के लिए अचूक ऊर्जा भविष्यवाणी

यह लेख ट्रांसफॉर्मर्स संस्करण 4.4.2 (19 मार्च, 2021 को जारी) परियोजना में बीईआरटी-संबंधित कोड के पाइटोरच संस्करण पर आधारित है। इसका विश्लेषण कोड संरचना, विशिष्ट कार्यान्वयन और सिद्धांतों और उपयोग के दृष्टिकोण से किया गया है।
मुख्य सामग्री:
1. BERT टोकनाइजेशन शब्द विभाजन मॉडल (BertTokenizer)
2. BERT मॉडल ऑन्टोलॉजी मॉडल (बर्टमॉडल)
    - बर्टएम्बेडिंग्स
    - बर्टएनकोडर
        - बर्टलेयर
            - बर्टअटेंशन
            - बर्टइंटरमीडिएट
            - बर्टआउटपुट
    - बर्टपूलर

***
## 1-टोकनीकरण शब्द विभाजन-बर्टटोकनाइज़र
BERT से संबंधित टोकननाइज़र मुख्य रूप से [`models/bert/tokenization_bert.py`](https://github.c) में लिखा गया हैom/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py)।


```अजगर
आयात संग्रह
ओएस आयात करें
आयातयूनिकोडडेटा
आयात सूची, वैकल्पिक, टपल टाइप करने से

ट्रांसफ़ॉर्मर्स.टोकनाइज़ेशन_यूटिल्स से प्रीट्रेन्डटोकनाइज़र, _is_control, _is_punctuation, _is_whitespace आयात करें
ट्रांसफॉर्मर.यूटिल्स से लॉगिंग आयात करें


लकड़हारा = लॉगिंग.get_logger(__name__)

VOCAB_FILES_NAMES = {"vocab_file": "vocab.txt"}

PRETRAINED_VOCAB_FILES_MAP = {
    "vocab_file": {
        "बर्ट-बीase-uncased": "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt",
    }
}

PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {
    "बर्ट-बेस-अनकेस्ड": 512,
}

PRETRAINED_INIT_CONFIGURATION = {
    "बर्ट-बेस-अनकेस्ड": {"do_lower_case": True},
}


डीईएफ़ लोड_वोकैब(वोकैब_फ़ाइल):
    """एक शब्दावली फ़ाइल को एक शब्दकोश में लोड करता है।"""
    शब्दावली = संग्रह.ऑर्डर्डडिक्ट()
    रीडर के रूप में open(vocab_file, "r", एन्कोडिंग = "utf-8") के साथ:
        टोकन = रीडर.रीडलाइन्स()
    सूचकांक के लिए, एन्युमेरा में टोकनते(टोकन):
        टोकन = टोकन.rstrip("\n")
        शब्दकोष[टोकन] = अनुक्रमणिका
    वापसी शब्दावली


डीईएफ़ व्हाइटस्पेस_टोकनाइज़(पाठ):
    """पाठ के एक टुकड़े पर मूल रिक्त स्थान की सफाई और विभाजन चलाता है।""
    टेक्स्ट = टेक्स्ट.स्ट्रिप()
    यदि पाठ नहीं है:
        वापस करना []
    टोकन = text.split()
    टोकन लौटाएं


क्लास बर्टटोकनाइज़र(प्रीट्रेन्डटोकनाइज़र):

    vocab_files_names = VOCAB_FILES_NAMES
    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
    pretrained_init_configuration = PRETRAINED_INआईटी_कॉन्फिगरेशन
    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES

    def __init__(
        खुद,
        vocab_file,
        do_lower_case=सत्य,
        do_basic_tokenize=सत्य,
        कभी_विभाजित=कोई नहीं,
        unk_token='[UNK]',
        sep_token='[SEP]',
        पैड_टोकन='[पीएडी]',
        cls_token='[सीएलएस]',
        मास्क_टोकन='[मास्क]',
        टोकननाइज़_चीनी_चार्स=सत्य,
        स्ट्रिप_एक्सेंट=कोई नहीं,
        **क्वार्ग्स
    ):
        सुपर().__init__(
            do_lower_case=do_lower_case,
            do_basic_tokenize=do_basic_tokenize,
            कभी_विभाजित नहीं = कभी_विभाजित नहीं,
            अनक_टोकन=अनक_टोकन,
            sep_token=sep_token,
            पैड_टोकन=पैड_टोकन,
            cls_token=cls_token,
            मास्क_टोकन=मास्क_टोकन,
            टोकननाइज_चीनी_चार्स=टोकनाइज_चीनी_चार्स,
            स्ट्रिप_एक्सेंट=स्ट्रिप_एक्सेंट,
            **क्वार्ग्स,
        )

        यदि नहीं os.path.isfile(vocab_file):
            वैल्यूएरर बढ़ाएं(
                च"नहीं कर सकता चGoogle पूर्व-प्रशिक्षित से शब्दावली लोड करने के लिए पथ '{vocab_file}' पर एक शब्दावली फ़ाइल डालें।
                "मॉडल उपयोग `टोकनाइज़र = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"
            )
        सेल्फ.वोकैब = लोड_वोकैब(वोकैब_फाइल)
        self.ids_to_tokens =collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])
        self.do_basic_tokenize = do_basic_tokenize
        यदि do_basic_tokenize:
            self.basic_tokenizer = BasicTokenizer(
                डीo_lower_case=do_lower_case,
                कभी_विभाजित नहीं = कभी_विभाजित नहीं,
                टोकननाइज_चीनी_चार्स=टोकनाइज_चीनी_चार्स,
                स्ट्रिप_एक्सेंट=स्ट्रिप_एक्सेंट,
            )
        सेल्फ.वर्डपीस_टोकनाइज़र = वर्डपीसटोकनाइज़र(वोकैब=सेल्फ.वोकैब, अनक_टोकन=सेल्फ.अनक_टोकन)

    @संपत्ति
    def do_lower_case(स्वयं):
        self.basic_tokenizer.do_lower_case लौटाएँ

    @संपत्ति
    def vocab_size(स्वयं):
        वापसी लेन(स्वयं.वोकैब)

    def get_vocab(स्वयं):
        वापसी निर्देश(सेल्फ.वोकैब, **सेल्फ.एडेड_टोकेंस_एनकोडर)

    def _टोकनाइज़(स्वयं, पाठ):
        स्प्लिट_टोकन = []
        यदि self.do_basic_tokenize:
            self.basic_tokenizer.tokenize(text, ever_split=self.all_special_tokens) में टोकन के लिए:

                # यदि टोकन नेवर_स्प्लिट सेट का हिस्सा है
                यदि self.basic_tokenizer.never_split में टोकन:
                    स्प्लिट_टोकन.जोड़ें(टोकन)
                अन्य:
                    स्प्लिट_टोकेंस += self.wordPiece_tokenizer.tokenize(toकेन)
        अन्य:
            स्प्लिट_टोकेंस = self.wordPiece_tokenizer.tokenize(पाठ)
        स्प्लिट_टोकन लौटाएँ

    def _convert_token_to_id(स्वयं, टोकन):
        """वोकैब का उपयोग करके एक टोकन (str) को एक आईडी में परिवर्तित करता है।"""
        self.vocab.get लौटाएं(टोकन, self.vocab.get(self.unk_token))

    def _convert_id_to_token(स्वयं, अनुक्रमणिका):
        """शब्दावली का उपयोग करके एक सूचकांक (पूर्णांक) को एक टोकन (str) में परिवर्तित करता है।""
        self.ids_to_tokens.get लौटाएं(सूचकांक, self.unk_token)

    def कन्वर्ट_टोकेन्स_टू_स्टअंगूठी(स्वयं, टोकन):
        """टोकन (स्ट्रिंग) के अनुक्रम को एक स्ट्रिंग में परिवर्तित करता है।"""
        आउट_स्ट्रिंग = " ".join(टोकन).replace(" ##", "").strip()
        वापसी_स्ट्रिंग

    def build_inputs_with_special_tokens(
        स्वयं, टोकन_आईडी_0: सूची[int], टोकन_आईडी_1: ​वैकल्पिक[सूची[int]] = कोई नहीं
    ) -> सूची[int]:
        """
        अनुक्रम वर्गीकरण कार्यों के लिए अनुक्रम या अनुक्रम की एक जोड़ी से मॉडल इनपुट बनाएं
        विशेष टोकन जोड़ना एक BERT अनुक्रमnce का निम्नलिखित प्रारूप है:
        - एकल अनुक्रम: ``[सीएलएस] एक्स [सितंबर]``
        - अनुक्रमों की जोड़ी: ``[सीएलएस] ए [सितंबर] बी [सितंबर]``
        तर्क:
            टोकन_आईडी_0 (:obj:`सूची[int]`):
                उन आईडी की सूची जिनमें विशेष टोकन जोड़े जाएंगे।
            टोकन_आईडी_1 (:obj:`सूची[int]`, `वैकल्पिक`):
                अनुक्रम जोड़े के लिए आईडी की वैकल्पिक दूसरी सूची।
        रिटर्न:
            :obj:`सूची[int]`: उपयुक्त एसपी के साथ `इनपुट आईडी <../glosary.html#input-ids>`__ की सूचीव्यक्तिगत टोकन.
        """
        यदि टोकन_आईडी_1 कोई नहीं है:
            वापसी [self.cls_token_id] + टोकन_ids_0 + [self.sep_token_id]
        सीएलएस = [self.cls_token_id]
        सितम्बर = [self.sep_token_id]
        वापसी सीएलएस + टोकन_आईडी_0 + सितंबर + टोकन_आईडी_1 + सितंबर

    डीईएफ़ get_special_tokens_mask(
        स्वयं, टोकन_आईडी_0: सूची[int], टोकन_आईडी_1: ​​वैकल्पिक[सूची[int]] = कोई नहीं, पहले से ही_विशेष_टोकन: बूल = गलत
    ) -> सूची[int]:
        """
        एक टोकन सूची से अनुक्रम आईडी पुनर्प्राप्त करें जो hचूँकि कोई विशेष टोकन नहीं जोड़ा गया है, इसलिए जोड़ते समय इस विधि को कॉल किया जाता है
        टोकननाइज़र ``prepare_for_model`` विधि का उपयोग करके विशेष टोकन।
        तर्क:
            टोकन_आईडी_0 (:obj:`सूची[int]`):
                आईडी की सूची.
            टोकन_आईडी_1 (:obj:`सूची[int]`, `वैकल्पिक`):
                अनुक्रम जोड़े के लिए आईडी की वैकल्पिक दूसरी सूची।
            पहले से ही_विशेष_टोकन हैं (:obj:`बूल`, `वैकल्पिक`, डिफ़ॉल्ट:obj:`गलत`):
                टोकन सूची पहले से ही प्रारूपित है या नहींमॉडल के लिए विशेष टोकन के साथ।
        रिटर्न:
            :obj:`सूची[int]`: श्रेणी में पूर्णांकों की एक सूची [0, 1]: एक विशेष टोकन के लिए 1, अनुक्रम टोकन के लिए 0।
        """

        यदि पहले से ही_विशेष_टोकन हैं:
            वापसी सुपर().get_special_tokens_mask(
                टोकन_आईडी_0=टोकन_आईडी_0, टोकन_आईडी_1=टोकन_आईडी_1, पहले से ही_विशेष_टोकन=सही है
            )

        यदि टोकन_आईडी_1 कोई नहीं है:
            वापसी [1] + ([0] * लेन(टोकन_आईडी_0)) + [1] + ([0] * लेन(टोकन_आईडीs_1)) + [1]
        वापसी [1] + ([0] * लेन(टोकन_आईडी_0)) + [1]

    def create_token_type_ids_from_sequences(
        स्वयं, टोकन_आईडी_0: सूची[int], टोकन_आईडी_1: ​वैकल्पिक[सूची[int]] = कोई नहीं
    ) -> सूची[int]:
        """
        अनुक्रम-जोड़ी वर्गीकरण कार्य में उपयोग के लिए पारित दो अनुक्रमों से एक मास्क बनाएं
        पेयर मास्क का प्रारूप निम्न है:
        ::
            0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
            |. पहला क्रम |. दूसरा क्रम |यदि :obj:`token_ids_1` :obj:`None` है, तो यह विधि केवल मास्क का पहला भाग (0s) लौटाती है।
        तर्क:
            टोकन_आईडी_0 (:obj:`सूची[int]`):
                आईडी की सूची.
            टोकन_आईडी_1 (:obj:`सूची[int]`, `वैकल्पिक`):
                अनुक्रम जोड़े के लिए आईडी की वैकल्पिक दूसरी सूची।
        रिटर्न:
            :obj:`सूची[int]`: दिए गए अनुसार `टोकन प्रकार आईडी <../glosary.html#token-type-ids>`_ की सूची
            क्रम.
        """
        सितंबर = [स्वयं.sep_token_id]
        सीएलएस = [self.cls_token_id]
        यदि टोकन_आईडी_1 कोई नहीं है:
            वापसी लेन(सीएलएस + टोकन_आईडी_0 + सितंबर) * [0]
        रिटर्न लेन(सीएलएस + टोकन_आईडी_0 + सितंबर) * [0] + लेन(टोकन_आईडी_1 + सितंबर) * [1]

    def save_vocabulary(स्वयं, save_directory: str, फ़ाइल नाम_उपसर्ग: वैकल्पिक[str] = कोई नहीं) -> Tuple[str]:
        सूचकांक = 0
        यदि os.path.isdir(save_directory):
            vocab_file = os.path.join(
                save_directory, (फ़ाइलनाम_उपसर्ग + "-" यदि फ़ाइलनाम_उपसर्ग अन्यथा "") + VOCAB_FILES_NAMES["vocab_file"]
            )
        अन्य:
            vocab_file = (फ़ाइल नाम_उपसर्ग + "-" यदि फ़ाइल नाम_उपसर्ग अन्यथा "") + save_directory
        लेखक के रूप में open(vocab_file, "w", एन्कोडिंग = "utf-8") के साथ:
            टोकन के लिए, sorted(self.vocab.items(), key=lambda kv: kv[1] में टोकन_इंडेक्स):
                यदि सूचकांक != टोकन_सूचकांक:
                    लकड़हारा.चेतावनी(
                        f"शब्दावली को {vocab_file} में सहेजा जा रहा है: शब्दावली सूचकांक लगातार नहीं हैं।""कृपया जांच लें कि शब्दावली दूषित तो नहीं है!"
                    )
                    सूचकांक = टोकन_सूचकांक
                लेखक.लिखें(टोकन + "\n")
                सूचकांक += 1
        वापसी (vocab_file,)


क्लास बेसिकटोकनाइज़र(ऑब्जेक्ट):

    def __init__(स्वयं, do_lower_case=सही, कभी_स्प्लिट=कोई नहीं, टोकननाइज_चीनी_चार्स=सही, स्ट्रिप_एक्सेंट=कोई नहीं):
        यदि नेवर_स्प्लिट कोई नहीं है:
            कभी_विभाजित नहीं = []
        self.do_lower_case = do_lower_case
        स्वयं.कभी_विभाजित नहीं= सेट(कभी_विभाजित नहीं)
        स्व.टोकनाइज_चीनी_चार्स = टोकनाइज_चीनी_चार्स
        स्वयं.स्ट्रिप_एक्सेंट = स्ट्रिप_एक्सेंट

    डीईएफ़ टोकननाइज़ (स्वयं, पाठ, कभी_स्प्लिट = कोई नहीं):
        """
        पाठ के एक टुकड़े का मूल टोकनाइजेशन, उप-शब्द टोकनाइजेशन के लिए केवल "सफेद रिक्त स्थान" पर विभाजित करें, देखें
        वर्डपीसटोकनाइज़र।
        तर्क:
            **never_split**: (`वैकल्पिक`) स्ट्र की सूची
                पिछड़े संगतता उद्देश्यों के लिए रखा गया। अब इसे सीधे बेस क्लास स्तर पर लागू किया गया है (देखें)।:func:`PreTrainedTokenizer.tokenize`) विभाजित न होने वाले टोकन की सूची।
        """
        # यूनियन() दो सेटों को जोड़कर एक नया सेट लौटाता है।
        नेवर_स्प्लिट = स्व.नेवर_स्प्लिट.यूनियन(सेट(नेवर_स्प्लिट)) यदि नेवर_स्प्लिट नहीं तो स्व.नेवर_स्प्लिट
        पाठ = self._clean_text(पाठ)

        # इसे बहुभाषी और चीनी के लिए 1 नवंबर, 2018 को जोड़ा गया था
        # मॉडल। यह अब अंग्रेजी मॉडल पर भी लागू होता है, लेकिन ऐसा नहीं है
        #mattersincetheEnglish मॉडईएलएस को किसी भी चीनी डेटा पर प्रशिक्षित नहीं किया गया था
        # और आम तौर पर उनमें कोई चीनी डेटा नहीं होता (चीनी होते हैं)।
        शब्दावली में # वर्ण क्योंकि विकिपीडिया में कुछ चीनी भाषाएँ हैं
        अंग्रेजी विकिपीडिया में # शब्द।)
        यदि self.tokenize_chinese_chars:
            पाठ = self._tokenize_chinese_chars(पाठ)
        मूल_टोकन = व्हाइटस्पेस_टोकनाइज़(पाठ)
        स्प्लिट_टोकन = []
        मूल_टोकन में टोकन के लिए:
            यदि टोकन कभी_स्प्लिट में नहीं है:यदि self.do_lower_case:
                    टोकन = टोकन.निचला()
                    यदि self.strip_accents ग़लत नहीं है:
                        टोकन = self._run_strip_accents(टोकन)
                एलिफ सेल्फ.स्ट्रिप_एक्सेंट:
                    टोकन = self._run_strip_accents(टोकन)
            स्प्लिट_टोकेंस.एक्सटेंड(स्वयं._रन_स्प्लिट_ऑन_पंक(टोकन, नेवर_स्प्लिट))

        आउटपुट_टोकेंस = व्हाईटस्पेस_टोकनाइज़(" ".join(split_tokens))
        आउटपुट_टोकन लौटाएँ

    def _run_strip_accents(स्वयं, पाठ):
        """पाठ के एक टुकड़े से उच्चारण को अलग करता है।"""
        टेक्स्ट = यूनिकोडडेटा.नॉर्मलाइज़ ("एनएफडी", टेक्स्ट)
        आउटपुट = []
        पाठ में चार के लिए:
            बिल्ली = यूनिकोडडेटा.श्रेणी(चार)
            अगर बिल्ली == "एमएन":
                जारी रखना
            आउटपुट.एपेंड(चार)
        वापसी ".join(आउटपुट)

    def _run_split_on_punc(स्वयं, पाठ, कभी_स्प्लिट=कोई नहीं):
        """विराम चिह्न को पाठ के एक टुकड़े पर विभाजित करता है।"""
        यदि नेवर_स्प्लिट कोई नहीं है और नेवर_स्प्लिट में टेक्स्ट है:
            वापस करनाn [पाठ]
        वर्ण = सूची(पाठ)
        मैं = 0
        प्रारंभ_नया_शब्द = सत्य
        आउटपुट = []
        जबकि मैं <लेन(वर्ण):
            चार = वर्ण[i]
            यदि _विराम चिह्न (चार) है:
                आउटपुट.एपेंड([चार])
                प्रारंभ_नया_शब्द = सत्य
            अन्य:
                यदि प्रारंभ_नया_शब्द:
                    आउटपुट.जोड़ें([])
                प्रारंभ_नया_शब्द = ग़लत
                आउटपुट[-1].जोड़ें(चार)
            मैं += 1

        वापसी [''.join(x) for x in output]

    def _tokenize_chinese_chars(स्वयं, पाठ):
        """किसी भी सीजेके चरित्र के आसपास रिक्त स्थान जोड़ता है।"""
        आउटपुट = []
        पाठ में चार के लिए:
            सीपी = ऑर्ड(चार)
            यदि self._is_chinese_char(cp):
                आउटपुट.एपेंड("")
                आउटपुट.एपेंड(चार)
                आउटपुट.एपेंड("")
            अन्य:
                आउटपुट.एपेंड (चार)
        वापसी ".join(आउटपुट)

    def _is_chinese_char(स्वयं, सीपी):
        """जांचता है कि क्या सीपी का कोडपॉइंट हैएक सीजेके चरित्र।"""
        # यह CJK यूनिकोड ब्लॉक में किसी भी चीज़ के रूप में "चीनी वर्ण" को परिभाषित करता है:
        # https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
        #
        # ध्यान दें कि सीजेके यूनिकोड ब्लॉक में सभी जापानी और कोरियाई अक्षर नहीं हैं,
        # अपने नाम के बावजूद आधुनिक कोरियाई हंगुल वर्णमाला एक अलग ब्लॉक है,
        # जैसे जापानी हिरागाना और कटकाना उन अक्षरों का उपयोग किया जाता है
        #शब्द स्थान-विभाजित हैं, अत: वे संधि नहीं हैंडी विशेष रूप से और संभाला
        # अन्य सभी भाषाओं की तरह।
        अगर (
            (सीपी >= 0x4ई00 और सीपी <= 0x9एफएफएफ)
            या (cp >= 0x3400 और cp <= 0x4DBF) #
            या (cp >= 0x20000 और cp <= 0x2A6DF) #
            या (cp >= 0x2A700 और cp <= 0x2B73F) #
            या (cp >= 0x2B740 और cp <= 0x2B81F) #
            या (cp >= 0x2B820 और cp <= 0x2CEAF) #
            या (cp >= 0xF900 और cp <= 0xFAFF)
            या (cp >= 0x2F800 और cp <= 0x2FA1F) #
        ): #सच लौटें

        विवरण झूठा है

    def _clean_text(स्वयं, पाठ):
        """पाठ पर अमान्य वर्ण हटाने और रिक्त स्थान साफ़ करने का कार्य करता है।"""
        आउटपुट = []
        पाठ में चार के लिए:
            सीपी = ऑर्ड(चार)
            यदि सीपी == 0 या सीपी == 0xएफएफएफडी या _is_control(चार):
                जारी रखना
            यदि_व्हाइटस्पेस(चार) है:
                आउटपुट.एपेंड("")
            अन्य:
                आउटपुट.एपेंड(चार)
        वापसी ".join(आउटपुट)


क्लास वर्डपीसटोकनिज़एर(ऑब्जेक्ट):
    """वर्डपीस टोकनाइजेशन चलाता है।""

    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):
        स्व.शब्दावली=शब्दावली
        self.unk_token = unk_token
        self.max_input_chars_per_word = max_input_chars_per_word

    डीईएफ़ टोकनेनाइज़ (स्वयं, पाठ):
        """
        पाठ के एक टुकड़े को उसके शब्द टुकड़ों में टोकनाइज़ करता है। यह प्रदर्शन करने के लिए एक लालची सबसे लंबे-मिलान-पहले एल्गोरिदम का उपयोग करता है
        दी गई शब्दावली का उपयोग करके टोकनीकरण।
        उदाहरण के लिए, :obj:`input = "unaffसक्षम"` आउटपुट के रूप में वापस आएगा :obj:`["un", "##aff", "##able"]`।
        तर्क:
          पाठ: एक एकल टोकन या रिक्त स्थान से अलग टोकन होना चाहिए
            पहले ही `बेसिकटोकनाइज़र` से गुजर चुका है।
        रिटर्न:
          वर्डपीस टोकन की एक सूची.
        """

        आउटपुट_टोकन = []
        Whitespace_tokenize(पाठ) में टोकन के लिए:
            वर्ण = सूची(टोकन)
            यदि लेन(वर्ण) > self.max_input_chars_per_word:
                आउटपुट_टोकन.जोड़ें(स्वयं.अंक_टोकन)जारी रखना

            बुरा है = ग़लत
            प्रारंभ=0
            उप_टोकन = []
            जबकि प्रारंभ <लेन(वर्ण):
                अंत = लेन(वर्ण)
                cur_substr = कोई नहीं
                जबकि प्रारंभ <अंत:
                    सबस्ट्र = ".जुड़ें(वर्ण[प्रारंभ:अंत])
                    यदि प्रारंभ > 0:
                        सबस्ट्र = "##" + सबस्ट्र
                    यदि self.vocab में सबस्ट्र:
                        cur_substr = पदार्थ
                        तोड़नाअंत-=1
                यदि cur_substr कोई नहीं है:
                    बुरा है = सत्य है
                    तोड़ना
                उप_टोकन.जोड़ें(cur_substr)
                प्रारंभ=अंत

            यदि खराब है:
                आउटपुट_टोकन.जोड़ें(स्वयं.अंक_टोकन)
            अन्य:
                आउटपुट_टोकन.विस्तार(उप_टोकन)
        आउटपुट_टोकन लौटाएँ
```

```
क्लास बर्टटोकनाइज़र(प्रीट्रेन्डटोकनाइज़र):
    """
    वर्डपीस के आधार पर एक BERT टोकनाइज़र का निर्माण करें।

    यह टोकनizer को :class:`~transformers.PreTrainedTokenizer` से विरासत में मिला है जिसमें अधिकांश मुख्य विधियाँ शामिल हैं।
    उन तरीकों के बारे में अधिक जानकारी के लिए उपयोगकर्ताओं को इस सुपरक्लास को देखना चाहिए।
    ...
    """
```

'बर्टटोकनाइज़र' 'बेसिकटोकनाइज़र' और 'वर्डपीसटोकनाइज़र' पर आधारित एक टोकननाइज़र है:
- बेसिकटोकनाइज़र प्रसंस्करण के पहले चरण के लिए ज़िम्मेदार है - विराम चिह्न, रिक्त स्थान इत्यादि के अनुसार वाक्यों को विभाजित करना, छोटे अक्षरों को एकीकृत करना है या नहीं, और अवैध वर्णों को साफ़ करना।
    - चीनी अक्षरों के लिए, प्रीप्रोसेसिंग (रिक्त स्थान जोड़कर) के माध्यम से शब्द द्वारा विभाजित;
    - साथ ही, आप निर्दिष्ट कर सकते हैं कि कुछ शब्दों को नेवर_स्प्लिट के माध्यम से विभाजित नहीं किया जाएगा;
    - यह चरण वैकल्पिक है (डिफ़ॉल्ट रूप से निष्पादित)।
- WordPieceTokenizer आगे शब्दों के आधार पर शब्दों को उपशब्दों में विघटित करता है।
    - सबवर्ड चार और शब्द दोनों के बीच हैयह कुछ हद तक शब्द के अर्थ को बरकरार रखता है, और अंग्रेजी में एकवचन और बहुवचन, काल और अपंजीकृत शब्दों की OOV (आउट-ऑफ-वोकैबुलरी) समस्या के कारण होने वाले शब्दावली विस्फोट का ख्याल रख सकता है और मूल को अलग कर सकता है। शब्द और काल प्रत्यय आदि। इससे शब्दावली सूची कम हो जाती है और प्रशिक्षण की कठिनाई भी कम हो जाती है;
    - उदाहरण के लिए, टोकननाइज़र शब्द को दो भागों में विभाजित किया जा सकता है: "टोकन" और "##ाइज़र"। ध्यान दें कि बाद वाले शब्द में "##" का अर्थ है कि यह पिछले शब्द का अनुसरण करता है।
BertTokenizer में निम्नलिखित सामान्य विधियाँ हैं:
- from_pretrained: शब्दावली फ़ाइल (vocab.txt) वाली निर्देशिका से एक टोकनाइज़र प्रारंभ करें;
- टोकननाइज़: पाठ (शब्द या वाक्य) को उपशब्दों की सूची में विघटित करें;
- Convert_tokens_to_ids: उपशब्दों की सूची को उपशब्दों के अनुरूप उपस्क्रिप्ट की सूची में परिवर्तित करें;
- Convert_ids_to_tokens: पिछले वाले के विपरीत;
- Convert_tokens_to_string: उपशब्द सूची को "##" के अनुसार वापस शब्दों या वाक्यों में विभाजित करें;
- एनकोड: एक वाक्य इनपुट के लिए, शब्द को विघटित करें और "[सीएलएस], एक्स, [एसईपी]" की संरचना बनाने के लिए विशेष शब्द जोड़ें और इसे दो वाक्य इनपुट के लिए शब्दावली में संबंधित सबस्क्रिप्ट की सूची में परिवर्तित करें; एकाधिक वाक्य केवल पहले दो को लेते हैं), शब्द को विघटित करें और "[सी" बनाने के लिए विशेष शब्द जोड़ेंएलएस], एक्स1, [एसईपी], एक्स2, [एसईपी]'' संरचना और एक सबस्क्रिप्ट सूची में परिवर्तित;
- डिकोड: एनकोड विधि के आउटपुट को पूर्ण वाक्य में बदला जा सकता है।
और, कक्षा की विधियाँ ही:


```अजगर
बीटी = BertTokenizer.from_pretrained('bert-base-uncased')
बीटी('मुझे प्राकृतिक भाषा की प्रगति पसंद है!')
# {'इनपुट_आईडी': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'टोकन_टाइप_आईडी': [0, 0, 0, 0, 0, 0, 0, 0], 'ध्यान_मास्क': [1, 1, 1, 1, 1, 1, 1, 1]}
```

    डाउनलोडिंग: 100%|██████████|232k/232k [00:00<00:00, 698kB/s]
    डाउनलोडिंग: 100%|██████████|28.0/28.0 [00:00<00:00, 11.1kB/s]
    डाउनलोडइनजी: 100%|██████████|466k/466k [00:00<00:00, 863kB/s]





    {'इनपुट_आईडी': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'टोकन_टाइप_आईडी': [0, 0, 0, 0, 0, 0, 0, 0], 'ध्यान_मास्क': [ 1, 1, 1, 1, 1, 1, 1, 1]}



***
## 2-मॉडल-बर्टमॉडल
BERT मॉडल से संबंधित कोड मुख्य रूप से [`/models/bert/modeling_bert.py`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.) में लिखा जाता है। py) , इस कोड में एक हजार से अधिक लाइनें हैं, जिसमें BERT मॉडल की मूल संरचना और उस पर आधारित फाइन-ट्यूनिंग मॉडल शामिल है।

आइए BERT मॉडल ऑन्कोलॉजी के विश्लेषण से शुरुआत करें:
```
क्लास बर्टमॉडल(बर्टप्रीट्रेन्डमॉडल):"""

    मॉडल एक एनकोडर (केवल आत्म-ध्यान के साथ) के साथ-साथ एक डिकोडर के रूप में भी व्यवहार कर सकता है, जिस स्थिति में एक परत
    'ध्यान है' में वर्णित वास्तुकला का पालन करते हुए, आत्म-ध्यान परतों के बीच क्रॉस-अटेंशन जोड़ा जाता है
    आपको बस इतना ही चाहिए <https://arxiv.org/abs/1706.03762>`__ आशीष वासवानी, नोम शज़ीर, निकी परमार, जैकब उस्ज़कोरिट द्वारा,
    लिलियन जोन्स, एडन एन. गोमेज़, लुकाज़ कैसर और इलिया पोलोसुखिन।

    एक डिकोडर के रूप में व्यवहार करने के लिए मॉडल को :obj के साथ आरंभ करने की आवश्यकता है:`is_decoder` कॉन्फ़िगरेशन का तर्क
    :obj:`True` पर सेट करें Seq2Seq मॉडल में उपयोग करने के लिए, मॉडल को :obj:`is_decoder` दोनों के साथ प्रारंभ करने की आवश्यकता है।
    तर्क और :obj:`add_cross_attention` को :obj:`True` पर सेट किया गया है; एक :obj:`encoder_hidden_states` को फिर एक के रूप में अपेक्षित किया जाता है
    फॉरवर्ड पास के लिए इनपुट।
    """
```
बर्टमॉडल मुख्य रूप से एक ट्रांसफार्मर एनकोडर संरचना है, जिसमें तीन भाग शामिल हैं:
1. एम्बेडिंग, BertEmbeddings वर्ग की एक इकाई, शब्द प्रतीक के आधार पर संबंधित वेक्टर प्रतिनिधित्व प्राप्त करती है;
2. एनकोडर, BertEncoder वर्ग की इकाई;
3. पूलर, बर्टपूलर वर्ग की इकाई, यह भाग वैकल्पिक है।

**नोट बर्टमॉडल भी उपलब्ध हैडिकोडर के रूप में कॉन्फ़िगर करें, लेकिन इस भाग पर नीचे चर्चा नहीं की गई है। **

निम्नलिखित BertModel की आगे प्रसार प्रक्रिया के दौरान प्रत्येक पैरामीटर का अर्थ और रिटर्न मान पेश करेगा:
```
def आगे(
        खुद,
        इनपुट_आईडी=कोई नहीं,
        ध्यान_मास्क=कोई नहीं,
        टोकन_टाइप_आईडी=कोई नहीं,
        स्थिति_आईडी=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        इनपुट्स_एम्बेड्स=कोई नहीं,
        एनकोडर_हिडन_स्टेट्स=कोई नहीं,
        एनकोडर_अटेंशन_मास्क=कोई नहीं,
        Past_key_values=कोई नहीं,
        उपयोग_कैश=कोई नहीं,
        आउटपुट_ध्यान=कोई नहीं,
        आउटपुट_हिडन_स्टेट्स=कोई नहीं,
        return_dict=कोई नहीं,
    ): ...
```
-इनपुट_आईडी: टोकननाइज़र शब्द विभाजन के बाद सबवर्ड से संबंधित सबस्क्रिप्ट सूची;
- ध्यान_मास्क: आत्म-ध्यान प्रक्रिया में, इस मास्क का उपयोग उस वाक्य के बीच अंतर को चिह्नित करने के लिए किया जाता है जहां उपशब्द स्थित है और पैडिंग, और पैडिंग भाग 0 से भरा हुआ है;
- टोकन_टाइप_आईडी: उपशब्द के वर्तमान वाक्य को चिह्नित करें (पहला वाक्य/दूसरा वाक्य/पैडिंग);
- पोजीशन_आईडी: उस वाक्य की स्थिति सबस्क्रिप्ट को चिह्नित करें जहां वर्तमान शब्द स्थित है;
- हेड_मास्क: कुछ परतों में कुछ ध्यान गणनाओं को अमान्य करने के लिए उपयोग किया जाता है;
- इनपुट_एम्बेड: यदि प्रदान किया गया है, तो इनपुट_आईडी की आवश्यकता नहीं है और यह सीधे एम्बेडिंग लुकअप प्रक्रिया में एम्बेडिंग के रूप में एनकोडर गणना में प्रवेश करेगा;
- एनकोडर_हिडन_स्टेट्स: यह भाग तब काम करता है जब बर्टमॉडल को डिकोडर के रूप में कॉन्फ़िगर किया गया है, और स्व-ध्यान के बजाय क्रॉस-अटेंशन करेगा;
- एनकोडर_अटेंशन_मास्क: ऊपर के समान, क्रॉस-अटेंशन में एनएनसी को चिह्नित करने के लिए उपयोग किया जाता हैदूसरी तरफ पैडिंग इनपुट;
- Past_key_values: क्रॉस-अटेंशन की लागत को कम करने के लिए यह पैरामीटर पूर्व-गणना किए गए K-V उत्पाद में पास होता प्रतीत होता है (क्योंकि यह भाग मूल रूप से दोहराई गई गणना है);
- उपयोग_कैश: पिछले पैरामीटर को सहेजेगा और डिकोडिंग को तेज करने के लिए इसे वापस पास करेगा;
- आउटपुट_अटेंशन: प्रत्येक मध्यवर्ती परत का ध्यान आउटपुट लौटाना है या नहीं;
- आउटपुट_हिडन_स्टेट्स: क्या प्रत्येक मध्यवर्ती परत का आउटपुट वापस करना है;
- रिटर्न_डिक्ट: आउटपुट को कुंजी-मूल्य जोड़े (मॉडलआउटपुट क्लास, जिसे टुपल के रूप में भी इस्तेमाल किया जा सकता है) के रूप में वापस करना है या नहीं, डिफ़ॉल्ट सत्य है।

** ध्यान दें कि यहां हेड_मास्क ध्यान गणना को अमान्य कर देता है, जो नीचे उल्लिखित ध्यान हेड प्रूनिंग से अलग है, लेकिन केवल इस गुणांक द्वारा कुछ ध्यान गणना परिणामों को गुणा करता है। **

आउटपुट भाग इस प्रकार है:
```
# बर्टमॉडल का आगे का प्रसार वापसी भाग
        यदि नहीं तो return_dict:
            वापसी (अनुक्रम_आउटपुट, पूल्ड_आउटपुट) + एनकोडर_आउटपुट[1:]

        रेटुआरएन बेसमॉडलआउटपुटविथपूलिंगएंडक्रॉसअटेंशन(
            अंतिम_छिपी_स्थिति=अनुक्रम_आउटपुट,
            पूलर_आउटपुट=पूल_आउटपुट,
            Past_key_values=encoder_outputs.past_key_values,
            hidden_states=encoder_outputs.hidden_states,
            ध्यान=एनकोडर_आउटपुट.ध्यान,
            क्रॉस_अटेंशन=एनकोडर_आउटपुट.क्रॉस_अटेंशन,
        )
```
यह देखा जा सकता है कि रिटर्न वैल्यू में न केवल एनकोडर और पूलर का आउटपुट शामिल है, बल्कि आसान पहुंच के लिए अन्य निर्दिष्ट आउटपुट भाग (hidden_states और ध्यान, आदि, यह हिस्सा encoder_outputs [1:]) में भी शामिल है:

```# BertEncoder का आगे का प्रसार रिटर्न भाग उपरोक्त encoder_outputs है
        यदि नहीं तो return_dict:
            वापसी टपल(
                वी
                v के लिए [
                    छुपे_राज्य,
                    अगला_डिकोडर_कैश,
                    सभी_छिपे हुए_राज्य,
                    सबका_स्वयं_ध्यान,
                    सभी का ध्यान,
                ]
                यदि v कोई नहीं है
            )
        वापसी BaseModelOutputWithPastAndCrossAttentions(
            अंतिम_छिपा हुआ_राज्य=छिपा हुआen_states,
            Past_key_values=next_decoder_cache,
            छुपे हुए राज्य=सभी_छिपे हुए राज्य,
            ध्यान=सभी_स्वयं_ध्यान,
            क्रॉस_अटेंशन=सभी_क्रॉस_अटेंशन,
        )
```

इसके अलावा, BertModel के पास BERT खिलाड़ियों को विभिन्न ऑपरेशन करने में सुविधा प्रदान करने के लिए निम्नलिखित विधियाँ भी हैं:

- get_input_embeddings: एम्बेडिंग में Word_embeddings निकालें, जो शब्द वेक्टर भाग है;
- set_input_embeddings: एम्बेडिंग में Word_embeddings को मान निर्दिष्ट करें;
- _prune_heads: ध्यान शीर्षों को काटने के लिए एक फ़ंक्शन प्रदान करता है। इनपुट {layer_num: इस परत में काटने योग्य शीर्षों की सूची} का एक शब्दकोश है, जो निर्दिष्ट परत के कुछ ध्यान शीर्षों को काट सकता है।

** प्रूनिंग एक जटिल ऑपरेशन है और इस पर ध्यान देने की आवश्यकता है जिसे बरकरार रखा जाएगाफोर्स हेड भाग के Wq, Kq, Vq और स्प्लिसिंग के बाद पूरी तरह से जुड़े हिस्से के वजन को एक नए छोटे वजन मैट्रिक्स में कॉपी किया जाता है (ध्यान दें कि कॉपी करने से पहले ग्रेड अक्षम है), और कतरनी वाले हेड को रोकने के लिए वास्तविक समय में रिकॉर्ड किया जाता है सबस्क्रिप्ट त्रुटियाँ. विवरण के लिए, बर्टअटेंशन अनुभाग में prune_heads विधि देखें।**


```अजगर
ट्रांसफॉर्मर.मॉडल.बर्ट.मॉडलिंग_बर्ट आयात से *
क्लास बर्टमॉडल(बर्टप्रीट्रेन्डमॉडल):
    """
    मॉडल एक एनकोडर (केवल आत्म-ध्यान के साथ) के साथ-साथ एक डिकोडर के रूप में भी व्यवहार कर सकता है, जिस स्थिति में एक परत
    'ध्यान है' में वर्णित वास्तुकला का पालन करते हुए, आत्म-ध्यान परतों के बीच क्रॉस-अटेंशन जोड़ा जाता है
    आपको बस <https://arxiv.org/abs/1706.0 चाहिए3762>`__ आशीष वासवानी, नोम शज़ीर, निकी परमार, जैकब उस्ज़कोरिट द्वारा,
    लिलियन जोन्स, एडन एन. गोमेज़, लुकाज़ कैसर और इलिया पोलोसुखिन।
    डिकोडर के रूप में व्यवहार करने के लिए मॉडल को कॉन्फ़िगरेशन के :obj:`is_decoder` तर्क के साथ प्रारंभ करने की आवश्यकता है
    :obj:`True` पर सेट करें Seq2Seq मॉडल में उपयोग करने के लिए, मॉडल को :obj:`is_decoder` दोनों के साथ प्रारंभ करने की आवश्यकता है।
    तर्क और :obj:`add_cross_attention` को :obj:`True` पर सेट किया गया है; एक :obj:`encoder_hidden_states` को फिर एक के रूप में अपेक्षित किया जाता है
    आई.एन.पीफॉरवर्ड पास के लिए ut.
    """

    def __init__(स्वयं, कॉन्फिग, add_pooling_layer=True):
        सुपर().__init__(config)
        self.config = config

        self.embeddings = BertEmbeddings(config)
        self.encoder = BertEncoder(config)

        self.pooler = BertPooler(config) यदि add_pooling_layer अन्य कोई नहीं

        self.init_weights()

    def get_input_embeddings(स्वयं):
        self.embeddings.word_embeddings लौटाएँ

    def set_input_embeddings(स्वयं, मान):
        self.embeddings.word_embeddings = मान

    डीईएफ़ _प्रून_हेड्स(स्वयं, हेड्स_टू_प्रून):
        """
        मॉडल के प्रमुखों की छँटाई करें
        क्लासप्रीट्रेन्डमॉडल
        """
        परत के लिए,heads_to_prune.items() में शीर्ष:
            self.encoder.layer[परत].ध्यान.prune_heads(heads)

    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format("बैच_आकार, अनुक्रम_लंबाई"))
    @add_code_sample_docstrings(टोकननाइज़र_क्लास=_TOKENIZER_FOR_DOC,
        चेकप्वाइंट=_CHECKPOINT_FOR_DOC,
        आउटपुट_प्रकार=बेसमॉडलआउटपुटविथपूलिंगएंडक्रॉसअटेंशन,
        config_class=_CONFIG_FOR_DOC,
    )
    def आगे(
        खुद,
        इनपुट_आईडी=कोई नहीं,
        ध्यान_मास्क=कोई नहीं,
        टोकन_टाइप_आईडी=कोई नहीं,
        स्थिति_आईडी=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        इनपुट्स_एम्बेड्स=कोई नहीं,
        एनकोडर_हिडन_स्टेट्स=कोई नहीं,
        एनकोडर_अटेंशन_मास्क=कोई नहीं,
        Past_key_values=कोई नहीं,
        उपयोग_सीएसीवह=कोई नहीं,
        आउटपुट_ध्यान=कोई नहीं,
        आउटपुट_हिडन_स्टेट्स=कोई नहीं,
        return_dict=कोई नहीं,
    ):
        आर"""
        एनकोडर_हिडन_स्टेट्स (:obj:`torch.FloatTensor` आकार का:obj:`(बैच_आकार, अनुक्रम_लंबाई, छिपा_आकार)`, `वैकल्पिक`):
            एनकोडर की अंतिम परत के आउटपुट पर छिपी-स्थितियों का अनुक्रम क्रॉस-अटेंशन में उपयोग किया जाता है
            मॉडल को डिकोडर के रूप में कॉन्फ़िगर किया गया है।
        एनकोडर_अटेंशन_मास्क (:obj:`torch.FloatTensor` आकार का:obj:`(batch_siज़ी, अनुक्रम_लंबाई)`, `वैकल्पिक`):
            एनकोडर इनपुट के पैडिंग टोकन सूचकांकों पर ध्यान केंद्रित करने से बचने के लिए इस मास्क का उपयोग किया जाता है
            यदि मॉडल को डिकोडर के रूप में कॉन्फ़िगर किया गया है तो क्रॉस-अटेंशन ``[0, 1]`` में चयनित है।
            - 1 उन टोकन के लिए जो **नकाबपोश** नहीं हैं,
            - 0 ऐसे टोकन के लिए जो **नकाबपोश** हैं।
        Past_key_values ​​​​(:obj:`tuple(tuple(torch.FloatTensor))` लंबाई:obj:`config.n_layers` प्रत्येक टुपल में 4 टेंसर शाpe :obj:`(बैच_आकार, num_heads, अनुक्रम_लंबाई - 1, एम्बेड_आकार_प्रति_सिर)`):
            इसमें ध्यान ब्लॉकों की पूर्व-गणना की गई कुंजी और मूल्य छिपी हुई स्थितियाँ शामिल हैं, जिनका उपयोग डिकोडिंग को तेज़ करने के लिए किया जा सकता है।
            यदि :obj:`past_key_values` का उपयोग किया जाता है, तो उपयोगकर्ता वैकल्पिक रूप से केवल अंतिम इनपुट कर सकता है :obj:`decoder_input_ids`
            (जिनके पास इस मॉडल को दी गई अपनी पिछली कुंजी मान स्थिति नहीं है) आकार की :obj:`(batch_size, 1)`
            सभी के बजाय :obj:`decoder_input_ids` आकार का :obj:`(bमिलान_आकार, अनुक्रम_लंबाई)`।
        उपयोग_कैश (:obj:`बूल`, `वैकल्पिक`):
            यदि :obj:`True` पर सेट किया जाता है, तो :obj:`past_key_values` कुंजी मान स्थितियाँ लौटा दी जाती हैं और गति बढ़ाने के लिए इसका उपयोग किया जा सकता है
            डिकोडिंग (देखें :obj:`past_key_values`)।
        """
        आउटपुट_अटेंशन = आउटपुट_अटेंशन यदि आउटपुट_अटेंशन कोई और नहीं है self.config.output_attentions
        आउटपुट_हिडन_स्टेट्स = (
            आउटपुट_हिडन_स्टेट्स यदि आउटपुट_हिडन_स्टेट्स कोई और नहीं है self.config.output_hidden_राज्य
        )
        रिटर्न_डिक्ट = रिटर्न_डिक्ट यदि रिटर्न_डिक्ट कोई और नहीं है self.config.use_return_dict

        यदि self.config.is_decoder:
            उपयोग_कैश = उपयोग_कैश यदि उपयोग_कैश कोई और नहीं है self.config.use_cache
        अन्य:
            उपयोग_कैश = गलत

        यदि इनपुट_आईडी कोई नहीं है और इनपुट_एम्बेड कोई नहीं है:
            मान बढ़ाएँ त्रुटि ("आप एक ही समय में इनपुट_आईडी और इनपुट_एम्बेड दोनों निर्दिष्ट नहीं कर सकते")
        elif इनपुट_आईडी कोई नहीं है:
            इनपुट_शएप = इनपुट_आईडी.आकार()
            बैच_आकार, seq_length = इनपुट_आकार
        elif इनपुट्स_एम्बेड्स कोई नहीं है:
            इनपुट_शेप = इनपुट्स_एम्बेड्स.साइज()[:-1]
            बैच_आकार, seq_length = इनपुट_आकार
        अन्य:
            वैल्यूएरर बढ़ाएं ("आपको या तो इनपुट_आईडी या इनपुट_एम्बेड निर्दिष्ट करना होगा")

        डिवाइस = इनपुट_आईडी.डिवाइस यदि इनपुट_आईडी कोई और नहीं है तो इनपुट_एम्बेड्स.डिवाइस

        # अतीत_कुंजी_मान_लंबाई
        Past_key_values_length = Past_key_values[0][0].shape[2] यदिPast_key_values ​​​​कोई और नहीं है 0

        यदि ध्यान_मास्क कोई नहीं है:
            ध्यान_मास्क = टॉर्च.ओन्स(((बैच_साइज, सीक_लेंथ + पास्ट_की_वैल्यू_लेंथ)), डिवाइस=डिवाइस)

        यदि टोकन_टाइप_आईडी कोई नहीं है:
            यदि hasattr(self.embeddings, "token_type_ids"):
                बफ़र्ड_टोकन_टाइप_आईडी = self.embeddings.token_type_ids[:, :seq_length]
                बफ़रेड_टोकन_टाइप_आईडी_विस्तारित = बफ़रेड_टोकन_टाइप_आईडी.विस्तार(बैच_आकार, seq_length)
                टोकन_टाइप_आईडी= बफ़रेड_टोकन_टाइप_आईडी_विस्तारित
            अन्य:
                टोकन_टाइप_आईड्स = टॉर्च.जीरोस (इनपुट_शेप, डीटाइप=टॉर्च.लॉन्ग, डिवाइस=डिवाइस)

        # हम आयामों का एक आत्म-ध्यान मास्क प्रदान कर सकते हैं [बैच_आकार, from_seq_length, to_seq_length]
        #स्वयं जिस स्थिति में हमें इसे सभी प्रमुखों के लिए प्रसारित करने योग्य बनाने की आवश्यकता है।
        एक्सटेंडेड_अटेंशन_मास्क: टॉर्च.टेन्सर = सेल्फ.गेट_एक्सटेंडेड_अटेंशन_मास्क(अटेंशन_मास्क, इनपुट_शेप, डिवाइस)

        # यदि 2डी या 3डी अटेंशन मास्क पीआर हैक्रॉस-अटेंशन के लिए प्रस्तुत किया गया
        # हमें [बैच_आकार, num_heads, seq_length, seq_length] को प्रसारण योग्य बनाने की आवश्यकता है
        यदि self.config.is_decoder और encoder_hidden_states कोई नहीं है:
            एनकोडर_बैच_आकार, एनकोडर_अनुक्रम_लंबाई, _ = एनकोडर_हिडन_स्टेट्स.आकार()
            एनकोडर_हिडन_शेप = (एनकोडर_बैच_आकार, एनकोडर_अनुक्रम_लंबाई)
            यदि एनकोडर_अटेंशन_मास्क कोई नहीं है:
                एनकोडर_अटेंशन_मास्क = टॉर्च.ओन्स(एनकोडर_हिडन_शेप, डिवाइस=डिवाइस)
            एनकोडर_एक्सटेंडेड_अटेंशन_मास्क = self.invert_attention_mask(एनकोडर_अटेंशन_मास्क)
        अन्य:
            एनकोडर_एक्सटेंडेड_अटेंशन_मास्क = कोई नहीं

        # यदि आवश्यक हो तो हेड मास्क तैयार करें
        हेड_मास्क में # 1.0 इंगित करता है कि हम सिर रखते हैं
        # ध्यान_प्रॉब्स का आकार bsz x n_heads x N x N है
        # इनपुट हेड_मास्क का आकार [num_heads] या [num_hidden_layers x num_heads] है
        # और हेड_मास्क को आकार में बदल दिया गया है [num_hidden_layers x बैच x num_heads x seq_length x seq_लंबाई]
        हेड_मास्क = self.get_head_mask(head_mask, self.config.num_hidden_layers)

        एम्बेडिंग_आउटपुट = self.embeddings(
            इनपुट_आईडी=इनपुट_आईडी,
            स्थिति_आईडी=स्थिति_आईडी,
            टोकन_टाइप_आईडी=टोकन_टाइप_आईडी,
            इनपुट्स_एम्बेड्स=इनपुट्स_एम्बेड्स,
            Past_key_values_length=past_key_values_length,
        )
        एनकोडर_आउटपुट = स्व.एनकोडर(
            एम्बेडिंग_आउटपुट,
            ध्यान_मास्क=विस्तारित_ध्यान_मास्क,
            हेड_एमपूछो=सिर_मास्क,
            एनकोडर_हिडन_स्टेट्स=एनकोडर_हिडन_स्टेट्स,
            एनकोडर_अटेंशन_मास्क=एनकोडर_एक्सटेंडेड_अटेंशन_मास्क,
            पास्ट_की_वैल्यू=पास्ट_की_वैल्यू,
            उपयोग_कैश=उपयोग_कैश,
            आउटपुट_अटेंशन=आउटपुट_अटेंशन,
            आउटपुट_हिडन_स्टेट्स=आउटपुट_हिडन_स्टेट्स,
            रिटर्न_डिक्ट=रिटर्न_डिक्ट,
        )
        अनुक्रम_आउटपुट = एनकोडर_आउटपुट[0]
        पूल्ड_आउटपुट = सेल्फ.पूलर(अनुक्रम_आउटपुट) यदि सेल्फ.पूलर कोई और नहीं है एनएक

        यदि नहीं तो return_dict:
            वापसी (अनुक्रम_आउटपुट, पूल्ड_आउटपुट) + एनकोडर_आउटपुट[1:]

        वापसी BaseModelOutputWithPoolingAndCrossAttentions(
            अंतिम_छिपी_स्थिति=अनुक्रम_आउटपुट,
            पूलर_आउटपुट=पूल_आउटपुट,
            Past_key_values=encoder_outputs.past_key_values,
            hidden_states=encoder_outputs.hidden_states,
            ध्यान=एनकोडर_आउटपुट.ध्यान,
            क्रॉस_अटेंशन=एनकोडर_आउटपुट.क्रॉस_अटेंशन,
        )
```

***
### 2.1-बर्टएम्बेडिंग्स
तीन भागों के योग से, हमें प्राप्त होता है:
![बर्ट-एम्बेडिंग](./pictures/3-0-embedding.png) चित्र: बर्ट-एम्बेडिंग

1. वर्ड_एम्बेडिंग, उपरोक्त उपशब्द के अनुरूप एम्बेडिंग।
2. टोकन_टाइप_एम्बेडिंग्स, उस वाक्य का प्रतिनिधित्व करने के लिए उपयोग किया जाता है जिसमें वर्तमान शब्द स्थित है, वाक्यों, पैडिंग और वाक्य जोड़े के बीच अंतर को अलग करने में सहायता के लिए।
3. पोजीशन_एम्बेडिंग, वाक्य में प्रत्येक शब्द की स्थिति एम्बेडिंग, शब्दों के क्रम को अलग करने के लिए उपयोग की जाती है। ट्रांसफार्मर पेपर में डिज़ाइन से भिन्न, इस ब्लॉक को साइनसॉइडल फ़ंक्शन द्वारा गणना की गई एक निश्चित एम्बेडिंग के बजाय प्रशिक्षित किया जाता है। आम तौर पर यह माना जाता है कि यह कार्यान्वयन स्केलेबिलिटी के लिए अनुकूल नहीं है (सीधे लंबे वाक्यों में स्थानांतरित करना मुश्किल है)।

लेयरनॉर्म+ड्रॉपआउट की एक परत से गुजरने के बाद तीन एम्बेडिंग को वजन और आउटपुट के बिना जोड़ा जाता है, आकार (बैच_आकार, अनुक्रम_लंबाई, छिपा हुआ_आकार) होता है।

** [यहां लेयरनॉर्म+ड्रॉपआउट का उपयोग क्यों करें? बैट के स्थान पर लेयरनॉर्म का उपयोग क्यों करें?chनॉर्म? आप एक अच्छे उत्तर का उल्लेख कर सकते हैं: ट्रांसफार्मर अन्य सामान्यीकरण विधियों के बजाय परत सामान्यीकरण का उपयोग क्यों करता है? ](https://www.zhihu.com/question/395811291/answer/1260290120)**


```अजगर
वर्ग BertEmbeddings(nn.मॉड्यूल):
    """शब्द, स्थिति और टोकन_प्रकार एम्बेडिंग से एम्बेडिंग का निर्माण करें।"""

    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)

        # self.LayerNorm TensorFlow मॉडल वेरिएबल नाम के साथ चिपकने और लोड करने में सक्षम होने के लिए साँप-आवरण वाला नहीं है
        # कोई भी TensorFlow चेकपॉइंट फ़ाइल
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)
        # पोजीशन_आईडी (1, लेन पोजीशन एम्ब) मेमोरी में सन्निहित है औरक्रमबद्ध होने पर निर्यात किया गया
        self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
        self.register_buffer("position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)))
        यदि संस्करण.पार्स(मशाल.__संस्करण__) > संस्करण.पार्से("1.6.0"):
            self.register_buffer(
                "टोकन_टाइप_आईडी",
                मशाल.शून्य (स्वयं.स्थिति_आईडी.आकार(), dtype=मशाल.लंबा, डिवाइस=स्वयं.स्थिति_आईडी.डिवाइस),
                लगातार=फ़ाअन्यथा,
            )

    def आगे(
        स्वयं, इनपुट_आईडी=कोई नहीं, टोकन_टाइप_आईडी=कोई नहीं, स्थिति_आईडी=कोई नहीं, इनपुट_एम्बेड्स=कोई नहीं, पास्ट_की_वैल्यू_लेंथ=0
    ):
        यदि इनपुट_आईडी कोई नहीं है:
            इनपुट_आकार = इनपुट_आईडी.आकार()
        अन्य:
            इनपुट_शेप = इनपुट्स_एम्बेड्स.साइज()[:-1]

        seq_length = इनपुट_आकार[1]

        यदि स्थिति_आईडी कोई नहीं है:
            पोजीशन_आईडी = स्व.पोजिशन_आईडी[:, पास्ट_की_वैल्यू_लेंथ : सेक_लेंथ + पास्ट_की_वैल्यू_लेंथ]

        #सेट्टिनटोकन_टाइप_आईड्स को कंस्ट्रक्टर में पंजीकृत बफर में डालें जहां यह सभी शून्य हैं, जो आमतौर पर होता है
        # जब इसका स्वतः-जनरेट होता है, तो पंजीकृत बफ़र टोकन_टाइप_आईडी पास किए बिना मॉडल का पता लगाने में उपयोगकर्ताओं की मदद करता है, हल करता है
        #अंक #5664
        यदि टोकन_टाइप_आईडी कोई नहीं है:
            यदि hasattr(स्वयं, "टोकन_टाइप_आईडी"):
                बफ़र्ड_टोकन_टाइप_आईडी = self.token_type_ids[:, :seq_length]
                बफ़रेड_टोकन_टाइप_आईड्स_विस्तारित = बफ़र्ड_टोकन_टाइप_आईड्स.एक्सपैंड(आईएनपी)ut_shape[0], seq_length)
                टोकन_टाइप_आईडी = बफ़रेड_टोकन_टाइप_आईडी_विस्तारित
            अन्य:
                टोकन_टाइप_आईडी = टॉर्च.जीरोस (इनपुट_शेप, डीटाइप = टॉर्च.लॉन्ग, डिवाइस = सेल्फ.पोजीशन_आईडी.डिवाइस)

        यदि इनपुट_एम्बेड कोई नहीं है:
            इनपुट्स_एम्बेड्स = self.word_embeddings(input_ids)
        टोकन_टाइप_एम्बेडिंग = self.token_type_embeddings(टोकन_टाइप_आईडी)

        एम्बेडिंग = इनपुट_एम्बेडिंग + टोकन_टाइप_एम्बेडिंग
        यदि self.position_embedding_type == "निरपेक्ष":
            स्थिति_एम्बेडिंग = स्व.स्थिति_एम्बेडिंग(स्थिति_आईडी)
            एम्बेडिंग += स्थिति_एम्बेडिंग
        एम्बेडिंग = self.LayerNorm(एम्बेडिंग)
        एम्बेडिंग = स्व.ड्रॉपआउट(एम्बेडिंग)
        वापसी एम्बेडिंग
```

***
### 2.2-बर्टएनकोडर

बर्टलेयर की कई परतों से युक्त, इस भाग के बारे में समझाने के लिए कुछ खास नहीं है, लेकिन एक विवरण का उल्लेख करना उचित है: प्रशिक्षण के दौरान मेमोरी के उपयोग को कम करने के लिए ग्रेडिएंट चेकपॉइंटिंग तकनीक का उपयोग करना।

**ग्रेडिएंट चेकपॉइंटिंग ग्रेडिएंट चेकपॉइंट है। यह सहेजे गए गणना ग्राफ नोड्स को कम करके मॉडल द्वारा कब्जा किए गए स्थान को संपीड़ित करता है, हालांकि, ग्रेडिएंट की गणना करते समय, अनस्टोर किए गए मानों को पुनर्गणना करने की आवश्यकता होती है सबलाइनियर मेमोरी कॉस्ट के साथ"। प्रक्रिया इस प्रकार है: **
![जीरेडिएंट-चेकपॉइंटिंग](./चित्र/3-1-ग्रेडिएंट-चेकपॉइंटिंग.gif) चित्र: ग्रेडिएंट-चेकपॉइंटिंग

BertEncoder में, ग्रेडिएंट चेकपॉइंट को torch.utils.checkpoint.checkpoint के माध्यम से कार्यान्वित किया जाता है, जो उपयोग करने के लिए अधिक सुविधाजनक है: आप दस्तावेज़ का उल्लेख कर सकते हैं: torch.utils.checkpoint - PyTorch 1.8.1 दस्तावेज़ीकरण जटिल, यहाँ कोई विस्तार नहीं।

एक परत और गहराई में जाकर, आप एनकोडर की एक निश्चित परत दर्ज करते हैं:


```अजगर
क्लास बर्टएनकोडर(एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.config = config
        self.layer = nn.ModuleList([BertLayer(config) for _ in रेंज(config.num_hidden_layers)])

    def आगे(
        खुद,
        छुपे_राज्य,
        ध्यान_मास्क=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        एनकोडर_हिडन_स्टेट्स=कोई नहीं,
        एनकोडर_अटेंशन_मास्क=कोई नहीं,
        Past_key_values=कोई नहीं,
        उपयोग_कैश=कोई नहीं,
        आउटपुट_ध्यान=गलत,
        आउटपुट_हिडन_स्टेट्स=गलत,
        return_dict=सत्य,
    ):
        all_hidden_states = () यदि आउटपुट_hidden_states अन्य कोई नहीं
        all_self_attentions = () यदि आउटपुट_ध्यान अन्य कोई नहीं
        ऑल_क्रॉस_एध्यान = () यदि आउटपुट_ध्यान और self.config.add_cross_attention अन्य कोई नहीं

        नेक्स्ट_डिकोडर_कैश = () यदि उपयोग_कैश अन्यथा कोई नहीं
        i के लिए, लेयर_मॉड्यूल एन्यूमरेट (सेल्फ.लेयर) में:
            यदि आउटपुट_हिडन_स्टेट्स:
                all_hidden_states = all_hidden_states + (hidden_states,)

            लेयर_हेड_मास्क = हेड_मास्क[i] यदि हेड_मास्क कोई नहीं है और कोई नहीं
            पास्ट_की_वैल्यू = पास्ट_की_वैल्यू[i] यदि पास्ट_की_वैल्यू कोई नहीं है और कोई नहीं

            यदि getattr(self.cऑनफ़िग, "ग्रेडिएंट_चेकपॉइंटिंग", ग़लत) और स्व.प्रशिक्षण:

                यदि उपयोग_कैश:
                    लकड़हारा.चेतावनी(
                        "`use_cache=True``config.gradient_checkpointing=True` के साथ असंगत है। सेटिंग"
                        "`use_cache=गलत`..."
                    )
                    उपयोग_कैश = गलत

                def create_custom_forward(मॉड्यूल):
                    डीईएफ़ कस्टम_फॉरवर्ड(*इनपुट):
                        रिटर्न मॉड्यूल (*इनपुट, Past_key_मूल्य, आउटपुट_ध्यान)

                    कस्टम_फ़ॉरवर्ड लौटाएँ

                लेयर_आउटपुट = टॉर्च.यूटिल्स.चेकपॉइंट.चेकपॉइंट(
                    create_custom_forward(layer_module),
                    छुपे_राज्य,
                    ध्यान_मास्क,
                    लेयर_हेड_मास्क,
                    एनकोडर_हिडन_स्टेट्स,
                    एनकोडर_अटेंशन_मास्क,
                )
            अन्य:
                लेयर_आउटपुट = लेयर_मॉड्यूल(
                    छुपे_एसटेट्स,
                    ध्यान_मास्क,
                    लेयर_हेड_मास्क,
                    एनकोडर_हिडन_स्टेट्स,
                    एनकोडर_अटेंशन_मास्क,
                    विगत_कुंजी_मूल्य,
                    आउटपुट_ध्यान,
                )

            छुपे हुए राज्य = परत_आउटपुट[0]
            यदि उपयोग_कैश:
                अगला_डिकोडर_कैश += (लेयर_आउटपुट[-1],)
            यदि आउटपुट_ध्यान:
                सभी_स्वयं_ध्यान = सभी_स्वयं_ध्यान + (परत_आउटपुट[1],)
                यदि self.config.add_cross_attention:
                    ऑल_क्रॉस_अटेंशन = ऑल_क्रॉस_अटेंशन + (लेयर_आउटपुट[2],)

        यदि आउटपुट_हिडन_स्टेट्स:
            all_hidden_states = all_hidden_states + (hidden_states,)

        यदि नहीं तो return_dict:
            वापसी टपल(
                वी
                v के लिए [
                    छुपे_राज्य,
                    अगला_डिकोडर_कैश,
                    सभी_छिपे हुए_राज्य,
                    सबका_स्वयं_ध्यान,सभी का ध्यान,
                ]
                यदि v कोई नहीं है
            )
        वापसी BaseModelOutputWithPastAndCrossAttentions(
            अंतिम_हिडन_स्टेट=हिडन_स्टेट्स,
            Past_key_values=next_decoder_cache,
            छुपे हुए राज्य=सभी_छिपे हुए राज्य,
            ध्यान=सभी_स्वयं_ध्यान,
            क्रॉस_अटेंशन=सभी_क्रॉस_अटेंशन,
        )
```

***
#### 2.2.1.1 बर्टअटेंशन

मैंने सोचा था कि ध्यान का कार्यान्वयन यहाँ था, लेकिन मुझे उम्मीद नहीं थी कि कोई अन्य स्तर होगा... उनमें से, स्वयं सदस्य बहु-प्रमुख ध्यान का कार्यान्वयन है।, और आउटपुट सदस्य ध्यान देने के बाद पूरी तरह से जुड़े +ड्रॉपआउट+अवशिष्ट+लेयरनॉर्म संचालन की एक श्रृंखला लागू करता है।

```
क्लास बर्टअटेंशन(एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.self = BertSelfAttention(config)
        self.output = BertSelfOutput(config)
        self.pruned_heads = सेट()
```
आइए पहले इस स्तर पर वापस जाएँ। यहां ऊपर उल्लिखित प्रूनिंग ऑपरेशन आता है, यानी, prune_heads विधि:
```
 def prune_heads(स्वयं, प्रमुख):
        यदि लेन(शीर्ष) == 0:
            वापस करना
        शीर्ष, सूचकांक = find_pruneable_heads_and_indices(
            प्रमुख, स्व.सेलf.num_attention_heads, self.self.attention_head_size, self.pruned_heads
        )

        #प्रून रैखिक परतें
        self.self.query = prune_linear_layer(self.self.query, अनुक्रमणिका)
        self.self.key = prune_linear_layer(self.self.key, अनुक्रमणिका)
        self.self.value = prune_linear_layer(self.self.value, सूचकांक)
        self.output.dense = prune_linear_layer(self.output.dense, Index, dim=1)

        # हाइपर पैराम्स को अपडेट करें और काटे गए हेड्स को स्टोर करें
        self.self.num_attention_heads = self.self.num_attention_heads - लेन(heads)
        स्वयं.स्वयं.सभी_सिर_आकार = स्वयं.स्व.ध्यान_सिर_आकार * स्वयं.स्वयं.संख्या_ध्यान_शीर्ष
        सेल्फ.प्रूनड_हेड्स = सेल्फ.प्रूनड_हेड्स.यूनियन(हेड्स)
```
यहां विशिष्ट कार्यान्वयन को इस प्रकार संक्षेप में प्रस्तुत किया गया है:
- `find_pruneable_heads_and_indices` उस शीर्ष का पता लगाना है जिसे काटने की आवश्यकता है और आयाम सूचकांक जिसे बनाए रखने की आवश्यकता है;

- `prune_linear_layer` सूचकांक के अनुसार Wk/Wq/Wv वजन मैट्रिक्स (पूर्वाग्रह के साथ) में अप्रयुक्त आयामों को एक नए मैट्रिक्स में स्थानांतरित करने के लिए जिम्मेदार है।
अगला मुख्य आकर्षण आता है - आत्म-ध्यान का विशिष्ट कार्यान्वयन।


```अजगर
क्लास बर्टअटेंशन(एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        सेlf.self = BertSelfAttention(config)
        self.output = BertSelfOutput(config)
        self.pruned_heads = सेट()

    def prune_heads(स्वयं, प्रमुख):
        यदि लेन(शीर्ष) == 0:
            वापस करना
        शीर्ष, सूचकांक = find_pruneable_heads_and_indices(
            शीर्ष, स्व.स्व.संख्या_ध्यान_प्रमुख, स्व.स्व.ध्यान_शीर्ष_आकार, स्व.प्रून्ड_सिर
        )

        #प्रून रैखिक परतें
        self.self.query = prune_linear_layer(self.self.query, अनुक्रमणिका)
        self.self.key = prune_linear_परत (स्वयं.स्वयं.कुंजी, अनुक्रमणिका)
        self.self.value = prune_linear_layer(self.self.value, सूचकांक)
        self.output.dense = prune_linear_layer(self.output.dense, Index, dim=1)

        # हाइपर पैराम्स को अपडेट करें और काटे गए हेड्स को स्टोर करें
        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)
        स्वयं.स्वयं.सभी_सिर_आकार = स्वयं.स्व.ध्यान_सिर_आकार * स्वयं.स्वयं.संख्या_ध्यान_शीर्ष
        सेल्फ.प्रूनड_हेड्स = सेल्फ.प्रूनड_हेड्स.यूनियन(हेड्स)

    def आगे(
        खुद,छुपे_राज्य,
        ध्यान_मास्क=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        एनकोडर_हिडन_स्टेट्स=कोई नहीं,
        एनकोडर_अटेंशन_मास्क=कोई नहीं,
        Past_key_value=कोई नहीं,
        आउटपुट_ध्यान=गलत,
    ):
        self_outputs = self.self(
            छुपे_राज्य,
            ध्यान_मास्क,
            सिर पर मुखौटा,
            एनकोडर_हिडन_स्टेट्स,
            एनकोडर_अटेंशन_मास्क,
            विगत_कुंजी_मूल्य,
            आउटपुट_ध्यान,
        )
        ध्यान_आउटपुट = सेlf.output(self_outputs[0],hidden_states)
        आउटपुट = (अटेंशन_आउटपुट,) + सेल्फ_आउटपुट[1:] # यदि हम उन्हें आउटपुट करते हैं तो ध्यान जोड़ें
        वापसी आउटपुट
```

***
##### 2.2.1.1.1 बर्टसेल्फअटेंशन

**चेतावनी: इस क्षेत्र को मॉडल का मुख्य क्षेत्र और सूत्रों को शामिल करने वाला एकमात्र स्थान कहा जा सकता है, इसलिए बहुत सारे कोड पोस्ट किए जाएंगे। **

आरंभीकरण भाग:
```
क्लास बर्टसेल्फअटेंशन(एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        यदि config.hidden_size % config.num_attention_heads != 0 और hasattr नहीं है(config, "embedding_size"):
            वैल्यूएरर बढ़ाएं("छिपा हुआ आकार (%d) ध्यान की संख्या का गुणक नहीं है"
                "प्रमुख (%d)" % (config.hidden_size, config.num_attention_heads)
            )

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        स्व.कुंजी = एनएन.लीनिकट(config.hidden_size, self.all_head_size)
        स्व.मान = nn.रैखिक(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
        यदि self.position_embedding_type == "relative_key" या self.position_embedding_type == "relative_key_query":
            self.max_position_embeddings = config.max_position_embeddings
            self.distance_embएडिंग = एनएन.एंबेडिंग(2 * कॉन्फिग.मैक्स_पोजिशन_एम्बेडिंग्स - 1, सेल्फ.अटेंशन_हेड_साइज)

        self.is_decoder = config.is_decoder
```

- क्वेरी, कुंजी, मान और एक ड्रॉपआउट के परिचित तीन भारों के अलावा, एक रहस्यमय स्थिति_एम्बेडिंग_प्रकार और डिकोडर टैग भी है;
- ध्यान दें किhidden_size और all_head_size प्रारंभ में समान हैं। इस चर को सेट करना अनावश्यक क्यों लगता है - यह स्पष्ट रूप से ऊपर दिए गए प्रूनिंग फ़ंक्शन के कारण है, कई ध्यान शीर्षों को काटने के बाद, all_head_size स्वाभाविक रूप से छोटा हो जाएगा;

-hidden_size num_attention_heads का पूर्णांक गुणज होना चाहिए। उदाहरण के तौर पर bert-base को लेते हुए, प्रत्येक ध्यान में 12 शीर्ष होते हैं,hidden_size 768 है, इसलिए प्रत्येक शीर्ष का आकारध्यान_head_size=768/12=64 है;- पोजीशन_एम्बेडिंग_टाइप क्या है? पता लगाने के लिए पढ़ते रहे।

फिर मुख्य बिंदु है, जो आगे की प्रसार प्रक्रिया है।

सबसे पहले, आइए बहु-प्रमुख आत्म-ध्यान के मूल सूत्र की समीक्षा करें:

$$MHA(Q, K, V) = Concat(head_1, ..., head_h)W^O$$
$$head_i = SDPA(QW_i^Q, KW_i^K, VW_i^V)$$
$$SDPA(Q, K, V) = Softmax(\frac{QK^T}{\sqrt(d_k)})V$$

जैसा कि हम सभी जानते हैं, इन ध्यान शीर्षों की गणना समानांतर में की जाती है, इसलिए उपरोक्त क्वेरी, कुंजी और मूल्य के तीन भार अद्वितीय हैं - ऐसा नहीं है कि सभी शीर्ष भार साझा करते हैं, बल्कि एक साथ "जुड़े" होते हैं।

**[मूल पेपर में मल्टी-हेड का कारण यह है कि मल्टी-हेड ध्यान मॉडल को एक ही ध्यान के साथ विभिन्न प्रतिनिधित्व उप-स्थानों से संयुक्त रूप से जानकारी प्राप्त करने की अनुमति देता हैसिर पर, औसत इसे रोकता है और एक और अधिक विश्वसनीय विश्लेषण है: ट्रांसफार्मर को मल्टी-हेड ध्यान की आवश्यकता क्यों है? ](https://www.zhihu.com/question/341222779/answer/814111138)**

आगे की विधि पर एक नजर डालें:
```
def transpose_for_scores(स्वयं, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        वापसी x.permute(0, 2, 1, 3)

    def आगे(
        खुद,
        छुपे_राज्य,
        ध्यान_मास्क=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        एनकोडर_हिडन_एसटेट्स=कोई नहीं,
        एनकोडर_अटेंशन_मास्क=कोई नहीं,
        Past_key_value=कोई नहीं,
        आउटपुट_ध्यान=गलत,
    ):
        Mixed_query_layer = self.query(hidden_states)

        # क्रॉस-अटेंशन की गणना का हिस्सा छोड़ें
        key_layer = self.transpose_for_scores(self.key(hidden_states))
        वैल्यू_लेयर = self.transpose_for_scores(self.value(hidden_states))
        query_layer = self.transpose_for_scores(mixed_query_layer)

        # कच्ची सामग्री प्राप्त करने के लिए "क्वेरी" और "कुंजी" के बीच डॉट उत्पाद लेंस्कोर पर.
        ध्यान_स्कोर = मशाल.मैटमूल(क्वेरी_लेयर, की_लेयर.ट्रांसपोज़(-1, -2))
        #...
```
यहां `transpose_for_scores` का उपयोग `hidden_size` को कई हेडर आउटपुट के आकार में विभाजित करने और मैट्रिक्स गुणन के लिए मध्य दो आयामों को स्थानांतरित करने के लिए किया जाता है;

यहां `key_layer/value_layer/query_laye`r का आकार है: (बैच_आकार, num_attention_heads, अनुक्रम_लंबाई, ध्यान_head_size);
यहां `अटेंशन_स्कोर्स` का आकार है: (बैच_आकार, संख्या_अटेंशन_हेड्स, अनुक्रम_लंबाई, अनुक्रम_लंबाई), जो एकाधिक शीर्षों की अलग-अलग गणनाओं द्वारा प्राप्त ध्यान मानचित्र के आकार के अनुरूप है।

इस बिंदु पर, कच्चे ध्यान स्कोर प्राप्त करने के लिए K और Q को गुणा किया जाता है, सूत्र के अनुसार, अगला कदम $d_k$ के आधार पर स्केलिंग करना होना चाहिएजी और सॉफ्टमैक्स ऑपरेशन करें। हालाँकि, जो सबसे पहले दिखाई देता है वह एक अजीब स्थितिगत एम्बेडिंग और आइंस्टीन योगों का एक समूह है:

```
 #...
        यदि self.position_embedding_type == "relative_key" या self.position_embedding_type == "relative_key_query":
            seq_length =hidden_states.size()[1]
            पोजीशन_आईडी_एल = टॉर्च.अरेंज(seq_length, dtype=torch.long, डिवाइस=hidden_states.device).view(-1, 1)
            पोजीशन_आईडी_आर = टॉर्च.अरेंज(seq_length, dtype=torch.long, डिवाइस=hidden_states.device).view(1, -1)
            दूरी = स्थिति_ids_l - स्थिति_ids_r
            पोजिशनल_एम्बेडिंग = सेल्फ.डिस्टेंस_एम्बेडिंग(दूरी + सेल्फ.मैक्स_पोजीशन_एम्बेडिंग - 1)
            पोजीशनल_एम्बेडिंग = पोजीशनल_एम्बेडिंग.टू(dtype=query_layer.dtype) # fp16 अनुकूलता

            यदि self.position_embedding_type == "relative_key":
                रिलेटिव_पोजिशन_स्कोर्स = टॉर्च.ईंसम ("बीएचएलडी,एलआरडी->बीएचएलआर", क्वेरी_लेयर, पोजिशनल_एम्बेडिंग)
                ध्यान_स्कोर = ध्यान_स्कोर + सापेक्ष_स्थिति_स्कोर
            एलिफ़self.position_embedding_type == "relative_key_query":
                रिलेटिव_पोजिशन_स्कोर्स_क्वेरी = टॉर्च.ईंसम("बीएचएलडी,एलआरडी->बीएचएलआर", क्वेरी_लेयर, पोजिशनल_एम्बेडिंग)
                रिलेटिव_पोजिशन_स्कोर्स_की = टॉर्च.ईंसम("बीएचआरडी,एलआरडी->बीएचएलआर", की_लेयर, पोजिशनल_एम्बेडिंग)
                ध्यान_स्कोर = ध्यान_स्कोर + सापेक्ष_स्थिति_स्कोर_क्वेरी + सापेक्ष_स्थिति_स्कोर_कुंजी
        #...
```
**[आइंस्टीन सारांश सम्मेलन के लिए, निम्नलिखित दस्तावेज़ देखें: torch.einsum - PyTorch 1.8.1 दस्तावेज़](https://pytorch.org/डॉक्स/स्थिर/जेनरेटेड/torch.einsum.html)**


विभिन्न पोजीशनल_एम्बेडिंग_प्रकारों के लिए, तीन ऑपरेशन हैं:

- निरपेक्ष: डिफ़ॉल्ट मान, इस भाग को संसाधित करने की आवश्यकता नहीं है;
- रिलेटिव_की: की_लेयर को प्रोसेस करें और इसे कुंजी-संबंधित स्थिति एन्कोडिंग के रूप में यहां पोजिशनल_एम्बेडिंग और कुंजी मैट्रिक्स के साथ गुणा करें;
- रिलेटिव_की_क्वेरी: कुंजी और मान दोनों को स्थितीय एन्कोडिंग के रूप में गुणा करें।

सामान्य ध्यान प्रक्रिया पर लौटें:
```
#...
        ध्यान_स्कोर = ध्यान_स्कोर / गणित.वर्ग(स्वयं.ध्यान_शीर्ष_आकार)
        यदि ध्यान_मास्क कोई नहीं है:
            # अटेंशन मास्क लागू करें (बर्टमॉडल फॉरवर्ड() फ़ंक्शन में सभी परतों के लिए पूर्व-गणना की गई)ध्यान_स्कोर = ध्यान_स्कोर + ध्यान_मास्क # यहाँ * के स्थान पर + क्यों है?

        # संभावनाओं पर ध्यान स्कोर को सामान्य करें।
        ध्यान_प्रोब्स = एनएन.सॉफ्टमैक्स(मंद=-1)(ध्यान_स्कोर)

        # यह वास्तव में प्रतीक्षा करने के लिए संपूर्ण टोकन खो रहा है, जो हो सकता है
        # थोड़ा असामान्य लगता है, लेकिन मूल ट्रांसफार्मर पेपर से लिया गया है।
        ध्यान_समस्याएँ = स्व.ड्रॉपआउट(ध्यान_समस्याएँ)

        #चाहें तो सिर पर मास्क लगाएं
        यदि हेड_मास्क कोई नहीं है:
            एध्यान_प्रॉब्स = ध्यान_प्रॉब्स * हेड_मास्क

        सन्दर्भ_परत = मशाल.मैटमूल(ध्यान_प्रोब्स, मूल्य_परत)

        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)

        आउटपुट = (संदर्भ_लेयर, ध्यान_प्रॉब्स) यदि आउटपुट_अटेंशन अन्य (संदर्भ_लेयर,)

        # डिकोडर रिटर्न वैल्यू भाग को छोड़ दें...
        वापसी आउटपुट
```

बड़ा सवाल: यहाँ परEntion_scores = ध्यान_स्कोर + ध्यान_मास्क क्या करता है? क्या इसे मास्क से गुणा नहीं किया जाना चाहिए?
- क्योंकि यहाँ ध्यान_मास्क को [निष्क्रिय रूप से हेरफेर किया गया है], जो भाग मूल रूप से 1 था उसे 0 में बदल दिया गया है, और जो भाग मूल रूप से 0 था (यानी, पैडिंग) एक बड़ी नकारात्मक संख्या बन गया है, इसलिए जोड़ के परिणामस्वरूप एक बड़ा परिणाम मिलता है नकारात्मक मान:
- [एक बड़ी ऋणात्मक संख्या] का उपयोग क्यों करें? क्योंकि इस तरह, सॉफ्टमैक्स ऑपरेशन के बाद, यह शब्द 0 के करीब दशमलव बन जाएगा।

```
(पीडीबी) ध्यान_मास्क
टेंसर([[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],
        [[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],
        [[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],
        ...,
        [[[-0., -0., -0., ..., -10000., -10000., -10000.]]],
        [[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],
        [[[ -0., -0., -0., ..., -10000., -10000., -10000.]]]],
       डिवाइस='क्यूडा:0')
```

तो, यह चरण कहाँ किया जाता है?
उत्तर modeling_bert.py में नहीं मिला, लेकिन एक विशेष वर्ग modeling_utils.py में पाया गया: class ModuleUtilsMixin, और सुराग इसके get_extensed_attention_mask विधि में पाए गए:

```
 डीईएफ़ गेट_एक्सटेंडेड_अटेंशन_मास्क(स्वयं, अटेंशन_मास्क: टेंसर, इनपुट_शेप: ट्यूपल[इंट], डिवाइस: डिवाइस) -> टेंसर:"""
        प्रसारण योग्य ध्यान और कारणात्मक मुखौटे बनाता है ताकि भविष्य और नकाबपोश टोकन को नजरअंदाज कर दिया जाए।

        तर्क:
            ध्यान_मास्क (:obj:`torch.Tensor`):
                ध्यान देने के लिए टोकन का संकेत देने वाले मास्क, अनदेखा करने के लिए टोकन के लिए शून्य।
            इनपुट_शेप (:obj:`Tuple[int]`):
                मॉडल में इनपुट का आकार.
            डिवाइस: (:obj:`torch.device`):
                मॉडल में इनपुट का उपकरण.

        रिटर्न:
            :obj:`टोरch.Tensor` विस्तारित ध्यान मास्क, :obj:`attention_mask.dtype` के समान dtype के साथ।
        """
        # इसका कुछ भाग हटा दें...

        # चूंकि अटेंशन_मास्क उन पदों के लिए 1.0 है जिनमें हम भाग लेना चाहते हैं और उनके लिए 0.0 है
        # नकाबपोश स्थिति, यह ऑपरेशन एक टेंसर बनाएगा जो 0.0 है
        # वे पद जिनमें हम भाग लेना चाहते हैं और नकाबपोश पदों के लिए -10000.0।
        # चूंकि हम इसे सॉफ्टमैक्स से पहले कच्चे स्कोर में जोड़ रहे हैं, यह है
        # प्रभावी रूप से इन्हें पूरी तरह हटाने जैसा ही है।एक्सटेंडेड_अटेंशन_मास्क = एक्सटेंडेड_अटेंशन_मास्क.टू(dtype=self.dtype) # fp16 अनुकूलता
        विस्तारित_ध्यान_मास्क = (1.0 - विस्तारित_ध्यान_मास्क) * -10000.0
        विस्तारित_अटेंशन_मास्क लौटाएं
```

तो, यह समारोह कब बुलाया गया था? इसका बर्टमॉडल से क्या लेना-देना है?
ठीक है, यहां 'बर्टमॉडल' का वंशानुक्रम विवरण आता है: 'बर्टमॉडल' 'बर्टप्रीट्रेन्डमॉडल' से विरासत में मिला है, जो 'प्रीट्रेन्डमॉडल' से विरासत में मिला है, और 'प्रीट्रेन्डमॉडल' तीन आधार वर्गों [एनएन.मॉड्यूल, मॉड्यूलयूटिल्समिक्सिन, जेनरेशनमिक्सिन] से विरासत में मिला है। ——कितना जटिल पैकेज है!

इसका मतलब यह है कि बर्टमॉडल ने किसी मध्यवर्ती चरण में मूल ध्यान_मास्क पर get_extensed_attention_mas को कॉल किया होगा।k, जिससे अटेंशन_मास्क मूल [1, 0] से बदलकर [0, -1e4] हो जाता है।

यह कॉल अंततः बर्टमॉडल (लाइन 944) को फॉरवर्ड पास के दौरान मिलती है:

```
  # हम आयामों का एक आत्म-ध्यान मास्क प्रदान कर सकते हैं [बैच_आकार, from_seq_length, to_seq_length]
        #स्वयं जिस स्थिति में हमें इसे सभी प्रमुखों के लिए प्रसारित करने योग्य बनाने की आवश्यकता है।
        एक्सटेंडेड_अटेंशन_मास्क: टॉर्च.टेन्सर = सेल्फ.गेट_एक्सटेंडेड_अटेंशन_मास्क(अटेंशन_मास्क, इनपुट_शेप, डिवाइस)

```
समस्या हल हो गई: यह विधि न केवल मुखौटे के मूल्य को बदलती है, बल्कि इसे एक ऐसे आकार में प्रसारित करती है जिसे सीधे ध्यान मानचित्र में जोड़ा जा सकता है।
जैसी कि आपसे अपेक्षा थी, हगिंगफेस।

इसके अलावा, उल्लेखनीय विवरणपास होना:

- प्रत्येक शीर्ष के आयामों के अनुसार स्केल, जो बर्ट-बेस के लिए 64 या 8 का वर्गमूल है;
- ध्यान_प्रॉब्स ने न केवल सॉफ्टमैक्स का प्रदर्शन किया, बल्कि ड्रॉपआउट का भी उपयोग किया। क्या ऐसा इसलिए है क्योंकि हम चिंतित हैं कि ध्यान मैट्रिक्स बहुत सघन है... यहां यह भी उल्लेख किया गया है कि यह असामान्य है, लेकिन मूल ट्रांसफार्मर पेपर ने यही किया है;
- हेड_मास्क पहले बताए गए मल्टी-हेड गणना के लिए मास्क है। यदि यह सेट नहीं है, तो डिफ़ॉल्ट सभी 1 है, और यह यहां काम नहीं करेगा;
- context_layer ध्यान मैट्रिक्स और मान मैट्रिक्स का उत्पाद है: मूल आकार है: (बैच_आकार, num_attention_heads, अनुक्रम_लंबाई, ध्यान_head_size);
- context_layer को स्थानांतरित करने और देखने के बाद, आकार को पुनर्स्थापित किया जाता है (बैच_आकार, अनुक्रम_लंबाई, छुपा_आकार)।



```अजगर
क्लास बर्टसेल्फअटेंशन(एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):सुपर().__init__()
        यदि config.hidden_size % config.num_attention_heads != 0 और hasattr नहीं है(config, "embedding_size"):
            वैल्यूएरर बढ़ाएं(
                f"छिपा हुआ आकार ({config.hidden_size}) ध्यान की संख्या का गुणक नहीं है"
                एफ"हेड्स ({config.num_attention_heads})"
            )

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        स्वयं.सर्व_सिर_आकार = स्व.संख्या_ध्यान_शीर्ष * स्व.ध्यान_सिर_आकार

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        स्व.मान = nn.रैखिक(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
        यदि self.position_embedding_type == "relative_key"या self.position_embedding_type == "relative_key_query":
            self.max_position_embeddings = config.max_position_embeddings
            self.distance_embedding = nn.embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)

        self.is_decoder = config.is_decoder

    def transpose_for_scores(स्वयं, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        वापसी x.permute(0, 2, 1, 3)def आगे(
        खुद,
        छुपे_राज्य,
        ध्यान_मास्क=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        एनकोडर_हिडन_स्टेट्स=कोई नहीं,
        एनकोडर_अटेंशन_मास्क=कोई नहीं,
        Past_key_value=कोई नहीं,
        आउटपुट_ध्यान=गलत,
    ):
        Mixed_query_layer = self.query(hidden_states)

        # यदि इसे क्रॉस-अटेंशन मॉड्यूल के रूप में त्वरित किया जाता है, तो कुंजियाँ
        # और मान एक एनकोडर से आते हैं, ध्यान मास्क होना चाहिए;
        # जैसे कि एन्कोडर के पैडिंग टोकन n हैंओटी में भाग लिया.
        is_cross_attention = encoder_hidden_states कोई नहीं है

        यदि is_cross_attention और Past_key_value कोई नहीं है:
            # k,v, क्रॉस_अटेंशन का पुन: उपयोग करें
            key_layer = Past_key_value[0]
            वैल्यू_लेयर = पास्ट_की_वैल्यू[1]
            ध्यान_मास्क = एनकोडर_ध्यान_मास्क
        एलिफ़ is_cross_attention:
            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
            वैल्यू_लेयर = self.transpose_for_scores(self.value(एनकोडर_हिडन_स्टेट्स))
            ध्यान_मास्क = एनकोडर_ध्यान_मास्क
        एलिफ पास्ट_की_वैल्यू कोई नहीं है:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            वैल्यू_लेयर = self.transpose_for_scores(self.value(hidden_states))
            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
            वैल्यू_लेयर = टॉर्च.कैट ([पास्ट_की_वैल्यू[1], वैल्यू_लेयर], डिम=2)
        अन्य:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            वैल्यू_लेयर = self.transpose_for_scores(self.value(hidden_states))

        query_layer = self.transpose_for_scores(mixed_query_layer)

        यदि self.is_decoder:
            # यदि क्रॉस_अटेंशन सभी क्रॉस अटेंशन कुंजी/वैल्यू_स्टेट्स के टुपल (टॉर्च.टेन्सर, टॉर्च.टेन्सर) को सेव करें।
            # क्रॉस_अटेंशन लेयर पर आगे कॉल करने पर सभी क्रॉस-अटेंशन का पुन: उपयोग किया जा सकता है
            # key/value_states (पहला "यदि" केस)
            # यदि यूनिडायरेक्शनल आत्म-ध्यान (डिकोडर) सहेजेंटुपल(टॉर्च.टेन्सर, टॉर्च.टेन्सर) का
            # सभी पिछले डिकोडर कुंजी/वैल्यू_स्टेट्स। एकदिशात्मक आत्म-ध्यान के लिए आगे की कॉल
            # पिछले डिकोडर कुंजी/वैल्यू_स्टेट्स को वर्तमान अनुमानित कुंजी/वैल्यू_स्टेट्स (तीसरा "एलिफ़" केस) से जोड़ सकता है
            # यदि एनकोडर द्वि-दिशात्मक आत्म-ध्यान `past_key_value` हमेशा `None` है
            पास्ट_की_वैल्यू = (की_लेयर, वैल्यू_लेयर)

        # मूल ध्यान स्कोर प्राप्त करने के लिए "क्वेरी" और "कुंजी" के बीच डॉट उत्पाद लें।
        अतention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))

        यदि self.position_embedding_type == "relative_key" या self.position_embedding_type == "relative_key_query":
            seq_length =hidden_states.size()[1]
            पोजीशन_आईडी_एल = टॉर्च.अरेंज(seq_length, dtype=torch.long, डिवाइस=hidden_states.device).view(-1, 1)
            पोजीशन_आईडी_आर = टॉर्च.अरेंज(seq_length, dtype=torch.long, डिवाइस=hidden_states.device).view(1, -1)
            दूरी = स्थिति_आईडी_एल- स्थिति_आईडी_आर
            पोजिशनल_एम्बेडिंग = सेल्फ.डिस्टेंस_एम्बेडिंग(दूरी + सेल्फ.मैक्स_पोजीशन_एम्बेडिंग - 1)
            पोजीशनल_एम्बेडिंग = पोजीशनल_एम्बेडिंग.टू(dtype=query_layer.dtype) # fp16 अनुकूलता

            यदि self.position_embedding_type == "relative_key":
                रिलेटिव_पोजिशन_स्कोर्स = टॉर्च.ईंसम ("बीएचएलडी,एलआरडी->बीएचएलआर", क्वेरी_लेयर, पोजिशनल_एम्बेडिंग)
                ध्यान_स्कोर = ध्यान_स्कोर + सापेक्ष_स्थिति_स्कोर
            एलिफ़ स्व.position_embedding_type == "relative_key_query":
                रिलेटिव_पोजिशन_स्कोर्स_क्वेरी = टॉर्च.ईंसम("बीएचएलडी,एलआरडी->बीएचएलआर", क्वेरी_लेयर, पोजिशनल_एम्बेडिंग)
                रिलेटिव_पोजिशन_स्कोर्स_की = टॉर्च.ईंसम("बीएचआरडी,एलआरडी->बीएचएलआर", की_लेयर, पोजिशनल_एम्बेडिंग)
                ध्यान_स्कोर = ध्यान_स्कोर + सापेक्ष_स्थिति_स्कोर_क्वेरी + सापेक्ष_स्थिति_स्कोर_कुंजी

        ध्यान_स्कोर = ध्यान_स्कोर / गणित.वर्ग(स्वयं.ध्यान_शीर्ष_आकार)
        अगर ध्यान_मास्ककोई नहीं है:
            # अटेंशन मास्क लागू करें (बर्टमॉडल फॉरवर्ड() फ़ंक्शन में सभी परतों के लिए पूर्व-गणना की गई)
            ध्यान_स्कोर = ध्यान_स्कोर + ध्यान_मास्क

        # संभावनाओं पर ध्यान स्कोर को सामान्य करें।
        ध्यान_प्रोब्स = एनएन.सॉफ्टमैक्स(मंद=-1)(ध्यान_स्कोर)

        # यह वास्तव में प्रतीक्षा करने के लिए संपूर्ण टोकन खो रहा है, जो हो सकता है
        # थोड़ा असामान्य लगता है, लेकिन मूल ट्रांसफार्मर पेपर से लिया गया है।
        ध्यान_प्रॉब्स = self.droपाउट(ध्यान_प्रॉब्स)

        #चाहें तो सिर पर मास्क लगाएं
        यदि हेड_मास्क कोई नहीं है:
            ध्यान_प्रॉब्स = ध्यान_प्रॉब्स * हेड_मास्क

        सन्दर्भ_परत = मशाल.मैटमूल(ध्यान_प्रोब्स, मूल्य_परत)

        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)

        आउटपुट = (संदर्भ_लेयर, ध्यान_प्रॉब्स) यदि कहांtput_attentions अन्य (संदर्भ_परत,)

        यदि self.is_decoder:
            आउटपुट = आउटपुट + (past_key_value,)
        वापसी आउटपुट
```

***
##### 2.2.1.1.2 बर्टसेल्फआउटपुट
```
क्लास BertSelfOutput(nn.Module):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)डीईएफ़ फ़ॉरवर्ड(स्वयं, छुपे_स्टेट्स, इनपुट_टेंसर):
        hidden_states = self.dense(hidden_states)
        छुपे हुए राज्य = स्व.ड्रॉपआउट(छिपे हुए राज्य)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        छुपे हुए_स्टेट्स वापस करें
```

**लेयरनॉर्म और ड्रॉपआउट का संयोजन यहां फिर से दिखाई देता है, लेकिन यहां ड्रॉपआउट पहले किया जाता है, और फिर अवशिष्ट कनेक्शन के बाद लेयरनॉर्म किया जाता है। अवशिष्ट कनेक्शन की आवश्यकता क्यों है, इसका सबसे सीधा उद्देश्य बहुत गहरी नेटवर्क परतों के कारण होने वाली प्रशिक्षण कठिनाई को कम करना और इसे मूल इनपुट के प्रति अधिक संवेदनशील बनाना है~**


```अजगर

क्लास BertSelfOutput(nn.Module):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        स्व.घना = एन.एन.रैखिक(config.hidden_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)

    डीईएफ़ फ़ॉरवर्ड(स्वयं, छुपे_स्टेट्स, इनपुट_टेंसर):
        hidden_states = self.dense(hidden_states)
        छुपे हुए राज्य = स्व.ड्रॉपआउट(छिपे हुए राज्य)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        छुपे हुए_स्टेट्स वापस करें
```

***
#### 2.2.1.2 बर्टइंटरमीडिएट

BertAtt देखना समाप्त कियाध्यान दें, ध्यान के पीछे एक पूर्ण कनेक्शन + सक्रियण ऑपरेशन भी है:
```
क्लास बर्टइंटरमीडिएट(एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        यदि isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        अन्य:
            self.intermediate_act_fn = config.hidden_act

    डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):
        hidden_states = self.dense(hidden_states)
        छुपे हुए_सेंटएट्स = self.intermediate_act_fn(hidden_states)
        छुपे हुए_स्टेट्स वापस करें
```

- यहां पूरा कनेक्शन विस्तारित है, उदाहरण के तौर पर बर्ट-बेस को लेते हुए, विस्तारित आयाम 3072 है, जो मूल आयाम 768 का 4 गुना है;
- यहां सक्रियण फ़ंक्शन का डिफ़ॉल्ट कार्यान्वयन गेलु (गॉसियन एरर लाइनर यूनिट्स (जीईएलयूएस)) है। बेशक, इसकी सीधे गणना नहीं की जा सकती है और इसे टैन (छोड़े गए) वाले एक अभिव्यक्ति के साथ अनुमानित किया जा सकता है।


```अजगर
क्लास बर्टइंटरमीडिएट(एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        यदि isinstance(config.hidden_act, str):
            self.intermediate_act_fn= ACT2FN[config.hidden_act]
        अन्य:
            self.intermediate_act_fn = config.hidden_act

    डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        छुपे हुए_स्टेट्स वापस करें
```

***
#### 2.2.1.3 बर्टआउटपुट

यहां एक और पूर्ण कनेक्शन + ड्रॉपआउट + लेयरनॉर्म और एक अवशिष्ट कनेक्शन है: अवशिष्ट कनेक्ट:
```
क्लास बर्टआउटपुट(एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.dense = nn.रैखिक(config.intermediate_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)

    डीईएफ़ फ़ॉरवर्ड(स्वयं, छुपे_स्टेट्स, इनपुट_टेंसर):
        hidden_states = self.dense(hidden_states)
        छुपे हुए राज्य = स्व.ड्रॉपआउट(छिपे हुए राज्य)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        छुपे हुए_स्टेट्स वापस करें
```

यहां यह नहीं कहा जा सकता कि ऑपरेशन का BertSelfOutput से कोई लेना-देना नहीं है, इसे केवल बिल्कुल वैसा ही कहा जा सकता है... दो घटक जिन्हें भ्रमित करना बहुत आसान है।निम्नलिखित सामग्री में BERT-आधारित एप्लिकेशन मॉडल, साथ ही BERT-संबंधित ऑप्टिमाइज़र और उपयोग भी शामिल हैं, जिन्हें अगले लेख में विस्तार से पेश किया जाएगा।


```अजगर
क्लास बर्टआउटपुट(एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)

    डीईएफ़ फ़ॉरवर्ड(स्वयं, छुपे_स्टेट्स, इनपुट_टेंसर):
        hidden_states = self.dense(hidden_states)
        छुपी_स्थितिs = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        छुपे हुए_स्टेट्स वापस करें
```

***
### 2.2.3 बर्टपूलर
यह परत बस वाक्य का पहला टोकन निकालती है, यानी, `[सीएलएस]` के अनुरूप वेक्टर, और फिर पूरी तरह से कनेक्टेड परत और एक सक्रियण फ़ंक्शन से गुज़रने के बाद इसे आउटपुट करती है: (यह हिस्सा वैकल्पिक है, क्योंकि वहां हैं कई पूलिंग अलग-अलग ऑपरेशन)

```
क्लास बर्टपूलर (एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        स्व.सक्रियण = nn.Tanh()

    डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):
        #हम"केवल छिपी हुई स्थिति को लेकर मॉडल को पूल करें
        # पहले टोकन के लिए.
        फर्स्ट_टोकन_टेंसर = छुपे हुए_स्टेट्स[:, 0]
        पूल्ड_आउटपुट = self.dense(first_token_tensor)
        पूल्ड_आउटपुट = स्व.सक्रियण(पूलेड_आउटपुट)
        पूल्ड_आउटपुट लौटाएँ
```


```अजगर
क्लास बर्टपूलर (एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        स्व.सक्रियण = nn.Tanh()

    डेएफ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):
        # हम केवल छिपी हुई स्थिति को लेकर मॉडल को "पूल" करते हैं
        # पहले टोकन के लिए.
        फर्स्ट_टोकन_टेंसर = छुपे हुए_स्टेट्स[:, 0]
        पूल्ड_आउटपुट = self.dense(first_token_tensor)
        पूल्ड_आउटपुट = स्व.सक्रियण(पूलेड_आउटपुट)
        पूल्ड_आउटपुट लौटाएँ
ट्रांसफार्मर.मॉडल.बर्ट.कॉन्फिगरेशन_बर्ट आयात से *
मशाल आयात करें
config = BertConfig.from_pretrained("bert-base-uncased")
बर्ट_पूलर = बर्टपूलर(कॉन्फिग=कॉन्फिग)
प्रिंटी ("बर्ट पूलर आकार में इनपुट: {}"। प्रारूप (config.hidden_size))
बैच_आकार = 1
seq_len = 2
छिपा_आकार = 768
x = टॉर्च.रैंड(बैच_आकार, seq_len, छिपा हुआ_आकार)
y = बर्ट_पूलर(x)
प्रिंट(y.आकार())
```

    बर्ट पूलर आकार में इनपुट: 768
    टॉर्च.आकार([1,768])



```अजगर

```

## सारांश
यह अनुभाग बर्ट मॉडल के कार्यान्वयन का विश्लेषण और अध्ययन करता है। मुझे आशा है कि पाठकों को बर्ट कार्यान्वयन की अधिक विस्तृत जानकारी मिल सकेगी।

यह ध्यान देने योग्य है कि हगिंगफेस द्वारा कार्यान्वित बर्ट मॉडल में, विभिन्न प्रकार की वीडियो मेमोरी सेविंग तकनीकों का उपयोग किया जाता है:

- ग्रेडिएंट चेकपॉइंट, जो आगे के प्रसार नोड्स को बरकरार नहीं रखता है और केवल उपयोग किए जाने पर गणना की जाती है; apply_chunking_to_forward, कई मिनी-बैचों और कम आयामों के अनुसार एफएफएन भाग की गणना करता है
- बर्टमॉडल में जटिल एनकैप्सुलेशन और कई घटक शामिल हैं। बर्ट-बेस को इस रूप में लेंउदाहरण के लिए, मुख्य घटक इस प्रकार हैं:
    - ड्रॉपआउट की कुल संख्या 1+(1+1+1)x12=37 बार दिखाई दी;
    - कुल लेयरनॉर्म 1+(1+1)x12=25 बार प्रदर्शित हुआ;
BertModel में बड़ी संख्या में पैरामीटर हैं। उदाहरण के तौर पर बर्ट-बेस को लेते हुए, इसका पैरामीटर आकार 109M है।

## आभार
यह लेख मुख्य रूप से झेजियांग विश्वविद्यालय के ली लुओकिउ द्वारा लिखा गया है, और इस परियोजना के छात्र इसे व्यवस्थित करने और सारांशित करने के लिए जिम्मेदार हैं।