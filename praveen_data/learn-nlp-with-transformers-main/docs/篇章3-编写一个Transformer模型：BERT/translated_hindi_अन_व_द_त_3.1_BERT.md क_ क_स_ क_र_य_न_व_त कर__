## प्रस्तावना
इस लेख में बहुत सारे स्रोत कोड और स्पष्टीकरण हैं। प्रत्येक मॉड्यूल को पैराग्राफ और क्षैतिज रेखाओं द्वारा विभाजित किया गया है, साथ ही, वेबसाइट प्रत्येक अनुभाग के बीच तेजी से कूदने में मदद करने के लिए एक साइडबार से सुसज्जित है इसे पढ़ने के बाद BERT की समझ। साथ ही, बर्ट स्रोत कोड को चरण दर चरण डीबग करने, संबंधित मॉड्यूल को डीबग करने और फिर स्पष्टीकरण की तुलना करने के लिए pycharm और vscode जैसे टूल का उपयोग करने की अनुशंसा की जाती है।इस अध्याय का tion.

इसमें शामिल ज्यूपिटर को [कोड बेस: अध्याय 3-एक ट्रांसफार्मर मॉडल लिखें: बीईआरटी, डाउनलोड] (https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7) में पाया जा सकता है। %AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AAट्रांसफॉर्मर%E6%A8%A1%E5%9E%8B%EF% बीसी%9एबर्ट)

सीखने के लिए यह अध्याय H[HuggingFace/Transformers, 48.9k Star](https://github.com/huggingface/transformers) पर आधारित होगा। इस अध्याय के सभी कोड [huggingface bert, कृपया ध्यान देंतीव्र संस्करण अपडेट के कारण, इसमें अंतर हो सकता है, कृपया संस्करण 4.4.2 देखें](https://github.com/huggingface/transformers/tree/master/src/transformers/models/bert)HuggingFace एक है चैटबॉट स्टार्टअप सेवा प्रदाता का मुख्यालय न्यूयॉर्क में है। इसने BERT प्रवृत्ति के संकेत को बहुत पहले ही पकड़ लिया और pytorch पर आधारित BERT मॉडल को लागू करना शुरू कर दिया। मूल प्रभाव को पुन: प्रस्तुत करते हुए इस परियोजना को मूल रूप से pytorch-pretrained-bert नाम दिया गया था -उपयोग के तरीकेइस शक्तिशाली मॉडल के आधार पर विभिन्न खेल और अनुसंधान को सुविधाजनक बनाने के लिए।

जैसे-जैसे उपयोगकर्ताओं की संख्या में वृद्धि हुई, परियोजना भी एक बड़े ओपन सोर्स समुदाय में विकसित हुई, जिसमें विभिन्न पूर्व-प्रशिक्षित भाषा मॉडलों को विलय किया गया और टेन्सरफ़्लो कार्यान्वयन जोड़ा गया, और 2019 की दूसरी छमाही में इसका नाम बदलकर ट्रांसफॉर्मर कर दिया गया। इस लेख को लिखने के समय तक ( 30 मार्च, 2021), परियोजना में 43k+ सितारे हैं, यह कहा जा सकता है कि ट्रांसफॉर्मर एक वास्तविक बुनियादी एनएलपी उपकरण बन गया है।

## इस अनुभाग की मुख्य सामग्री
![चित्र: BERT संरचना](./pictures/3-6-bert.png) चित्र: BERT संरचना, स्रोत IrEne: ट्रांसफार्मर के लिए व्याख्या योग्य ऊर्जा भविष्यवाणी

यह लेख ट्रांसफॉर्मर्स संस्करण 4.4.2 (19 मार्च, 2021 को जारी) परियोजना के पाइटोरच संस्करण के बीईआरटी-संबंधित कोड पर आधारित है, और कोड संरचना, विशिष्ट कार्यान्वयन और सिद्धांत और उपयोग के परिप्रेक्ष्य से इसका विश्लेषण करता है।
मुख्य सामग्री:

1. BERT टोकनाइजेशन शब्द विभाजन मॉडल (BertTokenizer)

2. BERT मॉडल ऑन्टोलॉजी मॉडल (BertModएल)

- बर्टएम्बेडिंग्स

- बर्टएनकोडर

- बर्टलेयर

- बर्टअटेंशन

- बर्टइंटरमीडिएट

- बर्टआउटपुट

- बर्टपूलर

***
## 1-टोकनीकरण शब्द विभाजन-बर्टटोकनाइज़र
BERT से संबंधित टोकननाइज़र मुख्य रूप से [`models/bert/tokenization_bert.py`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py) में लिखा गया है।

```अजगर
आयात संग्रह
ओएस आयात करें
आयातयूनिकोडडेटा
आयात सूची, वैकल्पिक, टपल टाइप करने से

ट्रांसफॉर्मर.टोकनाइजेशन_यूटी सेआईएलएस प्रीट्रेन्डटोकनाइज़र, _is_control, _is_punctuation, _is_whitespace आयात करता है
ट्रांसफॉर्मर.यूटिल्स से लॉगिंग आयात करें

लकड़हारा = लॉगिंग.get_logger(__name__)

VOCAB_FILES_NAMES = {"vocab_file" : "vocab.txt"}

PRETRAINED_VOCAB_FILES_MAP = {
"vocab_file": {
"बर्ट-बेस-अनकेस्ड": "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt",
}
}

PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {
"बर्ट-बेस-अनकेस्ड": 512,
}

PRETRAINED_INIT_CONFIGURATION = {
"बर्ट-बेस-अनकेस्ड": {"do_lower_case": True},
}डीईएफ़ लोड_वोकैब(वोकैब_फ़ाइल):
"""एक शब्दावली फ़ाइल को एक शब्दकोश में लोड करता है।"""
शब्दावली = संग्रह.ऑर्डर्डडिक्ट()
रीडर के रूप में open(vocab_file, "r", एन्कोडिंग = "utf-8") के साथ:
टोकन = रीडर.रीडलाइन्स()
सूचकांक के लिए, गणना में टोकन (टोकन):
टोकन = टोकन.rstrip("\n")
शब्दकोष[टोकन] = अनुक्रमणिका
वापसी शब्दावली

डीईएफ़ व्हाइटस्पेस_टोकनाइज़(पाठ):
"""पाठ के एक टुकड़े पर मूल रिक्त स्थान की सफाई और विभाजन चलाता है।""
टेक्स्ट = टेक्स्ट.स्ट्रिप()
यदि पाठ नहीं है:
वापस करना []
टोकन = text.split()
टोकन लौटाएं

क्लास बर्टटोकनाइज़र(पीपुनःप्रशिक्षितटोकनाइज़र):

vocab_files_names = VOCAB_FILES_NAMES
pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION
max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES

def __init__(
खुद,
vocab_file,
do_lower_case=सत्य,
do_basic_tokenize=सत्य,
कभी_विभाजित=कोई नहीं,
unk_token='[UNK]',
sep_token='[SEP]',
पैड_टोकन='[पीएडी]',
cls_token='[सीएलएस]',
मास्क_टोकन='[मास्क]',
टोकननाइज़_चीनी_चार्स=सत्य,
स्ट्रिप_एक्सेंट=कोई नहीं,
**क्वार्ग्स
):
बहुत अच्छा()।__इस में__(
do_lower_case=do_lower_case,
do_basic_tokenize=do_basic_tokenize,
कभी_विभाजित नहीं = कभी_विभाजित नहीं,
अनक_टोकन=अनक_टोकन,
sep_token=sep_token,
पैड_टोकन=पैड_टोकन,
cls_token=cls_token,
मास्क_टोकन=मास्क_टोकन,
टोकननाइज_चीनी_चार्स=टोकनाइज_चीनी_चार्स,
स्ट्रिप_एक्सेंट=स्ट्रिप_एक्सेंट,
**क्वार्ग्स,
)

यदि नहीं os.path.isfile(vocab_file):
वैल्यूएरर बढ़ाएं(
f"पथ '{vocab_file}' पर एक शब्दावली फ़ाइल नहीं मिल सकी। Google पूर्व-प्रशिक्षित से शब्दावली लोड करने के लिए"
"मॉडल उपयोग `टोकनाइज़र = BertTokenizer.from_पूर्वप्रशिक्षित(PRETRAINED_MODEL_NAME)`"
)
सेल्फ.वोकैब = लोड_वोकैब(वोकैब_फाइल)
self.ids_to_tokens =collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])
self.do_basic_tokenize = do_basic_tokenize
यदि do_basic_tokenize:
self.basic_tokenizer = BasicTokenizer(
do_lower_case=do_lower_case,
कभी_विभाजित नहीं = कभी_विभाजित नहीं,
टोकननाइज_चीनी_चार्स=टोकनाइज_चीनी_चार्स,
स्ट्रिप_एक्सेंट=स्ट्रिप_एक्सेंट,
)
सेल्फ.वर्डपीस_टोकनाइज़र = वर्डपीसटोकनाइज़र(वोकैब=सेल्फ.वोकैब, अनक_टोकन=सेल्फ.अनक_टोकन)

@प्रॉपerty
def do_lower_case(स्वयं):
self.basic_tokenizer.do_lower_case लौटाएँ

@संपत्ति
def vocab_size(स्वयं):
वापसी लेन(स्वयं.वोकैब)

def get_vocab(स्वयं):
रिटर्न डिक्ट (सेल्फ.वोकैब, **सेल्फ.एडेड_टोकेंस_एनकोडर)

def _टोकनाइज़(स्वयं, पाठ):
स्प्लिट_टोकन = []
यदि self.do_basic_tokenize:
self.basic_tokenizer.tokenize(text, ever_split=self.all_special_tokens) में टोकन के लिए:

# यदि टोकन नेवर_स्प्लिट सेट का हिस्सा है
यदि self.basic_tokenizer.never_split में टोकन:
स्प्लिट_टोकन.जोड़ें(टोकन)
अन्य:
स्प्लिट_टोकनs += self.wordPiece_tokenizer.tokenize(टोकन)
अन्य:
स्प्लिट_टोकेंस = self.wordPiece_tokenizer.tokenize(पाठ)
स्प्लिट_टोकन लौटाएँ

def _convert_token_to_id(स्वयं, टोकन):
"""वोकैब का उपयोग करके एक टोकन (str) को एक आईडी में परिवर्तित करता है।"""
self.vocab.get लौटाएं(टोकन, self.vocab.get(self.unk_token))

def _convert_id_to_token(स्वयं, अनुक्रमणिका):
"""शब्दावली का उपयोग करके एक सूचकांक (पूर्णांक) को एक टोकन (str) में परिवर्तित करता है।""
self.ids_to_tokens.get लौटाएं(सूचकांक, self.unk_token)

def Convert_tokens_to_string(स्वयं, टोकन):
"""परिवर्तन करता हैएक ही स्ट्रिंग में टोकन (स्ट्रिंग) का एक क्रम।"""
आउट_स्ट्रिंग = " ".join(टोकन).replace(" ##", "").strip()
वापसी_स्ट्रिंग

def build_inputs_with_special_tokens(
स्वयं, टोकन_आईडी_0: सूची[int], टोकन_आईडी_1: ​वैकल्पिक[सूची[int]] = कोई नहीं
) -> सूची[int]:
"""
अनुक्रम वर्गीकरण कार्यों के लिए अनुक्रम या अनुक्रम की एक जोड़ी से मॉडल इनपुट बनाएं
विशेष टोकन जोड़ने पर BERT अनुक्रम का प्रारूप निम्न होता है:
- एकल अनुक्रम: ``[सीएलएस] एक्स [सितंबर]``
- अनुक्रमों की जोड़ी: ``[सीएलएस] ए [सितंबर] बी [सितंबर]``
तर्क:
टोकन_आईडी_0 (:obj:`सूची[int]`):
उन आईडी की सूची जिनमें विशेष टोकन जोड़े जाएंगे।
टोकन_आईडी_1 (:obj:`सूची[int]`, `वैकल्पिक`):
अनुक्रम जोड़े के लिए आईडी की वैकल्पिक दूसरी सूची।
रिटर्न:
:obj:`List[int]`: उपयुक्त विशेष टोकन के साथ `इनपुट आईडी <../glosary.html#input-ids>`__ की सूची।
"""
यदि टोकन_आईडी_1 कोई नहीं है:
वापसी [self.cls_token_id] + टोकन_ids_0 + [self.sep_token_id]
सीएलएस = [self.cls_token_id]
सितम्बर = [self.sep_token_id]
वापसी सीएलएस + टोकन_आईडी_0 + सितंबर +टोकन_आईडी_1 + सितंबर

डीईएफ़ get_special_tokens_mask(
स्वयं, टोकन_आईडी_0: सूची[int], टोकन_आईडी_1: ​वैकल्पिक[सूची[int]] = कोई नहीं, पहले से ही_विशेष_टोकन है: बूल = गलत
) -> सूची[int]:
"""
किसी टोकन सूची से अनुक्रम आईडी प्राप्त करें जिसमें विशेष टोकन हों क्योंकि कोई विशेष टोकन नहीं जोड़ा गया है। जोड़ते समय इस विधि को कहा जाता है
टोकननाइज़र ``prepare_for_model`` विधि का उपयोग करके विशेष टोकन।
तर्क:
टोकन_आईडी_0 (:obj:`सूची[int]`):
आईडी की सूची.
टोकन_आईडी_1 (:obj:`सूची[int]`, `वैकल्पिक`):
एस के लिए आईडी की वैकल्पिक दूसरी सूचीअनुक्रम जोड़े.
पहले से ही_विशेष_टोकन हैं (:obj:`बूल`, `वैकल्पिक`, डिफ़ॉल्ट:obj:`गलत`):
टोकन सूची पहले से ही मॉडल के लिए विशेष टोकन के साथ स्वरूपित है या नहीं।
रिटर्न:
:obj:`सूची[int]`: श्रेणी में पूर्णांकों की एक सूची [0, 1]: एक विशेष टोकन के लिए 1, अनुक्रम टोकन के लिए 0।
"""

यदि पहले से ही_विशेष_टोकन हैं:
वापसी सुपर().get_special_tokens_mask(
टोकन_आईडी_0=टोकन_आईडी_0, टोकन_आईडी_1=टोकन_आईडी_1, पहले से ही_विशेष_टोकन=सही है
)

यदि टोकन_आईडी_1 कोई नहीं है:
वापसी [1] + ([0]* लेन(टोकन_आईडी_0)) + [1] + ([0] * लेन(टोकन_आईडी_1)) + [1]
वापसी [1] + ([0] * लेन(टोकन_आईडी_0)) + [1]

def create_token_type_ids_from_sequences(
स्वयं, टोकन_आईडी_0: सूची[int], टोकन_आईडी_1: ​वैकल्पिक[सूची[int]] = कोई नहीं
) -> सूची[int]:
"""
अनुक्रम-जोड़ी वर्गीकरण कार्य में उपयोग के लिए पारित दो अनुक्रमों से एक मास्क बनाएं
पेयर मास्क का प्रारूप निम्न है:
::
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1
|. पहला क्रम |. दूसरा क्रम |यदि :obj:`token_ids_1` है:obj:`None`,यह विधि केवल मास्क का पहला भाग (0s) लौटाती है।
तर्क:
टोकन_आईडी_0 (:obj:`सूची[int]`):
आईडी की सूची.
टोकन_आईडी_1 (:obj:`सूची[int]`, `वैकल्पिक`):
अनुक्रम जोड़े के लिए आईडी की वैकल्पिक दूसरी सूची।
रिटर्न:
:obj:`सूची[int]`: दिए गए अनुसार `टोकन प्रकार आईडी <../glosary.html#token-type-ids>`_ की सूची
क्रम.
"""
सितम्बर = [self.sep_token_id]
सीएलएस = [self.cls_token_id]
यदि टोकन_आईडी_1 कोई नहीं है:
वापसी लेन(सीएलएस + टोकन_आईडी_0 + सितंबर) * [0]
रिटर्न लेन (सीएलएस + टोकन_आईड्स_0 + सितंबर) * [0] + लेन (टूken_ids_1 + सितम्बर) * [1]

def save_vocabulary(स्वयं, save_directory: str, फ़ाइल नाम_उपसर्ग: वैकल्पिक[str] = कोई नहीं) -> Tuple[str]:
सूचकांक = 0
यदि os.path.isdir(save_directory):
vocab_file = os.path.join(
save_directory, (फ़ाइल नाम_उपसर्ग + "-" यदि फ़ाइल नाम_उपसर्ग अन्यथा "") + VOCAB_FILES_NAMES["vocab_file"]
)
अन्य:
vocab_file = (फ़ाइल नाम_उपसर्ग + "-" यदि फ़ाइल नाम_उपसर्ग अन्यथा "") + save_directory
लेखक के रूप में open(vocab_file, "w", एन्कोडिंग = "utf-8") के साथ:
टोकन के लिए, sorted(self.vocab.items(), key=l में टोकन_इंडेक्सअंब्दा केवी: केवी[1]):
यदि सूचकांक != टोकन_सूचकांक:
लकड़हारा.चेतावनी(
f"शब्दावली को {vocab_file} में सहेजा जा रहा है: शब्दावली सूचकांक लगातार नहीं हैं।"" कृपया जांचें कि शब्दावली दूषित नहीं है!"
)
सूचकांक = टोकन_सूचकांक
लेखक.लिखें(टोकन + "\n")
सूचकांक += 1
वापसी (vocab_file,)

क्लास बेसिकटोकनाइज़र(ऑब्जेक्ट):

def __init__(स्वयं, do_lower_case=सही, कभी_स्प्लिट=कोई नहीं, टोकननाइज_चीनी_चार्स=सही, स्ट्रिप_एक्सेंट=कोई नहीं):
यदि नेवर_स्प्लिट कोई नहीं है:
कभी_विभाजित नहीं = []
self.do_lower_case = do_lower_case
self.never_splयह = सेट (कभी_विभाजित नहीं)
स्व.टोकनाइज_चीनी_चार्स = टोकनाइज_चीनी_चार्स
स्वयं.स्ट्रिप_एक्सेंट = स्ट्रिप_एक्सेंट

डीईएफ़ टोकननाइज़ (स्वयं, पाठ, कभी_स्प्लिट = कोई नहीं):
"""
पाठ के एक टुकड़े का मूल टोकनाइजेशन, उप-शब्द टोकनाइजेशन के लिए केवल "सफेद रिक्त स्थान" पर विभाजित करें, देखें
वर्डपीसटोकनाइज़र।
तर्क:
**never_split**: (`वैकल्पिक`) स्ट्र की सूची
पिछड़े संगतता उद्देश्यों के लिए रखा गया। अब सीधे बेस क्लास स्तर पर लागू किया गया है (देखें:func:`PreTrainedTokenizer.tokenize`) विभाजित न होने वाले टोकन की सूची।
"""
#संघ() दो सेटों को जोड़कर एक नया सेट लौटाता है।
नेवर_स्प्लिट = स्व.नेवर_स्प्लिट.यूनियन(सेट(नेवर_स्प्लिट)) यदि नेवर_स्प्लिट नहीं तो स्व.नेवर_स्प्लिट
पाठ = self._clean_text(पाठ)

# इसे बहुभाषी और चीनी के लिए 1 नवंबर, 2018 को जोड़ा गया था
# मॉडल। यह अब अंग्रेजी मॉडल पर भी लागू होता है, लेकिन ऐसा नहीं है
# मामला यह है कि अंग्रेजी मॉडलों को किसी भी चीनी डेटा पर प्रशिक्षित नहीं किया गया था
# और आम तौर पर उनमें कोई चीनी डेटा नहीं होता (चीनी होते हैं)।
#शब्दावली में वर्ण क्योंकिविकिपीडिया में कुछ चीनी भाषाएँ हैं
अंग्रेजी विकिपीडिया में # शब्द।)
यदि self.tokenize_chinese_chars:
पाठ = self._tokenize_chinese_chars(पाठ)
मूल_टोकन = व्हाइटस्पेस_टोकनाइज़(पाठ)
स्प्लिट_टोकन = []
मूल_टोकन में टोकन के लिए:
यदि टोकन नेवर_स्प्लिट में नहीं है: यदि self.do_lower_case:
टोकन = टोकन.निचला()
यदि self.strip_accents ग़लत नहीं है:
टोकन = self._run_strip_accents(टोकन)
एलिफ सेल्फ.स्ट्रिप_एक्सेंट:
टोकन = self._run_strip_accents(टोकन)
स्प्लिट_टोकेंस.एक्सटेंड(स्वयं._रन_स्प्लिट_ऑन_पंक(टोकन, नेवर_स्प्लिटी))

आउटपुट_टोकेंस = व्हाईटस्पेस_टोकनाइज़(" ".join(split_tokens))
आउटपुट_टोकन लौटाएँ

def _run_strip_accents(स्वयं, पाठ):
"""पाठ के एक टुकड़े से उच्चारण को अलग करता है।"""
टेक्स्ट = यूनिकोडडेटा.नॉर्मलाइज़ ("एनएफडी", टेक्स्ट)
आउटपुट = []
पाठ में चार के लिए:
बिल्ली = यूनिकोडडेटा.श्रेणी(चार)
अगर बिल्ली == "एमएन":
जारी रखना
आउटपुट.एपेंड (चार)
वापसी ".join(आउटपुट)

def _run_split_on_punc(स्वयं, पाठ, कभी_स्प्लिट=कोई नहीं):
"""विराम चिह्न को पाठ के एक टुकड़े पर विभाजित करता है।"""
यदि नेवर_स्प्लिट कोई नहीं है और नेवर_स्प्लिट में टेक्स्ट है:
वापसी[मूलपाठ]
वर्ण = सूची(पाठ)
मैं = 0
प्रारंभ_नया_शब्द = सत्य
आउटपुट = []
जबकि मैं <लेन(वर्ण):
चार = वर्ण[i]
यदि _विराम चिह्न (चार) है:
आउटपुट.एपेंड([चार])
प्रारंभ_नया_शब्द = सत्य
अन्य:
यदि प्रारंभ_नया_शब्द:
आउटपुट.जोड़ें([])
प्रारंभ_नया_शब्द = ग़लत
आउटपुट[-1].जोड़ें(चार)
मैं += 1

वापसी [""। आउटपुट में x के लिए जुड़ें (x)]

def _tokenize_chinese_chars(स्वयं, पाठ):
"""किसी भी सीजेके चरित्र के आसपास रिक्त स्थान जोड़ता है।"""
आउटपुट = []
पाठ में चार के लिए:
सीपी = ऑर्ड(चार)
यदि self._is_chinese_char(cp):
आउटपुट.एपेंड("")
आउटपुट.एपीपेंड(चार)
आउटपुट.एपेंड("")
अन्य:
आउटपुट.एपेंड(चार)
वापसी ".join(आउटपुट)

def _is_chinese_char(स्वयं, सीपी):
"""जांचता है कि क्या सीपी सीजेके वर्ण का कोडपॉइंट है।""
# यह CJK यूनिकोड ब्लॉक में किसी भी चीज़ के रूप में "चीनी वर्ण" को परिभाषित करता है:
# https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
#
# ध्यान दें कि सीजेके यूनिकोड ब्लॉक में सभी जापानी और कोरियाई अक्षर नहीं हैं,
# अपने नाम के बावजूद आधुनिक कोरियाई हंगुल वर्णमाला एक अलग ब्लॉक है,
# जैसा कि जापानी हीरागाना हैऔर काताकाना। उन अक्षरों का उपयोग लिखने के लिए किया जाता है
#शब्दों को स्थान-विभाजित किया गया है, इसलिए उनका विशेष रूप से इलाज और प्रबंधन नहीं किया जाता है
# अन्य सभी भाषाओं की तरह।
अगर (
(सीपी >= 0x4ई00 और सीपी <= 0x9एफएफएफ)
या (cp >= 0x3400 और cp <= 0x4DBF) #
या (cp >= 0x20000 और cp <= 0x2A6DF) #
या (cp >= 0x2A700 और cp <= 0x2B73F) #
या (cp >= 0x2B740 और cp <= 0x2B81F) #
या (cp >= 0x2B820 और cp <= 0x2CEAF) #
या (cp >= 0xF900 और cp <= 0xFAFF)
या (cp >= 0x2F800 और cp <= 0x2FA1F) #
): #वापसी सत्य

विवरण झूठा है

डीईएफ़ _सीएलean_text(स्वयं, पाठ):
"""पाठ पर अमान्य वर्ण हटाने और रिक्त स्थान साफ़ करने का कार्य करता है।"""
आउटपुट = []
पाठ में चार के लिए:
सीपी = ऑर्ड(चार)
यदि सीपी == 0 या सीपी == 0xएफएफएफडी या _is_control(चार):
जारी रखना
यदि_व्हाइटस्पेस(चार) है:
आउटपुट.एपेंड("")
अन्य:
आउटपुट.एपेंड (चार)
वापसी ".join(आउटपुट)

क्लास वर्डपीसटोकनाइज़र(ऑब्जेक्ट):
"""वर्डपीस टोकनाइजेशन चलाता है।""

def __init__(self, vocab, unk_token, max_input_chars_per_word=100):
स्व.शब्दावली=शब्दावली
self.unk_token = unk_token
self.max_input_chars_per_शब्द = अधिकतम_इनपुट_चार_प्रति_शब्द

डीईएफ़ टोकनेनाइज़ (स्वयं, पाठ):
"""
पाठ के एक टुकड़े को उसके शब्द टुकड़ों में टोकनाइज़ करता है। यह प्रदर्शन करने के लिए एक लालची सबसे लंबे-मिलान-पहले एल्गोरिदम का उपयोग करता है
दी गई शब्दावली का उपयोग करके टोकनीकरण।
उदाहरण के लिए, :obj:`input = "unaffable"` आउटपुट के रूप में वापस आएगा :obj:`["un", "##aff", "##able"]`।
तर्क:
पाठ: एक एकल टोकन या रिक्त स्थान से अलग टोकन होना चाहिए
पहले ही `बेसिकटोकनाइज़र` से गुजर चुका है।
रिटर्न:
वर्डपीस टोकन की एक सूची.
"""

आउटपुट_टोकन = []
टोकन के लिएn व्हाईटस्पेस_टोकनाइज़(टेक्स्ट) में:
वर्ण = सूची(टोकन)
यदि लेन(वर्ण) > self.max_input_chars_per_word:
आउटपुट_टोकन.जोड़ें(स्वयं.अंक_टोकन)जारी रखें

बुरा है = ग़लत
प्रारंभ=0
उप_टोकन = []
जबकि प्रारंभ <लेन(वर्ण):
अंत = लेन(वर्ण)
cur_substr = कोई नहीं
जबकि प्रारंभ <अंत:
सबस्ट्र = ".जुड़ें(वर्ण[प्रारंभ:अंत])
यदि प्रारंभ > 0:
सबस्ट्र = "##" + सबस्ट्र
यदि self.vocab में सबस्ट्र:
cur_substr = पदार्थ
ब्रेकेंड -= 1
यदि cur_substr कोई नहीं है:
बुरा है = सत्य है
तोड़ना
उप_टोकन.जोड़ें(cur_substr)
प्रारंभ=अंत

यदि खराब है:
आउटपुट_टोकन.जोड़ें(स्वयं.अंक_टोकन)
अन्य:
आउटपुट_टोकन.विस्तार(उप_टोकन)
आउटपुट_टोकन लौटाएँ
```

```
क्लास बर्टटोकनाइज़र(प्रीट्रेन्डटोकनाइज़र):
"""
वर्डपीस के आधार पर एक BERT टोकनाइज़र का निर्माण करें।

यह टोकननाइज़र :class:`~transformers.PreTrainedTokenizer` से विरासत में मिला है जिसमें अधिकांश मुख्य विधियाँ शामिल हैं।
उन तरीकों के बारे में अधिक जानकारी के लिए उपयोगकर्ताओं को इस सुपरक्लास को देखना चाहिए।
...
"""
```

'बर्टटोकनाइज़र' 'बेसिकटोकनाइज़र' और 'वर्डपीसटोकनाइज़र' पर आधारित एक टोकननाइज़र है:
-बेसिकटोकनizer प्रसंस्करण के पहले चरण के लिए जिम्मेदार है - विराम चिह्न, रिक्त स्थान आदि द्वारा वाक्यों को विभाजित करना, और यह संभालना कि क्या लोअरकेस को एकीकृत करना है और अवैध वर्णों को साफ़ करना है।
- चीनी अक्षरों के लिए, प्रीप्रोसेसिंग (रिक्त स्थान जोड़कर) के माध्यम से शब्द द्वारा विभाजित;
- साथ ही, आप निर्दिष्ट कर सकते हैं कि कुछ शब्द नेवर_स्प्लिट के माध्यम से विभाजित नहीं हैं;
- यह चरण वैकल्पिक है (डिफ़ॉल्ट रूप से निष्पादित)।
- WordPieceTokenizer शब्दों के आधार पर शब्दों को उपशब्दों में विघटित करता है।
- उपशब्द चार और वर् के बीच हैडी, अर्थात, शब्द के अर्थ को एक निश्चित सीमा तक बनाए रखते हुए, यह अंग्रेजी में अपंजीकृत शब्दों की एकवचन और बहुवचन, काल और OOV (आउट-ऑफ-वोकैबुलरी) समस्याओं के कारण होने वाले शब्द सूची विस्फोट को भी ध्यान में रख सकता है। और मूल और काल प्रत्ययों को अलग करें, जिससे शब्द सूची और प्रशिक्षण की कठिनाई कम हो जाए;
- उदाहरण के लिए, टोकननाइज़र शब्द को दो भागों में विघटित किया जा सकता है: "टोकन" और "##ाइज़र"। ध्यान दें कि बाद वाले शब्द के "##" का अर्थ है कि यह इससे जुड़ा हैपिछला शब्द.
BertTokenizer में निम्नलिखित सामान्य विधियाँ हैं:
- from_pretrained: एक शब्दावली फ़ाइल (vocab.txt) वाली निर्देशिका से एक टोकननाइज़र प्रारंभ करें;
- टोकननाइज़: किसी पाठ (शब्द या वाक्य) को उपशब्दों की सूची में विघटित करें;
- Convert_tokens_to_ids: सबवर्ड की सूची को सबवर्ड सबस्क्रिप्ट की सूची में बदलें;
- Convert_ids_to_tokens: पिछले वाले के विपरीत;
- Convert_tokens_to_string: उपशब्द सूची को "##" द्वारा किसी शब्द या वाक्य में वापस जोड़ें;
- एन्कोड: एकल एस के लिएवाक्य इनपुट, शब्द को विघटित करें और "[सीएलएस], एक्स, [एसईपी]" की संरचना बनाने के लिए विशेष शब्द जोड़ें और इसे दो वाक्य इनपुट के लिए शब्दावली सबस्क्रिप्ट की सूची में परिवर्तित करें (कई वाक्यों के लिए केवल पहले दो), शब्द को विघटित करें और "[सीएलएस], एक्स1, [एसईपी], एक्स2, [एसईपी]" की संरचना बनाने के लिए विशेष शब्द जोड़ें और इसे एक सबस्क्रिप्ट सूची में परिवर्तित करें - डिकोड: एनकोड विधि के आउटपुट को बदल दिया जा सकता है एक पूरा वाक्य और कक्षा की अपनी विधियाँ: ```पायथन बीटी = BertTokenizer.from_pretrainएड('बर्ट-बेस-अनकेस्ड') बीटी('मुझे प्राकृतिक भाषा की प्रगति पसंद है!') # {'इनपुट_आईडी': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'टोकन_टाइप_आईडी': [ 0, 0, 0, 0, 0, 0, 0, 0], 'अटेंशन_मास्क': [1, 1, 1, 1, 1, 1, 1, 1]} ```

डाउनलोडिंग: 100%|██████████|232k/232k [00:00<00:00, 698kB/s]
डाउनलोडिंग: 100%|██████████|28.0/28.0 [00:00<00:00, 11.1kB/s]
डाउनलोडिंग: 100%|██████████|466k/466k [00:00<00:00, 863kB/s]

{'इनपुट_आईडी': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'टोकन_टाइप_आईडी': [0,0, 0, 0, 0, 0, 0, 0], 'अटेंशन_मास्क': [1, 1, 1, 1, 1, 1, 1, 1]}

***
## 2-मॉडल-बर्टमॉडल
और BERT मॉडल से संबंधित कोड मुख्य रूप से [`/models/bert/modeling_bert.py`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert) में लिखा गया है .py), जिसमें कोड की एक हजार से अधिक लाइनें हैं, जिसमें BERT मॉडल की मूल संरचना और उस पर आधारित फाइन-ट्यूनिंग मॉडल शामिल है।

आइए BERT मॉडल बॉडी के विश्लेषण से शुरुआत करें:
```
क्लास बर्टमॉडल(बीertPreTrainedModel):"""

मॉडल एक एनकोडर (केवल आत्म-ध्यान के साथ) के साथ-साथ एक डिकोडर के रूप में भी व्यवहार कर सकता है, जिस स्थिति में एक परत
'ध्यान है' में वर्णित वास्तुकला का पालन करते हुए, आत्म-ध्यान परतों के बीच क्रॉस-अटेंशन जोड़ा जाता है
आपको बस इतना ही चाहिए <https://arxiv.org/abs/1706.03762>`__ आशीष वासवानी, नोम शज़ीर, निकी परमार, जैकब उस्ज़कोरिट द्वारा,
लिलियन जोन्स, एडन एन. गोमेज़, लुकाज़ कैसर और इलिया पोलोसुखिन।

एक डिकोडर के रूप में व्यवहार करने के लिए मॉडल को :obj के साथ आरंभ करने की आवश्यकता है:कॉन्फ़िगरेशन का `is_decoder` तर्क
:obj:`True` पर सेट करें Seq2Seq मॉडल में उपयोग करने के लिए, मॉडल को :obj:`is_decoder` दोनों के साथ प्रारंभ करने की आवश्यकता है।
तर्क और :obj:`add_cross_attention` को :obj:`True` पर सेट किया गया है; एक :obj:`encoder_hidden_states` को फिर एक के रूप में अपेक्षित किया जाता है
फॉरवर्ड पास के लिए इनपुट।
"""
```
बर्टमॉडल मुख्य रूप से एक ट्रांसफार्मर एनकोडर संरचना है, जिसमें तीन भाग होते हैं:
1. एम्बेडिंग, अर्थात्, BertEmbeddings वर्ग की इकाइयाँ, जो संबंधित वेक्टर प्रतिनिधित्व प्राप्त करती हैंशब्द चिन्ह के अनुसार पर;
2. एनकोडर, अर्थात्, बर्टएनकोडर वर्ग की इकाइयाँ;
3. पूलर, अर्थात् बर्टपूलर वर्ग की इकाइयाँ, यह भाग वैकल्पिक है।

**ध्यान दें कि BertModel को डिकोडर के रूप में भी कॉन्फ़िगर किया जा सकता है, लेकिन इस भाग पर नीचे चर्चा नहीं की गई है।

निम्नलिखित BertModel के आगे प्रसार के दौरान प्रत्येक पैरामीटर का अर्थ और रिटर्न मान पेश करेगा:
```
def आगे(
खुद,
इनपुट_आईडी=कोई नहीं,
ध्यान_मास्क=कोई नहीं,
टोकन_टाइप_आईडी=कोई नहीं,
स्थिति_आईडी=कोई नहीं,
सिर_मुखौटा=कोई नहीं,
इनपुट्स_एम्बेड्स=कोई नहीं,
एनकोडर_हिडन_स्टेट्स=कोई नहीं,
एनकोडर_अटेंशन_मास्क=कोई नहीं,
Past_key_values=कोई नहीं,
उपयोग_कैश=कोई नहीं,
आउटपुट_ध्यान=कोई नहीं,
आउटपुट_हिडन_स्टेट्स=कोई नहीं,
return_dict=कोई नहीं,
): ...
```
-input_ids: टोकननाइज़र सेगमेंटेशन के बाद सबवर्ड से संबंधित सबस्क्रिप्ट सूची;
- ध्यान_मास्क: आत्म-ध्यान की प्रक्रिया में, इस मास्क का उपयोग उस वाक्य के बीच अंतर को चिह्नित करने के लिए किया जाता है जहां उपशब्द स्थित है और पैडिंग, और पैडिंग भाग को 0 से भरें;
- टोकन_टीpe_ids: उस वाक्य को चिह्नित करें जहां उपशब्द वर्तमान में स्थित है (पहला वाक्य/दूसरा वाक्य/पैडिंग);
- पोजीशन_आईडी: उस वाक्य की स्थिति सबस्क्रिप्ट को चिह्नित करें जहां वर्तमान शब्द स्थित है;
- हेड_मास्क: कुछ परतों की कुछ ध्यान गणनाओं को अमान्य करने के लिए उपयोग किया जाता है;
- इनपुट_एम्बेड: यदि प्रदान किया गया है, तो इनपुट_आईडी की आवश्यकता नहीं है, और एम्बेडिंग लुकअप प्रक्रिया को छोड़ दिया जाता है और सीधे एन्कोडर गणना में एम्बेडिंग के रूप में दर्ज किया जाता है;
- एनकोडर_हिडन_स्टेट्स: यह भाग तब काम करता है जब BertMओडेल को एक डिकोडर के रूप में कॉन्फ़िगर किया गया है, और आत्म-ध्यान के बजाय क्रॉस-अटेंशन किया जाएगा;
- एनकोडर_अटेंशन_मास्क: जैसा कि ऊपर बताया गया है, दूसरे छोर के इनपुट के क्रॉस-अटेंशनपैडिंग में एनकोडर को चिह्नित करने के लिए उपयोग किया जाता है;
- Past_key_values: क्रॉस-अटेंशन की लागत को कम करने के लिए यह पैरामीटर पूर्व-गणना किए गए K-V उत्पाद में पास होता प्रतीत होता है (क्योंकि यह भाग मूल रूप से दोहराई गई गणना है);
- उपयोग_कैश: पिछले पैरामीटर को सहेजें और डिकोडिंग को तेज करने के लिए इसे वापस पास करें;
- आउटपुट_अटेंशन: क्या करना हैप्रत्येक मध्यवर्ती परत का ध्यान आउटपुट लौटाएँ;
- आउटपुट_हिडन_स्टेट्स: क्या प्रत्येक मध्यवर्ती परत का आउटपुट वापस करना है;
- रिटर्न_डिक्ट: आउटपुट को कुंजी-मूल्य जोड़े (मॉडलआउटपुट क्लास, जिसे टपल के रूप में भी इस्तेमाल किया जा सकता है) के रूप में वापस करना है या नहीं, डिफ़ॉल्ट सत्य है।

**ध्यान दें कि यहां हेड_मास्क ध्यान गणना को अमान्य कर देता है, जो नीचे उल्लिखित ध्यान हेड प्रूनिंग से अलग है, और केवल इस कोए द्वारा कुछ ध्यान के गणना परिणामों को गुणा करता हैकुशल। **

आउटपुट भाग इस प्रकार है:
```
# बर्टमॉडल फॉरवर्ड प्रोपेगेशन रिटर्न पार्ट
यदि नहीं तो return_dict:
वापसी (अनुक्रम_आउटपुट, पूल्ड_आउटपुट) + एनकोडर_आउटपुट[1:]

returnrn BaseModelOutputWithPoolingAndCrossAttentions(
अंतिम_छिपी_स्थिति=अनुक्रम_आउटपुट,
पूलर_आउटपुट=पूल_आउटपुट,
Past_key_values=encoder_outputs.past_key_values,
hidden_states=encoder_outputs.hidden_states,
ध्यान=एनकोडर_आउटपुट.ध्यान,
क्रॉस_अटेंशन=एनकोडर_आउटपुट.क्रॉस_अटेंशन,
)
```
वह देखा जा सकता हैरिटर्न वैल्यू में न केवल एनकोडर और पूलर का आउटपुट शामिल है, बल्कि इसमें अन्य निर्दिष्ट आउटपुट भाग (hidden_states और ध्यान इत्यादि भी शामिल हैं, जिन्हें एनकोडर_आउटपुट [1:] में आसानी से एक्सेस किया जाता है):

```# BertEncoder आगे प्रसार रिटर्न भाग, यानी, उपरोक्त encoder_outputs
यदि नहीं तो return_dict:
वापसी टपल(
वी
v के लिए [
छुपे_राज्य,
अगला_डिकोडर_कैश,
सभी_छिपे हुए_राज्य,
सबका_स्वयं_ध्यान,
सभी का ध्यान,
]
यदि v कोई नहीं है
)
BaseModelOutputWithPastAndCr लौटाएँओस्सअटेंशन(
अंतिम_हिडन_स्टेट=हिडन_स्टेट्स,
Past_key_values=next_decoder_cache,
छुपे हुए राज्य=सभी_छिपे हुए राज्य,
ध्यान=सभी_स्वयं_ध्यान,
क्रॉस_अटेंशन=सभी_क्रॉस_अटेंशन,
)
```

इसके अलावा, BertModel के पास BERT खिलाड़ियों को विभिन्न ऑपरेशन करने में सुविधा प्रदान करने के लिए निम्नलिखित विधियाँ भी हैं:

- get_input_embeddings: एम्बेडिंग में Word_embeddings निकालें, यानी शब्द वेक्टर भाग;
- set_input_embeddings: एम्बेडिंग में Word_embeddings को मान निर्दिष्ट करें;
- _प्रून_हेड्स: एक प्रदान करता हैध्यान शीर्षों की छंटाई के लिए फ़ंक्शन, {लेयर_नम: इस परत में छंटाई करने वाले प्रमुखों की सूची} के शब्दकोश के रूप में इनपुट के साथ, जो निर्दिष्ट परत के कुछ ध्यान शीर्षों की छंटाई कर सकता है।

** प्रूनिंग एक जटिल ऑपरेशन है जिसके लिए ध्यान प्रमुखों को बनाए रखने की आवश्यकता होती है। सिर के हिस्से के Wq, Kq, Vq और पूरी तरह से जुड़े हिस्से के वजन को एक नए छोटे वजन मैट्रिक्स में जोड़ने के बाद कॉपी करें (ध्यान दें कि ग्रेड पहले से अक्षम है) नकल करना), और सब्सक्राइब को रोकने के लिए काटे गए सिरों को वास्तविक समय में रिकॉर्ड करनापीटी त्रुटियाँ। विवरण के लिए, बर्टअटेंशन भाग में prune_heads विधि देखें।**

```अजगर
ट्रांसफॉर्मर.मॉडल.बर्ट.मॉडलिंग_बर्ट आयात से *
क्लास बर्टमॉडल(बर्टप्रीट्रेन्डमॉडल):
"""
मॉडल एक एनकोडर (केवल आत्म-ध्यान के साथ) के साथ-साथ एक डिकोडर के रूप में भी व्यवहार कर सकता है, जिस स्थिति में एक परत
'ध्यान है' में वर्णित वास्तुकला का पालन करते हुए, आत्म-ध्यान परतों के बीच क्रॉस-अटेंशन जोड़ा जाता है
आपको बस इतना ही चाहिए <https://arxiv.org/abs/1706.03762>`__ आशीष वासवानी, नोम शज़ीर, नी द्वाराकी परमार, जैकब उस्ज़कोरिट,
लिलियन जोन्स, एडन एन. गोमेज़, लुकाज़ कैसर और इलिया पोलोसुखिन।
डिकोडर के रूप में व्यवहार करने के लिए मॉडल को कॉन्फ़िगरेशन के :obj:`is_decoder` तर्क के साथ प्रारंभ करने की आवश्यकता है
:obj:`True` पर सेट करें Seq2Seq मॉडल में उपयोग करने के लिए, मॉडल को :obj:`is_decoder` दोनों के साथ प्रारंभ करने की आवश्यकता है।
तर्क और :obj:`add_cross_attention` को :obj:`True` पर सेट किया गया है; एक :obj:`encoder_hidden_states` को फिर एक के रूप में अपेक्षित किया जाता है
फॉरवर्ड पास के लिए इनपुट।
"""

def __init__(self, config, add_pऊलिंग_लेयर = सत्य):
सुपर().__init__(config)
self.config = config

self.embeddings = BertEmbeddings(config)
self.encoder = BertEncoder(config)

self.pooler = BertPooler(config) यदि add_pooling_layer अन्य कोई नहीं

self.init_weights()

def get_input_embeddings(स्वयं):
self.embeddings.word_embeddings लौटाएँ

def set_input_embeddings(स्वयं, मान):
self.embeddings.wordrd_embeddings = मान

डीईएफ़ _प्रून_हेड्स(स्वयं, हेड्स_टू_प्रून):
"""
मॉडल के प्रून हेड्स हेड्स_टू_प्रून: {लेयर_नम का निर्देश: एच की सूचीइस परत में छँटाई करने के निर्देश} आधार देखें
क्लासप्रीट्रेन्डमॉडल
"""
परत के लिए,heads_to_prune.items() में शीर्ष:
self.encoder.layer[परत].ध्यान.prune_heads(heads)

@add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format("बैच_आकार, अनुक्रम_लंबाई"))
@add_code_sample_docstrings(टोकनाइज़र_क्लास=_TOKENIZER_FOR_DOC,
चेकप्वाइंट=_CHECKPOINT_FOR_DOC,
आउटपुट_प्रकार=बेसमॉडलआउटपुटविथपूलिंगएंडक्रॉसअटेंशन,
config_class=_CONFIG_FOR_DOC,
)
def आगे(
खुद,
इनपुट_आईडी=कोई नहीं,
ध्यान_मुखौटा=कोई नहीं,
टोकन_टाइप_आईडी=कोई नहीं,
स्थिति_आईडी=कोई नहीं,
हेड_मास्क=कोई नहीं,
इनपुट्स_एम्बेड्स=कोई नहीं,
एनकोडर_हिडन_स्टेट्स=कोई नहीं,
एनकोडर_अटेंशन_मास्क=कोई नहीं,
Past_key_values=कोई नहीं,
उपयोग_कैश=कोई नहीं,
आउटपुट_ध्यान=कोई नहीं,
आउटपुट_हिडन_स्टेट्स=कोई नहीं,
return_dict=कोई नहीं,
):
आर"""
एनकोडर_हिडन_स्टेट्स (:obj:`torch.FloatTensor` आकार का:obj:`(बैच_आकार, अनुक्रम_लंबाई, छिपा_आकार)`, `वैकल्पिक`):
एनकोडर की अंतिम परत के आउटपुट पर छिपी-स्थितियों का अनुक्रम क्रॉस-अटेंशन में उपयोग किया जाता है
मॉडल कॉन्फ़िगर किया गया हैडिकोडर के रूप में लाल।
एनकोडर_अटेंशन_मास्क (:obj:`torch.FloatTensor` आकार का:obj:`(बैच_आकार, अनुक्रम_लंबाई, छिपा हुआ_आकार)`, `वैकल्पिक`):
एनकोडर की अंतिम परत के आउटपुट पर छिपी-स्थितियों का अनुक्रम क्रॉस-अटेंशन में उपयोग किया जाता है
मॉडल को डिकोडर.ज़े, अनुक्रम_लंबाई)`, `वैकल्पिक`) के रूप में कॉन्फ़िगर किया गया है:
एनकोडर इनपुट के पैडिंग टोकन सूचकांकों पर ध्यान केंद्रित करने से बचने के लिए इस मास्क का उपयोग किया जाता है
यदि मॉडल को डिकोडर मान के रूप में कॉन्फ़िगर किया गया है तो क्रॉस-अटेंशन``[0, 1]`` में चयनित:
- 1 उन टोकन के लिए जो **नकाबपोश** नहीं हैं,
- 0 ऐसे टोकन के लिए जो **नकाबपोश** हैं।
Past_key_values ​​​​​(:obj:`tuple(tuple(torch.FloatTensor))` लंबाई का :obj:`config.n_layers` प्रत्येक टुपल में आकार के 4 टेंसर होते हैं:obj:`(बैच_आकार, num_heads, अनुक्रम_लंबाई - 1 , एंबेड_साइज_पर_हेड )`):
इसमें ध्यान ब्लॉकों की पूर्व-गणना की गई कुंजी और मूल्य छिपी हुई स्थितियाँ शामिल हैं, जिनका उपयोग डिकोडिंग को तेज़ करने के लिए किया जा सकता है।
यदि :obj:`past_key_values` का उपयोग किया जाता है, तो उपयोगकर्ता वैकल्पिक रूप से केवल अंतिम इनपुट कर सकता है:obj:`डिकोडर_इनपुट_आईडी`
(जिनके पास इस मॉडल को दी गई अपनी पिछली कुंजी मान स्थिति नहीं है) आकार की :obj:`(batch_size, 1)`
सभी के बजाय :obj:`decoder_input_ids` आकार का :obj:`(बैच_आकार, अनुक्रम_लंबाई)`।
उपयोग_कैश (:obj:`बूल`, `वैकल्पिक`):
यदि :obj:`True` पर सेट किया जाता है, तो :obj:`past_key_values` कुंजी मान स्थितियाँ लौटा दी जाती हैं और गति बढ़ाने के लिए इसका उपयोग किया जा सकता है
डिकोडिंग (देखें :obj:`past_key_values`)।
"""
आउटपुट_अटेंशन = आउटपुट_अटेंशन यदि आउटपुट_अटेंशन कोई और नहीं है self.config.output_attentionएस
आउटपुट_हिडन_स्टेट्स = (
आउटपुट_हिडन_स्टेट्स यदि आउटपुट_हिडन_स्टेट्स कोई और नहीं है self.config.output_hidden_states
)
रिटर्न_डिक्ट = रिटर्न_डिक्ट यदि रिटर्न_डिक्ट कोई और नहीं है self.config.use_return_dict

यदि self.config.is_decoder:
उपयोग_कैश = उपयोग_कैश यदि उपयोग_कैश कोई और नहीं है self.config.use_cache
अन्य:
उपयोग_कैश = गलत

यदि इनपुट_आईडी कोई नहीं है और इनपुट_एम्बेड कोई नहीं है:
मान बढ़ाएँ त्रुटि ("आप एक ही समय में इनपुट_आईडी और इनपुट_एम्बेड दोनों निर्दिष्ट नहीं कर सकते")
elif इनपुट_आईडी हैकोई नहीं:
इनपुट_आकार = इनपुट_आईडी.आकार()
बैच_आकार, seq_length = इनपुट_आकार
elif इनपुट्स_एम्बेड्स कोई नहीं है:
इनपुट_शेप = इनपुट्स_एम्बेड्स.साइज()[:-1]
बैच_आकार, seq_length = इनपुट_आकार
अन्य:
वैल्यूएरर बढ़ाएं ("आपको या तो इनपुट_आईडी या इनपुट_एम्बेड निर्दिष्ट करना होगा")

डिवाइस = इनपुट_आईडी.डिवाइस यदि इनपुट_आईडी कोई और नहीं है तो इनपुट_एम्बेड्स.डिवाइस

# अतीत_कुंजी_मान_लंबाई
Past_key_values_length = Past_key_values[0][0].shape[2] ifpast_key_values ​​​​नहीं है और कोई नहीं 0

यदि ध्यान_मास्क कोई नहीं है:
ध्यानआयन_मास्क = टॉर्च.ओन्स(((बैच_साइज, सेक_लेंथ + पास्ट_की_वैल्यू_लेंथ)), डिवाइस=डिवाइस)

यदि टोकन_टाइप_आईडी कोई नहीं है:
यदि hasattr(self.embeddings, "token_type_ids"):
बफ़र्ड_टोकन_टाइप_आईडी = self.embeddings.token_type_ids[:, :seq_length]
बफ़रेड_टोकन_टाइप_आईडी_विस्तारित = बफ़रेड_टोकन_टाइप_आईडी.विस्तार(बैच_आकार, seq_length)
टोकन_टाइप_आईडी= बफ़रेड_टोकन_टाइप_आईडी_विस्तारित
अन्य:
टोकन_टाइप_आईड्स = टॉर्च.जीरोस (इनपुट_शेप, डीटाइप=टॉर्च.लॉन्ग, डिवाइस=डिवाइस)

# हम एक आत्म-ध्यान मास्क प्रदान कर सकते हैंआयाम [बैच_आकार, from_seq_length, to_seq_length]
#स्वयं जिस स्थिति में हमें इसे सभी प्रमुखों के लिए प्रसारित करने योग्य बनाने की आवश्यकता है।
एक्सटेंडेड_अटेंशन_मास्क: टॉर्च.टेन्सर = सेल्फ.गेट_एक्सटेंडेड_अटेंशन_मास्क(अटेंशन_मास्क, इनपुट_शेप, डिवाइस)

# यदि क्रॉस-अटेंशन के लिए 2डी या 3डी अटेंशन मास्क प्रदान किया जाता है
# हमें [बैच_आकार, num_heads, seq_length, seq_length] को प्रसारण योग्य बनाने की आवश्यकता है
यदि self.config.is_decoder और encoder_hidden_states कोई नहीं है:
एनकोडर_बैच_आकार, एनकोडर_अनुक्रम_लेngth, _ = encoder_hidden_states.size()
एनकोडर_हिडन_शेप = (एनकोडर_बैच_आकार, एनकोडर_अनुक्रम_लंबाई)
यदि एनकोडर_अटेंशन_मास्क कोई नहीं है:
एनकोडर_अटेंशन_मास्क = टॉर्च.ओन्स(एनकोडर_हिडन_शेप, डिवाइस=डिवाइस)
एनकोडर_एक्सटेंडेड_अटेंशन_मास्क = self.invert_attention_mask(एनकोडर_अटेंशन_मास्क)
अन्य:
एनकोडर_एक्सटेंडेड_अटेंशन_मास्क = कोई नहीं

# यदि आवश्यक हो तो हेड मास्क तैयार करें
हेड_मास्क में # 1.0 इंगित करता है कि हम सिर रखते हैं
# ध्यान_प्रॉब्स का आकार bsz x n_heads x N x N है
# इनपुट हेड_मास्क का आकार है[num_heads] या [num_hidden_layers x num_heads]
# और हेड_मास्क को आकार में बदल दिया गया है [num_hidden_layers x बैच x num_heads x seq_length x seq_length]
हेड_मास्क = self.get_head_mask(head_mask, self.config.num_hidden_layers)

एम्बेडिंग_आउटपुट = self.embeddings(
इनपुट_आईडी=इनपुट_आईडी,
स्थिति_आईडी=स्थिति_आईडी,
टोकन_टाइप_आईडी=टोकन_टाइप_आईडी,
इनपुट्स_एम्बेड्स=इनपुट्स_एम्बेड्स,
Past_key_values_length=past_key_values_length,
)
एनकोडर_आउटपुट = स्व.एनकोडर(
एम्बेडिंग_आउटपुट,
ध्यान_मास्क=विस्तारित_ध्यानn_मास्क,
हेड_मास्क = हेड_मास्क,
एनकोडर_हिडन_स्टेट्स=एनकोडर_हिडन_स्टेट्स,
एनकोडर_अटेंशन_मास्क=एनकोडर_एक्सटेंडेड_अटेंशन_मास्क,
पास्ट_की_वैल्यू=पास्ट_की_वैल्यू,
उपयोग_कैश=उपयोग_कैश,
आउटपुट_अटेंशन=आउटपुट_अटेंशन,
आउटपुट_हिडन_स्टेट्स=आउटपुट_हिडन_स्टेट्स,
रिटर्न_डिक्ट=रिटर्न_डिक्ट,
)
अनुक्रम_आउटपुट = एनकोडर_आउटपुट[0]
पूल्ड_आउटपुट = सेल्फ.पूलर(अनुक्रम_आउटपुट) यदि सेल्फ.पूलर कोई नहीं है और कोई नहीं

यदि नहीं तो return_dict:
वापसी (अनुक्रम_आउटपुट, पूल्ड_आउटपुट) + एनकोडर_आउटपुट[1:]

वापस करनाबेसमॉडलआउटपुटविथपूलिंगएंडक्रॉसअटेंशन(
अंतिम_छिपी_स्थिति=अनुक्रम_आउटपुट,
पूलर_आउटपुट=पूल_आउटपुट,
Past_key_values=encoder_outputs.past_key_values,
hidden_states=encoder_outputs.hidden_states,
ध्यान=एनकोडर_आउटपुट.ध्यान,
क्रॉस_अटेंशन=एनकोडर_आउटपुट.क्रॉस_अटेंशन,
)
```

***
### 2.1-बर्टएम्बेडिंग्स
इसमें संक्षेप में तीन भाग शामिल हैं:
![बर्ट-एम्बेडिंग](./चित्र/3-0-एम्बेडिंग.पीएनजी) चित्र: बर्ट-एम्बेडिंग

1. वर्ड_एम्बेडिंग्स, टी में सबवर्ड के अनुरूप एम्बेडिंगवह पाठ के ऊपर है।
2. टोकन_टाइप_एम्बेडिंग, वाक्य को इंगित करने के लिए उपयोग किया जाता है जहां वर्तमान शब्द स्थित है, वाक्य और पैडिंग और वाक्य जोड़े के बीच अंतर को अलग करने में मदद करने के लिए।
3. पोजीशन_एंबेडिंग, वाक्य में प्रत्येक शब्द की स्थिति एम्बेडिंग, शब्दों के क्रम को अलग करने के लिए उपयोग की जाती है। ट्रांसफार्मर पेपर में डिज़ाइन के विपरीत, इस भाग को साइनसॉइडल फ़ंक्शन द्वारा गणना की गई निश्चित एम्बेडिंग के बजाय प्रशिक्षित किया जाता है यह कार्यान्वयनस्केलेबिलिटी के लिए अनुकूल नहीं है (सीधे लंबे वाक्यों में स्थानांतरित करना मुश्किल है)।

लेयरनॉर्म+ड्रॉपआउट की एक परत से गुजरने के बाद तीन एम्बेडिंग को वजन और आउटपुट के बिना जोड़ा जाता है, और इसका आकार (बैच_आकार, अनुक्रम_लंबाई, छिपा हुआ_आकार) होता है।

** [यहाँ LayerNorm+Dropout का उपयोग क्यों करें? Bat?chNorm के बजाय LayerNorm का उपयोग क्यों करें? आप एक अच्छे उत्तर का उल्लेख कर सकते हैं: ट्रांसफार्मर अन्य सामान्यीकरण विधियों के बजाय परत सामान्यीकरण का उपयोग क्यों करता है?](https://www.zhihu.com /प्रश्न/395811291/उत्तर/1260290120)**

```अजगर
वर्ग BertEmbeddings(nn.मॉड्यूल):
"""शब्द, स्थिति और टोकन_प्रकार एम्बेडिंग से एम्बेडिंग का निर्माण करें।"""

def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
self.position_embeddings = nn.embedding(config.max_position_embeddings, config.hidden_size)
self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)

# self.LayerNorm साँप नहीं हैTensorFlow मॉडल वेरिएबल नाम के साथ बने रहने और लोड करने में सक्षम होने के लिए ई-केस किया गया
# कोई भी TensorFlow चेकपॉइंट फ़ाइल
self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)
# पोजीशन_आईडी (1, लेन पोजीशन एंब) मेमोरी में सन्निहित है और क्रमबद्ध होने पर निर्यात किया जाता है
self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
self.register_buffer("position_ids", torch.arange(config.max_position_embeddings).expand((11)))
यदि संस्करण.पार्स(मशाल.__संस्करण__) > संस्करण.पार्से("1.6.0"):
self.register_buffer(
"टोकन_टाइप_आईडी",
मशाल.शून्य (स्वयं.स्थिति_आईडी.आकार(), dtype=मशाल.लंबा, डिवाइस=स्वयं.स्थिति_आईडी.डिवाइस),
लगातार=झूठा,
)

def आगे(
स्वयं, इनपुट_आईडी=कोई नहीं, टोकन_टाइप_आईडी=कोई नहीं, स्थिति_आईडी=कोई नहीं, इनपुट_एम्बेड्स=कोई नहीं, पास्ट_की_वैल्यू_लेंथ=0
):
यदि इनपुट_आईडी कोई नहीं है:
इनपुट_आकार = इनपुट_आईडी.आकार()
अन्य:
इनपुट_शेप = इनपुट्स_एम्बेड्स.साइज()[:-1]

seq_length = इनपुट_आकार[1]

यदि स्थिति_आईडी एन हैएक:
पोजीशन_आईडी = स्व.पोजिशन_आईडी[:, पास्ट_की_वैल्यू_लेंथ : सेक_लेंथ + पास्ट_की_वैल्यू_लेंथ]

# टोकन_टाइप_आईडी को कंस्ट्रक्टर में पंजीकृत बफर पर सेट करना जहां यह सभी शून्य है, जो आमतौर पर होता है
# जब यह स्वतः उत्पन्न होता है, तो पंजीकृत बफ़र उपयोगकर्ताओं को टोकन_टाइप_आईडी पास किए बिना मॉडल का पता लगाने में मदद करता है, हल करता है
#अंक #5664
यदि टोकन_टाइप_आईडी कोई नहीं है:
यदि hasattr(स्वयं, "टोकन_टाइप_आईडी"):
बफ़र्ड_टोकन_टाइप_आईडी = self.token_type_ids[:, :seq_length]
बफ़र्ड_टोकन_प्रकार_ids_expanded = बफर्ड_टोकन_टाइप_ids.expand(input_shape[0], seq_length)
टोकन_टाइप_आईडी = बफ़रेड_टोकन_टाइप_आईडी_विस्तारित
अन्य:
टोकन_टाइप_आईडी = टॉर्च.जीरोस (इनपुट_शेप, डीटाइप = टॉर्च.लॉन्ग, डिवाइस = सेल्फ.पोजीशन_आईडी.डिवाइस)

यदि इनपुट_एम्बेड कोई नहीं है:
इनपुट्स_एम्बेड्स = self.word_embeddings(input_ids)
टोकन_टाइप_एम्बेडिंग = self.token_type_embeddings(टोकन_टाइप_आईडी)

एम्बेडिंग = इनपुट_एम्बेडिंग + टोकन_टाइप_एम्बेडिंग
यदि self.position_embedding_type == "पूर्ण":
स्थिति_एम्बेडिंग्स = स्व.स्थिति_ईएम्बेडिंग(स्थिति_आईडी)
एम्बेडिंग += स्थिति_एम्बेडिंग
एम्बेडिंग = self.LayerNorm(एम्बेडिंग)
एम्बेडिंग = स्व.ड्रॉपआउट(एम्बेडिंग)
वापसी एम्बेडिंग
```

***
### 2.2-बर्टएन्कोडर

बर्टलेयर की कई परतें शामिल हैं। इस भाग के बारे में समझाने के लिए कुछ खास नहीं है, लेकिन इसमें उल्लेख करने लायक एक विवरण है: प्रशिक्षण के दौरान मेमोरी के उपयोग को कम करने के लिए ग्रेडिएंट चेकपॉइंटिंग तकनीक का उपयोग करना।

**ग्रेडिएंट चेकपॉइंटिंग एक ग्रेडिएंट चेकपॉइंट है यह मॉडल स्पेस को कम करके संपीड़ित करता हैसहेजे गए गणना ग्राफ़ नोड्स में, लेकिन ग्रेडिएंट की गणना करते समय, अनस्टोर्ड मान की पुनर्गणना करना आवश्यक है, "सबलाइनियर मेमोरी कॉस्ट के साथ डीप नेट को प्रशिक्षित करना" पेपर देखें। प्रक्रिया इस प्रकार है**
![ग्रेडिएंट-चेकपॉइंटिंग](./चित्र/3-1-ग्रेडिएंट-चेकपॉइंटिंग.gif) चित्र: ग्रेडिएंट-चेकपॉइंटिंग

BertEncoder में, ग्रेडिएंट चेकपॉइंट को torch.utils.checkpoint.checkpoint के माध्यम से कार्यान्वित किया जाता है, जिसका उपयोग करना सुविधाजनक है। आप दस्तावेज़ का संदर्भ ले सकते हैं: torch.utils.checkpoint -PyTorch 1.8.1 दस्तावेज़ीकरण इस तंत्र का विशिष्ट कार्यान्वयन अपेक्षाकृत जटिल है और इसका विस्तार यहां नहीं किया जाएगा।

गहराई में जाने पर, हम एनकोडर की एक परत दर्ज करते हैं:

```अजगर
क्लास बर्टएनकोडर(एनएन.मॉड्यूल):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.config = config
सेल्फ.लेयर = nn.ModuleList([BertLayer(config) for _ in रेंज(config.num_hidden_layers)])

def आगे(
खुद,
छुपे_राज्य,
ध्यान_मास्क=कोई नहीं,
हेड_मास्क=कोई नहीं,
एनकोडर_हिडन_स्टेट्स=कोई नहीं,
एनकोडर_अटेंशन_मास्क=गैरइ,
Past_key_values=कोई नहीं,
उपयोग_कैश=कोई नहीं,
आउटपुट_ध्यान=गलत,
आउटपुट_हिडन_स्टेट्स=गलत,
return_dict=सत्य,
):
all_hidden_states = () यदि आउटपुट_hidden_states अन्य कोई नहीं
all_self_attentions = () यदि आउटपुट_ध्यान अन्य कोई नहीं
all_cross_attentions = () यदि आउटपुट_attentions और self.config.add_cross_attention अन्य कोई नहीं

नेक्स्ट_डिकोडर_कैश = () यदि उपयोग_कैश अन्यथा कोई नहीं
i के लिए, लेयर_मॉड्यूल एन्यूमरेट (सेल्फ.लेयर) में:
यदि आउटपुट_हिडन_स्टेट्स:
all_hidden_states = all_hidden_states + (hidden_states,)

लाyer_head_mask = हेड_मास्क[i] यदि हेड_मास्क कोई नहीं है और कोई नहीं
पास्ट_की_वैल्यू = पास्ट_की_वैल्यू[i] यदि पास्ट_की_वैल्यू कोई नहीं है और कोई नहीं

यदि getattr(self.config, "gradient_checkpointing", False) और self.training:

यदि उपयोग_कैश:
लकड़हारा.चेतावनी(
"`use_cache=True``config.gradient_checkpointing=True` के साथ असंगत है। सेटिंग"
"`use_cache=गलत`..."
)
उपयोग_कैश = गलत

def create_custom_forward(मॉड्यूल):
डीईएफ़ कस्टम_फॉरवर्ड(*इनपुट):
रिटर्न मॉड्यूल (*इनपुट, पास्ट_की_वैल्यू, आउटपुट_अटेंटियोएनएस)

कस्टम_फ़ॉरवर्ड लौटाएँ

लेयर_आउटपुट = टॉर्च.यूटिल्स.चेकपॉइंट.चेकपॉइंट(
create_custom_forward(layer_module),
छुपे_राज्य,
ध्यान_मास्क,
लेयर_हेड_मास्क,
एनकोडर_हिडन_स्टेट्स,
एनकोडर_अटेंशन_मास्क,
)
अन्य:
लेयर_आउटपुट = लेयर_मॉड्यूल(
छुपे_राज्य,
ध्यान_मास्क,
लेयर_हेड_मास्क,
एनकोडर_हिडन_स्टेट्स,
एनकोडर_अटेंशन_मास्क,
विगत_कुंजी_मूल्य,
आउटपुट_ध्यान,
)

छुपे हुए राज्य = परत_आउटपुट[0]
यदि उपयोग_कैश:
अगला_डिकोडर_कैश += (लेयर_आउटपुट[-1],)
यदि आउटपुट_ध्यान:
सब_सेlf_attentions = all_self_attentions + (लेयर_आउटपुट[1],)
यदि self.config.add_cross_attention:
ऑल_क्रॉस_अटेंशन = ऑल_क्रॉस_अटेंशन + (लेयर_आउटपुट[2],)

यदि आउटपुट_हिडन_स्टेट्स:
all_hidden_states = all_hidden_states + (hidden_states,)

यदि नहीं तो return_dict:
वापसी टपल(
वी
v के लिए [
छुपे_राज्य,
अगला_डिकोडर_कैश,
सभी_छिपे हुए_राज्य,
सबका ध्यान, सबका ध्यान,
]
यदि v कोई नहीं है
)
वापसी BaseModelOutputWithPastAndCrossAttentions(
अंतिम_हिडन_स्टेट=हिडन_स्टेट्स,
अतीत_कुंजी_मान = अगला_डिकोडर_कैश,
छुपे हुए राज्य=सभी_छिपे हुए राज्य,
ध्यान=सभी_स्वयं_ध्यान,
क्रॉस_अटेंशन=सभी_क्रॉस_अटेंशन,
)
```

***
#### 2.2.1.1 बर्टअटेंशन

मैंने सोचा था कि ध्यान का कार्यान्वयन यहाँ था, लेकिन मुझे एक और स्तर नीचे जाने की उम्मीद नहीं थी... उनमें से, स्वयं सदस्य बहु-प्रमुख ध्यान का कार्यान्वयन है, और आउटपुट सदस्य ध्यान के बाद संचालन की एक श्रृंखला को लागू करता है, जिसमें शामिल हैं पूर्ण कनेक्शन + ड्रॉपआउट + अवशिष्ट + लेयरनॉर्म।

```
क्लास बर्टएटेन्टीपर(एनएन.मॉड्यूल):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.self = BertSelfAttention(config)
self.output = BertSelfOutput(config)
self.pruned_heads = सेट()
```
सबसे पहले, आइए इस परत पर वापस जाएं। ऊपर उल्लिखित प्रूनिंग ऑपरेशन यहां दिखाई देता है, अर्थात् prune_heads विधि:
```
def prune_heads(स्वयं, प्रमुख):
यदि लेन(शीर्ष) == 0:
वापस करना
शीर्ष, सूचकांक = find_pruneable_heads_and_indices(
शीर्ष, स्व.स्व.संख्या_ध्यान_प्रमुख, स्व.स्व.ध्यान_शीर्ष_आकार, स्व.प्रून्ड_सिर
)

#प्रून रैखिकपरतें
self.self.query = prune_linear_layer(self.self.query, अनुक्रमणिका)
self.self.key = prune_linear_layer(self.self.key, अनुक्रमणिका)
self.self.value = prune_linear_layer(self.self.value, सूचकांक)
self.output.dense = prune_linear_layer(self.output.dense, Index, dim=1)

# हाइपर पैराम्स को अपडेट करें और काटे गए हेड्स को स्टोर करें
self.self.num_attention_heads = self.self.num_attention_heads - len(heads)
स्वयं.स्वयं.सभी_सिर_आकार = स्वयं.स्व.ध्यान_सिर_आकार * स्वयं.स्वयं.संख्या_ध्यान_शीर्ष
self.pruned_heads = self.pruned_प्रमुख.संघ(प्रमुख)
```
यहां विशिष्ट कार्यान्वयन को इस प्रकार संक्षेप में प्रस्तुत किया गया है:
- `find_pruneable_heads_and_indices` छंटाई किए जाने वाले शीर्ष और बनाए रखे जाने वाले आयाम सूचकांक का पता लगाता है;

- `prune_linear_layer` उन आयामों को नए मैट्रिक्स में स्थानांतरित करने के लिए जिम्मेदार है जिन्हें सूचकांक के अनुसार Wk/Wq/Wv वजन मैट्रिक्स (पूर्वाग्रह के साथ) में काटा नहीं गया है।
इसके बाद, आइये मुख्य बिंदु पर आते हैं - आत्म-ध्यान का विशिष्ट कार्यान्वयन।

```अजगर
वर्ग BertAttention(nn.Moduले):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.self = BertSelfAttention(config)
self.output = BertSelfOutput(config)
self.pruned_heads = सेट()

def prune_heads(स्वयं, प्रमुख):
यदि लेन(शीर्ष) == 0:
वापस करना
शीर्ष, सूचकांक = find_pruneable_heads_and_indices(
शीर्ष, स्व.स्व.संख्या_ध्यान_प्रमुख, स्व.स्व.ध्यान_शीर्ष_आकार, स्व.प्रून्ड_सिर
)

#प्रून रैखिक परतें
self.self.query = prune_linear_layer(self.self.query, अनुक्रमणिका)
self.self.key = prune_linear_layer(self.self.key, अनुक्रमणिका)
स्वयं.स्वयं.मूल्य = prune_linear_layer(self.self.value, सूचकांक)
self.output.dense = prune_linear_layer(self.output.dense, Index, dim=1)

# हाइपर पैराम्स को अपडेट करें और काटे गए हेड्स को स्टोर करें
self.self.num_attention_heads = self.self.num_attention_heads - len(heads)
स्वयं.स्वयं.सभी_सिर_आकार = स्वयं.स्व.ध्यान_सिर_आकार * स्वयं.स्वयं.संख्या_ध्यान_शीर्ष
सेल्फ.प्रूनड_हेड्स = सेल्फ.प्रूनड_हेड्स.यूनियन(हेड्स)

def आगे(
स्वयं, छुपी हुई अवस्थाएँ,
ध्यान_मास्क=कोई नहीं,
हेड_मास्क=कोई नहीं,
एनकोडर_हिडन_स्टेट्स=कोई नहीं,
एनकोडर_ध्यान_मुखौटा=कोई नहीं,
Past_key_value=कोई नहीं,
आउटपुट_ध्यान=गलत,
):
self_outputs = self.self(
छुपे_राज्य,
ध्यान_मास्क,
सिर पर मुखौटा,
एनकोडर_हिडन_स्टेट्स,
एनकोडर_अटेंशन_मास्क,
विगत_कुंजी_मूल्य,
आउटपुट_ध्यान,
)
ध्यान_आउटपुट = self.output(self_outputs[0],hidden_states)
आउटपुट = (अटेंशन_आउटपुट,) + सेल्फ_आउटपुट[1:] # यदि हम उन्हें आउटपुट करते हैं तो ध्यान जोड़ें
वापसी आउटपुट
```

***
##### 2.2.1.1.1 बर्टसेल्फअटेंशन

**चेतावनी: यह मॉडल का मुख्य क्षेत्र है और फॉर्म से जुड़ा एकमात्र स्थान हैउल्लास, इसलिए बहुत सारे कोड पोस्ट किए जाएंगे।

आरंभीकरण भाग:
```
क्लास बर्टसेल्फअटेंशन(एनएन.मॉड्यूल):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
यदि config.hidden_size % config.num_attention_heads != 0 और hasattr नहीं है(config, "embedding_size"):
raise वैल्यूएरर ("छिपा हुआ आकार (%d) ध्यान की संख्या का गुणज नहीं है"
"प्रमुख (%d)" % (config.hidden_size, config.num_attention_heads)
)

self.num_attention_heads = config.num_attention_heads
self.attention_head_size = int(config.hidden_size/config.num_attention_heads)
self.all_head_size = self.num_attention_heads * self.attention_head_size

self.query = nn.Linear(config.hidden_size, self.all_head_size)
self.key = nn.Linear(config.hidden_size, self.all_head_size)
स्व.मान = nn.रैखिक(config.hidden_size, self.all_head_size)

self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
यदि self.position_embedding_type == "relative_key" याself.position_embedding_type == "relative_key_query":
self.max_position_embeddings = config.max_position_embeddings
self.distance_embedding = nn.embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)

self.is_decoder = config.is_decoder
```

- परिचित क्वेरी, कुंजी, मूल्य भार और एक ड्रॉपआउट के अलावा, एक रहस्यमय स्थिति_एम्बेडिंग_प्रकार और डिकोडर टैग भी है;

- ध्यान दें किhidden_size और all_head_size शुरुआत में समान हैंइस वेरिएबल को सेट करना आवश्यक है- जाहिर तौर पर ऊपर दिए गए प्रूनिंग फ़ंक्शन के कारण, कुछ ध्यान शीर्षों को काटने के बाद, all_head_size स्वाभाविक रूप से छोटा हो जाएगा;

- हिडन_साइज num_attention_heads का पूर्णांक गुणज होना चाहिए। उदाहरण के तौर पर बर्ट-बेस को लेते हुए, प्रत्येक अटेंशन में 12 हेड होते हैं, और हिडन_साइज 768 है, इसलिए प्रत्येक हेड का आकार अटेंशन_हेड_साइज = 768/12 = 64 है; - पोजीशन_एम्बेडिंग_टाइप क्या है? यदि आप पढ़ना जारी रखेंगे तो पता चल जाएगा।

फिर मुख्य बिंदु आगे का प्रसार हैप्रक्रिया।

सबसे पहले, आइए बहु-सिर आत्म-ध्यान के मूल सूत्र की समीक्षा करें:

$$MHA(Q, K, V) = Concat(head_1, ..., head_h)W^O$$
$$head_i = SDPA(QW_i^Q, KW_i^K, VW_i^V)$$
$$SDPA(Q, K, V) = Softmax(\frac{QK^T}{\sqrt(d_k)})V$$

और इन ध्यान शीर्षों की गणना समानांतर में की जाती है, इसलिए उपरोक्त क्वेरी, कुंजी और मूल्य भार अद्वितीय हैं - इसका मतलब यह नहीं है कि सभी शीर्ष भार साझा करते हैं, बल्कि एक साथ "जुड़े" होते हैं।

**[मूल पेपर में मल्टी-हेड का कारण वह मल्टी-हेड हैध्यान मॉडल को विभिन्न पदों पर अलग-अलग प्रतिनिधित्व उप-स्थानों से संयुक्त रूप से जानकारी प्राप्त करने की अनुमति देता है, औसत इसे रोकता है: ट्रांसफार्मर को मल्टी-हेड ध्यान की आवश्यकता क्यों है? zhihu.com/question/341222779/answer/814111138)**

आगे की विधि देखें:
```
def transpose_for_scores(स्वयं, x):
new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
x = x.view(*new_x_shape)
वापसी x.permute(0, 2, 1, 3)

def आगे(
खुद,
छुपे_राज्य,
ध्यान_मास्क=कोई नहीं,
हेड_मास्क=कोई नहीं,
एनकोडर_हिडन_स्टेट्स=कोई नहीं,
एनकोडर_अटेंशन_मास्क=कोई नहीं,
Past_key_value=कोई नहीं,
आउटपुट_ध्यान=गलत,
):
Mixed_query_layer = self.query(hidden_states)

# क्रॉस-अटेंशन गणना के भाग को छोड़ दें
key_layer = self.transpose_for_scores(self.key(hidden_states))
वैल्यू_लेयर = self.transpose_for_scores(self.value(hidden_states))
query_layer = self.transpose_for_scores(mixed_query_layer)

#लेनामूल ध्यान स्कोर प्राप्त करने के लिए "क्वेरी" और "कुंजी" के बीच डॉट उत्पाद।
ध्यान_स्कोर = मशाल.मैटमूल(क्वेरी_लेयर, की_लेयर.ट्रांसपोज़(-1, -2))
#...
```
यहां, `transpose_for_scores` का उपयोग `hidden_size` को कई हेड आउटपुट के आकार में विभाजित करने के लिए किया जाता है, और मैट्रिक्स गुणन के लिए मध्य दो आयामों को स्थानांतरित करने के लिए किया जाता है;

यहां, `key_layer/value_layer/query_layer`r का आकार है: (बैच_आकार, num_attention_heads, अनुक्रम_लंबाई, ध्यान_head_size);
यहाँ, `attention_scores` का आकार is: (बैच_आकार, संख्या_ध्यान_शीर्ष, अनुक्रम_लंबाई, अनुक्रम_लंबाई), जो अलग-अलग कई शीर्षों की गणना करके प्राप्त ध्यान मानचित्र के आकार के अनुरूप है।

इस बिंदु पर, हमने कच्चे ध्यान स्कोर प्राप्त करने के लिए K और Q का गुणन लागू किया है, सूत्र के अनुसार, अगला चरण $d_k$g द्वारा स्केल करना और एक सॉफ्टमैक्स ऑपरेशन करना होना चाहिए, हालांकि, पहली चीज़ जो दिखाई देती है यह एक अजीब स्थितिगत एम्बेडिंग है, और आइंस्टीन के योगों का एक समूह है:

```
#...
यदि स्व.स्थिति_एम्बेडिंग_प्रकार == "सापेक्ष_कुंजी" या स्वयं.स्थिति_एम्बेडिंग_प्रकार == "सापेक्ष_कुंजी_क्वेरी":
seq_length =hidden_states.size()[1]
पोजीशन_आईडी_एल = टॉर्च.अरेंज(seq_length, dtype=torch.long, डिवाइस=hidden_states.device).view(-1, 1)
पोजीशन_आईडी_आर = टॉर्च.अरेंज(seq_length, dtype=torch.long, डिवाइस=hidden_states.device).view(1, -1)
दूरी = स्थिति_आईडी_एल - स्थिति_आईडी_आर
पोजिशनल_एम्बेडिंग = सेल्फ.डिस्टेंस_एम्बेडिंग(दूरी + सेल्फ.मैक्स_पोजीशन_एम्बेडिंग - 1)
स्थितिगत_एम्बेडिंग = स्थितिtional_embedding.to(dtype=query_layer.dtype) # fp16 अनुकूलता

यदि self.position_embedding_type == "relative_key":
रिलेटिव_पोजिशन_स्कोर्स = टॉर्च.ईंसम ("बीएचएलडी,एलआरडी->बीएचएलआर", क्वेरी_लेयर, पोजिशनल_एम्बेडिंग)
ध्यान_स्कोर = ध्यान_स्कोर + सापेक्ष_स्थिति_स्कोर
elifself.position_embedding_type == "relative_key_query":
रिलेटिव_पोजिशन_स्कोर्स_क्वेरी = टॉर्च.ईंसम("बीएचएलडी,एलआरडी->बीएचएलआर", क्वेरी_लेयर, पोजिशनल_एम्बेडिंग)
रिलेटिव_पोजीशन_स्कोर्स_की = टॉर्च.ईंसम("बीएचआरडी,एलआरडी->बीएचएलआर", की_लेएर, पोजिशनल_एम्बेडिंग)
ध्यान_स्कोर = ध्यान_स्कोर + सापेक्ष_स्थिति_स्कोर_क्वेरी + सापेक्ष_स्थिति_स्कोर_कुंजी
#...
```
**[आइंस्टीन सारांश सम्मेलन के लिए, निम्नलिखित दस्तावेज़ देखें: torch.einsum - PyTorch 1.8.1 दस्तावेज़](https://pytorch.org/docs/stable/generated/torch.einsum.html)**

अलग-अलग पोजीशनल_एम्बेडिंग_प्रकार के लिए, तीन ऑपरेशन हैं:

- निरपेक्ष: डिफ़ॉल्ट मान, इस भाग को संसाधित करने की कोई आवश्यकता नहीं;

- रिलेटिव_की: की_लेयर को प्रोसेस करें और इसे पो से गुणा करेंकुंजी-संबंधित स्थिति एन्कोडिंग के रूप में यहां sional_embedding और कुंजी मैट्रिक्स;

- रिलेटिव_की_क्वेरी: स्थिति एन्कोडिंग के रूप में कुंजी और मान दोनों को गुणा करें।

सामान्य ध्यान प्रक्रिया पर वापस जाएँ:

```
#...
ध्यान_स्कोर = ध्यान_स्कोर / गणित.वर्ग(स्वयं.ध्यान_शीर्ष_आकार)
यदि ध्यान_मास्क कोई नहीं है:
# अटेंशन मास्क लागू करें (बर्टमॉडल फॉरवर्ड() फ़ंक्शन में सभी परतों के लिए प्रीकंप्यूटेड) अटेंशन_स्कोर्स = अटेंशन_स्कोर्स + अटेंशन_मास्क # यहां * के बजाय + क्यों है?

#ध्यान को सामान्य करेंआयन संभावनाओं को स्कोर करता है।
ध्यान_प्रोब्स = एनएन.सॉफ्टमैक्स(मंद=-1)(ध्यान_स्कोर)

# यह वास्तव में प्रतीक्षा करने के लिए संपूर्ण टोकन खो रहा है, जो हो सकता है
# थोड़ा असामान्य लगता है, लेकिन मूल ट्रांसफार्मर पेपर से लिया गया है।
ध्यान_समस्याएँ = स्व.ड्रॉपआउट(ध्यान_समस्याएँ)

#चाहें तो सिर पर मास्क लगाएं
यदि हेड_मास्क कोई नहीं है:
ध्यान_प्रॉब्स = ध्यान_प्रॉब्स * हेड_मास्क

सन्दर्भ_परत = मशाल.मैटमूल(ध्यान_प्रोब्स, मूल्य_परत)

context_layer = context_layer.permute(0, 2, 1, 3).contiguouएस()
new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
context_layer = context_layer.view(*new_context_layer_shape)

आउटपुट = (संदर्भ_लेयर, ध्यान_प्रॉब्स) यदि आउटपुट_अटेंशन अन्य (संदर्भ_लेयर,)

# डिकोडर रिटर्न वैल्यू भाग को छोड़ दें...
वापसी आउटपुट
```

प्रमुख प्रश्न: यहाँ पर ध्यान दें कि ention_scores = ध्यान_स्कोर + ध्यान_मास्क क्या कर रहा है? क्या इसे मास्क द्वारा गुणा नहीं किया जाना चाहिए?
- क्योंकि यहां अटेंशन मास्क के साथ छेड़छाड़ की गई है, जो हिस्सा थामूल रूप से 1 0 हो जाता है, और जो भाग मूल रूप से 0 था (यानी पैडिंग) एक बड़ी ऋणात्मक संख्या बन जाता है, इसलिए जोड़ने पर एक बड़ा ऋणात्मक मान प्राप्त होता है:
- [एक बड़ी ऋणात्मक संख्या] का उपयोग क्यों किया जाता है? क्योंकि सॉफ्टमैक्स ऑपरेशन के बाद, यह आइटम 0 के करीब दशमलव बन जाएगा।

```
(पीडीबी) ध्यान_मास्क
टेंसर([[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],
[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],
[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],
...,
[[[-0., -0.,-0., ..., -10000., -10000., -10000.]]],
[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],
[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]]],
डिवाइस='क्यूडा:0')
```

तो, यह चरण कहाँ क्रियान्वित किया गया है?
मुझे modelling_bert.py में उत्तर नहीं मिला, लेकिन मुझे modeling_utils.py में एक विशेष वर्ग मिला: class ModuleUtilsMixin, और इसकी get_extensed_attention_mask विधि में एक सुराग मिला:

```
डीईएफ़ गेट_एक्सटेंडेड_अटेंशन_मास्क(स्वयं, अटेंशन_मास्क: टेंसर, इनपुट_शेप: ट्यूपल[इंट], डिवाइस: डिवाइस) -> टेन्सया:"""
प्रसारण योग्य ध्यान और कारणात्मक मुखौटे बनाता है ताकि भविष्य और नकाबपोश टोकन को नजरअंदाज कर दिया जाए।

तर्क:
ध्यान_मास्क (:obj:`torch.Tensor`):
ध्यान देने के लिए टोकन का संकेत देने वाले मास्क, अनदेखा करने के लिए टोकन के लिए शून्य।
इनपुट_शेप (:obj:`Tuple[int]`):
मॉडल में इनपुट का आकार.
डिवाइस: (:obj:`torch.device`):
मॉडल में इनपुट का उपकरण.

रिटर्न:
:obj:`torch.Tensor` विस्तारित ध्यान मास्क, :obj:`attention_mask.dtype` के समान dtype के साथ।
"""
#भाग छोड़ा जा रहा है...# चूंकि अटेंशन_मास्क उन पदों के लिए 1.0 है जिनमें हम भाग लेना चाहते हैं और उनके लिए 0.0 है
# नकाबपोश स्थिति, यह ऑपरेशन एक टेंसर बनाएगा जो 0.0 है
# वे पद जिनमें हम भाग लेना चाहते हैं और नकाबपोश पदों के लिए -10000.0।
# चूंकि हम इसे सॉफ्टमैक्स से पहले कच्चे स्कोर में जोड़ रहे हैं, यह है
# प्रभावी रूप से इन्हें पूरी तरह से हटाने के समान है। विस्तारित_ध्यान_मास्क = विस्तारित_ध्यान_मास्क.टू(dtype=self.dtype) # fp16 अनुकूलता
विस्तारित_ध्यान_मास्क = (1.0 - विस्तारित_ध्यान_मास्क) * -10000.0
वापस करनाn विस्तारित_अटेंशन_मास्क
```

तो, इस फ़ंक्शन को कब बुलाया जाता है? इसका BertModel से क्या संबंध है?

ठीक है, यहां 'बर्टमॉडल' का वंशानुक्रम विवरण दिया गया है: 'बर्टमॉडल' 'बर्टप्रीट्रेंडमॉडल' से विरासत में मिला है, जो 'प्रीट्रेन्डमॉडल' से विरासत में मिला है, और 'प्रीट्रेन्डमॉडल' तीन आधार वर्गों [एनएन.मॉड्यूल, मॉड्यूलयूटिल्समिक्सिन, जेनरेशनमिक्सिन] से विरासत में मिला है। कितना जटिल एनकैप्सुलेशन है!

इसका मतलब यह है कि बर्टमॉडल ने किसी इंट पर मूल अटेंशन_मास्क पर get_extensed_attention_mask को कॉल किया होगाermediate चरण.k, जिससे ध्यान_मास्क मूल [1, 0] से [0, -1e4] में बदल जाता है।

अंततः, यह कॉल बर्टमॉडल (पंक्ति 944) के अग्रवर्ती प्रसार में पाई गई:

```
# हम आयामों का एक आत्म-ध्यान मास्क प्रदान कर सकते हैं [बैच_आकार, from_seq_length, to_seq_length]
#स्वयं जिस स्थिति में हमें इसे सभी प्रमुखों के लिए प्रसारित करने योग्य बनाने की आवश्यकता है।
एक्सटेंडेड_अटेंशन_मास्क: टॉर्च.टेन्सर = सेल्फ.गेट_एक्सटेंडेड_अटेंशन_मास्क(अटेंशन_मास्क, इनपुट_शेप, डिवाइस)

```
समस्या हल हो गई है: यहयह विधि न केवल मुखौटे के मूल्य को बदलती है, बल्कि इसे एक ऐसे आकार में भी प्रसारित करती है जिसे सीधे ध्यान मानचित्र में जोड़ा जा सकता है।
तुम तुम्हारे योग्य हो, हगिंगफेस।

इसके अलावा, ध्यान देने योग्य विवरण हैं:

- प्रत्येक शीर्ष के आयाम के अनुसार स्केल, बर्ट-बेस के लिए यह 64 का वर्गमूल है, यानी 8;
- ध्यान_प्रॉब्स न केवल सॉफ्टमैक्स करता है, बल्कि एक बार ड्रॉपआउट का भी उपयोग करता है। क्या ऐसा इसलिए है क्योंकि वे चिंतित हैं कि ध्यान मैट्रिक्स बहुत सघन है... यहां इसका भी उल्लेख किया गया हैयह बहुत ही असामान्य है, लेकिन मूल ट्रांसफार्मर पेपर ऐसा करता है;
- हेड_मास्क पहले बताए गए मल्टी-हेड गणना के लिए मास्क है। यदि यह सेट नहीं है, तो डिफ़ॉल्ट सभी 1 है, जो यहां काम नहीं करेगा;
- context_layer ध्यान मैट्रिक्स और मान मैट्रिक्स का उत्पाद है: मूल आकार है: (बैच_आकार, num_attention_heads, अनुक्रम_लंबाई, ध्यान_head_size);
- context_layer को स्थानांतरित करने और दृश्य ऑपरेशन निष्पादित करने के बाद, आकार को (बैच_आकार, अनुक्रम_लंबाई) में पुनर्स्थापित किया जाता हैएच, छिपा_आकार)।

```अजगर
क्लास बर्टसेल्फअटेंशन(एनएन.मॉड्यूल):
def __init__(self, config):super().__init__()
यदि config.hidden_size % config.num_attention_heads != 0 और hasattr नहीं है(config, "embedding_size"):
वैल्यूएरर बढ़ाएं(
f"छिपा हुआ आकार ({config.hidden_size}) ध्यान की संख्या का गुणक नहीं है"
एफ"हेड्स ({config.num_attention_heads})"
)

self.num_attention_heads = config.num_attention_heads
self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
स्वयं.सर्व_सिर_आकार = स्व.संख्या_ध्यान_शीर्ष * स्व.ध्यान_सिर_आकार

self.query = nn.Linear(config.hidden_size, self.all_head_size)

self.key = nn.Linear(config.hidden_size, self.all_head_size)

स्व.मान = nn.रैखिक(config.hidden_size, self.all_head_size)

self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")

यदि self.position_embedding_type == "relative_key" या self.position_embedding_type == "सापेक्ष_कुंजी_क्वेरी":
self.max_position_embeddings = config.max_position_embeddings
self.distance_embedding = nn.embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)

self.is_decoder = config.is_decoder

def transpose_for_scores(स्वयं, x):
new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
x = x.view(*new_x_shape)
वापसी x.permute(0, 2, 1, 3)def आगे(
खुद,
छुपे_राज्य,
ध्यान_मास्क=कोई नहीं,
हेड_मास्क=कोई नहीं,
एनकोडर_हिडन_स्टेट्स=कोई नहीं,
encoder_attention_mask=कोई नहीं,
Past_key_value=कोई नहीं,
आउटपुट_ध्यान=गलत,
):
Mixed_query_layer = self.query(hidden_states)

# यदि इसे क्रॉस-अटेंशन मॉड्यूल के रूप में त्वरित किया जाता है, तो कुंजियाँ
# और मान एक एनकोडर से आते हैं; ध्यान मास्क होना चाहिए
# ऐसा कि एनकोडर के पैडिंग टोकन पर ध्यान नहीं दिया जाता है।
is_cross_attention = encoder_hidden_states कोई नहीं है

यदि is_cross_attention और Past_key_value कोई नहीं है:
# k,v, क्रॉस_अटेंशन का पुन: उपयोग करें
key_layer = Past_key_value[0]
वैल्यू_लेयर = पास्ट_की_मूल्य[1]
ध्यान_मास्क = एनकोडर_ध्यान_मास्क
एलिफ़ is_cross_attention:
key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
वैल्यू_लेयर = self.transpose_for_scores(self.value(encoder_hidden_states))
ध्यान_मास्क = एनकोडर_ध्यान_मास्क
एलिफ पास्ट_की_वैल्यू कोई नहीं है:
key_layer = self.transpose_for_scores(self.key(hidden_states))
वैल्यू_लेयर = self.transpose_for_scores(self.value(hidden_states))
key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
वैल्यू_लेयर = कोrc.cat([past_key_value[1], value_layer], dim=2)
अन्य:
key_layer = self.transpose_for_scores(self.key(hidden_states))n_states))
वैल्यू_लेयर = self.transpose_for_scores(self.value(hidden_states))

query_layer = self.transpose_for_scores(mixed_query_layer)

यदि self.is_decoder:
# यदि क्रॉस_अटेंशन सभी क्रॉस अटेंशन कुंजी/वैल्यू_स्टेट्स के टुपल (टॉर्च.टेन्सर, टॉर्च.टेन्सर) को सेव करें।
# क्रॉस_अटेंशन लेयर पर आगे कॉल करने पर सभी क्रॉस-अटेंशन का पुन: उपयोग किया जा सकता है
# key/value_states (पहला "यदि" केस)
#ifuni-dइरेक्शनल सेल्फ-अटेंशन (डिकोडर) सेवटुपल (टॉर्च.टेन्सर, टॉर्च.टेन्सर)
# सभी पिछले डिकोडर कुंजी/वैल्यू_स्टेट्स। एकदिशात्मक आत्म-ध्यान के लिए आगे की कॉल
# पिछले डिकोडर कुंजी/वैल्यू_स्टेट्स को वर्तमान अनुमानित कुंजी/वैल्यू_स्टेट्स (तीसरा "एलिफ़" केस) से जोड़ सकता है
# यदि एनकोडर द्वि-दिशात्मक आत्म-ध्यान `past_key_value` हमेशा `None` है
पास्ट_की_वैल्यू = (की_लेयर, वैल्यू_लेयर)

# मूल ध्यान स्कोर प्राप्त करने के लिए "क्वेरी" और "कुंजी" के बीच डॉट उत्पाद लें।
ध्यान_अंक = मशाल.माtmul(query_layer, key_layer.transpose(-1, -2))

यदि self.position_embedding_type == "relative_key" या self.position_embedding_type == "relative_key_query":
seq_length =hidden_states.size()[1]
पोजीशन_आईडी_एल = टॉर्च.अरेंज(seq_length, dtype=torch.long, डिवाइस=hidden_states.device).view(-1, 1)
पोजीशन_आईडी_आर = टॉर्च.अरेंज(seq_length, dtype=torch.long, डिवाइस=hidden_states.device).view(1, -1)
दूरी = स्थिति_आईडी_एल- स्थिति_आईडी_आर
पोजिशनल_एम्बेडिंग = स्व.दूरी_एम्बेडिंग(दूरी + स्व.एमax_position_embeddings - 1)
पोजीशनल_एम्बेडिंग = पोजीशनल_एम्बेडिंग.टू(dtype=query_layer.dtype) # fp16 अनुकूलता

यदि self.position_embedding_type == "relative_key":
रिलेटिव_पोजिशन_स्कोर्स = टॉर्च.ईंसम ("बीएचएलडी,एलआरडी->बीएचएलआर", क्वेरी_लेयर, पोजिशनल_एम्बेडिंग)
ध्यान_स्कोर = ध्यान_स्कोर + सापेक्ष_स्थिति_स्कोर
elif self.position_embedding_type == "relative_key_query":
रिलेटिव_पोजिशन_स्कोर्स_क्वेरी = टॉर्च.ईंसम("बीएचएलडी,एलआरडी->बीएचएलआर", क्वेरी_लेयर, पोजिशनल_एम्बेडिंग)
सापेक्ष_स्थितिtion_scores_key = torch.einsum("bhrd,lrd->bhlr", key_layer,positional_embedding)
ध्यान_स्कोर = ध्यान_स्कोर + सापेक्ष_स्थिति_स्कोर_क्वेरी + सापेक्ष_स्थिति_स्कोर_कुंजी

ध्यान_स्कोर = ध्यान_स्कोर / गणित.वर्ग(स्वयं.ध्यान_शीर्ष_आकार)
यदि ध्यान_मास्क नहीं कोई नहीं:
# अटेंशन मास्क लागू करें (बर्टमॉडल फॉरवर्ड() फ़ंक्शन में सभी परतों के लिए पूर्व-गणना की गई)
ध्यान_स्कोर = ध्यान_स्कोर + ध्यान_मास्क

# संभावनाओं पर ध्यान स्कोर को सामान्य करें।
ध्यान_प्रॉब्स =एनएन.सॉफ्टमैक्स(मंद=-1)(ध्यान_स्कोर)

# यह वास्तव में प्रतीक्षा करने के लिए संपूर्ण टोकन खो रहा है, जो हो सकता है
# थोड़ा असामान्य लगता है, लेकिन मूल ट्रांसफार्मर पेपर से लिया गया है।
ध्यान_समस्याएँ = स्व.ड्रॉपआउट(ध्यान_समस्याएँ)

#चाहें तो सिर पर मास्क लगाएं
यदि हेड_मास्क कोई नहीं है:
ध्यान_प्रॉब्स = ध्यान_प्रॉब्स * हेड_मास्क

सन्दर्भ_परत = मशाल.मैटमूल(ध्यान_प्रोब्स, मूल्य_परत)

context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
new_context_layer_shape = context_layer.size()[:-2] + (स्वयं.सभी_सिर_आकार,)
context_layer = context_layer.view(*new_context_layer_shape)

आउटपुट = (संदर्भ_लेयर, ध्यान_प्रॉब्स) यदि आउटपुट_अटेंशन अन्य (संदर्भ_लेयर,)

यदि self.is_decoder:
आउटपुट = आउटपुट + (past_key_value,)
वापसी आउटपुट
```

***
##### 2.2.1.1.2 बर्टसेल्फआउटपुट
```
क्लास BertSelfOutput(nn.Module):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.dense = nn.Linear(config.hidden_size, config.hidden_size)
self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=confआईजी.लेयर_नॉर्म_ईपीएस)
self.dropout = nn.Dropout(config.hidden_dropout_prob)def आगे (स्वयं,hidden_states, इनपुट_tensor):
hidden_states = self.dense(hidden_states)
छुपे हुए राज्य = स्व.ड्रॉपआउट(छिपे हुए राज्य)
hidden_states = self.LayerNorm(hidden_states + input_tensor)
छुपे हुए_स्टेट्स वापस करें
```

**यहां फिर से, लेयरनॉर्म और ड्रॉपआउट का संयोजन दिखाई देता है, सिवाय इसके कि ड्रॉपआउट का उपयोग पहले किया जाता है, और फिर अवशिष्ट कनेक्शन को लेयरनॉर्म से पहले निष्पादित किया जाता हैसबसे सीधा उद्देश्य बहुत गहरी नेटवर्क परतों के कारण होने वाली प्रशिक्षण की कठिनाई को कम करना और मूल इनपुट के प्रति अधिक संवेदनशील होना है~**

```अजगर

क्लास BertSelfOutput(nn.Module):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.dense = nn.Linear(config.hidden_size, config.hidden_size)
self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)

डीईएफ़ फ़ॉरवर्ड(स्वयं, छुपे_स्टेट्स, इनपुट_टेंसर):
छुपे हुए राज्य = स्वयं.घना(hidden_states)
छुपे हुए राज्य = स्व.ड्रॉपआउट(छिपे हुए राज्य)
hidden_states = self.LayerNorm(hidden_states + input_tensor)
छुपे हुए_स्टेट्स वापस करें
```

***
#### 2.2.1.2 बर्टइंटरमीडिएट

बर्टअटेंशन पढ़ें, अटेंशन के बाद एक पूर्ण कनेक्शन + सक्रियण ऑपरेशन होता है:
```
क्लास बर्टइंटरमीडिएट(एनएन.मॉड्यूल):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
यदि isinstance(config.hidden_act, str):
self.intermediate_act_fn = अधिनियम2FN[config.hidden_act]
अन्य:
self.intermediate_act_fn = config.hidden_act

डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):
hidden_states = self.dense(hidden_states)
hidden_states = self.intermediate_act_fn(hidden_states)
छुपे हुए_स्टेट्स वापस करें
```

- यहां पूरा कनेक्शन विस्तारित है, उदाहरण के तौर पर बर्ट-बेस को लेते हुए, विस्तारित आयाम 3072 है, जो 768 के मूल आयाम से 4 गुना है;
- यहां सक्रियण फ़ंक्शन डिफ़ॉल्ट रूप से गेलू (गॉसियन एरर लाइनर यूनिट्स (जीईएलयूएस)) के रूप में कार्यान्वित किया जाता है।इसकी सीधे गणना नहीं की जा सकती है, लेकिन तन (छोड़े गए) वाले एक अभिव्यक्ति द्वारा इसका अनुमान लगाया जा सकता है।

```अजगर
क्लास बर्टइंटरमीडिएट(एनएन.मॉड्यूल):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
यदि isinstance(config.hidden_act, str):
self.intermediate_act_fn= ACT2FN[config.hidden_act]
अन्य:
self.intermediate_act_fn = config.hidden_act

डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):
hidden_states = self.dense(hidden_states)
छुपे हुए राज्य =self.intermediate_act_fn(hidden_states)
छुपे हुए_स्टेट्स वापस करें
```

***
#### 2.2.1.3 बर्टआउटपुट

यहां एक और पूर्ण कनेक्शन + ड्रॉपआउट + लेयरनॉर्म और एक अवशिष्ट कनेक्शन अवशिष्ट कनेक्ट है:
```
क्लास बर्टआउटपुट(एनएन.मॉड्यूल):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)

def आगे (स्वयं, छिपा हुआ)।_स्टेट्स, इनपुट_टेंसर):
hidden_states = self.dense(hidden_states)
छुपे हुए राज्य = स्व.ड्रॉपआउट(छिपे हुए राज्य)
hidden_states = self.LayerNorm(hidden_states + input_tensor)
छुपे हुए_स्टेट्स वापस करें
```

यहां ऑपरेशन BertSelfOutput से असंबंधित नहीं है, लेकिन यह बिल्कुल वैसा ही है... दो घटक जिन्हें भ्रमित करना बहुत आसान है। निम्नलिखित सामग्री में BERT-आधारित एप्लिकेशन मॉडल, साथ ही BERT-संबंधित ऑप्टिमाइज़र और उपयोग भी शामिल हैं, जो कि अगले लेख में विस्तार से प्रस्तुत किया जायेगा।

```अजगर
क्लास बर्टआउटपुट(एनएन.मॉड्यूल):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)

डीईएफ़ फ़ॉरवर्ड(स्वयं, छुपे_स्टेट्स, इनपुट_टेंसर):
hidden_states = self.dense(hidden_states)
छुपे हुए राज्य = स्व.ड्रॉपआउट(छिपे हुए राज्य)
hidden_states = self.LayerNorm(hidden_states + input_tensor)
छुपे हुए_स्टेट्स वापस करें```

***
### 2.2.3 बर्टपूलर
यह परत केवल वाक्य का पहला टोकन निकालती है, यानी, `[सीएलएस]` के अनुरूप वेक्टर, और फिर इसे आउटपुट के लिए पूरी तरह से कनेक्टेड परत और सक्रियण फ़ंक्शन के माध्यम से पास करती है: (यह हिस्सा वैकल्पिक है क्योंकि पूलिंग में कई हैं विभिन्न ऑपरेशन)

```
क्लास बर्टपूलर (एनएन.मॉड्यूल):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.dense = nn.Linear(config.hidden_size, config.hidden_size)
स्व.सक्रियण = nn.Tanh()

डीईएफ़ फॉरवर्ड(स्वयं, छुपी_स्टेटएस):
# हम केवल छिपी हुई स्थिति को लेकर मॉडल को "पूल" करते हैं
# पहले टोकन के लिए.
फर्स्ट_टोकन_टेंसर = छुपे हुए_स्टेट्स[:, 0]
पूल्ड_आउटपुट = self.dense(first_token_tensor)
पूल्ड_आउटपुट = स्व.सक्रियण(पूलेड_आउटपुट)
पूल्ड_आउटपुट लौटाएँ
```

```अजगर
क्लास बर्टपूलर (एनएन.मॉड्यूल):
def __init__(स्वयं, कॉन्फिग):
सुपर().__init__()
self.dense = nn.Linear(config.hidden_size, config.hidden_size)
स्व.सक्रियण = nn.Tanh()

डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):
# हम बस मॉडल को "पूल" करते हैंछिपी हुई स्थिति को संगत लेते हुए
# पहले टोकन के लिए.
फर्स्ट_टोकन_टेंसर = छुपे हुए_स्टेट्स[:, 0]
पूल्ड_आउटपुट = self.dense(first_token_tensor)
पूल्ड_आउटपुट = स्व.सक्रियण(पूलेड_आउटपुट)
पूल्ड_आउटपुट लौटाएँ
ट्रांसफार्मर.मॉडल.बर्ट.कॉन्फिगरेशन_बर्ट आयात से *
मशाल आयात करें
config = BertConfig.from_pretrained("bert-base-uncased")
बर्ट_पूलर = बर्टपूलर(कॉन्फिग=कॉन्फिग)
प्रिंट करें ("बर्ट पूलर आकार में इनपुट: {}"। प्रारूप (config.hidden_size))
बैच_आकार = 1
seq_len = 2
छिपा_आकार = 768
एक्स =मशाल.रैंड (बैच_आकार, seq_len, छुपे हुए_आकार)
y = बर्ट_पूलर(x)
प्रिंट(y.आकार())
```

बर्ट पूलर आकार में इनपुट: 768
टॉर्च.आकार([1,768])

```अजगर

```

## सारांश
यह अनुभाग बर्ट मॉडल के कार्यान्वयन का विश्लेषण और अध्ययन करता है, मुझे आशा है कि पाठकों को बर्ट कार्यान्वयन की अधिक विस्तृत समझ हो सकती है।

यह ध्यान देने योग्य है कि हगिंगफेस द्वारा कार्यान्वित बर्ट मॉडल में, विभिन्न प्रकार की मेमोरी-सेविंग तकनीकों का उपयोग किया जाता है:

- ग्रेडिएंट चेकपॉइंट, जो फॉरवर्ड प्रोपा को बरकरार नहीं रखता हैगेशन नोड्स और केवल जरूरत पड़ने पर ही उनकी गणना करता है; apply_chunking_to_forward, जो कई छोटे बैचों और कम आयामों में एफएफएन भाग की गणना करता है
- बर्टमॉडल में जटिल एनकैप्सुलेशन और कई घटक शामिल हैं, उदाहरण के लिए, मुख्य घटक इस प्रकार हैं:
- ड्रॉपआउट कुल मिलाकर 1+(1+1+1)x12=37 बार दिखाई देता है;
- लेयरनॉर्म कुल मिलाकर 1+(1+1)x12=25 बार प्रकट होता है;
BertModel में बहुत बड़ी संख्या में पैरामीटर हैं। उदाहरण के तौर पर bert-base को लेते हुए, इसका पैरामीटर वॉल्यूम 109M है।

## आभार
थीका लेख मुख्य रूप से झेजियांग विश्वविद्यालय के ली लुओकिउ द्वारा लिखा गया था, और इस परियोजना के छात्र इसे व्यवस्थित करने और सारांशित करने के लिए जिम्मेदार थे।