## प्रस्तावना
पिछले अनुभाग के बाद, हमने हगिंगफेस ओपन सोर्स कोड लाइब्रेरी में BERT मॉडल का गहन अध्ययन किया, इस अनुभाग में, हम BERT को लागू करने के तरीके के बारे में विस्तार से बताते हैं।

इसमें शामिल ज्यूपिटर को [कोड लाइब्रेरी: अध्याय 3-ट्रांसफॉर्मर मॉडल लिखना: बीईआरटी, डाउनलोड] (https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7) में पाया जा सकता है। % AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AAट्रांसफॉर्मर%E6%A8%A1%E5%9E%8B%EF% बीसी %9एबर्ट)

यह लेख ट्रांसफॉर्मर्स संस्करण 4.4.2 (19 मार्च, 2021 को जारी) परियोजना में बीईआरटी-संबंधित कोड के पाइटोरच संस्करण पर आधारित है, इसका विश्लेषण कोड संरचना, विशिष्ट कार्यान्वयन और सिद्धांतों और उपयोग सहित किया गया है निम्नलिखित सामग्री:

3. BERT-आधारित मॉडल अनुप्रयोग मॉडल
4. BERT प्रशिक्षण और अनुकूलन
5. बर्ट एनएलपी कार्यों को हल करता है
  - बर्टफॉरसीक्वेंसक्लासिफिकेशन
  - बर्टमल्टीचॉइस के लिए
  - बर्टफॉरटोकनक्लासिफिकेशन
  - बर्टफॉरक्वेश्चनआंसरिंग
6. BERT प्रशिक्षण और अनुकूलन
7. पूर्व प्रशिक्षण
  -फ़ाइन ट्यूनिंग
  -एडमडब्ल्यू
  - जोश में आना

## 3-बीईआरटी-आधारित मॉडल
BERT पर आधारित मॉडल /models/bert/modeling_bert.py में लिखे गए हैं, जिनमें BERT पूर्व-प्रशिक्षण मॉडल और BERT वर्गीकरण मॉडल शामिल हैं।

सबसे पहले, निम्नलिखित सभी मॉडल अमूर्त बेस क्लास 'बर्टप्रीट्रेन्डमॉडल' पर आधारित हैं, जो एक बड़े बेस क्लास 'प्रीट्रेन्डमॉडल' पर आधारित है। यहां हम `BertPreTrainedModel` के कार्यों पर ध्यान केंद्रित करते हैं:

मॉडल भार आरंभ करने और मॉडल लोड करते समय `PreTrainedModel` या वर्ग चर से विरासत में मिली कुछ टैग पहचान बनाए रखने के लिए उपयोग किया जाता है।
इसके बाद, हम पहले पूर्व-प्रशिक्षित मॉडल से शुरुआत करते हैं।

***
### 3.1 बर्टफॉरप्रीट्रेनिंग

जैसा कि हम सभी जानते हैं, BERT पूर्व-प्रशिक्षण कार्यों में दो शामिल हैं:

- नकाबपोश भाषा मॉडल (एमएलएम)।): वाक्य में बेतरतीब ढंग से कुछ शब्दों को `[MASK]` से बदलें, फिर प्रत्येक शब्द की जानकारी को एन्कोड करने के लिए वाक्य को BERT में पास करें, और अंत में उस स्थिति में सही शब्द की भविष्यवाणी करने के लिए `[MASK]` की एन्कोडेड जानकारी का उपयोग करें यह कार्य संदर्भ के आधार पर शब्दों के अर्थ को समझने के लिए मॉडल को प्रशिक्षित करने के लिए डिज़ाइन किया गया है;
- अगला वाक्य पूर्वानुमान (एनएसपी): इनपुट वाक्य जोड़े ए और बी को बीईआरटी में जोड़ता है, और यह अनुमान लगाने के लिए `[सीएलएस]` की एन्कोडिंग जानकारी का उपयोग करता है कि क्या बी ए का अगला वाक्य है। यह कार्य मॉडल को समझने के लिए प्रशिक्षित करने के लिए डिज़ाइन किया गया है पूर्वानुमानित वाक्यों के बीच संबंध.


![चित्र बर्ट पूर्व-प्रशिक्षण](./pictures/3-3-bert-lm.png) चित्र बर्ट पूर्व-प्रशिक्षण

कोड के अनुरूप, यह मॉडल जो दो कार्यों को एकीकृत करता है वह BertForPreTraining है, जिसमें दो घटक शामिल हैं:
```
क्लास बर्टफॉरप्रीट्रेनिंग(बर्टप्रीट्रेन्डमॉडल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__(config)

        self.bert = BertModel(config)
        self.cls = BertPreTrainingHeadएस(कॉन्फ़िगरेशन)

        self.init_weights()
    #...
```
यहां BertModel को पिछले अध्याय में विस्तार से पेश किया गया है (ध्यान दें कि डिफ़ॉल्ट `add_pooling_layer=True` यहां सेट है, यानी, `[CLS]` के अनुरूप आउटपुट NSP कार्य के लिए निकाला जाएगा), और `BertPreTrainingHeads `भविष्यवाणी मॉड्यूल दो कार्यों के लिए जिम्मेदार है:
```
क्लास बर्टप्रीट्रेनिंगहेड्स(एनएन.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.predictions = BertLMPredictionHead(config)
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    डीईएफ़ फ़ॉरवर्ड(स्वयं, अनुक्रम_आउटपुट, पूल्ड_आउटपुट):
        भविष्यवाणी_स्कोर = स्व.भविष्यवाणीons(अनुक्रम_आउटपुट)
        seq_relationship_score = self.seq_relationship(pooled_output)
        वापसी पूर्वानुमान_स्कोर, seq_relationship_score
```
एनकैप्सुलेशन की एक और परत: `BertPreTrainingHeads`` BertLMPredictionHead` और NSP कार्य का प्रतिनिधित्व करने वाली एक रैखिक परत को लपेटती है। एनएसपी से संबंधित कार्यों को यहां `बर्टXXXPredictionHead` के साथ समाहित नहीं किया गया है।

**वास्तव में, यह वर्ग संपुटित है, लेकिन इसे BertOnlyNSPHead कहा जाता है और इसका उपयोग यहां नहीं किया जाता है**

`बर्टप्रीट्रेनिंगहेड्स` का अन्वेषण जारी रखें:
```
क्लास BertLMPredictionHead(nn.मॉड्यूल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.transform = BertPredictionHeadTransform(conअंजीर)

        # आउटपुट भार इनपुट एम्बेडिंग के समान हैं, लेकिन हैं
        # प्रत्येक टोकन के लिए केवल-आउटपुट पूर्वाग्रह।
        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, पूर्वाग्रह=गलत)

        self.bias = nn.Parameter(torch.zeros(config.vocab_size))

        # दो वेरिएबल्स के बीच एक लिंक की आवश्यकता है ताकि पूर्वाग्रह को 'resize_token_embeddings' के साथ सही ढंग से आकार दिया जा सके
        self.decoder.bias = self.bias

    डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):
        छुपे हुए राज्य =स्व.परिवर्तन(hidden_states)
        hidden_states = self.decoder(hidden_states)
        छुपे हुए_स्टेट्स वापस करें
```

इस वर्ग का उपयोग श्रेणी के वर्गीकरण आउटपुट के रूप में प्रत्येक शब्द में `[MASK]` स्थिति के आउटपुट की भविष्यवाणी करने के लिए किया जाता है।

- यह वर्ग पूर्वानुमान भार के पूर्वाग्रह के रूप में एक ऑल-0 वेक्टर को पुन: प्रारंभ करता है;
- इस वर्ग का आउटपुट आकार [बैच_आकार, seq_length, vocab_size] है, जो प्रत्येक वाक्य में प्रत्येक शब्द की श्रेणी की भविष्यवाणी करने की संभावना मूल्य है (ध्यान दें कि यहां कोई सॉफ्टमैक्स नहीं है);
- एक अन्य संक्षिप्त वर्ग: BertPredictionHeadTransform, जिसका उपयोग कुछ रैखिक परिवर्तनों को पूरा करने के लिए किया जाता है:
```
क्लास BertPredictionHeadTransform(nn.Module):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_siज़ी)
        यदि isinstance(config.hidden_act, str):
            self.transform_act_fn = ACT2FN[config.hidden_act]
        अन्य:
            self.transform_act_fn = config.hidden_act
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    डीईएफ़ फॉरवर्ड (स्वयं, छुपे_स्टेट्स):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.transform_act_fn(hidden_states)
        hidden_states = self.LayerNorm(hidden_states)
        छुपे हुए_स्टेट्स वापस करें
```

`बर्टफॉरप्रीट्रेनिंग` पर वापस जाएं और देखना जारी रखें कि `नुकसान` के दो टुकड़े कैसे संसाधित होते हैं। इसका आगे का प्रसार बर्टमॉडल से अलग है, दो और इनपुट के साथ: `लेबल` और `नेक्स्ट_सेन्टेंस_लेबल`:

- लेबल: आकार [बैच_आकार, seq_length] है, जो एमएलएम कार्य के लेबल का प्रतिनिधित्व करता है। ध्यान दें कि मूल खुले शब्दों को -100 पर सेट किया गया है, ताकि कवर किए गए शब्दों में उनकी संबंधित आईडी होंगी, जो इसके विपरीत है। कार्य सेटिंग.

  - उदाहरण के लिए, मूल वाक्य यह है कि मैं एक सेब को [मास्क] करना चाहता हूं। यहां मैं इनपुट मॉडल को ईट शब्द के साथ कवर करता हूं, और संबंधित लेबल को [-100, -100, -100, [ईट के अनुरूप आईडी] पर सेट किया गया है। ], - 100, -100];
  - इसे -100 पर क्यों सेट करें और किसी अन्य नंबर पर क्यों नहीं? क्योंकि Torch.nn.CrossEntropyLoss डिफ़ॉल्ट रूप सेign_index=-100 है, जिसका अर्थ है कि 100 के लेबल वाले श्रेणी इनपुट के लिए हानि की गणना नहीं की जाएगी।

- अगला_सेन्टेंस_लेबल: यह इनपुट बहुत सरल है, जो 0 और 1 का बाइनरी लेबल है।

```
#...
    def आगे(
        खुद,
        इनपुट_आईडी=कोई नहीं,
        ध्यान_मास्क=कोई नहीं,
        टोकन_टाइप_आईडी=कोई नहीं,
        स्थिति_आईडी=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        इनपुट्स_एम्बेड्स=कोई नहीं,
        लेबल=कोई नहीं,
        अगला_वाक्य_लेबल=कोई नहीं,
        आउटपुट_ध्यान=कोई नहीं,
        आउटपुट_हिडन_स्टेट्स=कोई नहीं,
        return_dict=कोई नहीं,
    ): ...
```

हानि के अगले दो भागों का संयोजन:
```
 #...
        कुल हानि = कोई नहीं
        यदि लेबल कोई नहीं है और अगला_सेन्टेंस_लेबल कोई नहीं है:
            लॉस_एफसीटी = क्रॉसएंट्रॉपीलोसएस()
            नकाबपोश_एलएम_लॉस = लॉस_एफसीटी(भविष्यवाणी_स्कोर्स.व्यू(-1, सेल्फ.कॉन्फिग.वोकैब_साइज), लेबल्स.व्यू(-1))
            अगला_सेन्टेंस_लॉस = लॉस_एफसीटी(seq_relationship_score.view(-1, 2), नेक्स्ट_सेन्टेंस_लेबल.व्यू(-1))
            कुल_नुकसान = नकाबपोश_एलएम_नुकसान + अगला_वाक्य_नुकसान
        #...
```

प्रत्यक्ष जोड़ एक ऐसी सरल रणनीति है।
बेशक, इस कोड में BERT मॉडल भी शामिल है जो केवल एक ही लक्ष्य को पूर्व-प्रशिक्षित करना चाहता है (विशिष्ट विवरण का विस्तार नहीं किया जाएगा):
- BertForMaskedLM: केवल एमएलएम कार्य पर पूर्व-प्रशिक्षण करें;
  - BertOnlyMLMHead पर आधारित, जो BertLMPredictionHead के एनकैप्सुलेशन की एक और परत भी है;
- BertLMHeadModel: इस मॉडल और पिछले मॉडल के बीच अंतर यह है कि यह मॉडल एक डिकोडर के रूप में चलाया जाता है।पंक्ति संस्करण;
  - BertOnlyMLMHead पर भी आधारित;
- BertForNextSentencePrediction: केवल एनएसपी कार्यों पर पूर्व-प्रशिक्षण करें।
  - BertOnlyNSPHead पर आधारित, सामग्री एक रैखिक परत है।


```अजगर
_CHECKPOINT_FOR_DOC = "बर्ट-बेस-अनकेस्ड"
_CONFIG_FOR_DOC = "बर्टकॉन्फ़िग"
_TOKENIZER_FOR_DOC = "बर्टटोकनाइज़र"
ट्रांसफॉर्मर.मॉडल.बर्ट.मॉडलिंग_बर्ट आयात से *
ट्रांसफार्मर.मॉडल.बर्ट.कॉन्फिगरेशन_बर्ट आयात से *
क्लास बर्टफॉरप्रीट्रेनिंग(बर्टप्रीट्रेन्डमॉडल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__(config)

        self.bert = BertModel(config)self.cls = BertPreTrainingHeads(config)

        self.init_weights()

    def get_output_embeddings(स्वयं):
        self.cls.predictions.decoder लौटाएँ

    def set_output_embeddings(स्वयं, new_embeddings):
        self.cls.predictions.decoder = new_embeddings

    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format("बैच_आकार, अनुक्रम_लंबाई"))
    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)
    def आगे(
        खुद,इनपुट_आईडी=कोई नहीं,
        ध्यान_मास्क=कोई नहीं,
        टोकन_टाइप_आईडी=कोई नहीं,
        स्थिति_आईडी=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        इनपुट्स_एम्बेड्स=कोई नहीं,
        लेबल=कोई नहीं,
        अगला_वाक्य_लेबल=कोई नहीं,
        आउटपुट_ध्यान=कोई नहीं,
        आउटपुट_हिडन_स्टेट्स=कोई नहीं,
        return_dict=कोई नहीं,
    ):
        आर"""
        लेबल (:obj:`torch.LongTensor` आकार का ``(बैच_आकार, अनुक्रम_लंबाई)``, `वैकल्पिक`):
            नकाबपोश भाषा मॉडलिंग सूचकांकों की गणना के लिए लेबल``[-100, 0, ..., में होना चाहिए
            config.vocab_size]`` (देखें ``input_ids`` docstring) ``-100`` पर सेट सूचकांक वाले टोकन को नजरअंदाज कर दिया जाता है
            (नकाबपोश), हानि की गणना केवल ``[0, ..., config.vocab_size]`` में लेबल वाले टोकन के लिए की जाती है
        अगला_सेन्टेंस_लेबल (``मशाल.लॉन्गटेन्सर`` आकार का ``(बैच_आकार,)``, `वैकल्पिक`):
            अगले अनुक्रम पूर्वानुमान (वर्गीकरण) हानि की गणना के लिए लेबल एक अनुक्रम युग्म होना चाहिए
            (देखें :obj:`input_ids` docstriएनजी) सूचकांक ``[0, 1]`` में होने चाहिए:
            - 0 इंगित करता है कि अनुक्रम बी अनुक्रम ए की निरंतरता है,
            - 1 इंगित करता है कि अनुक्रम बी एक यादृच्छिक अनुक्रम है।
        kwargs (:obj:`Dict[str, कोई भी]`, वैकल्पिक, डिफ़ॉल्ट `{}`):
            उन विरासत तर्कों को छिपाने के लिए उपयोग किया जाता है जिन्हें बहिष्कृत कर दिया गया है।
        रिटर्न:
        उदाहरण::
ट्रांसफार्मर से BertTokenizer, BertForPreTraining आयात करें
मशाल आयात करें
टोकननाइज़र = BertTokenizer.from_pretrained('bert-base-uncased')
मॉडल = BertForPreTraining.from_pretrained('बर्ट-बेस-अनकेस्ड')
इनपुट = टोकननाइज़र ("हैलो, मेरा कुत्ता प्यारा है", return_tensors = "pt")
आउटपुट = मॉडल(**इनपुट)
भविष्यवाणी_लॉगिट्स = आउटपुट.भविष्यवाणी_लॉगिट्स
seq_relationship_logits = आउटपुट.seq_relationship_logits
        """
        रिटर्न_डिक्ट = रिटर्न_डिक्ट यदि रिटर्न_डिक्ट कोई और नहीं है self.config.use_return_dict

        आउटपुट = self.bert(
            इनपुट आईडी,
            ध्यान_मास्क = ध्यान_मास्क,
            टोकन_टाइप_आईडी=टोकन_टाइप_आईडी,
            स्थिति_आईडीs=स्थिति_आईडी,
            हेड_मास्क = हेड_मास्क,
            इनपुट्स_एम्बेड्स=इनपुट्स_एम्बेड्स,
            आउटपुट_अटेंशन=आउटपुट_अटेंशन,
            आउटपुट_हिडन_स्टेट्स=आउटपुट_हिडन_स्टेट्स,
            रिटर्न_डिक्ट=रिटर्न_डिक्ट,
        )

        अनुक्रम_आउटपुट, पूल्ड_आउटपुट = आउटपुट[:2]
        भविष्यवाणी_स्कोर, seq_relationship_score = self.cls(अनुक्रम_आउटपुट, पूल्ड_आउटपुट)

        कुल हानि = कोई नहीं
        यदि लेबल कोई नहीं है और अगला_सेन्टेंस_लेबल कोई नहीं है:
            हानि_एफसीटी = क्रॉसएंट्रॉपीलॉस ()
            नकाबपोश_एलएम_लॉस = लॉस_एफसीटी(भविष्यवाणी_स्कोर्स.व्यू(-1, सेल्फ.कॉन्फिग.वोकैब_साइज), लेबल्स.व्यू(-1))
            अगला_सेन्टेंस_लॉस = लॉस_एफसीटी(seq_relationship_score.view(-1, 2), नेक्स्ट_सेन्टेंस_लेबल.व्यू(-1))
            कुल_नुकसान = नकाबपोश_एलएम_नुकसान + अगला_वाक्य_नुकसान

        यदि नहीं तो return_dict:
            आउटपुट = (भविष्यवाणी_स्कोर, seq_relationship_score) + आउटपुट[2:]
            रिटर्न ((टोटल_लॉस,) + आउटपुट) यदि टोटल_लॉस अन्य कोई नहीं आउटपुट है

        आरBertForPreTrainingOutput(
            हानि=कुल_नुकसान,
            भविष्यवाणी_लॉगिट्स=भविष्यवाणी_स्कोर,
            seq_relationship_logits=seq_relationship_score,
            hidden_states=आउटपुट.hidden_states,
            ध्यान=आउटपुट.ध्यान,
        )

ट्रांसफार्मर से BertTokenizer, BertForPreTraining आयात करें
मशाल आयात करें
टोकननाइज़र = BertTokenizer.from_pretrained('bert-base-uncased')
मॉडल = BertForPreTraining.from_pretrained('bert-base-uncased')
इनपुट्स = टोकननाइज़र ("हैलो, एमआपका कुत्ता प्यारा है", return_tensors=”pt”)
आउटपुट = मॉडल(**इनपुट)
भविष्यवाणी_लॉगिट्स = आउटपुट.भविष्यवाणी_लॉगिट्स
seq_relationship_logits = आउटपुट.seq_relationship_logits
```

    BertForPreTraining के कुछ वेट को bert-base-uncased पर मॉडल चेकपॉइंट से आरंभ नहीं किया गया था और नए आरंभ किए गए हैं: ['cls.predictions.decoder.bias']
    भविष्यवाणियों और अनुमान के लिए इसका उपयोग करने में सक्षम होने के लिए आपको संभवतः इस मॉडल को डाउन-स्ट्रीम कार्य पर प्रशिक्षित करना चाहिए।



```अजगर
@add_start_docstrings(
    """बेरसीएलएम फ़ाइन-ट्यूनिंग के लिए शीर्ष पर `भाषा मॉडलिंग` वाला मॉडल, BERT_START_DOCSTRING
)
वर्ग BertLMHeadModel(BertPreTrainedModel):

    _keys_to_ignore_on_load_unexpected = [r"पूलर"]
    _keys_to_ignore_on_load_missing = [r"position_ids", r"predictions.decoder.bias"]

    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__(config)

        यदि config.is_decoder नहीं है:
            logger.warning ("यदि आप `BertLMHeadModel` को स्टैंडअलोन के रूप में उपयोग करना चाहते हैं, तो `is_decoder=True जोड़ें।`")

        खुद.बर्ट = बर्टमॉडल(कॉन्फ़िगरेशन, ऐड_पूलिंग_लेयर=गलत)
        self.cls = BertOnlyMLMHead(config)

        self.init_weights()

    def get_output_embeddings(स्वयं):
        self.cls.predictions.decoder लौटाएँ

    def set_output_embeddings(स्वयं, new_embeddings):
        self.cls.predictions.decoder = new_embeddings

    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format("बैच_आकार, अनुक्रम_लंबाई"))
    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)
    def आगे(
        खुद,
        इनपुट_आईडी=कोई नहीं,
        ध्यान_मास्क=कोई नहीं,
        टोकन_टाइप_आईडी=कोई नहीं,
        स्थिति_आईडी=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        इनपुट्स_एम्बेड्स=कोई नहीं,
        एनकोडर_हिडन_स्टेट्स=कोई नहीं,
        एनकोडर_अटेंशन_मास्क=कोई नहीं,
        लेबल=कोई नहीं,
        Past_key_values=कोई नहीं,
        उपयोग_कैश=कोई नहीं,
        आउटपुट_ध्यान=कोई नहीं,
        आउटपुट_हिडन_स्टेट्स=कोई नहीं,
        return_dict=कोई नहीं,
    ):
        आर"""
        encoder_hidden_states (:obj:`torch.FloatTensor` आकार का:obj:`(बैच_आकार, अनुक्रम_लंबाई, छिपा_आकार)`, `वैकल्पिक`):
            एनकोडर की अंतिम परत के आउटपुट पर छिपी-स्थितियों का अनुक्रम क्रॉस-अटेंशन में उपयोग किया जाता है
            मॉडल को डिकोडर के रूप में कॉन्फ़िगर किया गया है।
        एनकोडर_अटेंशन_मास्क (:obj:`torch.FloatTensor` आकार का:obj:`(बैच_आकार, अनुक्रम_लंबाई)`, `वैकल्पिक`):
            एनकोडर इनपुट के पैडिंग टोकन सूचकांकों पर ध्यान देने से बचने के लिए मास्कमें प्रयोग किया जाता है
            यदि मॉडल को डिकोडर के रूप में कॉन्फ़िगर किया गया है तो क्रॉस-अटेंशन ``[0, 1]`` में चयनित है।
            - 1 उन टोकन के लिए जो **नकाबपोश** नहीं हैं,
            - 0 ऐसे टोकन के लिए जो **नकाबपोश** हैं।
        लेबल (:obj:`torch.LongTensor` आकार का:obj:`(बैच_आकार, अनुक्रम_लंबाई)`, `वैकल्पिक`):
            बाएँ से दाएँ भाषा मॉडलिंग हानि की गणना के लिए लेबल (अगले शब्द सूचकांक में होना चाहिए)।
            ``[-100, 0, ..., config.vocab_size]`` (देखें ``input_ids`` docstring) ``-100`` पर सेट सूचकांक वाले टोकन हैं
            अनदेखा (नकाबपोश), नुकसान की गणना केवल n ``[0, ..., config.vocab_size]`` लेबल वाले टोकन के लिए की जाती है
        Past_key_values ​​​​(:obj:`tuple(tuple(torch.FloatTensor))` लंबाई का :obj:`config.n_layers` प्रत्येक टुपल में आकार के 4 टेंसर होते हैं:obj:`(बैच_आकार, num_heads, अनुक्रम_लंबाई - 1, एम्बेड_आकार_per_head )` ):
            इसमें ध्यान ब्लॉकों की पूर्व-गणना की गई कुंजी और मूल्य छिपी हुई स्थितियाँ शामिल हैं जिनका उपयोग गति बढ़ाने के लिए किया जा सकता हैडिकोडिंग
            यदि :obj:`past_key_values` का उपयोग किया जाता है, तो उपयोगकर्ता वैकल्पिक रूप से केवल अंतिम इनपुट कर सकता है :obj:`decoder_input_ids`
            (जिनके पास इस मॉडल को दी गई अपनी पिछली कुंजी मान स्थिति नहीं है) आकार की :obj:`(batch_size, 1)`
            सभी के बजाय :obj:`decoder_input_ids` आकार का :obj:`(बैच_आकार, अनुक्रम_लंबाई)`।
        उपयोग_कैश (:obj:`बूल`, `वैकल्पिक`):
            यदि :obj:`True` पर सेट किया जाता है, तो :obj:`past_key_values` कुंजी मान स्थितियाँ लौटा दी जाती हैं और गति बढ़ाने के लिए इसका उपयोग किया जा सकता हैऊपर
            डिकोडिंग (देखें :obj:`past_key_values`)।
        रिटर्न:
        उदाहरण::
            ट्रांसफार्मर से BertTokenizer, BertLMHeadModel, BertConfig आयात करें
            मशाल आयात करें
            टोकननाइज़र = BertTokenizer.from_pretrained('bert-base-cased')
            config = BertConfig.from_pretrained("bert-base-cased")
            config.is_decoder = सत्य
            मॉडल = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)
            इनपुट्स = टोकननाइज़र ("हैलो, मायकुत्ता प्यारा है", return_tensors=”pt”)
            आउटपुट = मॉडल(**इनपुट)
            भविष्यवाणी_लॉगिट्स = आउटपुट.लॉगिट्स
        """
        रिटर्न_डिक्ट = रिटर्न_डिक्ट यदि रिटर्न_डिक्ट कोई और नहीं है self.config.use_return_dict
        यदि लेबल कोई नहीं है:
            उपयोग_कैश = गलत

        आउटपुट = self.bert(
            इनपुट आईडी,
            ध्यान_मास्क = ध्यान_मास्क,
            टोकन_टाइप_आईडी=टोकन_टाइप_आईडी,
            स्थिति_आईडी=स्थिति_आईडी,
            हेड_मास्क = हेड_मास्क,इनपुट्स_एम्बेड्स=इनपुट्स_एम्बेड्स,
            एनकोडर_हिडन_स्टेट्स=एनकोडर_हिडन_स्टेट्स,
            एनकोडर_अटेंशन_मास्क=एनकोडर_अटेंशन_मास्क,
            पास्ट_की_वैल्यू=पास्ट_की_वैल्यू,
            उपयोग_कैश=उपयोग_कैश,
            आउटपुट_अटेंशन=आउटपुट_अटेंशन,
            आउटपुट_हिडन_स्टेट्स=आउटपुट_हिडन_स्टेट्स,
            रिटर्न_डिक्ट=रिटर्न_डिक्ट,
        )

        अनुक्रम_आउटपुट = आउटपुट[0]
        भविष्यवाणी_स्कोर = self.cls(अनुक्रम_आउटपुट)

        lm_los = कोई नहींयदि लेबल कोई नहीं है:
            # हम अगली-टोकन भविष्यवाणी कर रहे हैं; भविष्यवाणी स्कोर और इनपुट आईडी को एक-एक करके बदलें
            शिफ्ट_प्रेडिक्शन_स्कोर्स = प्रेडिक्शन_स्कोर्स[:, :-1, :].सन्निहित()
            लेबल = लेबल[:, 1:].सन्निहित()
            हानि_एफसीटी = क्रॉसएंट्रॉपीलॉस()
            lm_los = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))

        यदि नहीं तो return_dict:
            आउटपुट = (भविष्यवाणी_स्कोर,) + आउटपुट[2:]रिटर्न ((lm_los,) + आउटपुट) यदि lm_los कोई और नहीं आउटपुट है

        वापसी CausalLMOutputWithCrossAttentions(
            हानि=lm_नुकसान,
            लॉगिट=भविष्यवाणी_स्कोर,
            Past_key_values=outputs.past_key_values,
            hidden_states=आउटपुट.hidden_states,
            ध्यान=आउटपुट.ध्यान,
            क्रॉस_अटेंशन=आउटपुट.क्रॉस_अटेंशन,
        )

    डीईएफ़ तैयार_इनपुट_फॉर_जेनरेशन(स्वयं, इनपुट_आईडी, अतीत=कोई नहीं, ध्यान_मास्क=कोई नहीं, **मॉडल_क्वार्ग्स):इनपुट_शेप = इनपुट_आईड्स.शेप
        # यदि मॉडल को एनकोडर-डिकोडर मॉडल में डिकोडर के रूप में उपयोग किया जाता है, तो डिकोडर ध्यान मास्क तुरंत बनाया जाता है
        यदि ध्यान_मास्क कोई नहीं है:
            ध्यान_मास्क = इनपुट_आईडी.न्यू_ऑन्स (इनपुट_शेप)

        # यदि अतीत का उपयोग किया जाता है तो डिकोडर_इनपुट_आईडी काटें
        यदि अतीत कोई नहीं है:
            इनपुट_आईडी = इनपुट_आईडी[:, -1:]

        वापसी {"इनपुट_आईडी": इनपुट_आईडी, "अटेंशन_मास्क": अटेंशन_मास्क, "पास्ट_की_वैल्यू": अतीत}

    def _reorder_cache(स्वयं, अतीत, बीम_आईडीएक्स):
        पुन: व्यवस्थित_अतीत = ()
        अतीत में लेयर_पास्ट के लिए:
            पुनः क्रमित_पास्ट += (लेयर_पास्ट में पास्ट_स्टेट के लिए टुपल(पास्ट_स्टेट.इंडेक्स_सेलेक्ट(0, बीम_आईडीएक्स)),)
        पुन: व्यवस्थित_अतीत लौटें

ट्रांसफार्मर से BertTokenizer, BertLMHeadModel, BertConfig आयात करें
मशाल आयात करें
टोकननाइज़र = BertTokenizer.from_pretrained('bert-base-uncased')
config = BertConfig.from_pretrained("bert-base-uncased")
config.is_decoder = सत्य
मॉडल = BertLMHeadModel.from_pretrained('bert-base-uncased', conअंजीर=कॉन्फिग)
इनपुट = टोकननाइज़र ("हैलो, मेरा कुत्ता प्यारा है", return_tensors = "pt")
आउटपुट = मॉडल(**इनपुट)
भविष्यवाणी_लॉगिट्स = आउटपुट.लॉगिट्स
```

    BertLMHeadModel को आरंभ करते समय बर्ट-बेस-अनकेस्ड पर मॉडल चेकपॉइंट के कुछ वजन का उपयोग नहीं किया गया था: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
    - यदि आप BertLMHeadModel को किसी अन्य कार्य पर प्रशिक्षित मॉडल के चेकपॉइंट से या किसी अन्य आर्किटेक्चर के साथ प्रारंभ कर रहे हैं (उदाहरण के लिए BertForSequenceCl प्रारंभ करना) तो यह अपेक्षित हैBertForPreTraining मॉडल से एस्सिफिकेशन मॉडल)।
    - यदि आप किसी ऐसे मॉडल के चेकपॉइंट से BertLMHeadModel प्रारंभ कर रहे हैं, जिसके बिल्कुल समान होने की आप अपेक्षा करते हैं तो यह अपेक्षित नहीं है (BertForSequenceClassification मॉडल से BertForSequenceClassification मॉडल प्रारंभ करना)।



```अजगर
क्लास BertForNextSentencePrediction(BertPreTrainedModel):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__(config)

        self.bert = BertModel(config)
        self.cls = BertOnlyNSPHead(coएनएफआईजी)

        self.init_weights()

    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format("बैच_आकार, अनुक्रम_लंबाई"))
    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)
    def आगे(
        खुद,
        इनपुट_आईडी=कोई नहीं,
        ध्यान_मास्क=कोई नहीं,
        टोकन_टाइप_आईडी=कोई नहीं,
        स्थिति_आईडी=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        इनपुट्स_एम्बेड्स=कोई नहीं,
        लेबल=कोई नहीं,
        आउटपुट_ध्यान=कोई नहीं,
        उत्पादनt_hidden_states=कोई नहीं,
        return_dict=कोई नहीं,
        **क्वार्ग्स,
    ):
        आर"""
        लेबल (:obj:`torch.LongTensor` आकार का:obj:`(बैच_आकार,)`, `वैकल्पिक`):
            अगले अनुक्रम पूर्वानुमान (वर्गीकरण) हानि की गणना के लिए लेबल एक अनुक्रम युग्म होना चाहिए
            (देखें ``input_ids`` डॉकस्ट्रिंग) सूचकांक ``[0, 1]`` में होने चाहिए:
            - 0 इंगित करता है कि अनुक्रम बी अनुक्रम ए की निरंतरता है,
            - 1 इंगित करता है कि अनुक्रम बी एक यादृच्छिक अनुक्रम है।रिटर्न:
        उदाहरण::
ट्रांसफार्मर से BertTokenizer, BertForNextSentencePrediction आयात करें
मशाल आयात करें
टोकननाइज़र = BertTokenizer.from_pretrained('bert-base-uncased')
मॉडल = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
प्रॉम्प्ट = "इटली में, औपचारिक सेटिंग में, जैसे कि किसी रेस्तरां में, पिज़्ज़ा परोसा जाता है, बिना स्लाइस के प्रस्तुत किया जाता है।"
अगला_वाक्य = "नीले प्रकाश की तरंगदैर्घ्य कम होने के कारण आकाश नीला है।"
एन्कोडिंग = टोकननाइज़र (प्रॉम्प्ट, नेक्स्ट_सेन्टेंस, रिटर्न_टेंसर्स = 'पीटी')
आउटपुटs = मॉडल(**एन्कोडिंग, लेबल=मशाल.LongTensor([1]))
लॉगिट्स = आउटपुट.लॉगिट्स
एस्टर लॉगिट्स[0, 0] < लॉगिट्स[0, 1] # अगला वाक्य यादृच्छिक था
        """

        यदि kwargs में "next_sentence_label":
            चेतावनियाँ.चेतावनी(
                "`next_sentence_label` तर्क अप्रचलित है और भविष्य के संस्करण में हटा दिया जाएगा, इसके बजाय `लेबल` का उपयोग करें।"
                भविष्य की चेतावनी,
            )
            लेबल = kwargs.pop("next_sentence_label")

        रिटर्न_डिक्ट = रिटर्न_डिक्ट यदि रिटर्न_डीसीटी कोई और नहीं है self.config.use_return_dict

        आउटपुट = self.bert(
            इनपुट आईडी,
            ध्यान_मास्क = ध्यान_मास्क,
            टोकन_टाइप_आईडी=टोकन_टाइप_आईडी,
            स्थिति_आईडी=स्थिति_आईडी,
            हेड_मास्क = हेड_मास्क,
            इनपुट्स_एम्बेड्स=इनपुट्स_एम्बेड्स,
            आउटपुट_अटेंशन=आउटपुट_अटेंशन,
            आउटपुट_हिडन_स्टेट्स=आउटपुट_हिडन_स्टेट्स,
            रिटर्न_डिक्ट=रिटर्न_डिक्ट,
        )

        पूल्ड_आउटपुट = आउटपुट[1]

        seq_रिलेशनशिप_स्कोर्स = self.cls(पूल_आउटपुट)

        अगला_वाक्य_हानि = कोई नहीं
        यदि लेबल कोई नहीं है:
            हानि_एफसीटी = क्रॉसएंट्रॉपीलॉस()
            अगला_वाक्य_हानि = हानि_एफसीटी(seq_relationship_scores.view(-1, 2), labels.view(-1))

        यदि नहीं तो return_dict:
            आउटपुट = (seq_relationship_scores,) + आउटपुट[2:]
            रिटर्न ((नेक्स्ट_सेन्टेंस_लॉस,) + आउटपुट) यदि नेक्स्ट_सेन्टेंस_लॉस कोई और आउटपुट नहीं है

        NextSentencePredictorOutput लौटाएं(
            नुकसान=अगला_वाक्य_हानि,
            लॉगिट्स=seq_relationship_scores,
            hidden_states=आउटपुट.hidden_states,
            ध्यान=आउटपुट.ध्यान,
        )
ट्रांसफार्मर से BertTokenizer, BertForNextSentencePrediction आयात करें
मशाल आयात करें
टोकननाइज़र = BertTokenizer.from_pretrained('bert-base-uncased')
मॉडल = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
प्रॉम्प्ट = "इटली में, औपचारिक सेटिंग में, जैसे कि किसी रेस्तरां में, पिज़्ज़ा परोसा जाता है, बिना स्लाइस के प्रस्तुत किया जाता है।"
अगला_सेंटेnce = "नीले प्रकाश की तरंगदैर्घ्य कम होने के कारण आकाश नीला है।"
एन्कोडिंग = टोकननाइज़र (प्रॉम्प्ट, नेक्स्ट_सेन्टेंस, रिटर्न_टेंसर्स = 'पीटी')
आउटपुट = मॉडल(**एन्कोडिंग, लेबल=मशाल.लॉन्गटेन्सर([1]))
लॉगिट्स = आउटपुट.लॉगिट्स
एस्टर लॉगिट्स[0, 0] < लॉगिट्स[0, 1] # अगला वाक्य यादृच्छिक था
```

    डाउनलोडिंग: 100%|██████████|440M/440M [00:30<00:00, 14.5MB/s]
    BertForNextSentencePrediction को आरंभ करते समय बर्ट-बेस-अनकेस्ड पर मॉडल चेकपॉइंट के कुछ वज़न का उपयोग नहीं किया गया था: ['cls.predictions.bias','cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions .transform.LayerNorm.bias']
    - यह अपेक्षित है यदि आप किसी अन्य कार्य पर या किसी अन्य आर्किटेक्चर के साथ प्रशिक्षित मॉडल के चेकपॉइंट से BertForNextSentencePrediction प्रारंभ कर रहे हैं (उदाहरण के लिए BertForPreTraining मॉडल से BertForSequenceClassification मॉडल प्रारंभ करना)।
    - यदि आप आईएनआई हैं तो यह अपेक्षित नहीं हैकिसी मॉडल के चेकपॉइंट से BertForNextSentencePrediction को टियलाइज़ करना, जिसकी आप बिल्कुल समान होने की उम्मीद करते हैं (BertForSequenceClassification मॉडल से BertForSequenceClassification मॉडल को प्रारंभ करना)।


इसके बाद, हम विभिन्न फाइन-ट्यून मॉडल पेश करते हैं, जो मूल रूप से वर्गीकरण कार्य हैं:

![बर्ट: फाइनट्यून](./pictures/3-4-bert-ft.png) चित्र: बर्ट: फाइनट्यून

***
### 3.2 BertForSequenceClassification
इस मॉडल का उपयोग वाक्य वर्गीकरण (या प्रतिगमन) कार्यों के लिए किया जाता है, जैसे GLUE बेंचमार्क के विभिन्न कार्य।
- वाक्य वर्गीकरण का इनपुट एक वाक्य (जोड़ी) है, और आउटपुट एक एकल वर्गीकरण लेबल है।

संरचना बहुत सरल है, अर्थात, `बर्टमॉडल` (पूलिंग के साथ) एक ड्रॉपआउट से गुजरता है और आउटपुट वर्गीकरण के लिए एक रैखिक परत के बाद होता है:
```
क्लास बर्टफॉरसेकएन्सेक्लासिफिकेशन(बर्टप्रीट्रेन्डमॉडल):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config)
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        self.init_weights()
        #...
```

आगे प्रसार के दौरान, लेबल इनपुट को उपरोक्त पूर्व-प्रशिक्षित मॉडल की तरह पारित करने की आवश्यकता होती है।

- यदि आरंभिक num_labels=1 है, तो यह प्रतिगमन कार्य के लिए डिफ़ॉल्ट है और MSELoss का उपयोग करता है;

- अन्यथा इसे वर्गीकरण कार्य माना जाता है।


```अजगर
@add_start_docstrings("""
    शीर्ष पर अनुक्रम वर्गीकरण/प्रतिगमन शीर्ष के साथ बर्ट मॉडल ट्रांसफार्मर (पूल के शीर्ष पर एक परत रैखिक
    आउटपुट) उदाहरण के लिए GLUE कार्यों के लिए।
    """,
    BERT_START_DOCSTRING,
)
वर्ग BertForSequenceClassification(BertPreTrainedModel):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__(config)
        self.num_labels = config.num_labels
        self.config = config

        self.bert = BertModel(config)
        क्लासिफायर_ड्रॉपआउट = (
            config.classifier_dropout if config.classifier_dropout कोई और नहीं config.hidden_dropout_prob है
        )
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(क्लासिफायर_ड्रॉपआउट)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        self.init_weights()

    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format("बैच_आकार, अनुक्रम_लंबाई"))
    @add_code_sample_docstrings(
        टोकननाइज़र_क्लास=_TOKENIZER_FOR_DOC,
        चेकप्वाइंट=_CHECKPOINT_FOR_DOC,
        आउटपुट_प्रकार=अनुक्रमक्लासिफायरआउटपुट,
        config_class=_CONFIG_FOR_DOC,
    )
    def आगे(
        खुद,
        इनपुट_आईडी=कोई नहीं,
        ध्यान_मास्क=कोई नहीं,
        टोकन_टाइप_आईडी=कोई नहीं,
        स्थिति_आईडी=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        इनपुट्स_एम्बेड्स=कोई नहीं,
        लेबल=कोई नहीं,
        आउटपुट_ध्यान=कोई नहीं,
        आउटपुट_हिडन_स्टेट्स=कोई नहीं,
        return_dict=कोई नहीं,
    ):
        आर"""
        लेबल (:obj:`torch.LongTensor` आकार का:obj:`(बैच_आकार,)`, `वैकल्पिक`):
            एस की गणना के लिए लेबलअनुक्रम वर्गीकरण/प्रतिगमन हानि सूचकांक में होना चाहिए:obj:`[0, ...,
            config.num_labels - 1]` यदि :obj:`config.num_labels == 1` एक प्रतिगमन हानि की गणना की जाती है (मीन-स्क्वायर हानि),
            यदि :obj:`config.num_labels > 1` वर्गीकरण हानि की गणना की जाती है (क्रॉस-एन्ट्रॉपी)।
        """
        रिटर्न_डिक्ट = रिटर्न_डिक्ट यदि रिटर्न_डिक्ट कोई और नहीं है self.config.use_return_dict

        आउटपुट = self.bert(
            इनपुट आईडी,
            ध्यान_मास्क = ध्यान_मास्क,टोकन_टाइप_आईडी=टोकन_टाइप_आईडी,
            स्थिति_आईडी=स्थिति_आईडी,
            हेड_मास्क = हेड_मास्क,
            इनपुट्स_एम्बेड्स=इनपुट्स_एम्बेड्स,
            आउटपुट_अटेंशन=आउटपुट_अटेंशन,
            आउटपुट_हिडन_स्टेट्स=आउटपुट_हिडन_स्टेट्स,
            रिटर्न_डिक्ट=रिटर्न_डिक्ट,
        )

        पूल्ड_आउटपुट = आउटपुट[1]

        पूल्ड_आउटपुट = स्व.ड्रॉपआउट(पूल_आउटपुट)
        लॉगिट्स = self.classifier(pooled_output)

        हानि=कोई नहीं
        यदि लेबल कोई नहीं है:यदि self.config.problem_type कोई नहीं है:
                यदि self.num_labels == 1:
                    self.config.problem_type = "प्रतिगमन"
                elif self.num_labels > 1 और (लेबल.डीटाइप == टॉर्च.लॉन्ग या लेबल.डीटाइप == टॉर्च.इंट):
                    self.config.problem_type = "single_label_classification"
                अन्य:
                    self.config.problem_type = "multi_label_classification"

            यदि self.config.problem_type == "प्रतिगमन":
                हानि_एफसीटी= MSELoss()
                यदि self.num_labels == 1:
                    हानि = loss_fct(logits.squeeze(), labels.squeeze())
                अन्य:
                    हानि = हानि_एफसीटी (लॉगिट, लेबल)
            elif self.config.problem_type == "single_label_classification":
                हानि_एफसीटी = क्रॉसएंट्रॉपीलॉस()
                हानि = हानि_एफसीटी(लॉगिट्स.व्यू(-1, सेल्फ.नम_लेबल्स), लेबल्स.व्यू(-1))
            elif self.config.problem_type == "multi_label_classification":
                नुकसान_fct = BCEWithLogitsLoss()
                हानि = हानि_एफसीटी (लॉगिट, लेबल)
        यदि नहीं तो return_dict:
            आउटपुट = (लॉगिट्स,) + आउटपुट[2:]
            वापसी ((नुकसान,) + आउटपुट) यदि हानि कोई और नहीं आउटपुट है

        रिटर्न सीक्वेंसक्लासिफायरआउटपुट(
            हानि = हानि,
            लॉगिट्स=लॉगिट्स,
            hidden_states=आउटपुट.hidden_states,
            ध्यान=आउटपुट.ध्यान,
        )
```


```अजगर
ट्रांसफ़ॉर्मर्स.मॉडल.बर्ट.टोकनाइज़ेशन_बर्ट से बर्टटोकनाइज़र आयात करें
इधर-उधरm ट्रांसफार्मर.मॉडल.बर्ट.मॉडलिंग_बर्ट आयात BertForSequenceClassification
टोकननाइज़र = BertTokenizer.from_pretrained("bert-base-cased-finetuned-mrpc")
मॉडल = BertForSequenceClassification.from_pretrained("bert-base-cased-finetuned-mrpc")

क्लासेस = ["पैराफ्रेज़ नहीं", "पैराफ्रेज़ है"]

अनुक्रम_0 = "हगिंगफेस कंपनी न्यूयॉर्क शहर में स्थित है"
अनुक्रम_1 = "सेब आपके स्वास्थ्य के लिए विशेष रूप से खराब हैं"
अनुक्रम_2 = "हगिंगफेस का मुख्यालय मैनहट्टन में स्थित है"

#टोकनाइज़र wमैं स्वचालित रूप से किसी भी मॉडल विशिष्ट विभाजक (यानी <सीएलएस> और <एसईपी>) और टोकन को अनुक्रम में जोड़ूंगा, साथ ही ध्यान मास्क की गणना भी करूंगा।
पैराफ़्रेज़ = टोकननाइज़र(अनुक्रम_0, अनुक्रम_2, रिटर्न_टेंसर = "पीटी")
not_paraphrase = टोकननाइज़र(अनुक्रम_0, अनुक्रम_1, रिटर्न_टेंसर = "पीटी")

पैराफ़्रेज़_वर्गीकरण_लॉगिट्स = मॉडल(**पैराफ़्रेज़).लॉगिट्स
not_paraphrase_classification_logits = मॉडल(**not_paraphrase).logits

पैराफ्रेज़_परिणाम = टॉर्च.सॉफ्टमैक्स(पैराफ़्रेज़_क्लासिफिकेशन_लॉगिट्स, डिम=1).टोलिस्ट()[0]
not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]

# व्याख्या होनी चाहिए
रेंज(लेन(कक्षाएं) में i के लिए):
    प्रिंट(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")

# व्याख्या नहीं होनी चाहिए
रेंज(लेन(कक्षाएं) में i के लिए):
    प्रिंट(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")
```

    डाउनलोडिंग: 100%|██████████|213k/213k [00:00<00:00, 596kB/s]
    डाउनलोडिंग: 100%|██████████|29.0/29.0 [00:00<00:00, 12.4kB/s]डाउनलोडिंग: 100%|██████████|436k/436k [00:00<00:00, 808kB/s]
    डाउनलोडिंग: 100%|██████████|433/433 [00:00<00:00, 166kB/s]
    डाउनलोडिंग: 100%|██████████|433एम/433एम [00:29<00:00, 14.5एमबी/सेकंड]


    व्याख्या नहीं: 10%
    व्याख्या है: 90%
    व्याख्या नहीं: 94%
    व्याख्या है: 6%


***
### 3.3 BertForMultipleChoice

इस मॉडल का उपयोग RocStories/SWAG कार्यों जैसे बहुविकल्पीय कार्यों के लिए किया जाता है।
- बहुविकल्पीय कार्य का इनपुट चरणों में इनपुट वाक्यों का एक सेट है, और आउटपुट एक निश्चित वाक्य का चयन करने के लिए एक एकल लेबल है।
संरचना वाक्य वर्गीकरण के समान है, सिवाय इसके कि रैखिक परत का आउटपुट आयाम 1 है, अर्थात, प्रत्येक नमूने के कई वाक्यों के आउटपुट को प्रत्येक नमूने के पूर्वानुमान स्कोर के रूप में एक साथ जोड़ने की आवश्यकता है।
- वास्तव में, विशिष्ट ऑपरेशन के दौरान, प्रत्येक बैच के कई वाक्यों को एक साथ रखा जाता है, इसलिए एक बारसंसाधित इनपुट वाक्यों की संख्या [बैच_आकार, संख्या_चॉइस] है, इसलिए समान बैच आकार के साथ, इसमें वाक्य वर्गीकरण जैसे कार्यों की तुलना में अधिक वीडियो मेमोरी की आवश्यकता होती है, इसलिए आपको प्रशिक्षण के दौरान सावधान रहने की आवश्यकता है।

***
### 3.4 बर्टफॉरटोकनक्लासिफिकेशन
इस मॉडल का उपयोग अनुक्रम लेबलिंग (शब्द वर्गीकरण) के लिए किया जाता है, जैसे एनईआर कार्य।
- अनुक्रम लेबलिंग कार्य का इनपुट एक एकल वाक्य पाठ है, और आउटपुट प्रत्येक टोकन के अनुरूप श्रेणी लेबल है।
चूंकि प्रत्येक टोकन के अनुरूप आउटपुट को केवल कुछ के बजाय उपयोग करने की आवश्यकता होती है, यहां बर्टमॉडल को पूलिंग परत जोड़ने की आवश्यकता नहीं है;
- उसी समय, क्लास पैरामीटर `_keys_to_ignore_on_load_unexpected` को `[r"pooler"]` पर सेट किया गया है, जिसका अर्थ है कि मॉडल लोड करते समय अनावश्यक भार दिखाई देने पर कोई त्रुटि रिपोर्ट नहीं की जाएगी।


```अजगर
क्लास BertForMultipleChoice(BertPreTrainedModel):
    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__(config)

        self.bert = BertModel(config)स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, 1)

        self.init_weights()

    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format("बैच_आकार, संख्या_चॉइस, अनुक्रम_लंबाई"))
    @add_code_sample_docstrings(
        टोकननाइज़र_क्लास=_TOKENIZER_FOR_DOC,
        चेकप्वाइंट=_CHECKPOINT_FOR_DOC,
        आउटपुट_प्रकार=मल्टीपलचॉइसमॉडलआउटपुट,
        config_class=_CONFIG_FOR_DOC,
    )
    def आगे(खुद,
        इनपुट_आईडी=कोई नहीं,
        ध्यान_मास्क=कोई नहीं,
        टोकन_टाइप_आईडी=कोई नहीं,
        स्थिति_आईडी=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        इनपुट्स_एम्बेड्स=कोई नहीं,
        लेबल=कोई नहीं,
        आउटपुट_ध्यान=कोई नहीं,
        आउटपुट_हिडन_स्टेट्स=कोई नहीं,
        return_dict=कोई नहीं,
    ):
        आर"""
        लेबल (:obj:`torch.LongTensor` आकार का:obj:`(बैच_आकार,)`, `वैकल्पिक`):
            बहुविकल्पी वर्गीकरण हानि की गणना के लिए लेबल ``[0, ...,'' में होने चाहिए।num_choices-1]`` जहां :obj:`num_choices` इनपुट टेंसर के दूसरे आयाम का आकार है (देखें)।
            :obj:`input_ids` ऊपर)
        """
        रिटर्न_डिक्ट = रिटर्न_डिक्ट यदि रिटर्न_डिक्ट कोई और नहीं है self.config.use_return_dict
        num_choices = इनपुट_आईड्स.शेप[1] यदि इनपुट_आईड्स कोई और नहीं है तो इनपुट्स_एम्बेड्स.शेप[1]

        इनपुट_आईडी = इनपुट_आईडी.व्यू(-1, इनपुट_आईडी.आकार(-1)) यदि इनपुट_आईडी कोई नहीं है और कोई नहीं
        ध्यान_मास्क = ध्यान_मास्क.दृश्य(-1, ध्यान_मास्क.आकार(-1)) यदि ध्यान_मास्क कोई नहीं है और कोई नहीं
        टोकन_टाइप_आईडी = टोकन_टाइप_आईडी.व्यू(-1, टोकन_टाइप_आईडी.साइज(-1)) यदि टोकन_टाइप_आईडी कोई नहीं है और कोई नहीं
        स्थिति_आईडी = स्थिति_आईडी.दृश्य(-1, स्थिति_आईडी.आकार(-1)) यदि स्थिति_आईडी कोई नहीं है और कोई नहीं
        इनपुट्स_एम्बेड्स = (
            इनपुट्स_एम्बेड्स.व्यू(-1, इनपुट्स_एम्बेड्स.आकार(-2), इनपुट्स_एम्बेड्स.आकार(-1))
            यदि इनपुट्स_एम्बेड्स कोई नहीं है
            अन्यथा कोई नहीं
        )

        आउटपुट = self.bert(
            मैंnput_id,
            ध्यान_मास्क = ध्यान_मास्क,
            टोकन_टाइप_आईडी=टोकन_टाइप_आईडी,
            स्थिति_आईडी=स्थिति_आईडी,
            हेड_मास्क = हेड_मास्क,
            इनपुट्स_एम्बेड्स=इनपुट्स_एम्बेड्स,
            आउटपुट_अटेंशन=आउटपुट_अटेंशन,
            आउटपुट_हिडन_स्टेट्स=आउटपुट_हिडन_स्टेट्स,
            रिटर्न_डिक्ट=रिटर्न_डिक्ट,
        )

        पूल्ड_आउटपुट = आउटपुट[1]

        पूल्ड_आउटपुट = स्व.ड्रॉपआउट(पूल_आउटपुट)
        लॉगिट्स = self.classifier(pooled_output)resize_logits = logits.view(-1, num_choices)

        हानि=कोई नहीं
        यदि लेबल कोई नहीं है:
            हानि_एफसीटी = क्रॉसएंट्रॉपीलॉस()
            हानि = हानि_एफसीटी (पुनर्निर्मित_लॉगिट्स, लेबल)

        यदि नहीं तो return_dict:
            आउटपुट = (रीशेप्ड_लॉगिट्स,) + आउटपुट[2:]
            वापसी ((नुकसान,) + आउटपुट) यदि हानि कोई और नहीं आउटपुट है

        मल्टीपल चॉइसमॉडलआउटपुट लौटाएं(
            हानि = हानि,
            लॉगिट्स = पुनः आकार_लॉगिट्स,
            hidden_states=outputs.hidden_stateएस,
            ध्यान=आउटपुट.ध्यान,
        )

```


```अजगर
@add_start_docstrings(
    """
    शीर्ष पर एक टोकन वर्गीकरण हेड के साथ बर्ट मॉडल (छिपे हुए राज्यों के आउटपुट के शीर्ष पर एक रैखिक परत) उदाहरण के लिए
    नामित-इकाई-पहचान (एनईआर) कार्य।
    """,
    BERT_START_DOCSTRING,
)
वर्ग BertForTokenClassification(BertPreTrainedModel):

    _keys_to_ignore_on_load_unexpected = [r"पूलर"]

    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__(config)
        self.num_labels =config.num_labels

        self.bert = BertModel(config, add_pooling_layer=गलत)
        क्लासिफायर_ड्रॉपआउट = (
            config.classifier_dropout यदि config.classifier_dropout कोई नहीं है तो config.hidden_dropout_prob
        )
        स्व.ड्रॉपआउट = एनएन.ड्रॉपआउट(क्लासिफायर_ड्रॉपआउट)
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

        self.init_weights()

    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format("बैच_आकार, अनुक्रम_लंबाई"))@add_code_sample_docstrings(
        टोकननाइज़र_क्लास=_TOKENIZER_FOR_DOC,
        चेकप्वाइंट=_CHECKPOINT_FOR_DOC,
        आउटपुट_प्रकार=टोकनक्लासिफायरआउटपुट,
        config_class=_CONFIG_FOR_DOC,
    )
    def आगे(
        खुद,
        इनपुट_आईडी=कोई नहीं,
        ध्यान_मास्क=कोई नहीं,
        टोकन_टाइप_आईडी=कोई नहीं,
        स्थिति_आईडी=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        इनपुट्स_एम्बेड्स=कोई नहीं,
        लेबल=कोई नहीं,
        आउटपुट_ध्यान=कोई नहीं,
        आउटपुट_हिडन_स्टेट्स=कोई नहीं,
        वापसी_निर्देश=कोई नहीं,
    ):
        आर"""
        लेबल (:obj:`torch.LongTensor` आकार का:obj:`(बैच_आकार, अनुक्रम_लंबाई)`, `वैकल्पिक`):
            टोकन वर्गीकरण हानि की गणना के लिए लेबल ``[0, ..., config.num_labels - में होने चाहिए।
            1]``.
        """
        रिटर्न_डिक्ट = रिटर्न_डिक्ट यदि रिटर्न_डिक्ट कोई और नहीं है self.config.use_return_dict

        आउटपुट = self.bert(
            इनपुट आईडी,
            ध्यान_मास्क = ध्यान_मास्क,
            टोकन_टाइप_आईडी=टोकन_टाइप_आईडी,
            स्थिति_आईडी=स्थिति_आईडी,
            हेड_मास्क = हेड_मास्क,
            इनपुट्स_एम्बेड्स=इनपुट्स_एम्बेड्स,
            आउटपुट_अटेंशन=आउटपुट_अटेंशन,
            आउटपुट_हिडन_स्टेट्स=आउटपुट_हिडन_स्टेट्स,
            रिटर्न_डिक्ट=रिटर्न_डिक्ट,
        )

        अनुक्रम_आउटपुट = आउटपुट[0]

        अनुक्रम_आउटपुट = स्व.ड्रॉपआउट(अनुक्रम_आउटपुट)
        लॉगिट्स = स्व.वर्गीकरणकर्ता(अनुक्रम_आउटपुट)

        हानि=कोई नहीं
        यदि लेबल कोई नहीं है:
            हानि_एफसीटी = क्रॉसएंट्रोपाइलॉस()
            # हानि के केवल सक्रिय भाग ही रखें
            यदि ध्यान_मास्क कोई नहीं है:
                सक्रिय_नुकसान = ध्यान_मास्क.दृश्य(-1) == 1
                Active_logits = logits.view(-1, self.num_labels)
                सक्रिय_लेबल = मशाल.कहां(
                    सक्रिय_लॉस, लेबल्स.व्यू(-1), टॉर्च.टेंसर(लॉस_एफसीटी.इग्नोर_इंडेक्स).टाइप_अस(लेबल्स)
                )
                हानि = हानि_एफसीटी (सक्रिय_लॉगिट्स, सक्रिय_लेबल्स)
            अन्य:
                हानि = हानि_एफसीt(logits.view(-1, self.num_labels), labels.view(-1))

        यदि नहीं तो return_dict:
            आउटपुट = (लॉगिट्स,) + आउटपुट[2:]
            वापसी ((नुकसान,) + आउटपुट) यदि हानि कोई और नहीं आउटपुट है

        वापसी टोकनक्लासिफायरआउटपुट(
            हानि = हानि,
            लॉगिट्स=लॉगिट्स,
            hidden_states=आउटपुट.hidden_states,
            ध्यान=आउटपुट.ध्यान,
        )

```


```अजगर
ट्रांसफार्मर से BertForTokenClassification, BertTokenizer आयात करें
मशाल आयात करें

मॉडल = BertForTokenClassification.from_pretrained("dbmdz/bert-large-cased-finetuned-conll03-english")
टोकननाइज़र = BertTokenizer.from_pretrained("bert-base-cased")

लेबल_सूची = [
"ओ", # नामित इकाई के बाहर
"बी-एमआईएससी", # एक अन्य विविध इकाई के ठीक बाद एक विविध इकाई की शुरुआत
"आई-एमआईएससी", # विविध इकाई
"बी-पर", # किसी व्यक्ति के नाम की शुरुआत दूसरे व्यक्ति के नाम के ठीक बाद होती है
"मैं-पर", # व्यक्ति का नाम
"बी-ओआरजी", # एक संगठन के ठीक बाद दूसरे संगठन की शुरुआतsation
"आई-ओआरजी", #संगठन
"बी-एलओसी", # किसी स्थान की शुरुआत दूसरे स्थान के ठीक बाद
"आई-एलओसी" # स्थान
]

अनुक्रम = "हगिंग फेस इंक. न्यूयॉर्क शहर में स्थित एक कंपनी है। इसका मुख्यालय डंबो में है, इसलिए मैनहट्टन ब्रिज के बहुत करीब है।"

# विशेष टोकन के साथ टोकन प्राप्त करने के लिए थोड़ा सा हैक
टोकन = टोकनाइज़र.टोकनाइज़(टोकनाइज़र.डीकोड(टोकनाइज़र.एनकोड(अनुक्रम)))
इनपुट्स = टोकननाइज़र.एनकोड (अनुक्रम, रिटर्न_टेंसर्स = "पीटी")

आउटपुट = मॉडल(इनपुट).लॉगिट्स
भविष्यवाणीएनएस = टॉर्च.आर्गमैक्स(आउटपुट, मंद=2)
```

    डाउनलोडिंग: 100%|██████████|998/998 [00:00<00:00, 382kB/s]
    डाउनलोडिंग: 100%|██████████| 1.33जी/1.33जी [01:30<00:00, 14.7एमबी/सेकंड]



```अजगर
टोकन के लिए, ज़िप में भविष्यवाणी(टोकन, भविष्यवाणियाँ[0].numpy()):
    प्रिंट((टोकन, model.config.id2label[भविष्यवाणी]))
```

    ('[सीएलएस]', 'ओ')
    ('हू', 'आई-ओआरजी')
    ('##gging', 'I-ORG')
    ('फेस', 'आई-ओआरजी')
    ('इंक', 'आई-ओआरजी')
    ('.', 'ओ')
    ('आईएसओ')
    ('ए', 'ओ')
    ('कंपनी', 'ओ')
    ('आधारित', 'ओ')('मैं नहीं')
    ('नया', 'आई-एलओसी')
    ('यॉर्क', 'आई-एलओसी')
    ('शहर', 'आई-एलओसी')
    ('.', 'ओ')
    ('यह इतना')
    ('मुख्यालय', 'ओ')
    ('हैं', 'ओ')
    ('मैं नहीं')
    ('डी', 'आई-एलओसी')
    ('##UM', 'I-LOC')
    ('##बीओ', 'आई-एलओसी')
    (',', 'ओ')
    ('इसलिए', 'ओ')
    ('बहुत', 'ओ')
    ('बंद करें', 'ओ')
    ('बहुत')
    ('द', 'ओ')
    ('मैनहट्टन', 'आई-एलओसी')
    ('ब्रिज', 'आई-एलओसी')
    ('.', 'ओ')
    ('[सितंबर]', 'ओ')


***
### 3.5 BertForQuestionAnswering
इस मॉडल का उपयोग SQuAD कार्य जैसे प्रश्न उत्तर कार्यों को हल करने के लिए किया जाता है।
- प्रश्नोत्तरकार्य का इनपुट एक वाक्य युग्म है जिसमें एक प्रश्न + (बीईआरटी के लिए, यह केवल एक हो सकता है) उत्तर शामिल है, और आउटपुट प्रारंभिक स्थिति और समाप्ति स्थिति है जिसका उपयोग उत्तर में विशिष्ट पाठ को चिह्नित करने के लिए किया जाता है।
यहां दो आउटपुट की आवश्यकता है, अर्थात् प्रारंभिक स्थिति की भविष्यवाणी और अंतिम स्थिति की भविष्यवाणी। दोनों आउटपुट की लंबाई वाक्य की लंबाई के समान है, सबसे बड़े भविष्यवाणी मूल्य के अनुरूप सबस्क्रिप्ट को अनुमानित स्थिति के रूप में चुना जाता है।
- वाक्य की लंबाई से अधिक अवैध लेबल को एक उचित सीमा तक संपीड़ित (torch.clamp_) किया जाएगा।

उपरोक्त BERT स्रोत कोड का परिचय है। यहां BERT मॉडल के बारे में कुछ व्यावहारिक प्रशिक्षण विवरण दिए गए हैं।


```अजगर
@add_start_docstrings(
    """
    SQuAD (एक रैखिक) जैसे निष्कर्षण प्रश्न-उत्तर कार्यों के लिए शीर्ष पर स्पैन वर्गीकरण हेड वाला बर्ट मॉडल
    `स्पैन स्टार्ट लॉगिट्स` और `स्पैन एंड लॉगिट्स` की गणना करने के लिए छिपे हुए राज्य आउटपुट के शीर्ष पर परतें)।
    """,
    BERT_START_डॉकस्ट्रिंग,
)
वर्ग BertForQuestionAnswering(BertPreTrainedModel):

    _keys_to_ignore_on_load_unexpected = [r"पूलर"]

    def __init__(स्वयं, कॉन्फिग):
        सुपर().__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config, add_pooling_layer=गलत)
        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)

        self.init_weights()

    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format("बैच_आकार, अनुक्रम_लंबाई"))
    @add_code_sample_docstrings(
        टोकननाइज़र_क्लास=_TOKENIZER_FOR_DOC,
        चेकप्वाइंट=_CHECKPOINT_FOR_DOC,
        आउटपुट_प्रकार=प्रश्नउत्तर मॉडलआउटपुट,
        config_class=_CONFIG_FOR_DOC,
    )
    def आगे(
        खुद,
        इनपुट_आईडी=कोई नहीं,
        ध्यान_मास्क=कोई नहीं,
        टोकन_टाइप_आईडी=कोई नहीं,
        स्थिति_आईडी=कोई नहीं,
        हेड_मास्क=कोई नहीं,
        इनपुट्स_एम्बेड्स=कोई नहीं,
        प्रारंभ_स्थिति=कोई नहीं,
        अंत_स्थिति=कोई नहीं,
        आउटपुट_ध्यान=कोई नहीं,आउटपुट_हिडन_स्टेट्स=कोई नहीं,
        return_dict=कोई नहीं,
    ):
        आर"""
        प्रारंभ_स्थिति (:obj:`torch.LongTensor` आकार का:obj:`(बैच_आकार,)`, `वैकल्पिक`):
            टोकन वर्गीकरण हानि की गणना के लिए लेबल किए गए स्पैन की शुरुआत की स्थिति (सूचकांक) के लिए लेबल।
            पदों को अनुक्रम की लंबाई (:obj:`sequence_length` के बाहर की स्थिति) से जोड़ा जाता है
            हानि की गणना के लिए अनुक्रम को ध्यान में नहीं रखा जाता है।
        अंत_स्थिति (:obj:`torch.LongTensor` आकार का :obj:`(बैच_आकार,)`, `वैकल्पिक`):
            टोकन वर्गीकरण हानि की गणना के लिए लेबल किए गए स्पैन के अंत की स्थिति (सूचकांक) के लिए लेबल।
            पदों को अनुक्रम की लंबाई (:obj:`sequence_length` के बाहर की स्थिति) से जोड़ा जाता है
            हानि की गणना के लिए अनुक्रम को ध्यान में नहीं रखा जाता है।
        """
        रिटर्न_डिक्ट = रिटर्न_डिक्ट यदि रिटर्न_डिक्ट कोई और नहीं है self.config.use_return_dict

        आउटपुट = self.bert(
            इनपुट आईडी,
            ध्यान_मास्क = ध्यान_मास्क,
            टोकन_टाइप_आईडी=टोकन_टाइप_आईडी,
            स्थिति_आईडी=स्थिति_आईडी,
            हेड_मास्क = हेड_मास्क,
            इनपुट्स_एम्बेड्स=इनपुट्स_एम्बेड्स,
            आउटपुट_अटेंशन=आउटपुट_अटेंशन,
            आउटपुट_हिडन_स्टेट्स=आउटपुट_हिडन_स्टेट्स,
            रिटर्न_डिक्ट=रिटर्न_डिक्ट,
        )

        अनुक्रम_आउटपुट = आउटपुट[0]

        लॉगिट्स = self.qa_outputs(sequence_output)
        स्टार्ट_लॉगिट्स, एंड_लॉगिट्स= लॉगिट्स.स्प्लिट(1, मंद=-1)
        स्टार्ट_लॉगिट्स = स्टार्ट_लॉगिट्स.स्क्वीज़(-1).सन्निहित()
        End_logits = End_logits.squeeze(-1).contiguous()

        कुल हानि = कोई नहीं
        यदि स्टार्ट_पोजीशन कोई नहीं है और एंड_पोजीशन कोई नहीं है:
            # यदि हम मल्टी-जीपीयू पर हैं, तो स्प्लिट एक आयाम जोड़ता है
            यदि len(start_positions.size()) > 1:
                प्रारंभ_स्थितियां = प्रारंभ_स्थितियां.निचोड़ें(-1)
            यदि len(end_positions.size()) > 1:
                अंत_स्थिति = अंत_स्थितिएनएस.निचोड़(-1)
            # कभी-कभी प्रारंभ/अंत स्थिति हमारे मॉडल इनपुट के बाहर होती है, हम इन शर्तों को अनदेखा कर देते हैं
            इग्नोर_इंडेक्स = स्टार्ट_लॉगिट्स.साइज(1)
            स्टार्ट_पोजीशन = स्टार्ट_पोजीशन.क्लैंप(0, इग्नोर_इंडेक्स)
            एंड_पोजीशन = एंड_पोजीशन.क्लैंप(0, इग्नोर_इंडेक्स)

            लॉस_एफसीटी = क्रॉसएंट्रॉपी लॉस (इग्नोर_इंडेक्स = इग्नोर_इंडेक्स)
            स्टार्ट_लॉस = लॉस_एफसीटी (स्टार्ट_लॉगिट्स, स्टार्ट_पोजिशन)
            एंड_लॉस = लॉस_एफसीटी (एंड_लॉगिट्स, एंड_पोजिशन)कुल_लॉस = (स्टार्ट_लॉस + एंड_लॉस) / 2

        यदि नहीं तो return_dict:
            आउटपुट = (स्टार्ट_लॉगिट्स, एंड_लॉगिट्स) + आउटपुट[2:]
            रिटर्न ((टोटल_लॉस,) + आउटपुट) यदि टोटल_लॉस अन्य कोई नहीं आउटपुट है

        वापसी प्रश्नउत्तरमॉडलआउटपुट(
            हानि=कुल_नुकसान,
            स्टार्ट_लॉगिट्स=स्टार्ट_लॉगिट्स,
            एंड_लॉगिट्स=एंड_लॉगिट्स,
            hidden_states=आउटपुट.hidden_states,
            ध्यान=आउटपुट.ध्यान,
        )

```


```अजगर
स्थानांतरण सेमैं ऑटोटोकनाइज़र, ऑटोमॉडलफॉरक्वेस्टियनआंसरिंग आयात करता हूं
मशाल आयात करें

टोकननाइजर = ऑटोटोकनाइजर.फ्रॉम_प्रीट्रेन्ड('बर्ट-लार्ज-अनकेस्ड-होल-वर्ड-मास्किंग-फाइनट्यून्ड-स्क्वाड')
मॉडल = AutoModelForQuestionAnswering.from_pretrained("बर्ट-लार्ज-अनकेस्ड-होल-वर्ड-मास्किंग-फाइनट्यून्ड-स्क्वाड")

टेक्स्ट = "🤗 ट्रांसफॉर्मर (पहले पाइटोरच-ट्रांसफॉर्मर और पाइटोरच-प्रीट्रेन्ड-बर्ट के नाम से जाना जाता था) प्राकृतिक भाषा समझ (एनएलयू) के लिए सामान्य प्रयोजन आर्किटेक्चर (बीईआरटी, जीपीटी-2, रोबर्टा, एक्सएलएम, डिस्टिलबर्ट, एक्सएलनेट…) प्रदान करता है।) और 100+ भाषाओं में 32+ से अधिक पूर्व-प्रशिक्षित मॉडल और TensorFlow 2.0 और PyTorch के बीच गहरी अंतरसंचालनीयता के साथ प्राकृतिक भाषा पीढ़ी (एनएलजी)।

प्रश्न = [
"ट्रांसफॉर्मर में कितने पूर्व-प्रशिक्षित मॉडल उपलब्ध हैं?",
"ट्रांसफॉर्मर क्या प्रदान करता है?",
"🤗 ट्रांसफॉर्मर किन फ्रेमवर्क के बीच अंतरसंचालनीयता प्रदान करता है?",
]

प्रश्नों में प्रश्न के लिए:
    इनपुट = टोकननाइज़र(प्रश्न, पाठ, add_special_tokens=True, return_tensors='pt')
    इनपुट_आईडी = इनपुट["इनपुट_आईडी"].टोलिस्ट()[0]आउटपुट = मॉडल(**इनपुट)
    उत्तर_स्टार्ट_स्कोर = आउटपुट.स्टार्ट_लॉगिट्स
    उत्तर_एंड_स्कोर = आउटपुट.एंड_लॉगिट्स
    उत्तर_स्टार्ट = टॉर्च.आर्गमैक्स(
        उत्तर_प्रारंभ_स्कोर
    ) # स्कोर के आर्गमैक्स के साथ उत्तर की सबसे संभावित शुरुआत प्राप्त करें
    उत्तर_एंड = टॉर्च.आर्गमैक्स(उत्तर_एंड_स्कोर) + 1 # स्कोर के आर्गमैक्स के साथ उत्तर का सबसे संभावित अंत प्राप्त करें
    उत्तर = टोकनाइज़र.कन्वर्ट_टोकेन्स_टू_स्ट्रिंग(टोकनाइज़र.कनवर्ट_आईडी_टू_टोकेन्स(इनपुट_आईडी[उत्तर_प्रारंभ:उत्तर_अंत]))प्रिंट(f"प्रश्न: {प्रश्न}")
    प्रिंट करें (f"उत्तर: {उत्तर}")
```

    डाउनलोडिंग: 100%|██████████|443/443 [00:00<00:00, 186kB/s]
    डाउनलोडिंग: 100%|██████████|232k/232k [00:00<00:00, 438kB/s]
    डाउनलोडिंग: 100%|██████████|466k/466k [00:00<00:00, 845kB/s]
    डाउनलोडिंग: 100%|██████████| 28.0/28.0 [00:00<00:00, 10.5kB/s]
    डाउनलोडिंग: 100%|██████████| 1.34जी/1.34जी [01:28<00:00, 15.1एमबी/सेकेंड]


    प्रश्न: ट्रांसफार्मर में कितने पूर्व प्रशिक्षित मॉडल उपलब्ध हैं?
    उत्तर: 32 से अधिक+
    प्रश्न: 🤗 ट्रांसफार्मर क्या प्रदान करता है?
    उत्तर: सामान्य प्रयोजन वास्तुकला
    प्रश्न: 🤗 ट्रांसफार्मर किस फ्रेमवर्क के बीच अंतरसंचालनीयता प्रदान करते हैं?
    उत्तर: टेंसरफ्लो 2. 0 और पाइटोरच


***
## BERT प्रशिक्षण और अनुकूलन
### 4.1 पूर्व प्रशिक्षण
पूर्व-प्रशिक्षण चरण में, प्रसिद्ध 15% और 80% मुखौटा अनुपात के अलावा, ध्यान देने योग्य एक बात पैरामीटर साझाकरण है।
केवल BERT ही नहीं, हगिंगफेस द्वारा कार्यान्वित PLM के शब्द एम्बेडिंग और नकाबपोश भाषा मॉडल के पूर्वानुमान भार को आरंभीकरण प्रक्रिया के दौरान साझा किया जाता है:
```
क्लास प्रीट्रेंडमॉडल(एनएन.मॉड्यूल, मॉड्यूलयूटिल्समिक्सिन, जेनरेशनमिक्सिन):
    #...
    डीईएफ़ टाई_वेट्स(सेवाम):
        """
        इनपुट एम्बेडिंग और आउटपुट एम्बेडिंग के बीच वजन बांधें।

        यदि :obj:`torchscript` ध्वज कॉन्फ़िगरेशन में सेट है, तो पैरामीटर साझाकरण को संभाल नहीं सकता है इसलिए हम क्लोनिंग कर रहे हैं
        इसके बजाय वजन.
        """
        आउटपुट_एम्बेडिंग = self.get_output_embeddings()
        यदि आउटपुट_एम्बेडिंग कोई नहीं है और self.config.tie_word_embeddings:
            self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())

        यदि self.confg.is_encoder_decoder और self.config.tie_encoder_decoder:
            यदि hasattr(स्वयं, self.base_model_prefix):
                स्वयं = getattr(स्वयं, self.base_model_prefix)
            self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)
    #...
```

ऐसा क्यों होना चाहिए क्योंकि वर्ड_एम्बेडिंग और भविष्यवाणी का वजन बहुत बड़ा है। उदाहरण के तौर पर बर्ट-बेस को लेते हुए, इसका आकार (30522, 768) है, जो प्रशिक्षण की कठिनाई को कम करता है।


***
### 4.2 फ़ाइन-ट्यूनिंग
फ़ाइन-ट्यूनिंग डाउनस्ट्रीम कार्य चरण है, और ध्यान देने योग्य दो बातें हैं।
#### 4.2.1 एडमडब्ल्यू
सबसे पहले, आइए BERT के ऑप्टिमाइज़र का परिचय दें: एडमडब्ल्यू (एडमवेटडेकेऑप्टिमाइज़र)।

यह अनुकूलक I से आता हैएडम के वजन घटाने की त्रुटि को ठीक करने के लिए एक नई विधि सीएलआर 2017 के सर्वश्रेष्ठ पेपर में प्रस्तावित है: "एडम में वजन घटाने के नियमितीकरण को ठीक करना"। पेपर बताता है कि एल2 नियमितीकरण और वजन क्षय ज्यादातर मामलों में समतुल्य नहीं हैं, और केवल एसजीडी अनुकूलन के मामले में समतुल्य हैं जबकि अधिकांश ढांचे एडम+एल2 नियमितीकरण के लिए वजन क्षय विधि का उपयोग करते हैं;

एडमडब्ल्यू एडम+एल2 नियमितीकरण पर आधारित एक बेहतर एल्गोरिदम है। सामान्य एडम+एल2 से अंतर इस प्रकार है:

![चित्र: एडमडब्ल्यू](./चित्र/3-5-एडमडब्ल्यू.पीएनजी) चित्र: एडमडब्ल्यू

एडमडब्ल्यू के विश्लेषण के लिए कृपया देखें:

- एडमडब्ल्यू और सुपर-कन्वर्जेंस अब तंत्रिका जाल को प्रशिक्षित करने का सबसे तेज़ तरीका है [1]
- पेपरप्लेनेट: यह 9102 वर्ष पुराना है, अब एडम + एल2 नियमितीकरण का उपयोग न करें [2]

आमतौर पर, हम क्षय प्रक्रिया में भाग लेने के लिए मॉडल के वजन वाले हिस्से को चुनेंगे, जबकि दूसरा हिस्सा (लेयरनॉर्म के वजन सहित) नहीं चुनता है।भाग लें (कोड का मूल स्रोत हगिंगफेस का उदाहरण होना चाहिए)
अनुपूरक: जहां तक ​​ऐसा करने के कारण का सवाल है, मुझे अभी तक कोई उचित उत्तर नहीं मिला है।

```
# मॉडल: एक बर्ट-आधारित-मॉडल ऑब्जेक्ट
    # सीखने की दर: पाठ वर्गीकरण के लिए डिफ़ॉल्ट 2e-5
    परम_ऑप्टिमाइज़र = सूची (मॉडल.नामांकित_पैरामीटर())
    no_decay = ['पूर्वाग्रह', 'LayerNorm.bias', 'LayerNorm.weight']
    ऑप्टिमाइज़र_ग्रुपेड_पैरामीटर = [
        {'पैराम्स': [एन के लिए पी, पैरा_ऑप्टिमाइज़र में पी यदि कोई नहीं है(
            nd में n के लिए nd में no_decay)], 'weight_decay': 0.01},
        {'पैराम्स': [एन के लिए पी, पैरा_ऑप्टिमाइज़र में पी यदि कोई हो(nd में n के लिए nd में no_decay)], 'weight_decay': 0.0}
    ]
    ऑप्टिमाइज़र = एडमडब्ल्यू (ऑप्टिमाइज़र_ग्रुप्ड_पैरामीटर,
                      एलआर=लर्निंग_रेट)
    #...
```

#### 4.2.2 वार्मअप

BERT के प्रशिक्षण की एक अन्य विशेषता वार्मअप है, जिसका अर्थ है:

प्रशिक्षण की शुरुआत में एक छोटी सीखने की दर (0 से शुरू) का उपयोग करें, और मॉडल को प्रवेश करने से रोकने के लिए इसे धीरे-धीरे एक निश्चित संख्या में चरणों (जैसे 1000 कदम) के भीतर सामान्य आकार (जैसे ऊपर 2e-5) तक बढ़ाएं। स्थानीय इष्टतम समय से पहले और अति-फिटिंग गठबंधन;
- बाद के प्रशिक्षण में बड़े पैरामीटर परिवर्तनों से बचने के लिए प्रशिक्षण के बाद के चरणों में सीखने की दर को धीरे-धीरे 0 तक कम करें।
- हगिंगफेस के कार्यान्वयन में, कई वार्मअप रणनीतियों का उपयोग किया जा सकता है:
```
TYPE_TO_SCHEDULER_FUNCTION = {
    शेड्यूलर प्रकार.LINEAR: get_linear_schedule_with_warmup,
    शेड्यूलर प्रकार.COSINE: get_cosine_schedule_with_warmup,
    शेड्यूलर प्रकार.COSINE_WITH_RESARTTS: get_cosine_with_hard_restarts_schedule_with_warmup,
    शेड्यूलर प्रकार.बहुपद: get_बहुपद_decay_schedule_with_warmup,
    शेड्यूलर प्रकार.CONSTANT: get_constant_schedule,
    शेड्यूलर प्रकार.CONSTANT_WITH_WARMUP: get_constant_schedule_with_warmup,
}
```
विशेष रूप से:
- स्थिर: निश्चित सीखने की दर को अपरिवर्तित रखें;
- CONSTANT_WITH_WARMUP: प्रत्येक चरण में सीखने की दर को रैखिक रूप से समायोजित करें;
- रैखिक: ऊपर उल्लिखित दो-चरणीय समायोजन;
- कोसाइन: दो-चरण समायोजन के समान, सिवाय इसके कि यह त्रिकोणमितीय फ़ंक्शन वक्र समायोजन का उपयोग करता है;
- COSINE_WITH_RESARTTS: प्रशिक्षण के दौरान उपरोक्त COSINE समायोजनों को n बार दोहराएं;- बहुपद: घातीय वक्र के अनुसार दो-चरणीय समायोजन।
विशिष्ट उपयोग के लिए, ट्रांसफार्मर/ऑप्टिमाइज़ेशन.py देखें:
सबसे अधिक इस्तेमाल की जाने वाली विधि get_linear_scheduler_with_warmup है, जो सीखने की दर का एक रैखिक दो-चरण समायोजन है।

```
def get_scheduler(
    नाम: यूनियन[str, शेड्यूलरटाइप],
    अनुकूलक: अनुकूलक,
    num_warmup_steps: वैकल्पिक[int] = कोई नहीं,
    num_training_steps: वैकल्पिक[int] = कोई नहीं,
): ...

```

उपरोक्त ट्रांसफॉर्मर लाइब्रेरी (संस्करण 4.4.2) में बीईआरटी एप्लिकेशन के प्रासंगिक कोड का विशिष्ट कार्यान्वयन विश्लेषण है। पाठकों के साथ चर्चा करने के लिए आपका स्वागत है।

## आभार
यह लेख मुख्य रूप से झेजियांग विश्वविद्यालय के ली लुओकिउ द्वारा लिखा गया है, और इस परियोजना के छात्र इसे व्यवस्थित करने और सारांशित करने के लिए जिम्मेदार हैं।


```अजगर

```