इस लेख में शामिल जप्टर नोटबुक [अध्याय 4 कोड बेस](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%) में है AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1).

प्रासंगिक डेटासेट और मॉडल को तुरंत डाउनलोड करने के लिए इस ट्यूटोरियल को सीधे Google कोलाब नोटबुक का उपयोग करके खोलने की अनुशंसा की जाती है।
यदि आप इस नोटबुक को Google कोलैब में खोल रहे हैं, तो आपको ट्रांसफॉर्मर और 🤗डेटासेट लाइब्रेरीज़ को इंस्टॉल करने की आवश्यकता हो सकती हैस्थापित करने के लिए निम्न आदेश।

```अजगर
! पिप इंस्टाल डेटासेट ट्रांसफॉर्मर "सैक्रेब्लू>=1.4.12,<2.0.0" वाक्य टुकड़ा
```

यदि आप इस नोटबुक को स्थानीय रूप से खोल रहे हैं, तो कृपया सुनिश्चित करें कि आपने ट्रांसफार्मर-क्विक-स्टार्ट-ज़ेड रीडमी फ़ाइल में सभी निर्भरताओं को ध्यान से पढ़ा और स्थापित किया है। आप इस नोटबुक का एक मल्टी-जीपीयू वितरित प्रशिक्षण संस्करण भी पा सकते हैं [यहां] (https)। ://github.com/huggingface/transformers/tree/master/examples/seq2seq)।

# अनुवाद के लिए ट्रांसफार्मर मॉडल को फाइन-ट्यूनिंग करनाकार्य

इस नोटबुक में, हम दिखाएंगे कि प्राकृतिक भाषा प्रसंस्करण में अनुवाद कार्यों को हल करने के लिए [🤗 ट्रांसफॉर्मर्स] (https://github.com/huggingface/transformers) रिपॉजिटरी से मॉडल का उपयोग कैसे करें। हम [WMT डेटासेट] (http) का उपयोग करेंगे ://www.statmt.org/wmt16/) यह अनुवाद कार्यों के लिए सबसे अधिक उपयोग किए जाने वाले डेटासेट में से एक है।

एक उदाहरण नीचे दिया गया है:

![अनुवाद कार्य पर विजेट अनुमान](https://github.com/huggingface/notebooks/blob/master/examples/images/translation.png?raw=1)अनुवाद कार्य के लिए, हम दिखाएंगे कि एक सरल डेटासेट लोडिंग का उपयोग कैसे करें और आर में ट्रेनर इंटरफ़ेस का उपयोग करके मॉडल को संबंधित फ़ाइन-ट्यून के लिए भी उपयोग करें।

```अजगर
मॉडल_चेकपॉइंट = "हेलसिंकी-एनएलपी/ओपस-एमटी-एन-आरओ"
# एक मॉडल चेकपॉइंट चुनें
```

जब तक पूर्व-प्रशिक्षित ट्रांसफार्मर मॉडल में seq2seq संरचना की एक प्रमुख परत होती है, यह नोटबुक सैद्धांतिक रूप से किसी भी अनुवाद कार्य को हल करने के लिए विभिन्न प्रकार के ट्रांसफार्मर मॉडल [मॉडल पैनल] (https://huggingface.co/models) का उपयोग कर सकता है।

इस एआर मेंटिकिकल, हम अनुवाद कार्यों के लिए पूर्व-प्रशिक्षित [`हेलसिंकी-एनएलपी/ओपस-एमटी-एन-रो`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ro) चेकपॉइंट का उपयोग करते हैं।

## डेटा लोड हो रहा है

हम डेटा और संबंधित मूल्यांकन विधियों को लोड करने के लिए 🤗 डेटासेट लाइब्रेरी का उपयोग करेंगे। डेटा लोडिंग और मूल्यांकन विधि लोडिंग के लिए केवल लोड_डेटासेट और लोड_मेट्रिक के सरल उपयोग की आवश्यकता होती है। हम WMT डेटासेट में अंग्रेजी/रोमानियाई द्विभाषी अनुवाद का उपयोग करते हैं।

```अजगर
डेटासेट से लोड_डेटासेट, लोड_मेट्रिक आयात करें

raw_डेटासेट = लोड_डेटासेट ("wmt16", "ro-en")
मीट्रिक = लोड_मेट्रिक("सैक्रेब्लू")
```

डाउनलोडिंग: 2.81kB [00:00, 523kB/s]
डाउनलोडिंग: 3.19kB [00:00, 758kB/s]
डाउनलोडिंग: 41.0kB [00:00, 11.0MB/s]

डेटासेट wmt16/ro-en को डाउनलोड करना और तैयार करना (डाउनलोड: अज्ञात आकार, उत्पन्न: अज्ञात आकार, पोस्ट-प्रोसेस्ड: अज्ञात आकार, कुल: अज्ञात आकार) को /Users/niepig/.cache/huggingface/datasets/wmt16/ro-en पर। /1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a...

डाउनलोडिंग: 100%|██████████|225एम/225एम[00:18<00:00, 12.2एमबी/सेकेंड]
डाउनलोडिंग: 100%|██████████|23.5M/23.5M [00:16<00:00, 1.44MB/s]
डाउनलोडिंग: 100%|██████████|38.7एम/38.7एम [00:03<00:00, 9.82एमबी/सेकंड]

डेटासेट wmt16 डाउनलोड किया गया और /Users/niepig/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a के लिए तैयार किया गया। बाद की कॉलें इस डेटा का पुन: उपयोग करेंगी लोड हो रहा है: 5.40kB [00:00, 2.08MB/s ]

यह डेटासेट ऑब्जेक्ट स्वयं एक [`DatasetDict`](https://huggingface.co/docs/datasets/package_refe) हैrence/main_classes.html#datasetdict) डेटा संरचना। प्रशिक्षण सेट, सत्यापन सेट और परीक्षण सेट के लिए, आपको संबंधित डेटा प्राप्त करने के लिए केवल संबंधित कुंजी (ट्रेन, सत्यापन, परीक्षण) का उपयोग करने की आवश्यकता है।

```अजगर
raw_datasets
```

डेटासेटडिक्ट({
ट्रेन: डेटासेट({
विशेषताएं: ['अनुवाद'],
num_rows: 610320
})
सत्यापन: डेटासेट({
विशेषताएं: ['अनुवाद'],
num_rows: 1999})
परीक्षण: डेटासेट({
विशेषताएं: ['अनुवाद'],
num_rows: 1999
})
})

डेटा विभाजन कुंजी (ट्रेन, सत्यापन या परीक्षण) दी गईऔर एक सबस्क्रिप्ट, आप डेटा देख सकते हैं।

```अजगर
raw_डेटासेट["ट्रेन"][0]
# हम देख सकते हैं कि एक अंग्रेजी वाक्य एन एक रोमानियाई वाक्य आरओ से मेल खाता है
```

{'अनुवाद': {'एन': 'संसद की सदस्यता: मिनट देखें',
'ro': 'Componenţa Parlamentului: a se vedea procesul-verbal'}}

यह समझने के लिए कि डेटा कैसा दिखता है, निम्न फ़ंक्शन प्रदर्शन के लिए डेटासेट से यादृच्छिक रूप से कुछ उदाहरणों का चयन करेगा।

```अजगर
डेटासेट आयात करें
यादृच्छिक आयात करें
पांडा को पीडी के रूप में आयात करें
fromIPythऑन.डिस्प्ले आयात डिस्प्ले, HTML

def show_random_elements(डेटासेट, num_examples=5):
assert num_examples <= len(डेटासेट), "डेटासेट में मौजूद तत्वों से अधिक तत्व नहीं चुन सकते।"
चुनता है = []
रेंज में _ के लिए (num_examples):
पिक = रैंडम.रैंडिंट(0, लेन(डेटासेट)-1)
चयन करते समय:
पिक = रैंडम.रैंडिंट(0, लेन(डेटासेट)-1)
चुनता है.जोड़ें(चुनें)

डीएफ = पीडी.डेटाफ़्रेम(डेटासेट[चुनता है])
कॉलम के लिए, dataset.features.items() टाइप करें:
यदि isinstance(type,datasets.ClassLabel):
डीएफ[कॉलम] = डीएफ[कॉलम].ट्रांसफॉर्म(लैम्बदा i: टाइप.नाम[i])
डिस्प्ले(HTML(df.to_html()))
```

```अजगर
show_random_elements(raw_datasets["ट्रेन"])
```

<तालिका सीमा = "1" वर्ग = "डेटाफ़्रेम">
<सिर>
<tr शैली = "पाठ-संरेखण: दाएं;">
<थ></थ>
<th>अनुवाद</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>{'en': 'मुझे विश्वास नहीं है कि यह सही रास्ता है।', 'ro': 'मुझे इस बात का पूरा भरोसा है कि यह एक भिन्न प्रकार है।'}</td></tr>
<tr>
<th>1</th>
<td>{'en': 'यूरोपीय रसायन एजेंसी में कुल 104 नई नौकरियाँ सृजित हुईं, जो मुख्य रूप से पर्यवेक्षण करती हैंहमारे REACH प्रोजेक्ट्स को देखें। उत्पाद निर्माण के लिए एजेंसियों का निर्माण करें, देखभाल करें, विशेष रूप से, पहुंच के लिए सर्वोत्तम सेवाएं प्रदान करें।'}</td>
</tr>
<tr>
<th>2</th>
<td>{'en': 'उपरोक्त को ध्यान में रखते हुए, क्या परिषद यह कहेगी कि संयुक्त फ्रोंटेक्स ऑपरेशन में तुर्की की भागीदारी के लिए चर्चा किस स्तर पर पहुंच गई है?', 'ro': 'क्या टूर होगाकिश सरकार फ्रोंटेक्स में तुर्की ओपेरा की नगर पालिका के सदस्यों को रेफरल प्रदान करने में सक्षम होगी?'}</td>
</tr>
<tr>
<थ>3</थ>
<td>{'en': 'अब हमें डर है कि यदि इस निर्देश का दायरा बढ़ाया जाता है, तो निर्देश का वही हश्र होगा जो 'मेड इन' मूल अंकन शुरू करने के पिछले प्रयास का हुआ था - दूसरे शब्दों में, कि यह होगा एक बार फिर परिषद द्वारा अवरुद्ध किया जाएगा।', 'आरओ': 'हमें यकीन नहीं है, लेकिन हम एक निर्देश के लिए आवेदन करने में सक्षम होंगे क्योंकि हम पहले ही ठीक वैसा ही परिणाम देख चुके हैंts, हमने एक मार्काजुलुई मूल "मेड इन" भी पेश किया है, जो एक अच्छा उदाहरण भी है, हम देश का नक्शा भी देख सकते हैं।'}</td>
</tr>
<tr>
<थ>4</थ>
<td>{'en': 'देश 6.58 के स्कोर के साथ नौ पायदान गिरकर 85वें स्थान पर आ गया।', 'ro': '6.58 के स्कोर के साथ देश का स्कोर 85 कम है।'}</td >
</tr>
</tbody>
</तालिका>

मीट्रिक [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric) वर्ग है, मीट्रिक और उपयोग के उदाहरण देखें:

```अजगर
मीट्रिक
```

मीट्रिक(नाम: "सैक्रेब्लू", विशेषताएँ: {'भविष्यवाणियाँ': मान(dtype='string', id='अनुक्रम'), 'संदर्भ': अनुक्रम(सुविधा=मान(dtype='string', id='अनुक्रम '), लंबाई=-1, आईडी='संदर्भ')}, उपयोग: """
अपने पर्याप्त आँकड़ों के साथ BLEU स्कोर तैयार करता है
एक स्रोत से एक या अधिक संदर्भों के विरुद्ध।

तर्क:
भविष्यवाणियाँ: सिस्टम स्ट्रीम (खंडों का एक क्रम)
संदर्भ: एक या अधिक संदर्भ धाराओं की सूची (प्रत्येक खंडों का एक क्रम)
चिकना: चिकना करने की विधिउपयोग
स्मूथ_वैल्यू: 'फर्श' स्मूथनिंग के लिए, उपयोग करने योग्य फर्श
बल: उस डेटा को अनदेखा करें जो पहले से ही टोकनयुक्त दिखता है
लोअरकेस: डेटा को लोअरकेस करें
टोकननाइज़: उपयोग करने के लिए टोकननाइज़र
रिटर्न:
'स्कोर': BLEU स्कोर,
'गिनती': गिनती,
'कुल': कुल,
'परिशुद्धता': परिशुद्धता,
'बीपी': संक्षिप्तता दंड,
'sys_len': पूर्वानुमान की लंबाई,
'ref_len': संदर्भ लंबाई,
उदाहरण:

>>> भविष्यवाणियां = ["हैलो जनरल केनोबी", "फू बार फूबार"]
>>> संदर्भ = [["हैलो जनरल केनोबी", "हैलो!"], ["फू बार फूबार", "फू बार फूबार"]]
>>> sacrebleu = datasets.load_metric("sacrebleu")
>>> परिणाम = sacrebleu.compute(भविष्यवाणियां=भविष्यवाणियां, संदर्भ=संदर्भ)
>>> प्रिंट(सूची(परिणाम.कुंजियाँ()))
['स्कोर', 'गिनती', 'कुल', 'परिशुद्धता', 'बीपी', 'sys_len', 'ref_len']
>>> प्रिंट(राउंड(परिणाम["स्कोर"], 1))
100.0
""", संग्रहीत उदाहरण: 0)

हम स्कोर की गणना करने के लिए पूर्वानुमानों और लेबलों की तुलना करने के लिए `गणना` विधि का उपयोग करते हैं। विशिष्ट के लिए पूर्वानुमानों और लेबलों दोनों की एक सूची होनी चाहिएप्रारूप:

```अजगर
फेक_प्रेड्स = ["हैलो देयर", "जनरल केनोबी"]
फेक_लेबल्स = [["हैलो देयर"], ["जनरल केनोबी"]]
metric.compute(भविष्यवाणियां=नकली_प्रेड, संदर्भ=नकली_लेबल)
```

{'स्कोर': 0.0,
'गिनती': [4, 2, 0, 0],
'कुल': [4, 2, 0, 0],
'परिशुद्धता': [100.0, 100.0, 0.0, 0.0],
'बीपी': 1.0,
'sys_len': 4,
'रेफ_लेन': 4}

## डेटा प्रीप्रोसेसिंग

मॉडल में डेटा फीड करने से पहले, हमें डेटा को प्रीप्रोसेस करना होगा। प्रीप्रोसेसिंग टूल को टोकननाइज़र कहा जाता हैपुट, फिर टोकन को प्री-मॉडल में आवश्यक संबंधित टोकन आईडी में परिवर्तित करता है, और फिर उन्हें मॉडल द्वारा आवश्यक इनपुट प्रारूप में परिवर्तित करता है।

डेटा प्रीप्रोसेसिंग के उद्देश्य को प्राप्त करने के लिए, हम अपने टोकननाइज़र को इंस्टेंट करने के लिए AutoTokenizer.from_pretrained विधि का उपयोग करते हैं, जो सुनिश्चित करता है:

- हमें एक टोकननाइज़र मिलता है जो एक-एक करके पूर्व-प्रशिक्षित मॉडल से मेल खाता है।
- निर्दिष्ट मॉडल चेकपॉइंट के अनुरूप टोकननाइज़र का उपयोग करते समय, हम आवश्यक शब्दावली भी डाउनलोड करते हैंy मॉडल, अधिक सटीक रूप से, टोकन शब्दावली।

इस डाउनलोड की गई टोकन शब्दावली को कैश कर दिया जाएगा ताकि दोबारा उपयोग करने पर इसे दोबारा डाउनलोड न किया जा सके।

```अजगर
ट्रांसफार्मर से ऑटोटोकनाइज़र आयात करें
# `सेंटेंसपीस` इंस्टॉल करने की आवश्यकता: पिप इंस्टॉल सेंटेंसपीस

टोकननाइज़र = AutoTokenizer.from_pretrained(model_checkpoint)
```

डाउनलोडिंग: 100%|██████████| 1.13k/1.13k [00:00<00:00, 466kB/s]
डाउनलोडिंग: 100%|██████████|789k/789k [00:00<00:00, 882kB/s]
डाउनलोडिंग: 100%|██████████|817k/817k [00:00<00:00, 902kB/s]
डाउनलोडिंग: 100%|██████████| 1.39M/1.39M [00:01<00:00, 1.24MB/s]
डाउनलोडिंग: 100%|██████████|42.0/42.0 [00:00<00:00, 14.6kB/s]

उदाहरण के तौर पर हमारे द्वारा उपयोग किए जाने वाले mBART मॉडल को लें, हमें स्रोत भाषा और लक्ष्य भाषा को सही ढंग से सेट करने की आवश्यकता है। यदि आप अन्य द्विभाषी कॉर्पोरा का अनुवाद करना चाहते हैं, तो कृपया [यहां] (https://huggingface.co/facebook/mbart-large) देखें। -cc25) हम स्रोत और लक्ष्य भाषाओं की सेटिंग्स की जांच कर सकते हैं:

```अजगर
यदि model_checkpoint में "mbart":
टोकननाइजर.एसrc_lang = "en-XX"
टोकननाइज़र.tgt_lang = "ro-RO"
```

टोकननाइज़र एकल टेक्स्ट या टेक्स्ट की एक जोड़ी को प्रीप्रोसेस कर सकता है। टोकननाइज़र प्रीप्रोसेसिंग के बाद प्राप्त डेटा पूर्व-प्रशिक्षित मॉडल के इनपुट प्रारूप से मिलता है

```अजगर
टोकननाइज़र ("हैलो, यह एक वाक्य!")
```

{'इनपुट_आईडी': [125, 778, 3, 63, 141, 9191, 23, 0], 'अटेंशन_मास्क': [1, 1, 1, 1, 1, 1, 1, 1]}

ऊपर देखी गई टोकन आईडी, यानी इनपुट_आईड्स, आम तौर पर पूर्व-प्रशिक्षित मॉडल के नाम के साथ भिन्न होती हैं। इसका कारण यह है कि अलग-अलग पूर्व-प्रशिक्षित मॉडयदि आप प्री-ट्रेनिंग के दौरान अलग-अलग नियम निर्धारित करते हैं, तो टोकननाइज़र प्रीप्रोसेसिंग का इनपुट प्रारूप मॉडल आवश्यकताओं को पूरा करेगा। प्रीप्रोसेसिंग के बारे में अधिक जानकारी के लिए, कृपया [यह ट्यूटोरियल](https://huggingface.co/transformers/preprocessing) देखें। .html)

किसी वाक्य को टोकनाइज़ करने के अलावा, हम वाक्यों की सूची को भी टोकनाइज़ कर सकते हैं।

```अजगर
टोकननाइज़र(["हैलो, यह एक वाक्य!", "यह एक और वाक्य है।"])
```

{'इनपुट_आईडी': [[125, 778, 3, 63, 141, 9191, 23, 0], [187, 32, 716, 9191, 2, 0]], 'अटेंशन_मास्क': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}

नोट: मॉडल के लिए अनुवाद लक्ष्य तैयार करने के लिए, हम लक्ष्य के अनुरूप विशेष टोकन को नियंत्रित करने के लिए `as_target_tokenizer` का उपयोग करते हैं:

```अजगर
टोकननाइज़र.as_target_tokenizer() के साथ:
प्रिंट (टोकनाइज़र ("हैलो, यह एक वाक्य!"))
मॉडल_इनपुट = टोकननाइज़र ("हैलो, यह एक वाक्य!")
टोकन = टोकनाइज़र.कन्वर्ट_आईड्स_टू_टोकन्स(मॉडल_इनपुट['इनपुट_आईड्स'])
# प्रिंट करें और विशेष टोकन देखें
प्रिंट करें ('टोकन: {}'.प्रारूप(टोकन))
```

{'इनपुट_आईडी': [10334, 1204, 3, 15, 8915, 27, 452, 59, 29579, 581, 23, 0], 'अटेंशन_मास्क': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
टोकन: ['हेल', 'लो', ',', '', 'दिस', 'ओ', 'ने', 'से', 'एनटेन', 'सीई', '!', '</s >']

यदि आप T5 का उपयोग कर रहे हैं, तो पूर्व-प्रशिक्षित मॉडल के चेकपॉइंट्स को विशेष उपसर्गों की जांच करने की आवश्यकता है। T5 मॉडल को यह बताने के लिए विशेष उपसर्गों का उपयोग करता है कि क्या करना है। विशिष्ट उपसर्ग उदाहरण इस प्रकार हैं:

```अजगर
यदि मॉडल_चेकपॉइंट ["t5-छोटा", "t5-बेस", "t5-बड़ा" में है, "t5-3b", "t5-11b"]:
उपसर्ग = "अंग्रेजी से रोमानियाई में अनुवाद करें:"
अन्य:
उपसर्ग = ""
```

अब हम अपने प्रीप्रोसेसिंग फ़ंक्शन को बनाने के लिए सब कुछ एक साथ रख सकते हैं, जब हम नमूनों को प्रीप्रोसेस करते हैं, तो हम यह सुनिश्चित करने के लिए `truncation=True` पैरामीटर का भी उपयोग करेंगे कि हमारे लंबे वाक्यों को छोटा कर दिया गया है। डिफ़ॉल्ट रूप से, हम छोटे वाक्यों के लिए स्वचालित रूप से पैड लगाते हैं।

```अजगर
अधिकतम_इनपुट_लंबाई = 128
अधिकतम_लक्ष्य_लंबाई = 128
source_lang = "एन"
लक्ष्य_लैंग = "आरओ"

डीईएफ़ प्रीप्रोसेस_फंक्शन(उदाहरण):
इनपुट्स = [उपसर्ग + पूर्व[sस्रोत]]e_lang] उदाहरणों में पूर्व के लिए["अनुवाद"]]
लक्ष्य = [उदा[target_lang] उदाहरणों में पूर्व के लिए["अनुवाद"]]
model_inputs = टोकननाइज़र (इनपुट, max_length=max_input_length, ट्रंकेशन=True)

# लक्ष्य के लिए टोकननाइज़र सेटअप करें
टोकननाइज़र.as_target_tokenizer() के साथ:
लेबल = टोकननाइज़र (लक्ष्य, अधिकतम लंबाई = अधिकतम लक्ष्य_ लंबाई, काट-छाँट = सत्य)

model_inputs["लेबल"] = लेबल["input_ids"]
मॉडल_इनपुट लौटाएं
```

उपरोक्त प्रीप्रोसेसिंग फ़ंक्शन एक नमूना या एकाधिक नमूना उदाहरण संसाधित कर सकता हैएकाधिक नमूनों को संसाधित करता है, एकाधिक नमूनों के पूर्व-संसाधित होने के बाद परिणाम वापस आ जाता है।

```अजगर
प्रीप्रोसेस_फंक्शन(कच्चा_डेटासेट['ट्रेन'][:2])
```

{'इनपुट_आईडी': [[393, 4462, 14, 1137, 53, 216, 28636, 0], [24385, 14, 28636, 14, 4646, 4622, 53, 216, 28636, 0]], 'ध्यान_मास्क' : [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'लेबल': [ [42140, 494, 1750, 53, 8, 59, 903, 3543, 9, 15202, 0], [36199, 6612, 9, 15202, 122, 568, 35788, 21549, 53, 8, 59, 903, 3543 , 9, 15202, 0]]}

इसके बाद, डेटासेट में सभी नमूनों को प्रीप्रोसेसिंग फ़ंक्शन तैयार_ट्रेन_फीचर्स को नमूने पर सभी (मैप) पर लागू करने के लिए मैप फ़ंक्शन का उपयोग करके प्रीप्रोसेस किया जाता है।

```अजगर
टोकनयुक्त_डेटासेट = raw_datasets.map(प्रीप्रोसेस_फ़ंक्शन, बैच = सही)
```

100%|██████████|611/611 [02:32<00:00, 3.99ba/s]
100%|██████████|2/2 [00:00<00:00, 3.76ba/s]
100%|██████████| 2/2 [00:00<00:00, 3.89ba/s]

इससे भी बेहतर, अगली बार पुनर्गणना से बचने के लिए लौटाए गए परिणाम स्वचालित रूप से कैश किए जाते हैंसंसाधित (लेकिन ध्यान रखें कि यदि इनपुट बदल गया है, तो यह कैश से प्रभावित हो सकता है!) डेटासेट लाइब्रेरी फ़ंक्शन यह निर्धारित करने के लिए इनपुट पैरामीटर का पता लगाएगा कि क्या कोई परिवर्तन है, तो कैश्ड डेटा होगा यदि कोई परिवर्तन होता है, तो इसे पुन: संसाधित किया जाएगा, लेकिन यदि इनपुट पैरामीटर अपरिवर्तित रहते हैं, तो जब आप इनपुट बदलना चाहते हैं तो कैश को साफ़ करना सबसे अच्छा है। साफ़ करने का तरीका `load_from_cache_file=False `पैरामीटर इसके अलावा, `बीऊपर इस्तेमाल किया गया मिलान=सही पैरामीटर टोकननाइज़र की एक विशेषता है, क्योंकि यह समानांतर में इनपुट को संसाधित करने के लिए कई थ्रेड्स का उपयोग करेगा।

## ट्रांसफार्मर मॉडल को माइक्रोएडजस्ट करें

अब जब डेटा तैयार है, तो हमें अपने पूर्व-प्रशिक्षित मॉडल को डाउनलोड और लोड करना होगा, और फिर पूर्व-प्रशिक्षित मॉडल को फाइन-ट्यून करना होगा क्योंकि हम seq2seq कार्य कर रहे हैं, हमें एक मॉडल वर्ग की आवश्यकता है जो इस कार्य को हल कर सके वर्ग `AutoModelForSeq2SeqLM` टोकननाइज़र के समान, `from_pretrained` विधि भी हमें डाउनलोड करने में मदद कर सकती हैd मॉडल को लोड करें, और यह मॉडल को कैश भी करेगा ताकि हम मॉडल को बार-बार डाउनलोड न करें।

```अजगर
ट्रांसफार्मर से AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer आयात करें

मॉडल = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

डाउनलोडिंग: 100%|██████████|301एम/301एम [00:19<00:00, 15.1एमबी/सेकेंड]

चूँकि हमारा फाइन-ट्यूनिंग कार्य मशीनी अनुवाद है, और हम एक पूर्व-प्रशिक्षित seq2seq मॉडल लोड करते हैं, हम ऐसा नहीं करते हैं यह हमें संकेत देगा कि कुछ बेजोड़ न्यूमॉडल को लोड करते समय आरएएल नेटवर्क मापदंडों को हटा दिया गया था (उदाहरण के लिए, पूर्व-प्रशिक्षित भाषा मॉडल के तंत्रिका नेटवर्क प्रमुख को हटा दिया गया था, और मशीन अनुवाद के तंत्रिका नेटवर्क प्रमुख को यादृच्छिक रूप से आरंभ किया गया था)।

`Seq2SeqTrainer` प्रशिक्षण उपकरण प्राप्त करने के लिए, हमें 3 तत्वों की आवश्यकता है, जिनमें से सबसे महत्वपूर्ण है प्रशिक्षण सेटिंग्स/पैरामीटर [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers .Seq2SeqTrainingArguments)।इस प्रशिक्षण सेटिंग में वे सभी गुण शामिल हैं जो प्रशिक्षण प्रक्रिया को परिभाषित कर सकते हैं

```अजगर
बैच_आकार = 16
तर्क = Seq2SeqTrainingArguments(
"परीक्षण-अनुवाद",
मूल्यांकन_रणनीति = "युग",
सीखने की दर=2e-5,
प्रति_डिवाइस_ट्रेन_बैच_आकार=बैच_आकार,
प्रति_डिवाइस_eval_batch_size=बैच_आकार,
वज़न_क्षय=0.01,
save_total_limit=3,
num_train_epochs=1,
भविष्यवाणी_साथ_उत्पन्न=सत्य,
fp16=गलत,
)
```

उपरोक्त मूल्यांकन_रणनीति = "युग" पैरामीटर प्रशिक्षण कोड को बताता है कि हम एक वैध कार्य करेंगेप्रति युग एक बार मूल्यांकन।

इस नोटबुक से पहले बैच_आकार परिभाषित किया गया है।

चूँकि हमारा डेटासेट बड़ा है और `Seq2SeqTrainer` मॉडलों को सहेजता रहेगा, हमें इसे अधिकतम `save_total_limit=3` मॉडलों को सहेजने के लिए कहना होगा।

अंत में, हमें अपने संसाधित इनपुट को मॉडल में फीड करने के लिए डेटा कोलेटर डेटा की आवश्यकता होती है।

```अजगर
data_colllator = DataCollatorForSeq2Seq(टोकनाइज़र, मॉडल=मॉडल)
```

`Seq2SeqTrainer` की स्थापना के बाद जो आखिरी चीज बची है वह यह है कि हमें मूल्यांकन पद्धति को परिभाषित करने की आवश्यकता है। हम `मेट्रिक` टी का उपयोग करते हैंo मूल्यांकन पूरा करें। मूल्यांकन करने से पहले मॉडल पूर्वानुमान भेजें, हम डेटा की कुछ पोस्ट-प्रोसेसिंग भी करेंगे:

```अजगर
एनपी के रूप में सुन्न आयात करें

डीईएफ़ पोस्टप्रोसेस_टेक्स्ट(प्रीड्स, लेबल्स):
प्रीड्स = [प्रीड्स में प्रीड के लिए प्रीड.स्ट्रिप()]
लेबल = [[लेबल.स्ट्रिप()] लेबल में लेबल के लिए]

रिटर्न प्रीड्स, लेबल

डीईएफ़ कंप्यूट_मेट्रिक्स(eval_preds):
प्रीड्स, लेबल्स = eval_preds
यदि isinstance(preds, tuple):
प्रीड्स = प्रीड्स[0]
डिकोडेड_प्रेड्स = टोकननाइजर.बैच_डीकोड(प्रीड्स, स्किप_स्पेशल_टोकेंस=ट्रू)

# -100 इंच बदलेंलेबल क्योंकि हम उन्हें डिकोड नहीं कर सकते।
लेबल = np.where(लेबल !=-100, लेबल, टोकननाइज़र.पैड_टोकन_आईडी)
डिकोडेड_लेबल्स = टोकननाइजर.बैच_डीकोड(लेबल्स, स्किप_स्पेशल_टोकेंस=ट्रू)

# कुछ सरल पोस्ट-प्रोसेसिंग
डिकोडेड_प्रेड्स, डिकोडेड_लेबल्स = पोस्टप्रोसेस_टेक्स्ट(डिकोडेड_प्रेड्स, डिकोडेड_लेबल्स)

परिणाम = metric.compute(भविष्यवाणियां=डीकोडेड_प्रीड्स, संदर्भ=डीकोडेड_लेबल्स)
परिणाम = {"ब्लू": परिणाम["स्कोर"]}

Prediction_lens = [np.count_nonzero(pred != टोकननाइज़र.pad_token_id) for pred in preds]
परिणाम["जनरल_len"] = np.mean(prediction_lens)result = {k: राउंड(v, 4) for k, v in result.items()}
वापसी परिणाम
```

अंत में, सभी पैरामीटर/डेटा/मॉडल को `Seq2SeqTrainer` पर पास करें

```अजगर
ट्रेनर = Seq2SeqTrainer(
नमूना,
तर्क,
ट्रेन_डेटासेट=टोकनयुक्त_डेटासेट["ट्रेन"],
eval_dataset=टोकनयुक्त_डेटासेट["सत्यापन"],
डेटा_कोलेटर=डेटा_कोलेटर,
टोकननाइजर=टोकनाइजर,
कंप्यूट_मेट्रिक्स = कंप्यूट_मेट्रिक्स
)
```

प्रशिक्षण को बेहतर बनाने के लिए `ट्रेन` विधि को कॉल करें।

```अजगर
ट्रेनर.ट्रेन()
```

अंत में, टी को मत भूलनाओ जांचें कि मॉडल कैसे अपलोड करें और मॉडल को](https://huggingface.co/transformers/model_sharing.html) पर अपलोड करें। [🤗मॉडल हब](https://huggingface.co/models) पर जाएं इस नोटबुक की शुरुआत की तरह, मॉडल नाम का उपयोग करके सीधे अपने मॉडल का उपयोग करें।

```अजगर

```