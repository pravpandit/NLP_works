{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "इस लेख में शामिल जप्टर नोटबुक [अध्याय 4 कोड आधार](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB) में है %A04 -%E4%BD%BF%E7%94%A8ट्रांसफॉर्मर%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1)।\n",
    "\n",
    "इस ट्यूटोरियल को सीधे खोलने के लिए Google Colab नोटबुक का उपयोग करने की अनुशंसा की जाती है, ताकि आप प्रासंगिक डेटा सेट और मॉडल जल्दी से डाउनलोड कर सकें।\n",
    "यदि आप इस नोटबुक को Google के कोलाब में खोल रहे हैं, तो आपको ट्रांसफॉर्मर और 🤗डेटासेट लाइब्रेरी इंस्टॉल करने की आवश्यकता हो सकती है। इसे स्थापित करने के लिए निम्नलिखित कमांड को अनकम्मेंट करें।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOsHUjgdIrIW"
   },
   "outputs": [],
   "source": [
    "! pip install datasets transformers \"sacrebleu>=1.4.12,<2.0.0\" sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "यदि आप इस नोटबुक को स्थानीय रूप से खोल रहे हैं, तो कृपया सुनिश्चित करें कि आपने ट्रांसफार्मर-क्विक-स्टार्ट-ज़ेडएच की रीडमी फ़ाइल में सभी आश्रित पुस्तकालयों को ध्यान से पढ़ा और स्थापित किया है। आप यहां इस नोटबुक का मल्टी-जीपीयू वितरित प्रशिक्षण संस्करण भी पा सकते हैं।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# अनुवाद कार्यों को हल करने के लिए ट्रांसफार्मर मॉडल को फाइन-ट्यून करें"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "इस नोटबुक में, हम दिखाएंगे कि प्राकृतिक भाषा प्रसंस्करण में अनुवाद कार्यों को हल करने के लिए [🤗 ट्रांसफॉर्मर्स] (https://github.com/huggingface/transformers) कोड बेस से मॉडल का उपयोग कैसे करें। हम [WMT डेटासेट](http://www.statmt.org/wmt16/) डेटासेट का उपयोग करेंगे। यह अनुवाद कार्यों के लिए सबसे अधिक उपयोग किए जाने वाले डेटासेट में से एक है।\n",
    "\n",
    "एक उदाहरण नीचे दिया गया है:\n",
    "\n",
    "![अनुवाद कार्य पर विजेट अनुमान](https://github.com/huggingface/notebooks/blob/master/examples/images/translation.png?raw=1)\n",
    "\n",
    "अनुवाद कार्य के लिए, हम दिखाएंगे कि ट्रांसफार्मर में ट्रेनर इंटरफ़ेस का उपयोग करके डेटासेट को आसानी से कैसे लोड किया जाए और संबंधित प्रशिक्षण के लिए मॉडल को कैसे ठीक किया जाए।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rJvQjiUqPjhM"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-ro\" \n",
    "# एक मॉडल चेकपॉइंट चुनें"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "जब तक पूर्व-प्रशिक्षित ट्रांसफार्मर मॉडल में seq2seq संरचना की मुख्य परत होती है, यह नोटबुक सैद्धांतिक रूप से किसी भी अनुवाद कार्य को हल करने के लिए विभिन्न प्रकार के ट्रांसफार्मर मॉडल [मॉडल पैनल] (https://huggingface.co/models) का उपयोग कर सकता है।\n",
    "\n",
    "इस लेख में, हम अनुवाद करने के लिए पहले से प्रशिक्षित [`हेलसिंकी-एनएलपी/ओपस-एमटी-एन-रो`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ro) चेकपॉइंट का उपयोग करते हैं काम।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## डेटा डाउनलोड करें"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "हम डेटा और संबंधित मूल्यांकन विधियों को लोड करने के लिए 🤗 डेटासेट लाइब्रेरी का उपयोग करेंगे। डेटा लोडिंग और मूल्यांकन मोड लोडिंग के लिए केवल लोड_डेटासेट और लोड_मेट्रिक के सरल उपयोग की आवश्यकता होती है। हम WMT डेटासेट से अंग्रेजी/रोमानियाई द्विभाषी अनुवाद का उपयोग करते हैं।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 2.81kB [00:00, 523kB/s]                    \n",
      "Downloading: 3.19kB [00:00, 758kB/s]                    \n",
      "Downloading: 41.0kB [00:00, 11.0MB/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wmt16/ro-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/niepig/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 225M/225M [00:18<00:00, 12.2MB/s]\n",
      "Downloading: 100%|██████████| 23.5M/23.5M [00:16<00:00, 1.44MB/s]\n",
      "Downloading: 100%|██████████| 38.7M/38.7M [00:03<00:00, 9.82MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wmt16 downloaded and prepared to /Users/niepig/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 5.40kB [00:00, 2.08MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"wmt16\", \"ro-en\")\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "यह डेटासेट ऑब्जेक्ट स्वयं एक [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict) डेटा संरचना है, प्रशिक्षण सेट, सत्यापन सेट और परीक्षण सेट के लिए, आपको केवल इसकी आवश्यकता है संबंधित डेटा का उपयोग कुंजी (ट्रेन, सत्यापन, परीक्षण) का उपयोग करके प्राप्त किया जा सकता है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWiVUF0jIrIv",
    "outputId": "3151a9fc-7239-4471-a8f0-548dd68d5a89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 610320\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "डेटा विभाजन कुंजी (ट्रेन, सत्यापन, या परीक्षण) और सबस्क्रिप्ट दिए जाने पर, आप डेटा देख सकते हैं।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6HrpprwIrIz",
    "outputId": "69f3873e-2d1f-4614-e43e-9e654277245c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Membership of Parliament: see Minutes',\n",
       "  'ro': 'Componenţa Parlamentului: a se vedea procesul-verbal'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]\n",
    "# हम देख सकते हैं कि एक अंग्रेजी एन वाक्य रोमानियाई भाषा आरओ से मेल खाता है"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "यह समझने के लिए कि डेटा कैसा दिखता है, निम्न फ़ंक्शन प्रदर्शित करने के लिए डेटा सेट से यादृच्छिक रूप से कुछ उदाहरणों का चयन करेगा।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "SZy5tRB_IrI7",
    "outputId": "93e16172-d927-457d-fcab-04dcb4d2ef29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'I do not believe that this is the right course.', 'ro': 'Nu cred că acesta este varianta corectă.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'A total of 104 new jobs were created at the European Chemicals Agency, which mainly supervises our REACH projects.', 'ro': 'Un total de 104 noi locuri de muncă au fost create la Agenția Europeană pentru Produse Chimice, care, în special, supraveghează proiectele noastre REACH.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'en': 'In view of the above, will the Council say what stage discussions for Turkish participation in joint Frontex operations have reached?', 'ro': 'Care este stadiul negocierilor referitoare la participarea Turciei la operațiunile comune din cadrul Frontex?'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'en': 'We now fear that if the scope of this directive is expanded, the directive will suffer exactly the same fate as the last attempt at introducing 'Made in' origin marking - in other words, that it will once again be blocked by the Council.', 'ro': 'Acum ne temem că, dacă sfera de aplicare a directivei va fi extinsă, aceasta va avea exact aceeaşi soartă ca ultima încercare de introducere a marcajului de origine \"Made in”, cu alte cuvinte, că va fi din nou blocată la Consiliu.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'en': 'The country dropped nine slots to 85th, with a score of 6.58.', 'ro': 'Ţara a coborât nouă poziţii, pe locul 85, cu un scor de 6,58.'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnjDIuQ3IrI-"
   },
   "source": [
    "मेट्रिक [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric) वर्ग का एक उदाहरण है। मीट्रिक और उपयोग के उदाहरण देखें:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5o4rUteaIrI_",
    "outputId": "4814f907-6225-4af0-ee63-376699dc79ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions: The system stream (a sequence of segments)\n",
       "    references: A list of one or more reference streams (each a sequence of segments)\n",
       "    smooth: The smoothing method to use\n",
       "    smooth_value: For 'floor' smoothing, the floor to use\n",
       "    force: Ignore data that looks already tokenized\n",
       "    lowercase: Lowercase the data\n",
       "    tokenize: The tokenizer to use\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "    >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "    >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "    >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "    >>> print(round(results[\"score\"], 1))\n",
       "    100.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "हम स्कोर की गणना करने के लिए पूर्वानुमानों और लेबलों की तुलना करने के लिए `गणना` पद्धति का उपयोग करते हैं। पूर्वानुमान और लेबल दोनों को एक सूची बनाने की आवश्यकता है। विशिष्ट प्रारूप के लिए नीचे दिया गया उदाहरण देखें:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XN1Rq0aIrJC",
    "outputId": "d130ad50-c6ca-42bc-8b14-31021feb620d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [4, 2, 0, 0],\n",
       " 'totals': [4, 2, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [[\"hello there\"], [\"general kenobi\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## डेटा प्रीप्रोसेसिंग"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "मॉडल में डेटा फीड करने से पहले, हमें डेटा को प्रीप्रोसेस करना होगा। प्रीप्रोसेसिंग टूल को टोकननाइज़र कहा जाता है। टोकनाइज़र पहले इनपुट को टोकनाइज़ करता है, फिर टोकन को प्री-मॉडल में आवश्यक संबंधित टोकन आईडी में परिवर्तित करता है, और फिर इसे मॉडल द्वारा आवश्यक इनपुट प्रारूप में परिवर्तित करता है।\n",
    "\n",
    "डेटा प्रीप्रोसेसिंग के उद्देश्य को प्राप्त करने के लिए, हम अपने टोकननाइज़र को इंस्टेंट करने के लिए AutoTokenizer.from_pretrained विधि का उपयोग करते हैं, जो सुनिश्चित करता है:\n",
    "\n",
    "- हमें एक टोकननाइज़र मिलता है जो पूर्व-प्रशिक्षित मॉडल वन-टू-वन से मेल खाता है।\n",
    "- निर्दिष्ट मॉडल चेकपॉइंट के अनुरूप टोकननाइज़र का उपयोग करते समय, हम मॉडल द्वारा आवश्यक शब्दावली लाइब्रेरी भी डाउनलोड करते हैं, सटीक होने के लिए, टोकन शब्दावली।\n",
    "\n",
    "\n",
    "डाउनलोड किए गए टोकन शब्दावली को कैश किया जाएगा ताकि दोबारा उपयोग करने पर इसे दोबारा डाउनलोड न किया जा सके।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.13k/1.13k [00:00<00:00, 466kB/s]\n",
      "Downloading: 100%|██████████| 789k/789k [00:00<00:00, 882kB/s]\n",
      "Downloading: 100%|██████████| 817k/817k [00:00<00:00, 902kB/s]\n",
      "Downloading: 100%|██████████| 1.39M/1.39M [00:01<00:00, 1.24MB/s]\n",
      "Downloading: 100%|██████████| 42.0/42.0 [00:00<00:00, 14.6kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# `सेंटेंसपीस` इंस्टॉल करने की आवश्यकता: पिप इंस्टॉल सेंटेंसपीस\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLRyc5J9PjhS"
   },
   "source": [
    "उदाहरण के तौर पर हम जिस mBART मॉडल का उपयोग करते हैं, उसे लेते हुए हमें स्रोत भाषा और लक्ष्य भाषा को सही ढंग से सेट करने की आवश्यकता है। यदि आप अन्य द्विभाषी कॉर्पोरा का अनुवाद करना चाहते हैं, तो कृपया [यहां] (https://huggingface.co/facebook/mbart-large-cc25) देखें। हम स्रोत और लक्ष्य भाषा सेटिंग्स की जाँच कर सकते हैं:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kmXG36baPjhS"
   },
   "outputs": [],
   "source": [
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"ro-RO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "टोकननाइज़र या तो एक टेक्स्ट या टेक्स्ट की एक जोड़ी को प्रीप्रोसेस कर सकता है। टोकननाइज़र प्रीप्रोसेसिंग के बाद प्राप्त डेटा प्री-ट्रेनिंग मॉडल इनपुट प्रारूप से मिलता है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5hBlsrHIrJL",
    "outputId": "072ee20c-db1d-4ba1-a98a-119405ea9552"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [125, 778, 3, 63, 141, 9191, 23, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo_0B1M2IrJM"
   },
   "source": [
    "ऊपर देखी गई टोकन आईडी, जो कि इनपुट_आईडी हैं, आम तौर पर पूर्व-प्रशिक्षित मॉडल के नाम के साथ बदलती रहती हैं। इसका कारण यह है कि विभिन्न पूर्व-प्रशिक्षण मॉडल पूर्व-प्रशिक्षण के दौरान अलग-अलग नियम निर्धारित करते हैं। लेकिन जब तक टोकननाइजर और मॉडल के नाम सुसंगत हैं, तब तक टोकननाइजर द्वारा पूर्व-संसाधित इनपुट प्रारूप मॉडल की जरूरतों को पूरा करेगा। प्रीप्रोसेसिंग के बारे में अधिक जानकारी के लिए, कृपया [यह ट्यूटोरियल](https://huggingface.co/transformers/preprocessing.html) देखें\n",
    "\n",
    "किसी वाक्य को टोकनाइज़ करने के अलावा, हम वाक्यों की सूची को भी टोकनाइज़ कर सकते हैं।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LkLffVlKPjhT",
    "outputId": "f144d050-fc84-4a1a-9fc2-25281b681441"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[125, 778, 3, 63, 141, 9191, 23, 0], [187, 32, 716, 9191, 2, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uVqYJrePjhT"
   },
   "source": [
    "नोट: मॉडल के लिए अनुवादित लक्ष्य तैयार करने के लिए, हम लक्ष्य के अनुरूप विशेष टोकन को नियंत्रित करने के लिए `as_target_tokenizer` का उपयोग करते हैं:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgCW0X0FPjhT",
    "outputId": "352c44ab-f025-4cf6-98d1-786f6f07111a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [10334, 1204, 3, 15, 8915, 27, 452, 59, 29579, 581, 23, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "tokens: ['▁Hel', 'lo', ',', '▁', 'this', '▁o', 'ne', '▁se', 'nten', 'ce', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer(\"Hello, this one sentence!\"))\n",
    "    model_input = tokenizer(\"Hello, this one sentence!\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(model_input['input_ids'])\n",
    "# प्रिंट करें और विशेष टोकन देखें\n",
    "    print('tokens: {}'.format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "यदि आप T5 पूर्व-प्रशिक्षित मॉडल की चौकियों का उपयोग कर रहे हैं, तो आपको विशेष उपसर्ग की जाँच करने की आवश्यकता है। T5 मॉडल को किए जाने वाले विशिष्ट कार्यों को बताने के लिए विशेष उपसर्गों का उपयोग करता है। विशिष्ट उपसर्गों के उदाहरण इस प्रकार हैं:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xS1JJSdmPjhU"
   },
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to Romanian: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CezpZ8gFPjhU"
   },
   "source": [
    "अब हम अपना प्रीप्रोसेसर फ़ंक्शन बनाने के लिए सब कुछ एक साथ रख सकते हैं। जब हम नमूने को प्रीप्रोसेस करते हैं, तो हम यह सुनिश्चित करने के लिए `truncation=True` पैरामीटर का भी उपयोग करेंगे कि हमारे अत्यधिक लंबे वाक्यों को छोटा कर दिया गया है। डिफ़ॉल्ट रूप से, हम छोटे वाक्यों को स्वचालित रूप से पैड कर देंगे।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"ro\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "# लक्ष्य के लिए टोकननाइज़र सेटअप करें\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "उपरोक्त प्रीप्रोसेसिंग फ़ंक्शन एक नमूना या एकाधिक नमूना उदाहरणों को संसाधित कर सकता है। यदि कई नमूनों को संसाधित किया जाता है, तो कई नमूनों को पूर्व-संसाधित करने के बाद परिणामों की एक सूची वापस कर दी जाती है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-b70jh26IrJS",
    "outputId": "89b26088-d2d2-4312-81d8-b0f5e62dd6a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[393, 4462, 14, 1137, 53, 216, 28636, 0], [24385, 14, 28636, 14, 4646, 4622, 53, 216, 28636, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[42140, 494, 1750, 53, 8, 59, 903, 3543, 9, 15202, 0], [36199, 6612, 9, 15202, 122, 568, 35788, 21549, 53, 8, 59, 903, 3543, 9, 15202, 0]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets['train'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "इसके बाद, डेटासेट में सभी नमूने पूर्व-संसाधित होते हैं। प्रसंस्करण की विधि मैप फ़ंक्शन का उपयोग करना और सभी नमूनों पर प्रीप्रोसेसिंग फ़ंक्शन तैयार_ट्रेन_फीचर्स को लागू करना है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DDtsaJeVIrJT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611/611 [02:32<00:00,  3.99ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.76ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.89ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "इससे भी बेहतर, अगली बार संसाधित होने पर पुनर्गणना से बचने के लिए लौटाए गए परिणाम स्वचालित रूप से कैश किए जाएंगे (लेकिन कृपया यह भी ध्यान दें कि यदि इनपुट बदल दिया जाता है, तो यह कैश से प्रभावित हो सकता है!)। डेटासेट लाइब्रेरी फ़ंक्शन यह निर्धारित करने के लिए इनपुट पैरामीटर का पता लगाएगा कि क्या परिवर्तन हैं। यदि कोई परिवर्तन नहीं हैं, तो कैश्ड डेटा का उपयोग किया जाएगा। यदि परिवर्तन हैं, तो डेटा को पुन: संसाधित किया जाएगा। लेकिन यदि इनपुट पैरामीटर अपरिवर्तित रहते हैं और आप इनपुट बदलना चाहते हैं, तो कैश को साफ़ करना और समायोजित करना सबसे अच्छा है। साफ़ करने का तरीका `load_from_cache_file=False` पैरामीटर का उपयोग करना है। इसके अलावा, ऊपर इस्तेमाल किया गया `बैच = ट्रू` पैरामीटर टोकननाइज़र की एक विशेषता है, जिसके बारे में माना जाता है कि यह एक ही समय में समानांतर में इनपुट को संसाधित करने के लिए कई थ्रेड्स का उपयोग करता है।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## ट्रांसफार्मर मॉडल को फाइन-ट्यून करें"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "अब जब डेटा तैयार हो गया है, तो हमें अपने पूर्व-प्रशिक्षित मॉडल को डाउनलोड और लोड करना होगा, और फिर पूर्व-प्रशिक्षित मॉडल को फाइन-ट्यून करना होगा। चूँकि हम seq2seq कार्य कर रहे हैं, हमें एक मॉडल वर्ग की आवश्यकता है जो इस कार्य को हल कर सके। हम वर्ग `AutoModelForSeq2SeqLM` का उपयोग करते हैं। टोकननाइज़र के समान, `from_pretrained` विधि भी हमें मॉडल को डाउनलोड और लोड करने में मदद कर सकती है, यह मॉडल को कैश भी करेगी ताकि मॉडल बार-बार डाउनलोड न हो।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TlqNaB8jIrJW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 301M/301M [00:19<00:00, 15.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "चूँकि हमारा फाइन-ट्यूनिंग कार्य मशीन अनुवाद है, और हम पूर्व-प्रशिक्षित seq2seq मॉडल लोड कर रहे हैं, मॉडल लोड करते समय हमें कुछ बेमेल तंत्रिका नेटवर्क मापदंडों को फेंकने के लिए प्रेरित नहीं किया जाएगा (उदाहरण के लिए: पूर्व-प्रशिक्षित तंत्रिका नेटवर्क प्रमुख- प्रशिक्षित भाषा मॉडल को फेंक दिया गया था, जबकि मशीनी अनुवाद के लिए तंत्रिका नेटवर्क प्रमुख को बेतरतीब ढंग से आरंभ किया गया था)।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "`Seq2SeqTrainer` प्रशिक्षण उपकरण प्राप्त करने के लिए, हमें 3 तत्वों की भी आवश्यकता है, जिनमें से सबसे महत्वपूर्ण है प्रशिक्षण सेटिंग्स/पैरामीटर [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html# ट्रांसफार्मर.Seq2SeqTrainingArguments)। इस प्रशिक्षण सेटिंग में वे सभी विशेषताएँ शामिल हैं जो प्रशिक्षण प्रक्रिया को परिभाषित करती हैं"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test-translation\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "उपरोक्त मूल्यांकन_रणनीति = \"युग\" पैरामीटर प्रशिक्षण कोड बताता है: हम प्रत्येक युग के लिए एक सत्यापन मूल्यांकन करेंगे।\n",
    "\n",
    "उपरोक्त बैच_आकार इस नोटबुक से पहले परिभाषित किया गया था।\n",
    "\n",
    "चूँकि हमारा डेटा सेट अपेक्षाकृत बड़ा है और `Seq2SeqTrainer` मॉडलों को सहेजना जारी रखेगा, हमें इसे अधिकतम `save_total_limit=3` मॉडलों को सहेजने के लिए कहना होगा।\n",
    "\n",
    "अंत में, हमें अपने संसाधित इनपुट को मॉडल में फीड करने के लिए एक डेटा कोलेटर की आवश्यकता होती है।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZMdgZaOoPjhX"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "`Seq2SeqTrainer` की स्थापना के बाद एक आखिरी काम बचा है, वह यह है कि हमें मूल्यांकन पद्धति को परिभाषित करने की आवश्यकता है। हम मूल्यांकन पूरा करने के लिए `मीट्रिक` का उपयोग करते हैं। मूल्यांकन में मॉडल पूर्वानुमानों को फीड करने से पहले, हम कुछ डेटा पोस्ट-प्रोसेसिंग भी करेंगे:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "# लेबल में -100 बदलें क्योंकि हम उन्हें डिकोड नहीं कर सकते।\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "# कुछ सरल पोस्ट-प्रोसेसिंग\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "अंत में, सभी पैरामीटर/डेटा/मॉडल को `Seq2SeqTrainer` पर पास करें"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "प्रशिक्षण को बेहतर बनाने के लिए `ट्रेन` विधि को कॉल करें।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXOyGJtqPjhZ"
   },
   "source": [
    "अंत में, यह देखना न भूलें कि मॉडल कैसे अपलोड करें, मॉडल को](https://huggingface.co/transformers/model_sharing.html) से [🤗 मॉडल हब](https://huggingface.co/models) पर अपलोड करें। . फिर आप मॉडल नाम का उपयोग करके सीधे अपने मॉडल का उपयोग कर सकते हैं जैसे आपने इस नोटबुक की शुरुआत में किया था।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jeq1Cq2yPjhZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "4.6-生成任务-机器翻译",
   "provenance": []
  },
  "interpreter": {
   "hash": "3bfce0b4c492a35815b5705a19fe374a7eea0baaa08b34d90450caf1fe9ce20b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
