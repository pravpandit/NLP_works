{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.8 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Large language models (LLMs) have revolutionized the field of natural language processing, which previously relied primarily on explicit rule-based systems and simpler statistical methods. The advent of LLMs has introduced new deep learning-driven methods that have driven advances in understanding, generating, and translating human language.\n",
    "- Modern LLMs are primarily trained in two steps.\n",
    "- First, they are pre-trained on a large unlabeled corpus of text, using the prediction of the next word in a sentence as a “label.”\n",
    "- Then, they are fine-tuned on a smaller, labeled target dataset to perform instructions or classification tasks.\n",
    "- LLMs are based on the transformer architecture. The key idea of ​​the transformer architecture is the attention mechanism, which enables the LLM to selectively visit every word in the entire input sequence when generating output.\n",
    "- The original transformer architecture consists of an encoder for parsing text and a decoder for generating text.\n",
    "- LLMs for generating text and performing instructions, such as GPT-3 and ChatGPT, only implement the decoder module, simplifying the architecture. Pre-training LLMs requires large datasets of billions of words. In this book, we will implement and train LLMs on a small dataset for educational purposes, but will also show how to load publicly available model weights.\n",
    "- Although the general pre-training task for GPT-like models is to predict the next word in a sentence, theseLLMs exhibit “emergent” properties, such as the ability to classify, translate, or summarize text.\n",
    "- Once an LLM is pre-trained, the resulting base model can be more effectively fine-tuned for a variety of downstream tasks.\n",
    "- LLMs fine-tuned on custom datasets can outperform generic LLMs on specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Readers with a background in machine learning may note that traditional machine learning models and deep neural networks trained via conventional supervised learning paradigms usually require label information. However, this is not the case during the pre-training phase of LLMs. In this phase, LLMs utilize self-supervised learning, where the model generates its own labels from the input data. This concept is introduced later in this chapter.\n",
    "\n",
    "[2] GPT-3, a language model worth $4.6 million. https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
