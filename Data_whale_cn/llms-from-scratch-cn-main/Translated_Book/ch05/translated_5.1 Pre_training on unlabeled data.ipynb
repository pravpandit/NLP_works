{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae559a1",
   "metadata": {},
   "source": [
    "# Chapter 5 Pre-training on untokenized data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b02c54",
   "metadata": {},
   "source": [
    "**Introduction**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ed84e",
   "metadata": {},
   "source": [
    "- Calculate the loss of the training set and validation set to evaluate the quality of the text generated by the LLM during training\n",
    "- Implement the training function and pre-train the LLM\n",
    "- Save and load the model weights to continue training the LLM\n",
    "- Load OpenAI's pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c209fb",
   "metadata": {},
   "source": [
    "In the previous chapters, we implemented data sampling, attention mechanism, and wrote the code for the LLM architecture. In this chapter, we will focus on how to implement the training function and pre-train the LLM, as shown in Figure 5.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f55f31",
   "metadata": {},
   "source": [
    "**Figure 5.1 A mental model of the three main stages of building an LLM, including pre-training the LLM on a general text dataset and fine-tuning it on a token dataset. This chapter will focus primarily on pre-training the LLM, including implementing the training code, evaluating performance, and saving and loading model weights. **"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f4fdd61",
   "metadata": {},
   "source": [
    "![fig5.1](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-5-1.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783519e0",
   "metadata": {},
   "source": [
    "As shown in Figure 5.1, we will further learn basic model evaluation techniques in order to measure the quality of generated text, which is a key step in optimizing LLM during training. In addition, we will also explore how to load pre-trained weights, which will provide a solid foundation for us to fine-tune LLM in subsequent chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582a3ea",
   "metadata": {},
   "source": [
    "**Weight parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f117c0",
   "metadata": {},
   "source": [
    "In the context of LLMs and other deep learning models, weights refer to the trainable parameters that need to be adjusted during the learning process. These weights are also called weight parameters or simply parameters. In frameworks like PyTorch, these weights are stored in linear layers, such as the ones we used when implementing the multi-head attention module in Chapter 3 and the GPTModel in Chapter 4. After initializing a layer (`new_layer = torch.nn.Linear(...)`), we can access its weights through the `.weight` attribute, that is, `new_layer.weight`. In addition, for convenience, PyTorch allows direct access to all trainable parameters of the model, including weights and biases, through the `model.parameters()` method, which we will use later when implementing model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fa8b4",
   "metadata": {},
   "source": [
    "## 5.1 Evaluating the Text Generation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220ac20",
   "metadata": {},
   "source": [
    "We will start from the code in the previous chapter, introduce how to use LLM for text generation, and then discuss basic methods for evaluating the quality of generated text. The content of this section and the rest of this chapter is summarized in Figure 5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2058d5ab",
   "metadata": {},
   "source": [
    "**Figure 5.2 The main topics of this chapter are as follows. We first review the text generation content in the previous chapter, and then implement the basic techniques for model evaluation during the pre-training stage. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d0449",
   "metadata": {},
   "source": [
    "![fig5.2](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-5-2.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e7627",
   "metadata": {},
   "source": [
    "As shown in Figure 5.2, in the next section we will review the text generation content set up at the end of the previous chapter, and then in the subsequent sections we will delve into text evaluation and calculate training and validation losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aefa0d9",
   "metadata": {},
   "source": [
    "### 5.1.1 Generating text using GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beee80f",
   "metadata": {},
   "source": [
    "In this section, we will initialize LLM and briefly review the text generation process implemented in Chapter 4. First, we will initialize a GPT model, which will be evaluated and trained in this chapter. We will use the GPTModel class and GPT_CONFIG_124M dictionary from Chapter 4 to complete the model initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64eae149",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from chapter04 import GPTModel\n",
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257,\n",
    "\"context_length\": 256, #A\n",
    "\"emb_dim\": 768,\n",
    "\"n_heads\": 12,\n",
    "\"n_layers\": 12,\n",
    "\"drop_rate\": 0.1, #B\n",
    "\"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97ef59",
   "metadata": {},
   "source": [
    "For the GPT_CONFIG_124M dictionary, the only adjustment we made compared to the previous chapter was to reduce the context length (context_length) to 256 tokens. This change reduces the computational pressure of model training and makes it possible to train on an ordinary laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ad472",
   "metadata": {},
   "source": [
    "The 124 million parameter GPT-2 model was originally configured to process 1024 tokens. After the training process is finished, we will update the context size setting at the end of this chapter and load the pre-trained weights to work with the model configured for a context length of 1024 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d6219",
   "metadata": {},
   "source": [
    "With the GPTmodel example, we use the `generate_text_simple` function introduced in the previous chapter and introduce two practical functions, `text_to_token_ids` and `token_ids_to_text`. These functions facilitate the conversion between text and token representations, and we will use them frequently in this chapter. To provide a clearer understanding, we show this process through Figure 5.3 before diving into the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fb30b",
   "metadata": {},
   "source": [
    "**Figure 5.3 The text generation process involves encoding the text into token IDs, which are then processed into logit vectors by LLM. These logit vectors are then converted back to token IDs and finally decoded into text form. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b74da",
   "metadata": {},
   "source": [
    "![fig5.3](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-5-3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8aaf9",
   "metadata": {},
   "source": [
    "Figure 5.3 depicts the three steps of text generation using the GPT model. First, the tokenizer converts the input text into a series of token IDs, as described in Chapter 2. Second, the model takes these token IDs and generates corresponding logits, which are vectors that represent the probability distribution of each token in the vocabulary, as described in Chapter 4. Finally, these logits are converted back to token IDs, which the tokenizer decodes into human-readable text, completing the cycle from text input to text output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c65de",
   "metadata": {},
   "source": [
    "We implemented the following code for the text generation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f955fbdf",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from chapter04 import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # 添加批次维度\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # 删除批次维度\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac02310",
   "metadata": {},
   "source": [
    "Using the previous code, the model generates the following text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee5054",
   "metadata": {},
   "source": [
    "```\n",
    "Output text:\n",
    "Every effort you make is a waste of time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55c67f",
   "metadata": {},
   "source": [
    "From the output, it is clear that the model is not yet able to generate coherent text because it has not yet been trained. In order to define what is \"coherent\" or \"high-quality\" text, we need to implement a numerical method to evaluate the generated content. This method will allow us to monitor and improve the performance of the model throughout the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ced9d3",
   "metadata": {},
   "source": [
    "The following sections describe how we compute a loss metric for the generated output. This loss serves as a measure of training progress and a sign of success. Additionally, in a subsequent section on fine-tuning the LLM, we will review other methods for evaluating model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac4dbe",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "### 5.1.2 Calculating text generation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a758181",
   "metadata": {},
   "source": [
    "In this section, we will explore a technique for calculating text generation loss to quantitatively evaluate the quality of text generated during training. We will gradually parse this topic in depth through a practical example to make the concept clearer and easier to practice. Let's first briefly review the data loading in Chapter 2 and the `generate_text_simple` function in Chapter 4 to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6d3342",
   "metadata": {},
   "source": [
    "Figure 5.4 clearly depicts the entire process from input text to LLM-generated text in a five-step process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5e953",
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "**Figure 5.4 For the three inputs shown on the left side of the image, we compute a vector for each input token that contains the probability scores for each token in the vocabulary. The index position with the highest probability score in each vector represents the most likely next token ID. The token IDs associated with the highest probability scores are selected and mapped back to a text that represents the text generated by the model. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a713e2",
   "metadata": {},
   "source": [
    "![fig5.4](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-5-4.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4c3f7",
   "metadata": {},
   "source": [
    "The text generation pipeline in Figure 5.4 details the inner workings of the `generate_text_simple` function from Chapter 4. We need to perform these same initial steps before calculating the loss in generated text quality later in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd17c35",
   "metadata": {},
   "source": [
    "Figure 5.4 outlines the text generation process for a small vocabulary of only 7 tokens to display this image on a single page. However, our GPT model uses a large vocabulary of 50,257 words; therefore, in the following code, token IDs will range from 0 to 50,256, not just 0 to 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ceb14",
   "metadata": {},
   "source": [
    "In addition, for simplicity, Figure 5.4 shows only one text example (\"every effort moves\"). In the following code examples that implement the steps in Figure 5.4, we will use two input examples (\"every effort moves\" and \"I really like\") as input to the GPT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664aaa24",
   "metadata": {},
   "source": [
    "Consider two input examples that have been converted to corresponding token IDs, corresponding to step 1 in Figure 5.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce0dbb8d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100], # [\"every effort moves\",\n",
    "                       [40, 1107, 588]]) # \"I really like\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa56920",
   "metadata": {},
   "source": [
    "Matching these inputs, the `targets` contain the token IDs we aim for the model to produce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df02aa75",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345 ], # [\" effort moves you\",\n",
    "                        [588, 428, 11311]]) # \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089e68e",
   "metadata": {},
   "source": [
    "Note that the target is the same as the input, just shifted one position forward, a concept we discussed when implementing data loaders in Chapter 2. This shifting strategy is crucial for training the model to predict the next element in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c2bdf",
   "metadata": {},
   "source": [
    "We feed the input into the model to compute the logistic vectors for two input examples, each consisting of three toekns, and apply the softmax function to convert these logistic values ​​into probability scores, which corresponds to step 2 in Figure 5.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de5d2a00",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): #A\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1) # 词表中每个 token 的概率\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd63e6",
   "metadata": {},
   "source": [
    "The resulting probability score (probas) tensor dimensions are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fedd0",
   "metadata": {},
   "source": [
    "`torch.Size([2, 3, 50257])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07faf720",
   "metadata": {},
   "source": [
    "The first number, 2, represents the two examples (rows) in the input, also known as the batch size. The second number, 3, represents the number of tokens in each input (row). The last number corresponds to the dimensionality of the embedding, which is determined by the vocabulary size, as we discussed in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6370a8",
   "metadata": {},
   "source": [
    "After converting the logical values ​​to probabilities through the softmax function, these probability scores are converted back to text using the `generate_text_simple` function we implemented in Chapter 4, as shown in steps 3-5 of Figure 5.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05668740",
   "metadata": {},
   "source": [
    "We can implement steps 3 and 4 by applying the argmax function to the probability scores to obtain the corresponding token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bed6affe",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f47d35",
   "metadata": {},
   "source": [
    "Considering that we have two input batches, each containing 3 tokens, applying the argmax function to the probability scores (as shown in step 3 of Figure 5.4) will produce two sets of outputs, each containing 3 predicted token IDs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae4dad",
   "metadata": {},
   "source": [
    "```\n",
    "Token IDs:\n",
    "tensor([[[16657], # First batch\n",
    "[ 339],\n",
    "[42826]],\n",
    "[[49906], # Second batch\n",
    "[29669],\n",
    "[41751]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c07d1",
   "metadata": {},
   "source": [
    "Finally, step 5 converts the token ID back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d529bbb",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55640177",
   "metadata": {},
   "source": [
    "When we decode these tokens, we find that these output tokens are completely different from the target tokens we want the model to generate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ae407",
   "metadata": {},
   "source": [
    "```\n",
    "Targets batch 1: effort moves you\n",
    "Outputs batch 1: Armed heNetflix\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a968f03",
   "metadata": {},
   "source": [
    "The random text generated by the model is different from the target text because it has not been trained yet. Now, we will use a method called \"loss\" to numerically evaluate the performance of the text generated by the model, as shown in Figure 5.4. This method is not only important for measuring the quality of generated text, but also the basis for implementing subsequent training functions. We will use this function to update the weights of the model to improve the quality of generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b9da4",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "**Figure 5.5 In the remainder of this section, we will implement the text evaluation function. In the next section, we will apply this evaluation function to the entire dataset used for model training. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d15903",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "![fig5.5](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-5-5.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b7f61",
   "metadata": {
    "metadata": {},
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "In the rest of this section, we will implement parts of the text evaluation process, as shown in Figure 5.5. This process is to measure the \"distance\" between the generated token and the correct prediction (the target). In the training function later in this chapter, we will use this information to adjust the model weights so that the generated text is closer to (ideally the same as) the target text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d3ae9",
   "metadata": {},
   "source": [
    "The goal of model training is to improve the softmax probability of the index position corresponding to the correct target token ID, as shown in Figure 5.6. This softmax probability is also used in the evaluation indicators we will implement in the subsequent parts of this section to numerically evaluate the output generated by the model: the higher the probability of the correct position, the better the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c573c",
   "metadata": {},
   "source": [
    "**Figure 5.6 Before training, the model randomly generates the probability vector of the next token. The goal of model training is to maximize the probability value corresponding to the target token ID. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16fe1d9",
   "metadata": {},
   "source": [
    "![fig5.6](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-5-6.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b4cee",
   "metadata": {},
   "source": [
    "Note that Figure 5.6 shows the softmax probabilities for a compact vocabulary of only 7 tokens, so that all information is integrated into a single graph. This means that the initial random values ​​will be around 1/7 (about 0.14)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21753a03",
   "metadata": {},
   "source": [
    "However, the vocabulary we use in the GPT-2 model has 50,257 tokens, so most of the initial probabilities will be around 0.00002 (1/50,257)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff61eb16",
   "metadata": {},
   "source": [
    "For each of the two input texts, we can print the initial softmax probability score corresponding to the target token with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a37ab176",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([3.9836e-05, 1.6783e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c3dbf",
   "metadata": {},
   "source": [
    "The probabilities of the 3 target token IDs for each batch are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199e6c0",
   "metadata": {},
   "source": [
    "```\n",
    "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
    "Text 2: tensor([3.9836e-05, 1.6783e-05, 4.7559e-06])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d38314",
   "metadata": {},
   "source": [
    "The training goal of LLM is to maximize these probability values, making them as close to 1 as possible. This way, when the model generates the next token, it will always choose the target token - the next word in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b81f29",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08f388",
   "metadata": {},
   "source": [
    "How can we maximize the softmax probability values ​​corresponding to the target tokens? In general, we update the weights of the model so that the model outputs a higher value for each token ID we want to generate. The weight updates are done through a process called backpropagation, which is a standard technique for training deep neural networks (see Sections A.3 to A.7 of Appendix A for more details on backpropagation and model training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d808b98",
   "metadata": {},
   "source": [
    "Backpropagation requires a loss function, which is used to calculate the difference between the model's predicted output (here, the probability corresponding to the target token ID) and the actual expected output. This loss function is used to measure the degree of deviation between the model's prediction and the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0177e07",
   "metadata": {},
   "source": [
    "In the rest of this section, we will compute the loss of probability scores for two batches of examples, `target_probas_1` and `target_probas_2`. The main steps are shown in Figure 5.7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b414e8",
   "metadata": {},
   "source": [
    "**Figure 5.7 Calculating the loss involves multiple steps. Steps 1 to 3 are used to calculate the token probabilities corresponding to the target tensor. Then, in steps 4 to 6, these probabilities are log-transformed and averaged. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5ab98",
   "metadata": {},
   "source": [
    "![fig5.7](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-5-7.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad3584",
   "metadata": {},
   "source": [
    "Since we have already calculated `target_probas_1` and `target_probas_2` according to steps 1-3 in Figure 5.7, we will proceed to step 4 and apply the logarithmic function to these probability scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a052472f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -10.1307, -10.9951, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935fe09",
   "metadata": {},
   "source": [
    "The results are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9709916",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([ -9.5042, -10.3796, -11.3677, -10.1308, -10.9951, -12.2561])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb412c76",
   "metadata": {},
   "source": [
    "In mathematical optimization, it is more convenient to work with the logarithms of probability scores rather than directly with the scores themselves. This topic is beyond the scope of this book, but I have given a lecture on this in detail, which you can find a link to in the References section of Appendix B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87175393",
   "metadata": {},
   "source": [
    "Next, we combine these log-probabilities into a single score by computing the mean (step 5 in Figure 5.7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78673f2a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7722)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0cd22",
   "metadata": {},
   "source": [
    "The resulting average log-odds score is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1717f1d",
   "metadata": {},
   "source": [
    "`tensor(-10.7722)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf39ba6",
   "metadata": {},
   "source": [
    "Our goal is to make the mean log probability as close to 0 as possible by updating the model weights during training, which we will achieve in Section 5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e24ea8",
   "metadata": {},
   "source": [
    "However, in deep learning, a common approach is not to increase the mean log probability to 0, but to reduce the negative mean log probability to 0. The negative mean log probability is the mean log probability multiplied by -1, which corresponds to step 6 in Figure 5.7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c071178",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7722)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ea370",
   "metadata": {},
   "source": [
    "This prints the tensor `(10.7722)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c57f70",
   "metadata": {},
   "source": [
    "This negative value (-10.7722 becomes 10.7722) is called cross entropy loss in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff9142",
   "metadata": {},
   "source": [
    "PyTorch comes in handy here because it has a built-in `cross_entropy` function that can handle all six steps in Figure 5.7 for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a8f12",
   "metadata": {},
   "source": [
    "**Cross entropy loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea2a44",
   "metadata": {},
   "source": [
    "Cross-entropy loss is a common metric in machine learning and deep learning that measures the difference between two probability distributions — typically the true distribution of labels (in this case, the tokens in the dataset) and the predicted distribution of the model (e.g., the token probabilities generated by an LLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5c1ab",
   "metadata": {},
   "source": [
    "In the field of machine learning, especially in frameworks like PyTorch, the `cross_entropy` function is used to calculate a metric for discrete outcomes, which is similar to the negative mean log probability of the target token given the probability of the token generated by the model. Therefore, the terms cross entropy and negative mean log probability are often used interchangeably in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eff884",
   "metadata": {},
   "source": [
    "Before applying the cross entropy function, let's briefly review the shapes of the logits and target tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "886cbd2a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb02d0e",
   "metadata": {},
   "source": [
    "The shape is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4a899",
   "metadata": {},
   "source": [
    "```\n",
    "Logits shape: torch.Size([2, 3, 50257])\n",
    "Targets shape: torch.Size([2, 3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c2449",
   "metadata": {},
   "source": [
    "As we can see, the logits tensor has three dimensions: batch size, number of tokens, and vocabulary size. The targets tensor has two dimensions: batch size and number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c15b1",
   "metadata": {},
   "source": [
    "For the cross entropy loss function in PyTorch, we want to flatten these tensors by concatenating them across the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9282e764",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cccf45",
   "metadata": {},
   "source": [
    "The resulting tensor dimensions are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a4bc9",
   "metadata": {},
   "source": [
    "```\n",
    "Flattened logits: torch.Size([6, 50257])\n",
    "Flattened targets: torch.Size([6])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbad9f",
   "metadata": {},
   "source": [
    "Note that the target values ​​are the token IDs we expect the LLM to generate, while the logits contain the unscaled output values ​​of the model before passing through the softmax function to obtain probability scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12c46a",
   "metadata": {},
   "source": [
    "Earlier, we applied the softmax function, selected the probability scores corresponding to the target ID, and calculated the negative mean log probability. PyTorch's `cross_entropy` function will handle all of these steps for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bd2964f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7722)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79090bfa",
   "metadata": {},
   "source": [
    "The resulting loss is the same as we obtained earlier when we manually implemented the steps shown in Figure 5.7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36aaf7",
   "metadata": {},
   "source": [
    "```\n",
    "tensor(10.7722)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68aea09",
   "metadata": {},
   "source": [
    "Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdbd3b3",
   "metadata": {},
   "source": [
    "Perplexity is a commonly used evaluation metric, often used together with cross entropy loss to evaluate model performance for tasks such as LLM. It provides a more understandable way to help us understand the uncertainty of the model when predicting the next token in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13622f5e",
   "metadata": {},
   "source": [
    "Perplexity measures how well the probability distribution predicted by the model matches the actual distribution of words in the dataset. Similar to loss, lower perplexity indicates that the model's predictions are closer to the actual distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d21e8b",
   "metadata": {},
   "source": [
    "The perplexity can be calculated using the formula `perplexity = torch.exp(loss)`. When we apply this formula to the previously calculated loss value, the result is `tensor(47678.8633)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31060ae7",
   "metadata": {},
   "source": [
    "Perplexity is often considered easier to understand than the raw loss value because it represents the uncertainty of the model about the effective vocabulary size at each step. In this case, it means that out of the 47,678 words or tokens in the vocabulary, the model is unsure which one will be the next token to be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823685d",
   "metadata": {},
   "source": [
    "In this section, we compute the loss on two small text inputs for illustration. In the next section, we will compute the loss on the entire training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86d71d",
   "metadata": {},
   "source": [
    "### 5.1.3 Calculate the training set and validation set loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5283b",
   "metadata": {},
   "source": [
    "In this section, we first prepared the training and validation sets that will be used to train the LLM later in this chapter. Next, we calculated the cross entropy of the training and validation sets, as shown in Figure 5.8, which is an important part of the model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981b9bb9",
   "metadata": {},
   "source": [
    "**Figure 5.8 In the previous section, we have calculated the cross entropy loss. Now we will apply this loss calculation method to the entire text dataset that we will use for model training. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b3be5",
   "metadata": {},
   "source": [
    "![fig5.8](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-5-8.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e10d71",
   "metadata": {},
   "source": [
    "To compute the training and validation losses shown in Figure 5.8, we used a very small text dataset, the short stories \"The Verdict\" by Edith Wharton, which we already used in Chapter 2. By choosing text in the public domain, we avoided any issues related to usage rights. In addition, the reason we chose such a small dataset is that it allows us to execute the code examples in a few minutes on a standard laptop, even without a high-end GPU, which is very beneficial for teaching purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adff469",
   "metadata": {},
   "source": [
    "For those interested, you can also use the additional code provided with this book to prepare a larger dataset of over 60,000 public domain books from Project Gutenberg and train an LLM on this data (see Appendix D for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8acee64",
   "metadata": {},
   "source": [
    "**Cost of Pre-training LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e93779",
   "metadata": {},
   "source": [
    "To better understand the scale of our project, we can refer to the training of the 7 billion parameter Llama 2 model, a relatively well-known and publicly available LLM. This model took 184,320 hours to run on an expensive A100 GPU, processing 2 trillion tokens. At the time of writing, running an 8xA100 cloud server on AWS costs about $30 per hour. A rough estimate puts the total training cost of such an LLM at about $690,000 (184,320 hours divided by 8, then multiplied by $30)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93bbc9",
   "metadata": {},
   "source": [
    "The following code loads the short story \"The Verdict\" that we used in Chapter 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6926455",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c9e5d",
   "metadata": {},
   "source": [
    "After loading the dataset, we can check the number of characters and tokens in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6f673dc",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b958a9",
   "metadata": {},
   "source": [
    "The output is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe0f35",
   "metadata": {},
   "source": [
    "```\n",
    "Characters: 20479\n",
    "Tokens: 5145\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ba1c8",
   "metadata": {},
   "source": [
    "Although this text only has 5145 tokens, it may seem too small to train a large LLM. However, as we mentioned before, this is for educational purposes, allowing us to run the code in minutes rather than weeks. In addition, we will load OpenAI's pre-trained weights into our GPTModel code at the end of this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c247c",
   "metadata": {},
   "source": [
    "Next, we split the dataset into training and validation sets and use the data loader from Chapter 2 to prepare batches of data for LLM training. This process is visually demonstrated in Figure 5.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c35863",
   "metadata": {},
   "source": [
    "**Figure 5.9 In the process of preparing the data loader, we first split the input text into training and validation sets. Next, we tokenize the text (for simplicity, only the training set part is shown here) and divide the tokenized text into blocks of user-specified length (6 in this case). Finally, we shuffle the order of the lines and organize the divided text into batches (in this case, the batch size is 2) so that we can use them for model training. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2f796",
   "metadata": {},
   "source": [
    "![fig5.9](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-5-9.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352860c9",
   "metadata": {},
   "source": [
    "For ease of visualization, we set the maximum length to 6 in Figure 5.9, mainly due to space limitations. However, in our actual implementation of the data loader, we set the maximum length to the context length of 256 tokens supported by LLM, which allows LLM to be exposed to longer text during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee334578",
   "metadata": {},
   "source": [
    "**Training with variable length**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db7aa9",
   "metadata": {},
   "source": [
    "We train the model with chunks of data of similar size for simplicity and efficiency reasons. However, in practice, it is also beneficial to train the LLM with inputs of different lengths, which helps the model better adapt to various types of inputs when used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2d431e",
   "metadata": {},
   "source": [
    "To achieve the data splitting and loading shown in Figure 5.9, we first define a train_ratio, using 90% of the data for training and the remaining 10% as validation data during model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4c6fbca",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560b8c0",
   "metadata": {},
   "source": [
    "With the train_data and val_data subsets, we can now create the corresponding data loaders, reusing the `create_dataloader_v1` code from Chapter 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32f557f7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from chapter02 import create_dataloader_v1\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    "    )\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1839c4b",
   "metadata": {},
   "source": [
    "In the above code, we choose a smaller batch size to reduce the consumption of computing resources because we are dealing with a very small dataset. However, in practice, it is common to use a batch size of 1024 or larger to train LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e5b0f",
   "metadata": {},
   "source": [
    "As an optional check, we can iterate over the data loaders to ensure they were created correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25ce9931",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da7292",
   "metadata": {},
   "source": [
    "We can see the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d64f2d7",
   "metadata": {},
   "source": [
    "```\n",
    "Train loader:\n",
    "torch.Size([2, 256]) torch.Size([2, 256])\n",
    "torch.Size([2, 256]) torch.Size([2, 256])\n",
    "torch.Size([2, 256]) torch.Size([2, 256])\n",
    "torch.Size([2, 256]) torch.Size([2, 256])\n",
    "torch.Size([2, 256]) torch.Size([2, 256])\n",
    "torch.Size([2, 256]) torch.Size([2, 256])\n",
    "torch.Size([2, 256]) torch.Size([2, 256])\n",
    "torch.Size([2, 256]) torch.Size([2, 256])\n",
    "torch.Size([2, 256]) torch.Size([2, 256])\n",
    "Validation loader:\n",
    "torch.Size([2, 256]) torch.Size([2, 256])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21880e54",
   "metadata": {},
   "source": [
    "According to the running results of the preceding code, we have a total of 9 training batches, each batch contains 2 samples, and each sample has 256 tokens. Since we only allocate 10% of the data for the validation process, there is only one validation batch, which contains 2 input examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0725e62",
   "metadata": {},
   "source": [
    "As we expected, the input data (x) and target data (y) have the same shape (batch size times the number of tokens in each batch) because the target is the input data shifted one position backwards, which we discussed in Chapter 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89261410",
   "metadata": {},
   "source": [
    "Next, we will implement a utility function that calculates the cross entropy loss for a specific batch returned by the training and validation loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7bf3e7d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device) #A\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e4c17",
   "metadata": {},
   "source": [
    "We can now use this utility function calc_loss_batch , which computes the loss for a single batch, to implement the following function calc_loss_loader , which computes the loss for all batches sampled by a given data loader:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa26ed3",
   "metadata": {},
   "source": [
    "**Code Listing 5.2 Functions for calculating training and validation losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ed33e8a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader) #A\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader)) #B\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item() #C\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches #D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193f725",
   "metadata": {},
   "source": [
    "By default, the `calc_loss_batch` function iterates over all batches in the given data loader, accumulates the losses of each batch in the `total_loss` variable, and then calculates and averages the losses of all batches. Alternatively, we can specify a smaller number of batches through the `num_batches` parameter to speed up evaluation during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59aa004",
   "metadata": {},
   "source": [
    "Now, let’s apply this calc_loss_batch function to our train and validation set loaders to see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08bc4c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #A\n",
    "model.to(device)\n",
    "train_loss = calc_loss_loader(train_loader, model, device) #B\n",
    "val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d897af",
   "metadata": {},
   "source": [
    "The resulting loss values ​​are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e531252",
   "metadata": {},
   "source": [
    "```\n",
    "Training loss: 10.98758347829183\n",
    "Validation loss: 10.98110580444336\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3427743",
   "metadata": {},
   "source": [
    "Since the model has not been trained yet, the loss value is relatively high. In contrast, if the model can learn how to generate the next token in the order of the training set and the validation set, the loss value will be close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a73b3",
   "metadata": {},
   "source": [
    "Now that we have a way to measure the quality of generated text, we will next train the LLM to reduce this loss so that it performs better at generating text, as shown in Figure 5.10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abd6ac",
   "metadata": {},
   "source": [
    "**Figure 5.10 We have reviewed the text generation process and implemented basic model evaluation techniques to calculate the loss of the training set and validation set. Next, we will move into understanding the training function and pre-training the LLM. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd95f9",
   "metadata": {},
   "source": [
    "![fig5.10](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-5-10.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721518a",
   "metadata": {},
   "source": [
    "As shown in Figure 5.10, the following section will focus on the pre-training of LLM. After the model training is completed, we will adopt different text generation strategies and save and load the pre-trained model weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
