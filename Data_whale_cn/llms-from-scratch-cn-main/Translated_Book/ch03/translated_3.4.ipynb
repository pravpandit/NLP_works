{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Implementing Self-Attention with Trainable Weights\n",
    "\n",
    "In this section, we will implement the self-attention mechanism that is widely used in the original Transformer architecture, the GPT model, and most other popular large language models. This self-attention mechanism is also called scaled dot\n",
    "product attention. Figure 3.13 depicts how this self-attention mechanism fits into the broader context of building large language models.\n",
    "\n",
    "**Figure 3.13 shows how the self-attention mechanism we encode in this section fits into the overall context of this book and chapter. In the previous section, we implemented a simplified attention mechanism in order to understand the basic principles of the attention mechanism. In this section, we will build on this by adding trainable weights. In subsequent sections, we will further extend this self-attention mechanism by adding causal masks and multi-head attention. **\n",
    "\n",
    "![3.13](../img/fig-3-13.jpg)\n",
    "\n",
    "As shown in Figure 3.13, the self-attention mechanism with trainable weights builds on the previous concept: we want to calculate the context vector as a weighted sum of the input vector based on a specific input element. As you can see, it has only a few minor differences compared to the basic self-attention mechanism we wrote before in Section 3.3.\n",
    "\n",
    "The most significant difference is the introduction of the weight matrix that is updated during model training. ThisThese trainable weight matrices are crucial because they enable the model (especially the attention module inside the model) to learn to produce \"good\" context vectors. (Note that we will train the large language model in Chapter 5.)\n",
    "\n",
    "We will discuss this self-attention mechanism in two subsections. First, we will write the code step by step as before. Second, we will organize the code into a compact Python class that can be imported into our large language model architecture in Chapter 4.\n",
    "\n",
    "## 3.4.1 Calculating Attention Weights Step by Step\n",
    "\n",
    "We will implement the self-attention mechanism step by step by introducing three trainable weight matrices Wq, Wk, and Wv. These three matrices are used to project the embedded input token x(i) into query vectors, key vectors, and value vectors, as shown in Figure 3.14.\n",
    "\n",
    "** Figure 3.14 shows the first step of implementing the self-attention mechanism with trainable weight matrices. In this step, for each input element x, we calculate its corresponding query (q), key (k), and value (v) vectors. As in the previous section, we treat the second input x(2) as the query input. The query vector q(2) is obtained by matrix multiplication of the input x(2) with the query weight matrix Wq. Similarly, we use the weight matrices Wk and Wv to calculate the key vector and value vector respectively through corresponding matrix multiplication operations. **\n",
    "\n",
    "![3.14](../img/fig-3-14.jpg)\n",
    "\n",
    "In Section 3.3.1, we defined the second input element x(2) as the query vector to compute the simplified attention weights and obtain the context vector z(2). In Section 3.3.2, we generalized this computation to all context vectors z(1) to z(T) for the six-word input sentence \"Your journey starts with one step.\"\n",
    "\n",
    "Again, for illustration purposes, we will first compute only one context vector z(2). In the next section, we will modify this code to compute all context vectors.\n",
    "\n",
    "Let's start by defining some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in models like GPT, the input and output dimensions are usually the same, but to better illustrate the calculation process, we choose different input (d_in=3) and output (d_out=2) dimensions here.\n",
    "\n",
    "Next, we initialize the three weight matrices Wq, Wk, and Wv shown in Figure 3.14:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we set requires_grad to False for clearer output during demonstration, but if we were to use these weight matrices for model training, we would set requires_grad to True so that these matrices can be updated during model training.\n",
    "\n",
    "Next, as shown in Figure 3.14, we compute the query, key, and value vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query \n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the query output, this produces a two-dimensional vector because we set the number of columns of the corresponding weight matrix to 2 via d_out:\n",
    "```python\n",
    "tensor([0.4306, 1.4551])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Parameters vs. Attention Weights\n",
    "\n",
    "Note that in the weight matrix W, the word \"weight\" is short for \"weight parameters\", which are the values ​​of the neural network that are optimized during the training process. It should not be confused with the attention weights. As we have seen in the previous section, the attention weights determine how much the context vector depends on different parts of the input, that is, how much the network pays attention to different parts of the input.\n",
    "\n",
    "That is, the weight parameters are the basic learned coefficients that define the network connections, while the attention weights are dynamic, context-specific values.\n",
    "\n",
    "Although our temporary goal is only to calculate a context vector z(2), we still need the key and value vectors of all input elements because they participate in calculating the attention weights based on the query vector q(2), as shown in Figure 3.14.\n",
    "\n",
    "We can obtain all the key and value vectors by matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that we have successfully projected the 6 input tokens from the 3D space to the 2D embedding space:\n",
    "```python\n",
    "keys.shape: torch.Size([6, 2])\n",
    "values.shape: torch.Size([6, 2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to calculate the attention score, as shown in Figure 3.15.\n",
    "\n",
    "**Figure 3.15 The attention score calculation is a dot product calculation, similar to the simplified self-attention mechanism we used in Section 3.3. The new aspect is that instead of directly calculating the dot product between the input elements, we use the query vector and key vector obtained by transforming the input through their respective weight matrices. **\n",
    "\n",
    "![3.15](../img/fig-3-15.jpg)\n",
    "\n",
    "First, let's calculate the attention score ω22:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the unnormalized attention score result:\n",
    "```python\n",
    "tensor(1.8524)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can extend this calculation to all attention scores via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output we can quickly see that the second element of the output matches the attn_score_22 we calculated previously:\n",
    "```python\n",
    "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to go from attention scores to attention weights, as shown in Figure 3.16.\n",
    "\n",
    "**Figure 3.16 After calculating the attention scores ω, the next step is to normalize these scores using the softmax function to obtain the attention weights α. **\n",
    "\n",
    "![3.16](../img/fig-3-16.jpg)\n",
    "\n",
    "Next, as shown in Figure 3.16, we calculate the attention weights by scaling the attention scores and using the softmax function we used before. Unlike before, we now scale the attention scores by dividing by the square root of the embedding dimension of the key, (note that taking the square root is mathematically the same as exponentiating to 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained attention weights are as follows:\n",
    "```python\n",
    "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logic behind scaled dot product attention\n",
    "\n",
    "The reason for normalizing by the size of the embedding dimension is to improve training performance by avoiding small gradients. For example, when scaling up the embedding dimension, for large language models like GPT, whose dimensions often exceed a thousand, large dot products may produce very small gradients during backpropagation due to the softmax function applied. As the dot product increases, the softmax function behaves more like a step function, causing the gradient to approach zero. These small gradients can greatly slow down learning or cause training to stagnate.\n",
    "\n",
    "The reason this self-attention mechanism is also called scaled dot product attention is that it scales by the square root of the embedding dimension.\n",
    "\n",
    "The last step is to calculate the context vector, as shown in Figure 3.17. \n",
    "\n",
    "**Figure 3.17 In the last step of the self-attention calculation, we calculate the context vector by combining all the value vectors by the attention weights. **\n",
    "\n",
    "![3.17](../img/fig-3-17.jpg)\n",
    "\n",
    "Similar to Section 3.3, we calculated the context vector by weighted sum of input vectors, now we calculate the context vector by weighted sum of value vectors. Here, the attention weight acts as a weighting factor to measure the corresponding importance of each value vector. Similar to Section 3.3, we can use matrix multiplication to get the output in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attn_weights_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m context_vec_2 \u001b[38;5;241m=\u001b[39m \u001b[43mattn_weights_2\u001b[49m \u001b[38;5;241m@\u001b[39m values\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(context_vec_2)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attn_weights_2' is not defined"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated tensor content is as follows:\n",
    "```python\n",
    "tensor([0.3061, 0.8210])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only computed one context vector, z(2). In the next section, we will extend the code to compute all context vectors z(1) to z(T) in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why query, key, and value?\n",
    "\n",
    "In the context of attention mechanisms, the terms \"key\", \"query\", and \"value\" are borrowed from the fields of information retrieval and databases, where similar concepts are used to store, search, and retrieve information.\n",
    "\n",
    "A \"query\" is similar to a search query in a database. It represents the item that the model is currently focusing on or trying to understand (e.g., a word or token in a sentence). The query is used to explore other parts of the input sequence to determine how much attention should be given to them.\n",
    "\n",
    "A \"key\" is similar to the keys used for indexing and searching in a database. In an attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match against queries.\n",
    "\n",
    "A \"value\" in this context is similar to the value of a key-value pair in a database. It represents the actual content or representation of the input item. Once the model determines which keys (which parts of the input) are most relevant to the query (the currently focused item), it retrieves the corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Implementing a compact self-attention Python class\n",
    "\n",
    "In the previous chapters, we detailed the steps for calculating the self-attention output. This was done mainly to facilitate step-by-step explanation and demonstration. However, in practice, especially considering the implementation of the large language model to be introduced in the next chapter, it is more efficient to integrate these codes into a Python class. As shown below:\n",
    "\n",
    "### Listing 3.1 Compact self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    " \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this PyTorch code, SelfAttention_v1 is a class that inherits from nn.Module, which is the basic unit of PyTorch models and provides the functions required to create and manage model layers.\n",
    "\n",
    "The ‘__init__’ method is responsible for initializing the trainable weight matrices (W_query, W_key, and W_value) for query, key, and value respectively, each of which transforms the input dimension d_in to the output dimension d_out.\n",
    "\n",
    "During the forward pass, through the forward method, we calculate the attention scores (attn_scores) by multiplying the query and the key and normalize them using the softmax function. Finally, we weight the values ​​by these normalized attention scores to create a context vector.\n",
    "\n",
    "We can use this class in the following ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the input contains six embedding vectors, we get a matrix containing six context vectors:\n",
    "```python\n",
    "tensor([[0.2996, 0.8053],\n",
    "[0.3061, 0.8210],\n",
    "[0.3058, 0.8203],\n",
    "[0.2948, 0.7939],\n",
    "[0.2927, 0.7891],\n",
    "[0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check shows that the second row ([0.3061, 0.8210]) actually corresponds to the contents of context_vec_2 in the previous section.\n",
    "\n",
    "Figure 3.18 provides an overview of the self-attention mechanism we just implemented.\n",
    "\n",
    "**Figure 3.18 In the self-attention mechanism, we transform the input vectors in the input matrix X by three weight matrices Wq, Wk, and Wv. We then compute the attention weight matrices based on the generated query (Q) and key (K). Using these attention weights and values ​​(V), we compute the context vector (Z). For visual simplicity, this figure only shows a single input text containing n tokens, rather than a batch of multiple inputs. This simplified representation into a 2D matrix makes the process more intuitive to visualize and understand. **\n",
    "\n",
    "![3.18](../img/fig-3-18.jpg)\n",
    "\n",
    "As shown in Figure 3.18, self-attention involves three trainable weight matrices Wq, Wk, and Wv. These matrices transform input data into queries, keys, and values, and are the core components of the attention mechanism. As the model is exposed to more data during training, these trainable weights are adjusted accordingly, which we will discuss in detail in the following chapters.\n",
    "\n",
    "We can further improve SelfAttention by using PyTorch's nn.Linear layer_v1. These layers can perform matrix multiplication efficiently when no bias units are used. In addition, a significant advantage of using nn.Linear over manually implementing nn.Parameter(torch.rand(...)) is that it has an optimized weight initialization scheme, which helps to achieve more stable and efficient model training.\n",
    "\n",
    "### Listing 3.2 Self-attention class using PyTorch's Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    " \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use SelfAttention_v2 in the same way as SelfAttention_v1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is: \n",
    "```python\n",
    "tensor([[-0.0739, 0.0713],\n",
    "[-0.0748, 0.0703],\n",
    "[-0.0749, 0.0702],\n",
    "[-0.0760, 0.0685],\n",
    "[-0.0763, 0.0679],\n",
    "[-0.0754, 0.0693]], grad_fn=<MmBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the outputs of SelfAttention_v1 and SelfAttention_v2 are different because they use different weight matrix initialization schemes. nn.Linear uses a more complex weight initialization scheme than nn.Parameter(torch.rand(d_in, d_out)) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2\n",
    "\n",
    "Note that the nn.Linear in SelfAttention_v2 uses a different weight initialization scheme than the nn.Parameter(torch.rand(d_in, d_out)) in SelfAttention_v1, which causes the two mechanisms to produce different results. To check that the implementations of SelfAttention_v1 and SelfAttention_v2 are otherwise similar, we can transfer the weight matrix of the SelfAttention_v2 object to the SelfAttention_v1 object so that both objects produce the same results.\n",
    "\n",
    "Your task is to correctly assign the weights of the SelfAttention_v2 instance to the SelfAttention_v1 instance. To do this, you need to understand the relationship between the weights in the two versions. (Hint: nn.Linear stores the weight matrix in transposed form.) After completing the weight assignment, you should find that the two instances produce the same output.\n",
    "\n",
    "In the next section, we will enhance the self-attention mechanism, in particular, we will incorporate elements of causality and multi-head attention. The improvement in causality involves modifying the attention mechanism to prevent the model from accessing future information in the sequence.This is crucial for tasks like language modeling, where the prediction of each word can only depend on the previous words.\n",
    "\n",
    "The component of multi-head attention involves splitting the attention mechanism into multiple \"heads\". Each head learns a different aspect of the data, allowing the model to simultaneously focus on information from different locations in different representation subspaces. This improves the model's performance on complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minitorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
