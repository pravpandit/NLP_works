{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3f5f82",
   "metadata": {},
   "source": [
    "## 3.2 Using attention mechanism to capture data dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd722cbb",
   "metadata": {},
   "source": [
    "As mentioned earlier, before transformer LLM, RNNs were commonly used for language modeling tasks such as language translation. RNNs are good for translating short sentences, but not for longer texts because they do not have direct access to previous words in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cd985",
   "metadata": {},
   "source": [
    "A major drawback of this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder, as shown in Figure 3.4 in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d08f54",
   "metadata": {},
   "source": [
    "Therefore, in 2014, researchers developed the so-called Bahdanau attention mechanism for RNNs (named after the first author of the corresponding paper), which modifies the encoder-decoder RNN so that the decoder can selectively access different parts of the input sequence at each decoding step, as shown in Figure 3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8421d19",
   "metadata": {},
   "source": [
    "Figure 3.5 Using the attention mechanism, the text generation decoder part of the network can selectively visit all input tokens. This means that some input tokens are more important than others for generating a given output token. The importance is determined by the so-called attention weights, which we will calculate later. Note that this figure shows the general idea behind attention and does not describe the exact implementation of the Bahdanau mechanism, which is an RNN method outside the scope of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553837ba",
   "metadata": {},
   "source": [
    "![image-20240422132957951](../img/fig-3-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f381cb8",
   "metadata": {},
   "source": [
    "Interestingly, just three years later, researchers discovered that the RNN architecture was not necessary for building deep neural networks for natural language processing and proposed the original Transformer architecture (discussed in Chapter 1), whose self-attention mechanism was inspired by the Bahdanau attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a986f2a9",
   "metadata": {},
   "source": [
    "Self-attention is a mechanism that allows each position in an input sequence to attend to all positions in the same sequence when computing a representation of the sequence. Self-attention is a key component of contemporary LLMs based on transformer architectures, such as the GPT family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187245d5",
   "metadata": {},
   "source": [
    "This chapter focuses on how to encode and understand this self-attention mechanism used in GPT-like models, as shown in Figure 3.6. In the next chapter, we will encode the rest of the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb7d25",
   "metadata": {},
   "source": [
    "Figure 3.6 Self-attention is a mechanism in the Transformer that computes a more efficient representation of the input, allowing each position in the sequence to interact with and weigh its importance with all other positions in the same sequence. In this chapter, we will code this self-attention mechanism from scratch, and then encode the rest of the GPT-like LLM in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c034d79",
   "metadata": {},
   "source": [
    "![image-20240422133126835](../img/fig-3-6.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
