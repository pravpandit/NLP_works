{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa309c4a",
   "metadata": {},
   "source": [
    "# 2 Text data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c769445",
   "metadata": {},
   "source": [
    "**In this chapter**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef7704",
   "metadata": {},
   "source": [
    "- Prepare text for large language model training\n",
    "- Split text into vocabulary and sub-vocabulary tokens\n",
    "- Byte pair encoding is a more advanced way to segment text\n",
    "- Use a sliding window method to sample training examples\n",
    "- Convert tokens to vectors for input into large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c667f",
   "metadata": {},
   "source": [
    "In the previous chapter, we explored the general structure of Large Language Models (LLMs) and learned that they are pre-trained on large amounts of text. \n",
    "Specifically, we focus on decoder-only large language models (LLMs) based on the transducer architecture, which is the basis of ChatGPT and other popular GPT-like LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a008526",
   "metadata": {},
   "source": [
    "During the pre-training phase, large language models (LLMs) process text word by word.\n",
    "Training large language models with millions to billions of parameters using the next word prediction task can produce models with remarkable capabilities.\n",
    "These models can be further fine-tuned to follow general instructions or perform specific target tasks.\n",
    "But before deploying and training large language models (LLMs) in the following chapters, we need to prepare the training dataset first, which is also the main content of this chapter, as shown in Figure 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d6b77",
   "metadata": {},
   "source": [
    "**Figure 2.1 shows the three main stages of building a large language model (LLM): pre-training the LLM on a general text dataset, and fine-tuning it on an annotated dataset. This chapter explains and structures the data preparation and sampling process that provides pre-trained text data for the LLM. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f6e5a",
   "metadata": {},
   "source": [
    "![fig2.1](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-2-1.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28b74e",
   "metadata": {},
   "source": [
    "In this chapter, you will learn how to prepare input text for training large language models (LLMs).\n",
    "This includes segmenting text into individual vocabulary and sub-vocabulary tokens, and then encoding them into vector representations for use by large language models (LLMs).\n",
    "You will also learn about advanced tokenization schemes such as byte pair encoding, which have been used in popular large language models (LLMs) such as GPT.\n",
    "Finally, we will introduce sampling and data loading strategies that are used to generate the input-output pairs required for training large language models (LLMs) in subsequent chapters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cell)",
   "language": "python",
   "name": "cell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
