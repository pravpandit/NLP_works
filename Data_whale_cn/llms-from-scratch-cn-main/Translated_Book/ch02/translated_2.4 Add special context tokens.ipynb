{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cdf73ca",
   "metadata": {},
   "source": [
    "# 2.4 Adding special context tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1019c5ac",
   "metadata": {},
   "source": [
    "In the previous section, we implemented a simple tokenizer and applied it to a paragraph in our training set. In this section, we will modify this tokenizer to handle unknown words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5b6a1",
   "metadata": {},
   "source": [
    "We will also discuss the use and addition of special contextual tags that can enhance the model's understanding of context or other relevant information in the text. For example, these special tags can include markers for unknown words and document boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9bb7be",
   "metadata": {},
   "source": [
    "In particular, we will modify the vocabulary and tokenizer implemented in the previous section SimpleTokenizerV2 to support two new tokens <|UNK|> and <|CONTENT|> as shown in Figure 2.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8457f8c",
   "metadata": {},
   "source": [
    "**Figure 2.9 We add special tokens to our vocabulary to handle specific contexts. For example, we add the <|UNK|> token to represent new and unknown words that are not part of the training data and therefore not part of the existing vocabulary. In addition, we also add a <|CONTENT|> token that we can use to separate two unrelated text sources. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490fa60b",
   "metadata": {},
   "source": [
    "![fig2.20](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-2-20.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233806c3",
   "metadata": {},
   "source": [
    "As shown in Figure 2.9, we can modify the tokenizer to use the <|UNK|> token if it encounters a word that is not part of the vocabulary. In addition, we add tokens between unrelated text. For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book after the previous text source, as shown in Figure 2.10.\n",
    "This helps the LLM understand that although these text sources are connected for training, they are actually unrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ded03a",
   "metadata": {},
   "source": [
    "**Figure 2.10 When processing multiple independent text sources, we add tokens called <|endoftext|> between these texts. These <|endoftext|> tokens serve as markers, marking the beginning and end of a specific paragraph, which allows LLM to process and understand the text more effectively. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc76cd1",
   "metadata": {},
   "source": [
    "![fig2.21](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-2-21.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792917b",
   "metadata": {},
   "source": [
    "现在让我们修改词汇表，以包含这两个特殊的token，<unk>以及<|endoftext|>，并将它们添加到我们在上一节中创建的唯一词表中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38439456",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[43mpreprocessed\u001b[49m)))\n\u001b[0;32m      2\u001b[0m all_tokens\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|endoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|unk|>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m vocab \u001b[38;5;241m=\u001b[39m {token:integer \u001b[38;5;28;01mfor\u001b[39;00m integer,token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_tokens)}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed' is not defined"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a776a",
   "metadata": {},
   "source": [
    "According to the output of the print statement, the new vocabulary size is 1161 (the vocabulary size in the previous section was 1159)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688ecd4",
   "metadata": {},
   "source": [
    "As an extra quick check, let's print the last 5 words of our updated vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08e3a9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mvocab\u001b[49m\u001b[38;5;241m.\u001b[39mitems())[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:]):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(item)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd79d47",
   "metadata": {},
   "source": [
    "The above code prints the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb990f9",
   "metadata": {},
   "source": [
    "('younger', 1156)\n",
    "('your', 1157)\n",
    "('yourself', 1158)\n",
    "('<|endoftext|>', 1159)\n",
    "('<|unk|>', 1160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725cfe23",
   "metadata": {},
   "source": [
    "Based on the output of the above code, we can confirm that the two new special tokens have indeed been successfully merged into the vocabulary. Next, we adjust the tokenizer in Listing 2.3 accordingly, as shown in Listing 2.4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930dca4b",
   "metadata": {},
   "source": [
    "**Listing 2.4 A simple text tokenizer that handles unknown words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a26133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed] #A\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #B\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273cda30",
   "metadata": {},
   "source": [
    "Compared to the SimpleTokenizerV1 we implemented in Listing 2.3 in the previous section, the new SimpleTokenizerV2 replaces unknown words with <|UNK|> tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3040c",
   "metadata": {},
   "source": [
    "Now let's try this new tagger in practice. To do this, we will use a simple text example that is made by concatenating two separate and unrelated sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a04bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb6a7dc",
   "metadata": {},
   "source": [
    "The output is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b8c26",
   "metadata": {},
   "source": [
    "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64915c5",
   "metadata": {},
   "source": [
    "Next, let's tokenize the vocab we created earlier in Listing 2.2 using SimpleTokenizerV2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162d3403",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m SimpleTokenizerV2(\u001b[43mvocab\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f532e55",
   "metadata": {},
   "source": [
    "This will print the following token ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d238cf",
   "metadata": {},
   "source": [
    "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7988b14",
   "metadata": {},
   "source": [
    "We can see that the token ID list contains 1159, which is the <|endoftext|> delimiter token, and two 1160s, which are used to mark unknown words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c9571",
   "metadata": {},
   "source": [
    "Let's de-tokenize the text as a quick sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643173b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c9917",
   "metadata": {},
   "source": [
    "The output looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d93b6",
   "metadata": {},
   "source": [
    "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2a152",
   "metadata": {},
   "source": [
    "By comparing the de-tokenized text above with the original input text, we know that the training dataset, Edith Wharton's short story The Verdict, does not contain the words \"Hello\" and \"palace\".\n",
    "\n",
    "So far, we have discussed tokenization, which is an important step in processing text as input to LLMs. Depending on the LLM, some researchers also consider other special tokens, such as:\n",
    "\n",
    "·[BOS] (beginning of sequence): This token marks the beginning of the text. LLM indicates where a paragraph of content begins. </br>\n",
    "·[EOS] (end of sequence): This token is at the end of the text and is particularly useful when connecting multiple unrelated texts, similar to <|text|>. For example, when merging two different Wikipedia articles or books, the [EOS] token indicates where one article ends and the next begins. </br>\n",
    "·[PAD] (padding): When training LLMs with a batch size greater than 1, the batch may contain texts of different lengths. To ensure that all texts have the same length, the [PAD] tag is used to extend or \"pad\" shorter texts to the length of the longest text in the batch. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6120349",
   "metadata": {},
   "source": [
    "Note that the tokenizer used for the GPT model does not require any of the tokens mentioned above, but only uses the <|内文|> token for simplicity. The <|内文|\" is a token similar to the [EOS] token mentioned above. Additionally, the <|内文|\" is also used for padding. However, as we will explore in subsequent sections, when training on batches of inputs, we typically use masking, which means that we do not pay attention to the padded tokens. Therefore, choosing a specific token for padding becomes irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3367570",
   "metadata": {},
   "source": [
    "Additionally, the tokenizer used for the GPT model does not use the <|UNK|> tag for out-of-vocabulary words. Instead, the GPT model uses a byte pair encoding tokenizer that breaks words into sub-word units, which we will discuss in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78d469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch",
   "language": "python",
   "name": "llm_from_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
