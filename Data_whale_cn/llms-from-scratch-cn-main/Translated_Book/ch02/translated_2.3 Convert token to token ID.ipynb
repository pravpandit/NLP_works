{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda8a8f9",
   "metadata": {},
   "source": [
    "# 2.3 Convert token to token ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50c3e5",
   "metadata": {},
   "source": [
    "In the previous section, we split a short story by Edith Wharton into individual tokens.\n",
    "In this section, we will convert these tokens from Python strings to integer representations, generating so-called token IDs.\n",
    "This conversion is an intermediate step before converting the token IDs into embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c244c8c8",
   "metadata": {},
   "source": [
    "In order to map the previously generated tokens to token IDs, we first need to build a so-called vocabulary.\n",
    "This vocabulary defines how we map each unique word and special character to a unique integer, as shown in Figure 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13d6b8",
   "metadata": {},
   "source": [
    "**Figure 2.6 We build the vocabulary by splitting the entire text in the training dataset into individual tokens.\n",
    "These individual tokens are then sorted alphabetically and duplicate tokens are removed.\n",
    "These unique tokens are then clustered into a vocabulary that defines a mapping from each unique token to a unique integer value.\n",
    "The vocabulary shown is intentionally kept small for illustration purposes and does not include punctuation or special characters for simplicity. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843aae2",
   "metadata": {},
   "source": [
    "![fig2.6](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-2-6.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b01652",
   "metadata": {},
   "source": [
    "In the previous section, we tokenized the short stories of Edith Wharton and assigned them to a Python variable called \"preprocessed\".\n",
    "Now, let's create a list of all the unique tokens and sort them alphabetically to determine the size of the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90a181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "response = requests.get(url)\n",
    "raw_text = response.text\n",
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if\n",
    "item.strip()]\n",
    "\n",
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c385a",
   "metadata": {},
   "source": [
    "After determining that the vocabulary has 1159 words through the above code, we create the vocabulary and print its first 50 words for illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b545d",
   "metadata": {},
   "source": [
    "### Code Example 2.2 Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32897865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindle:', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n",
      "('Her', 51)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in\n",
    "enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    " print(item)\n",
    " if i > 50:\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e173b",
   "metadata": {},
   "source": [
    "The output is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e78b4a",
   "metadata": {},
   "source": [
    "('!', 0) \\\n",
    "('\"', 1) \\\n",
    "(\"'\", 2) \\\n",
    "... \\\n",
    "('Has', 49) \\\n",
    "('He', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6aa26",
   "metadata": {},
   "source": [
    "As we can see from the output above, this dictionary contains a single token associated with a unique integer label.\n",
    "Our next goal is to apply this vocabulary to convert new text into tokenIDs, as shown in Figure 2.7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f260d9",
   "metadata": {},
   "source": [
    "**Figure 2.7 Starting with a new text sample, we tokenize the text and use the vocabulary to convert the text tokens into token IDs.\n",
    "This vocabulary is built based on the entire training set and can be applied to the training set itself and any new text examples.\n",
    "The vocabulary shown below will not include punctuation or special characters for simplicity. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ca144",
   "metadata": {},
   "source": [
    "![fig2.7](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-2-7.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105417eb",
   "metadata": {},
   "source": [
    "Later in the book, when we want to convert the output of a large language model (LLM) from numbers back to text, we will also need a way to convert token IDs back to text.\n",
    "To do this, we can create a reverse version of the vocabulary that maps token IDs back to their corresponding tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1de2d",
   "metadata": {},
   "source": [
    "Let's implement a complete tokenizer class in Python, including an encode method that splits text into tokens and performs a string-to-integer mapping through the vocabulary to generate token IDs.\n",
    "In addition, we also implement a decode method that performs the reverse integer-to-string mapping to convert the token ID back to text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ac1bf",
   "metadata": {},
   "source": [
    "The code for this tokenizer is shown in Code Example 2.3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4407e",
   "metadata": {},
   "source": [
    "### Code Example 2.3 Implementing a simple text tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2603c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab #A\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} #B\n",
    " \n",
    "    def encode(self, text): #C\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed\n",
    "if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    " \n",
    "    def decode(self, ids): #D\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])  \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #E\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b5e459",
   "metadata": {},
   "source": [
    "Using the SimpleTokenizerV1 Python class described above, we can now instantiate new tokenizer objects with an existing vocabulary, which we can then use to encode and decode text, as shown in Figure 2.8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e495f",
   "metadata": {},
   "source": [
    "**Figure 2.8 There are two common methods for tokenizer implementation: one is the encoding method and the other is the decoding method.\n",
    "The encoding method receives sample text, splits it into individual tokens, and converts these tokens into token IDs through a vocabulary.\n",
    "The decoding method receives token IDs, converts them back to text tokens, and concatenates these text tokens into natural text. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae01fd",
   "metadata": {},
   "source": [
    "![fig2.8](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-2-8.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e245b96",
   "metadata": {},
   "source": [
    "Let's try this out in practice by instantiating a new tokenizer object from the SimpleTokenizerV1 class and using it to tokenize a short story by Edith Wharton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623bb612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn\n",
    "said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef8106f",
   "metadata": {},
   "source": [
    "The above code prints out the token ID of the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2e527",
   "metadata": {},
   "source": [
    "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7,\n",
    "39, 873, 1136, 773, 812, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535622e0",
   "metadata": {},
   "source": [
    "Next, let's see if we can convert these token IDs back into text using the decode method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0bc417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234e096",
   "metadata": {},
   "source": [
    "This will output the following text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82cb8b",
   "metadata": {},
   "source": [
    "'\" It's the last he painted, you know,\" Mrs. Gisburn said\n",
    "with pardonable pride.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae48908",
   "metadata": {},
   "source": [
    "Based on the output above, we can see that the decode method successfully converts the tokenID back to the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7a151",
   "metadata": {},
   "source": [
    "So far, so good.\n",
    "With this, we have built a tokenizer that is able to tokenize and decode text based on a fragment from our training set.\n",
    "Now, let's apply it to a new example of text that was not included in our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe01788d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m         preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m         preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m---> 10\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m         preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m         preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m---> 10\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55442846",
   "metadata": {},
   "source": [
    "Executing the above code will result in the following error:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b8330",
   "metadata": {},
   "source": [
    "...\n",
    "KeyError: 'Hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777ed61",
   "metadata": {},
   "source": [
    "The problem is that the word \"Hello\" does not appear in the short story \"The Verdict\".\n",
    "So, this word is not included in the vocabulary we built earlier.\n",
    "This highlights the importance of considering using a large and diverse training set to expand the vocabulary when working with large language models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68448d76",
   "metadata": {},
   "source": [
    "In the next section, we will further test the tokenizer on text containing unknown vocabulary, and we will also discuss additional special tokens that can be used to provide more context to the Large Language Model (LLM) during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cell)",
   "language": "python",
   "name": "cell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
