{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda8a8f9",
   "metadata": {},
   "source": [
    "# 2.6 Using sliding windows for data sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50c3e5",
   "metadata": {},
   "source": [
    "In the previous section, we detailed the tokenization step and the process of converting string tokens into integer token IDs. Before creating the embedding of the LLM, we need to generate the input-target pairs required to train the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c244c8c8",
   "metadata": {},
   "source": [
    "What do these input-target pairs look like? As we learned in Chapter 1, LLMs are pre-trained by predicting the next word in a text, as shown in Figure 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13d6b8",
   "metadata": {},
   "source": [
    "**Figure 2.12 Given a text sample, an input block is extracted as an input subsample of the LLM. The task of the LLM during training is to predict the next word after the input block. During training, we mask out all words after the target word. Note that the text has been tokenized before being processed by the LLM. For ease of illustration, the tokenization step is omitted in this figure. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843aae2",
   "metadata": {},
   "source": [
    "![fig2.12](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-2-12.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b01652",
   "metadata": {},
   "source": [
    "In this chapter, we implemented a data loader that uses a sliding window approach to obtain the input-target pairs described in Figure 2.12 from the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687ffa8",
   "metadata": {},
   "source": [
    "First, we will tokenize the entire The Verdict short story using the BPE tokenizer introduced in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b90a181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c385a",
   "metadata": {},
   "source": [
    "After applying the BPE tokenizer to the training set, 5145 tokens are obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d24dc",
   "metadata": {},
   "source": [
    "Next, we will remove the first 50 toekns from the dataset in order to present more attractive text passages in subsequent steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0174a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4320b",
   "metadata": {},
   "source": [
    "When creating an input-target pair for the next word prediction task, a simple and intuitive approach is to create two variables x and y. x is used to store the input token sequence, while y is used to store the target token sequence. The target sequence is composed of each token in the input sequence shifted one position to the right. Thus, an input-target pair is formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aba6df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #A\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d58328",
   "metadata": {},
   "source": [
    "Running the above code will print the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58311589",
   "metadata": {},
   "source": [
    "```\n",
    "x: [290, 4920, 2241, 287]\n",
    "y: [4920, 2241, 287, 257]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6bdd3",
   "metadata": {},
   "source": [
    "After we generate the corresponding target data by shifting the input data one position to the right, we can refer to Figure 2.12 and follow the steps below to create the next word prediction task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf4f75b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a1a42",
   "metadata": {},
   "source": [
    "The above code will print the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7beab",
   "metadata": {},
   "source": [
    "```\n",
    "[290] ----> 4920\n",
    "[290, 4920] ----> 2241\n",
    "[290, 4920, 2241] ----> 287\n",
    "[290, 4920, 2241, 287] ----> 257\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a7575",
   "metadata": {},
   "source": [
    "The content on the left side of the arrow (---->) refers to the input received by the LLM, and the token ID on the right side of the arrow represents the target token ID that the LLM should predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37865c93",
   "metadata": {},
   "source": [
    "For better understanding, we repeat the previous code, but this time we convert the token ID back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def4b071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5326f0b",
   "metadata": {},
   "source": [
    "The following output shows the text format of the input and output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb7f17",
   "metadata": {},
   "source": [
    "```\n",
    "and ----> established\n",
    "and established ----> himself\n",
    "and established himself ----> in\n",
    "and established himself in ----> a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962cc5f",
   "metadata": {},
   "source": [
    "Now that we have created our input-target pairs, we can train our LLM in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0031bc0",
   "metadata": {},
   "source": [
    "Before we convert tokens into embedding vectors, there is one last task to complete. As we mentioned at the beginning of this chapter, we also need to implement an efficient data loader that iterates over the input dataset and returns input-target pairs. These inputs and targets are PyTorch tensors, which can be understood as multidimensional arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61ab29",
   "metadata": {},
   "source": [
    "Specifically, we want to return two tensors: one is the input tensor, which contains the text that the LLM has seen; the other is the target tensor, which contains the target that the LLM wants to predict, as shown in Figure 2.13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70acbd",
   "metadata": {},
   "source": [
    "**Figure 2.13 To implement an efficient data loader, we store all inputs into a tensor called x, where each row represents an input context. At the same time, we create another tensor called y to store the corresponding predicted targets (i.e., the next word), which are obtained by shifting the input content one position to the right. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af4d55",
   "metadata": {},
   "source": [
    "![fig2.13](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-2-13.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee3a31",
   "metadata": {},
   "source": [
    "For ease of understanding, Figure 2.13 shows the token in string format, but in the code implementation, we will directly operate the token ID. This is because the `encode` method of the BPE tokenizer combines the two steps of tokenization and conversion to token ID into one step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a9223",
   "metadata": {},
   "source": [
    "To implement an efficient data loader, we will use PyTorch's built-in Dataset and DataLoader classes. For additional information and guidance on installing PyTorch, see Section A.1.3, \"Installing PyTorch\" in Appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa6eb9",
   "metadata": {},
   "source": [
    "The code of the dataset class is shown in Code Example 2.5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4407e",
   "metadata": {},
   "source": [
    "### Code Example 2.5 A Dataset for Batch Input and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2603c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt) #A\n",
    "        for i in range(0, len(token_ids) - max_length, stride): #B\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self): #C\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx): #D\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be19f1",
   "metadata": {},
   "source": [
    "In Listing 2.5, we built a class called `GPTDatasetV1`, which is a subclass of PyTorch's `Dataset` class. This class specifies how to extract individual samples from the dataset, each of which contains a certain number of token IDs stored in the `input_chunk` tensor (the number is determined by the `max_length` parameter). The `target_chunk` tensor holds the target corresponding to the input. To understand this process more deeply, it is recommended that you continue reading to see what the data returned looks like when the dataset is used in conjunction with PyTorch's DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661df5f2",
   "metadata": {},
   "source": [
    "If you are not familiar with the structure of PyTorch's Dataset class, as shown in Listing 2.5, I recommend that you read Section A.6, \"Building Efficient Data Loaders\" in Appendix A. There, you will find a detailed explanation of the basic structure and usage of PyTorch's Dataset and DataLoader classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a3f78",
   "metadata": {},
   "source": [
    "The following code will use `GPTDatasetV1` to batch load input data through PyTorch's `DataLoader`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b11e56",
   "metadata": {},
   "source": [
    "### Listing 2.6, a batch data loader for generating input-target pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "642ff507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4,\n",
    "        max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") #A\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #B\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last) #C\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d898d",
   "metadata": {},
   "source": [
    "To gain a more intuitive understanding of how the GPTDatasetV1 class in Listing 2.5 and the create_dataloader_v1 function in Listing 2.6 work together, we will test the data loader with a batch size of 1 in an LLM with a context size of 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3197531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    dataloader = create_dataloader_v1(\n",
    "        raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "    data_iter = iter(dataloader) #A\n",
    "    first_batch = next(data_iter)\n",
    "    print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3335ce",
   "metadata": {},
   "source": [
    "Executing the above code will print the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef651c1",
   "metadata": {},
   "source": [
    "```\n",
    "[tensor([[ 40, 367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e401b36",
   "metadata": {},
   "source": [
    "The `first_batch` variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs. Since `max_length` is 4, both tensors contain only 4 token IDs. It should be noted that the input size of 4 here is relatively small and is only used for demonstration. When training language models in practice, the input size is usually at least 256.\n",
    "\n",
    "To illustrate the meaning of `stride=1`, let's get another batch of data from this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "181d6113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6144c",
   "metadata": {},
   "source": [
    "The second batch of data is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0bc04",
   "metadata": {},
   "source": [
    "```\n",
    "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2daa0c",
   "metadata": {},
   "source": [
    "By comparing the first and second batches of data, we can see that the token IDs of the second batch are shifted one position to the right compared to the first batch (for example, the second ID of the first batch input is 367, which is exactly the first ID of the second batch input). The `stride` parameter determines the number of positions that the input is shifted between batches, which simulates the concept of a sliding window, as shown in Figure 2.14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a866b46",
   "metadata": {},
   "source": [
    "**Figure 2.14 In the process of creating multiple batches from the input dataset, we slide an input window over the text. If the stride is set to 1, then when generating the next batch, we move the input window to the right by 1 position. If the stride is set to the size of the input window, then overlap between batches can be avoided. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b942805",
   "metadata": {},
   "source": [
    "![fig2.14](https://github.com/Pr04Ark/llms-from-scratch-cn/blob/trans01/Translated_Book/img/fig-2-14.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88ecbe",
   "metadata": {},
   "source": [
    "**Exercise 2.2 Loading data using data loaders with different step sizes and context sizes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93820358",
   "metadata": {},
   "source": [
    "To further understand how the data loader works, you can try running it with different settings, such as max_length=2 and stride=2 , or max_length=8 and stride=2 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6104e53",
   "metadata": {},
   "source": [
    "So far, we have sampled batch sizes of 1 from the data loader, which is helpful for demonstration purposes. If you have experience with deep learning, you probably know that smaller batch sizes require less memory during training, but can result in more noise when updating the model. Just like in regular deep learning, batch size is a hyperparameter that requires trade-offs and experimentation when training LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c392de9",
   "metadata": {},
   "source": [
    "Before we dive into the last two sections on how to create embedding vectors from token IDs, letâ€™s take a quick look at how to use data loaders for sampling with batch sizes greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "344db195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  306,    11,   475,   465],\n",
      "        [  338,   523, 14348,     0],\n",
      "        [  423,   284,  1394, 26148],\n",
      "        [  465,  1182,   284,   804],\n",
      "        [  355,   339,  3830,   612],\n",
      "        [22665,  4252, 10899,    13],\n",
      "        [   13,   383, 18098,   373],\n",
      "        [  683,  1207,  8344,   803]])\n",
      "\n",
      "Targets:\n",
      " tensor([[   11,   475,   465,  2951],\n",
      "        [  523, 14348,     0,   632],\n",
      "        [  284,  1394, 26148,   526],\n",
      "        [ 1182,   284,   804,   510],\n",
      "        [  339,  3830,   612,   290],\n",
      "        [ 4252, 10899,    13,   198],\n",
      "        [  383, 18098,   373,  3940],\n",
      "        [ 1207,  8344,   803,   306]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf19cea",
   "metadata": {},
   "source": [
    "The output is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b66ba",
   "metadata": {},
   "source": [
    "```\n",
    "Inputs:\n",
    "tensor([[ 306, 11, 475, 465],\n",
    "[ 338, 523, 14348, 0],\n",
    "[ 423, 284, 1394, 26148],\n",
    "[ 465, 1182, 284, 804],\n",
    "[ 355, 339, 3830, 612],\n",
    "[22665, 4252, 10899, 13],\n",
    "[ 13, 383, 18098, 373],\n",
    "[ 683, 1207, 8344, 803]])\n",
    "\n",
    "Targets:\n",
    "tensor([[ 11, 475, 465, 2951],\n",
    "[ 523, 14348, 0, 632],\n",
    "[ 284, 1394, 26148, 526],\n",
    "[ 1182, 284, 804, 510],\n",
    "[ 339, 3830, 612, 290],\n",
    "[ 4252, 10899, 13, 198],\n",
    "[ 383, 18098, 373, 3940],\n",
    "[ 1207, 8344, 803, 306]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896585c",
   "metadata": {},
   "source": [
    "Note that we increased the stride to 4. This is to make full use of the dataset (no words are skipped) and also to avoid overlap between batches. Too much overlap may increase the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3438e06",
   "metadata": {},
   "source": [
    "In the next two sections of this chapter, we will implement the Embedding layer. Its role is to convert the token ID into a continuous vector representation, which will be used as the input of the LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
