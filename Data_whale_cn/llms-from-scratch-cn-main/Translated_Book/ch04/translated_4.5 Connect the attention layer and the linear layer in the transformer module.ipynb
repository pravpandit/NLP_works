{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be7fb0b",
   "metadata": {},
   "source": [
    "# 4.5 Connect the attention layer and the linear layer in the transformer module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adbb40c",
   "metadata": {},
   "source": [
    "In this section, we will implement the transfomer module, which is a fundamental building block of GPT and other large language model (LLM) architectures.\n",
    "This module is repeated more than a dozen times in the 124 million parameter GPT-2 architecture and combines several concepts we introduced previously: multi-head attention, layer normalization, dropout, feed-forward layers, and GELU activation functions, as shown in Figure 4.13.\n",
    "In the next section, we will connect this transfomer module to the rest of the GPT architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49496c5",
   "metadata": {},
   "source": [
    "**Figure 4.13 Illustration of the transfomer module.\n",
    "The bottom of the figure shows the input tokens, which have been embedded into 768-dimensional vectors.\n",
    "Each row corresponds to the vector representation of a token.\n",
    "The output of the transfomer module is a vector with the same dimensions as the input, which can then be fed to subsequent layers in the LLM. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41853c9f",
   "metadata": {},
   "source": [
    "![fig4.13](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-4-13.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda05b2",
   "metadata": {},
   "source": [
    "As shown in Figure 4.13, the transformer module combines multiple components, including the masked multi-head attention module in Chapter 3 and the feedforward module we implemented in Section 4.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e660c",
   "metadata": {},
   "source": [
    "When the transformer module processes an input sequence, each element in the sequence (e.g., a word or subword token) is represented by a vector of fixed size (768 dimensions in the case of Figure 4.13).\n",
    "The operations inside the transformer module, including the multi-head attention and feed-forward layers, are designed to transform these vectors in a way that preserves their dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ea84b",
   "metadata": {},
   "source": [
    "The idea behind the self-attention mechanism in the multi-head attention module is that it is able to identify and analyze the relationships between elements in the input sequence.\n",
    "Meanwhile, the feed-forward network modifies the data independently at each position.\n",
    "This combination not only allows for a more nuanced understanding and processing of the input, but also enhances the model’s overall ability to handle complex data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad2b03",
   "metadata": {},
   "source": [
    "In the following code, we can create the Transformer module as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b878325",
   "metadata": {},
   "source": [
    "### Code Example 4.6 GPT Transformer Module Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57d53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a3199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "#A\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "        \n",
    "        shortcut = x #B\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut #C\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f6344",
   "metadata": {},
   "source": [
    "The code given above defines a TransformerBlock class in PyTorch, including a multi-head attention mechanism (MultiHeadAttention) and a feedforward network (FeedForward), both of which are configured according to the provided configuration dictionary (for example, GPT_CONFIG_124M)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c84673",
   "metadata": {},
   "source": [
    "Layer normalization (LayerNorm) is applied before these two components, and dropout is applied after them, to regularize the model and prevent overfitting.\n",
    "This is also called Pre-LayerNorm.\n",
    "In older architectures, such as the original transformer module, layer normalization is applied after self-attention and the feedforward network, known as Post-LayerNorm, which often leads to poor training dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238742e6",
   "metadata": {},
   "source": [
    "The class also implements the forward pass, where each component is followed by a shortcut connection that adds the block’s input to its output.\n",
    "This key feature helps the gradient flow through the network during training and improves learning for deep models as explained in Section 4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d75abd",
   "metadata": {},
   "source": [
    "Using the GPT_CONFIG_124M dictionary we defined earlier, let's instantiate a transfomer module and feed it some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4592fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"] * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(cfg[\"emb_dim\"] * 4, cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads #A\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) #B\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "        'mask',\n",
    "         torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x) #C\n",
    "        queries = self.W_query(x) #C\n",
    "        values = self.W_value(x) #C\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)#D\n",
    "\n",
    "        keys = keys.transpose(1, 2) #E\n",
    "        queries = queries.transpose(1, 2) #E\n",
    "        values = values.transpose(1, 2) #E\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3) #F\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] #G\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) #H\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) #I\n",
    "#J\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) #K\n",
    "        return context_vec\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0bc169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9528c",
   "metadata": {},
   "source": [
    "The output looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988db16",
   "metadata": {},
   "source": [
    "Input shape: torch.Size([2, 4, 768]) \\\n",
    "Output shape: torch.Size([2, 4, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c115f72",
   "metadata": {},
   "source": [
    "From the code output we can see that the transformer module maintains the dimensionality of the input in its output, which shows that the transformer architecture does not change the shape of the data sequence as it is processed throughout the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd353ef",
   "metadata": {},
   "source": [
    "Preserving shape throughout the transformer module architecture is not accidental, but a key aspect of its design.\n",
    "This design enables it to be effectively applied to a wide range of sequence-to-sequence tasks, where each output vector corresponds directly to an input vector, maintaining a one-to-one relationship.\n",
    "However, as we learned in Chapter 3, the output is a context vector that encapsulates information about the entire input sequence.\n",
    "This means that while the physical dimensions of the sequence (length and feature size) remain unchanged when passing through the transformer module, the content of each output vector is re-encoded to incorporate contextual information from the entire input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1279d7",
   "metadata": {},
   "source": [
    "The transformer module implemented in this section gives us all the building blocks, as shown in Figure 4.14, needed to implement the GPT architecture in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82d8a5",
   "metadata": {},
   "source": [
    "**Figure 4.14 shows a mental model of the different concepts we have implemented in this chapter so far.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21776f36",
   "metadata": {},
   "source": [
    "![fig4.14](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-4-14.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379f576",
   "metadata": {},
   "source": [
    "As shown in Figure 4.14, the transfomer module combines layer normalization, a feed-forward network (including GELU activations), and shortcut connections, which we introduced earlier in this chapter.\n",
    "As we will see in the upcoming chapters, this transfomer module will form the main component of the GPT architecture we will implement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cell)",
   "language": "python",
   "name": "cell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
