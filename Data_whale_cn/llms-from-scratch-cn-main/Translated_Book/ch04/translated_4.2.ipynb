{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1baec45",
   "metadata": {},
   "source": [
    "## 4.2 Normalizing activations using layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa08967",
   "metadata": {},
   "source": [
    "Training deep neural networks with many layers can sometimes be challenging due to problems such as vanishing or exploding gradients. These problems lead to unstable training dynamics, making it difficult for the network to effectively adjust its weights, which means that the learning process has a hard time finding a set of parameters (weights) for the neural network that minimizes the loss function. In other words, the network has a hard time learning the underlying patterns in the data to a level that allows it to make accurate predictions or decisions. (If you are not familiar with neural network training and gradient concepts, you can find a brief introduction to these concepts in Appendix A, Section A.4, “Automatic Differentiation Made Easy: An Introduction to PyTorch”. However, a deep mathematical understanding of gradients is not required to follow the content of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ecf2b",
   "metadata": {},
   "source": [
    "In this section, we will implement layer normalization to improve the stability and efficiency of neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe24ad6",
   "metadata": {},
   "source": [
    "The main idea behind layer normalization is to adjust the activations (outputs) of a neural network layer so that they have a mean of 0 and a variance of 1, also known as unit variance. This adjustment speeds up convergence to effective weights and ensures consistent, reliable training. As we saw in the previous section, based on the DummyLayerNorm placeholder, in GPT-2 and modern transformer architectures, layer normalization is usually applied before and after the multi-head attention module and before the final output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9459b0b0",
   "metadata": {},
   "source": [
    "Before implementing layer normalization in code, Figure 4.5 provides a visual overview of how layer normalization works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101131d",
   "metadata": {},
   "source": [
    "Figure 4. Illustration of layer 5 normalization, where the 5 layer outputs (also called activations) are normalized to have a mean of zero and a variance of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4338794",
   "metadata": {},
   "source": [
    "![image-20240422135247478](../img/fig-4-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1ba1c",
   "metadata": {},
   "source": [
    "We can recreate the example shown in Figure 4.5 with the following code, where we implemented a neural network layer with 5 inputs and 6 outputs, which we applied to two input examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #A\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7901a3c",
   "metadata": {},
   "source": [
    "This will print the following tensor, where the first line lists the layer output for the first input, and the second line lists the layer output for the second line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
    "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
    "        grad_fn=<ReluBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b8b15",
   "metadata": {},
   "source": [
    "The neural network layer we encoded consists of a linear layer and a non-linear activation function called ReLU (short for Rectified Linear Unit), which is a standard activation function in neural networks. If you are not familiar with ReLU, it simply thresholds negative inputs to 0, ensuring that the layer only outputs positive values, which explains why the resulting layer output does not contain any negative values. (Note that we will use another, more complex activation function in GPT, which we will introduce in the next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4219cc",
   "metadata": {},
   "source": [
    "Before applying layer normalization to these outputs, let's check the mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30826f",
   "metadata": {},
   "source": [
    "The output is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean:\n",
    "    tensor([[0.1324],\n",
    "            [0.2170]], grad_fn=<MeanBackward1>)\n",
    "Variance:\n",
    "    tensor([[0.0231],\n",
    "            [0.0398]], grad_fn=<VarBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb269d67",
   "metadata": {},
   "source": [
    "The first row in the average tensor above contains the average of the first input row, and the second output row contains the average of the second input row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2aad91",
   "metadata": {},
   "source": [
    "Using keepdim=True in operations such as mean or variance computation ensures that the output tensor maintains the same shape as the input tensor, even if the operation reduces the tensor along the dimension specified by dim. For example, without keepdim=True, the returned mean tensor would be a 2D vector [0.1324, 0.2170] instead of a 2D matrix [[0.1324], [0.2170]]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c4a90",
   "metadata": {},
   "source": [
    "The dim parameter specifies the dimension in the tensor over which the statistic (here, the mean or variance) is computed, as shown in Figure 4.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd68fe9b",
   "metadata": {},
   "source": [
    "Figure 4.6 Illustration of the dim parameter when computing the mean of a tensor. For example, if we have a 2D tensor (matrix) of dimension [rows, columns], using dim=0 will perform the operation across the rows (vertically, as shown at the bottom), producing output that aggregates the data for each column. Using dim=1 or dim=-1 will perform the operation across the columns (horizontally, as shown at the top), producing output that aggregates the data for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a5e20",
   "metadata": {},
   "source": [
    "![image-20240422135636907](../img/fig-4-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb12e38",
   "metadata": {},
   "source": [
    "​ As shown in Figure 4.6, for a 2D tensor (such as a matrix), using dim=-1 for operations such as mean or variance calculations is the same as using dim=1. This is because -1 refers to the last dimension of the tensor, which corresponds to the columns in a 2D tensor. Later, when adding layer normalization to the GPT model, the model generates a 3D tensor with shape [batch_size, num_tokens, embedding_size], we can still use dim=-1 to normalize the last dimension, avoiding the change from dim=1 to dim=2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5325ca63",
   "metadata": {},
   "source": [
    "Next, let's apply layer normalization to the layer output we obtained previously. This operation consists of subtracting the mean and dividing by the square root of the variance (also known as the standard deviation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e13187",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637af25d",
   "metadata": {},
   "source": [
    "From the results, we can see that the normalization layer output (which now also contains negative values) has a mean of zero and a variance of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea812b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalized layer outputs:\n",
    "        tensor([[ 0.6159, 1.4126, -0.8719, 0.5872, -0.8719, -0.8719],\n",
    "                [-0.0189, 0.1121, -1.0876, 1.5173, 0.5647, -1.0876]],\n",
    "               grad_fn=<DivBackward0>)\n",
    "Mean:\n",
    "    tensor([[2.9802e-08],\n",
    "            [3.9736e-08]], grad_fn=<MeanBackward1>)\n",
    "Variance:\n",
    "    tensor([[1.],\n",
    "            [1.]], grad_fn=<VarBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd9a15",
   "metadata": {},
   "source": [
    "​ Note that the value 2.9802e-08 in the output tensor is the scientific notation for 2.9802 × 10-8, or 0.00000000298 in decimal form. This value is very close to 0, but it is not exactly 0 because computers have limited precision in representing numbers and small numerical errors may accumulate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c577985",
   "metadata": {},
   "source": [
    "For improved readability, we can also turn off scientific notation when printing tensor values ​​by setting sci_mode to False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e739ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "Mean:\n",
    "    tensor([[ 0.0000],\n",
    "            [ 0.0000]], grad_fn=<MeanBackward1>)\n",
    "Variance:\n",
    "    tensor([[1.],\n",
    "            [1.]], grad_fn=<VarBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810265c",
   "metadata": {},
   "source": [
    "So far in this section, we have encoded and applied layer normalization step by step. Now let's encapsulate this process in a PyTorch module that can be used later in the GPT model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773faacb",
   "metadata": {},
   "source": [
    "**Listing 4.2 A-layer normalized class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\tdef __init__(self, emb_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.eps = 1e-5\n",
    "\t\tself.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "\t\tself.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\tdef forward(self, x):\n",
    "\t\tmean = x.mean(dim=-1, keepdim=True)\n",
    "\t\tvar = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\t\tnorm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\t\treturn self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20827f45",
   "metadata": {},
   "source": [
    "This particular implementation of layer normalization operates on the last dimension of the input tensor x, which represents the embedding dimension (emb_dim). The variable eps is a small constant (epsilon) added to the variance to prevent division by zero during the normalization process. scale and shift are two trainable parameters (same as the dimensions of the input) that LLM automatically adjusts during training if it determines that doing so will improve the model's performance on the task it was trained on. This allows the model to learn the appropriate scaling and shifting that best suits the data it is processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea7351",
   "metadata": {},
   "source": [
    "Bias Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de0ee3",
   "metadata": {},
   "source": [
    "In our variance calculation method, we opted into this implementation detail by setting unbiased=False . For those who are curious about what this means, in the variance calculation, we divide by the number of inputs n in the variance formula. This approach does not apply the Bessel correction, which typically uses n-1 in the denominator instead of n to adjust for bias in the sample variance estimate. This decision leads to what is known as biased estimation. For large-scale language models (LLMs), where the embedding dimension n is very large, the difference between using n and n-1 is almost negligible. We chose this approach to ensure compatibility with the normalization layers of the GPT-2 model and because it reflects the default behavior of TensorFlow, which is used to implement the original GPT-2 model. Using a similar setting ensures that our approach is compatible with the pre-trained weights we will load in Chapter 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336e78c",
   "metadata": {},
   "source": [
    "Now let's try the LayerNorm module in practice and apply it to a batch input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836af1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8d756",
   "metadata": {},
   "source": [
    "From the results, we can see that the layer normalization code works as expected and normalizes the values ​​of each of the two inputs so that they have a mean of 0 and a variance of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65693732",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean:\n",
    "    tensor([[ -0.0000],\n",
    "            [ 0.0000]], grad_fn=<MeanBackward1>)\n",
    "Variance:\n",
    "    tensor([[1.0000],\n",
    "            [1.0000]], grad_fn=<VarBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c085356",
   "metadata": {},
   "source": [
    "In this section, we introduced one of the building blocks required to implement the GPT architecture, as shown in the mental model in Figure 4.7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a88bb",
   "metadata": {},
   "source": [
    "Figure 4.7 A mental model outlining the different building blocks we implemented in this chapter to assemble the GPT architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dcaf81",
   "metadata": {},
   "source": [
    "![image-20240422140325104](../img/fig-4-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0e0ee",
   "metadata": {},
   "source": [
    "In the next section, we will look at the GELU activation function, which is one of the activation functions used in LLM instead of the traditional ReLU function we used in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7881c5",
   "metadata": {},
   "source": [
    "**Layer Normalization vs Batch Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c57101",
   "metadata": {},
   "source": [
    "​ If you are familiar with batch normalization, a common and traditional neural network normalization method, you may wonder how it compares to layer normalization. Unlike batch normalization, which normalizes across the batch dimension, layer normalization normalizes across the feature dimension. LLMs typically require significant computational resources, and the available hardware or specific use case can dictate the batch size during training or inference. Since layer normalization normalizes each input independently of the batch size, it provides greater flexibility and stability in these scenarios. This is particularly useful for distributed training or when deploying models in resource-constrained environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
