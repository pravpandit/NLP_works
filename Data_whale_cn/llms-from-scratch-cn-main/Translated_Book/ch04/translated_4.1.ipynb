{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae559a1",
   "metadata": {},
   "source": [
    "# Chapter 4 Implementing a GPT model from scratch to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b02c54",
   "metadata": {},
   "source": [
    "**Introduction**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65964c57",
   "metadata": {},
   "source": [
    "- Code a GPT-like Large Language Model (LLM) that can be trained to generate human-like text \n",
    "- Normalize layer activations to stabilize neural network training \n",
    "- Add shortcut connections in deep neural networks to train models more efficiently \n",
    "- Implement transformer modules to create GPT models of various sizes \n",
    "- Calculate the number of parameters and storage requirements of GPT models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c209fb",
   "metadata": {},
   "source": [
    "In the previous chapter, you learned and programmed the multi-head attention mechanism, one of the core components of LLM. In this chapter, we will now encode the other building blocks of LLM and assemble them into a GPT-like model that we will train in the next chapter to generate human-like text, as shown in Figure 4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5efc3f",
   "metadata": {},
   "source": [
    "Figure 4.1 A mental model that encodes the three main stages of LLM. Pretraining the LLM on a generic text dataset and fine-tuning it on a labeled dataset. This chapter focuses on implementing the LLM architecture, which we will train in the next chapter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f4fdd61",
   "metadata": {},
   "source": [
    "![image-20240422133749839](../img/fig-4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783519e0",
   "metadata": {},
   "source": [
    "The LLM architecture referenced in Figure 4.1 consists of several building blocks, which we will implement in this chapter. In the next section, we will start with a top-down view of the model architecture before looking at the individual components in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fa8b4",
   "metadata": {},
   "source": [
    "## 4.1 Writing the LLM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beee80f",
   "metadata": {},
   "source": [
    "LLMs, such as GPT (which stands for Generative Pretrained Transformer), are large deep neural network architectures designed to generate new text one word (or token) at a time. However, despite their large size, the model architecture is not as complex as you might think, as many of its components are repeated, as we will see later. Figure 4.2 provides a top-down view of a GPT-like LLM, with its main components highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ad472",
   "metadata": {},
   "source": [
    "Figure 4.2 The mental model of the GPT model. Next to the embedding layer, it consists of one or more transformer modules, which contain the masked multi-head attention module we implemented in the previous chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b74da",
   "metadata": {},
   "source": [
    "![image-20240422133908887](../img/fig-4-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7237970",
   "metadata": {},
   "source": [
    "As shown in Figure 4.2, we have introduced several aspects, such as input tokenization and embedding, and the masked multi-head attention module. The focus of this chapter will be on implementing the core structure of the GPT model, including its transformer module, and then we will train it to generate human-like text in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d3ae9",
   "metadata": {},
   "source": [
    "In previous chapters, we used small embedding dimensions for simplicity, ensuring that concepts and examples could fit comfortably on a single page. Now, in this chapter, we will scale up to the size of a small GPT-2 model, specifically the smallest version with 124 million parameters, as described in the paper \"Language Models as Unsupervised Multi-Task Learners\" by Radford et al. Note that while the original report mentioned 117 million parameters, this has since been corrected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401bb3ab",
   "metadata": {},
   "source": [
    "Chapter 6 will focus on how to load pre-trained weights into our implementation and tune them to large GPT-2 models with 345, 762, and 1.542 billion parameters. In the context of deep learning and LLMs like GPT, the term \"parameters\" refers to the trainable weights of a model. These weights are essentially the internal variables of the model that are adjusted and optimized during the training process to minimize a specific loss function. This optimization allows the model to learn from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075ff94",
   "metadata": {},
   "source": [
    "For example, in a neural network layer represented by a 2,048x2,048-dimensional weight matrix (or tensor), each element of that matrix is ​​a parameter. Since there are 2,048 rows and 2,048 columns, the total number of parameters in that layer is 2,048 times 2,048, which equals 4,194,304 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a4bae",
   "metadata": {},
   "source": [
    "GPT-2 and GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ffc50",
   "metadata": {},
   "source": [
    "​ Note that we focus on GPT-2 because OpenAI has made the weights of the pretrained model publicly available, which we will load into our implementation in Chapter 6. GPT-3 is essentially the same in terms of model architecture, except that it scales up from 1.5 billion parameters in GPT-2 to 175 billion parameters in GPT-3, and it is trained using much more data. At the time of this writing, the weights for GPT-3 are not yet publicly available. GPT-2 is also a better choice for learning how to implement LLM because it can run on a single laptop, whereas GPT-3 requires a GPU cluster for both training and inference. According to Lambda Labs, it would take 355 years to train GPT-3 on a single V100 datacenter GPU and 665 years to train GPT-3 on a consumer-grade RTX 8000 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa26bc",
   "metadata": {},
   "source": [
    "We specify the configuration of the small GPT-2 model via the following Python dictionary, which we will use in subsequent code examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\t\"vocab_size\": 50257,      # Vocabulary size\n",
    "    \"context_length\": 1024,   # Context length\n",
    "    \"emb_dim\": 768,           # Embedding dimension\n",
    "    \"n_heads\": 12,            # Number of attention heads\n",
    "    \"n_layers\": 12,           # Number of layers\n",
    "    \"drop_rate\": 0.1,         # Dropout rate\n",
    "    \"qkv_bias\": False         # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5bdac9",
   "metadata": {},
   "source": [
    "In the GPT_CONFIG_124M dictionary, we use concise variable names for clarity and to prevent long code lines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578f3e3",
   "metadata": {},
   "source": [
    "- \"vocab_size\" refers to the 50,257-word vocabulary used by the BPE tokenizer in Chapter 2.\n",
    "- \"context_length\" represents the maximum number of input tokens that the model can handle via positional embeddings discussed in Chapter 2.\n",
    "- \"emb_dim\" represents the embedding size, which converts each token into a 768-dimensional vector.\n",
    "- \"n_heads\" represents the attention head count in the multi-head attention mechanism implemented in Chapter 3.\n",
    "- \"n_layers\" specifies the number of transformer blocks in the model, which will be elaborated in later chapters.\n",
    "- \"drop_rate\" represents the strength of the dropout mechanism (0.1 means a 10% drop in hidden units) to prevent overfitting, as described in Chapter 3.\n",
    "- \"qkv_bias\" determines whether to include bias vectors in the linear layers of the multi-head attention for query, key, and value calculations. As is customary with modern LLMs, we will initially disable this, but we will revisit it in Chapter 6 when we load OpenAI’s pretrained GPT-2 weights into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5fd3bf",
   "metadata": {},
   "source": [
    "Using the configuration above, we will start this chapter by implementing the GPT placeholder architecture (DummyGPTModel) in this section, as shown in Figure 4.3. This will give us a global view of how everything fits together and what other components we need to write in the upcoming sections to assemble the complete GPT model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67566077",
   "metadata": {},
   "source": [
    "Figure 4.3 A mental model outlining the order in which we encode the GPT architecture. In this chapter, we will start with the GPT backbone (a placeholder architecture) and then discuss the individual core parts and finally assemble them into the transformer module of the final GPT architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a4b7c6",
   "metadata": {},
   "source": [
    "![image-20240422134328260](../img/fig-4-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc685535",
   "metadata": {},
   "source": [
    "The numbered boxes shown in Figure 4.3 illustrate the order in which we process the various concepts needed to encode the final GPT architecture. We will start at step 1, with a placeholder GPT backbone that we call DummyGPTModel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f01dad",
   "metadata": {},
   "source": [
    "**Listing 4.1 Placeholder GPT model architecture class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "        *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]) #A\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"]) #B\n",
    "        self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "class DummyTransformerBlock(nn.Module): #C\n",
    "    def __init__(self, cfg):\n",
    "    \tsuper().__init__()\n",
    "    def forward(self, x): #D\n",
    "    \treturn x\n",
    "class DummyLayerNorm(nn.Module): #E\n",
    "    def __init__(self, normalized_shape, eps=1e-5): #F\n",
    "    \tsuper().__init__()\n",
    "    def forward(self, x):\n",
    "    \treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69969d27",
   "metadata": {},
   "source": [
    "The DummyGPTModel class in this code uses PyTorch's neural network module (nn.Module). The model architecture in the DummyGPTModel class consists of token and position embeddings, dropout, a series of transformer blocks (DummyTransformerBlock), final layer normalization (DummyLayerNorm), and a linear output layer (out_head). Configurations are passed in via a Python dictionary, for example, the GPT_CONFIG_124M dictionary we created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc70ece",
   "metadata": {},
   "source": [
    "The forward method describes the flow of data through the model: it computes token and position embeddings for the input indices, applies dropout, processes the data through a transformer module, applies normalization, and finally generates logits using a linear output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13ef4b",
   "metadata": {},
   "source": [
    "The above code already works, as we will see later in this section after preparing the input data. However, for now, note that in the above code, we have used placeholders (DummyLayerNorm and DummyTransformerBlock) to implement the transformer block and layer normalization, which we will develop in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fee0c5",
   "metadata": {},
   "source": [
    "Next, we will prepare the input data and initialize a new GPT model to illustrate its use. Figure 4.4 provides a high-level overview of how data flows into and out of the GPT model, building on the numbers we saw in Chapter 2 (where we encoded the tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099417b4",
   "metadata": {},
   "source": [
    "Figure 4.4 shows a high-level overview of how input data is labeled, embedded, and fed to the GPT model. Note that in the DummyGPTClass we encoded earlier, token embeddings were handled in the GPT model. In an LLM, the input token dimensions of the embeddings typically match the output dimensions. The output embeddings here represent the context vectors we discussed in Chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed7f95",
   "metadata": {},
   "source": [
    "![image-20240422134652565](../img/fig-4-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79dcc6a",
   "metadata": {},
   "source": [
    "To implement the steps shown in Figure 4.4, we tokenize batches of two text inputs to the GPT model using the tiktoken tokenizer introduced in Chapter 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea35069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93c468",
   "metadata": {},
   "source": [
    "The resulting token IDs for both texts are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[ 6109, 3626, 6100, 345], #A\n",
    "        [ 6109, 1110, 6622, 257]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f3aaf",
   "metadata": {},
   "source": [
    "Next, we initialize a new 124M parameter DummyGPTModel instance and feed it the tokenized batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ee9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416827b6",
   "metadata": {},
   "source": [
    "The model output (often called logit) is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a253a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Output shape: torch.Size([2, 4, 50257])\n",
    "tensor([[[-1.2034, 0.3201, -0.7130, ..., -1.5548, -0.2390, -0.4667],\n",
    "         [-0.1192, 0.4539, -0.4432, ..., 0.2392, 1.3469, 1.2430],\n",
    "         [ 0.5307, 1.6720, -0.4695, ..., 1.1966, 0.0111, 0.5835],\n",
    "         [ 0.0139, 1.6755, -0.3388, ..., 1.1586, -0.0435, -1.0400]],\n",
    "        [[-1.0908, 0.1798, -0.9484, ..., -1.6047, 0.2439, -0.4530],\n",
    "         [-0.7860, 0.5581, -0.0610, ..., 0.4835, -0.0077, 1.6621],\n",
    "         [ 0.3567, 1.2698, -0.6398, ..., -0.0162, -0.1296, 0.3717],\n",
    "         [-0.2407, -0.7349, -0.5102, ..., 2.0057, -0.3694, 0.1814]]],\n",
    "       grad_fn=<UnsafeViewBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038eead",
   "metadata": {},
   "source": [
    "The output tensor has two rows corresponding to two text samples. Each text sample consists of 4 tokens; each token is a 50,257-dimensional vector, matching the size of the tokenizer vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a3546",
   "metadata": {},
   "source": [
    "The embedding has 50,257 dimensions because each dimension refers to a unique token in the vocabulary. At the end of this chapter, when we implement the post-processing code, we will convert these 50,257-dimensional vectors back into token IDs, which we can then decode into words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3c403",
   "metadata": {},
   "source": [
    "Now that we have a top-down understanding of the GPT architecture and its inand outputs, we will code the various placeholders in the following sections, starting with the actual layer normalization class that will replace the DummyLayerNorm in the previous code snippet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
