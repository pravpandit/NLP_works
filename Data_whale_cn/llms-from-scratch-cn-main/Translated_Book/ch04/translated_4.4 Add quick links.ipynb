{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb87f0a8",
   "metadata": {},
   "source": [
    "# 4.4 Add quick links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c4c4e",
   "metadata": {},
   "source": [
    "Next, let’s discuss the concept behind shortcut connections, which are also known as skip or residual connections.\n",
    "Initially, shortcut connections were proposed for deep networks in computer vision (particularly residual networks) to alleviate the challenge of vanishing gradients.\n",
    "The vanishing gradient problem refers to the problem that the gradients (which guide weight updates during training) gradually become smaller as the layers backpropagate, making it difficult to train early layers, as shown in Figure 4.12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489704c",
   "metadata": {},
   "source": [
    "**Figure 4.12 is a comparison between a deep neural network consisting of 5 layers without shortcut connections (left) and with shortcut connections (right).\n",
    "A shortcut connection involves adding the input of a layer to its output, effectively creating an alternate path that bypasses certain layers.\n",
    "The gradients shown in Figure 1.1 represent the average absolute gradient for each layer, which we will calculate in the code example below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c76b5f",
   "metadata": {},
   "source": [
    "![fig4.12](https://github.com/datawhalechina/llms-from-scratch-cn/blob/main/Translated_Book/img/fig-4-12.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1388332",
   "metadata": {},
   "source": [
    "As shown in Figure 4.12, shortcut connections create a shorter alternate path for gradients to flow through the network by skipping one or more layers. This is achieved by adding the output of one layer to the output of the following layer.\n",
    "This is why these connections are also called skip connections.\n",
    "They play a vital role in backpropagation during training to keep the gradients flowing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de7fbe",
   "metadata": {},
   "source": [
    "In the following code example, we implement the neural network shown in Figure 4.12 and see how to add shortcut connections in the forward method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c73a3",
   "metadata": {},
   "source": [
    "### Code Example 4.5 A Neural Network Illustrated with Shortcut Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7a3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import GELU\n",
    "\n",
    "\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "# Implement a 5-layer neural network\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "       ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "# Calculate the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "# Check if shortcut connection can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f506ca",
   "metadata": {},
   "source": [
    "This code implements a 5-layer deep neural network, each consisting of a linear layer and a GELU activation function.\n",
    "In the forward pass, we iteratively pass the input through the layers, optionally adding the shortcut connections shown in Figure 4.12 if the self.use_shortcut attribute is set to True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da8d36",
   "metadata": {},
   "source": [
    "Let's use this code to initialize a neural network without shortcut connections.\n",
    "Here, each layer will be initialized so that it accepts an example with 3 input values ​​and returns 3 output values.\n",
    "The last layer returns a single output value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac40afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # 为初始权重指定随机种子以确保可重现性\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "     layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec95be",
   "metadata": {},
   "source": [
    "Next, we will implement a function to calculate the gradient in the backward pass of the model through the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64db0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "# Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "   \n",
    "# Calculate the loss based on how close the target and output are\n",
    "# What is the output format?\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "# Backpropagate to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "# Output the mean absolute gradient of the weights\n",
    "           print(f\"{name} has gradient mean of {param.grad.abs().mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7a32c",
   "metadata": {},
   "source": [
    "In the code above, we specify a loss function that calculates how close the model output is to a user-specified target (which is 0 here for simplicity).\n",
    "Then, when loss.backward() is called, PyTorch calculates the gradient of the loss for each layer in the model.\n",
    "We can iterate over the weight parameters via model.named_parameters().\n",
    "Let’s assume that a given layer has a 3×3 matrix of weight parameters.\n",
    "In this case, the layer will have 3×3 gradient values, and we print the average absolute gradient of these 3×3 gradient values ​​to get a single gradient value for each layer, so that gradients between layers can be more easily compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f78ee",
   "metadata": {},
   "source": [
    "In short, the .backward() method is a convenient way in PyTorch to calculate the loss gradients required during model training, without having to implement the mathematical operations of gradient calculations ourselves, making the use of deep neural networks easier.\n",
    "If you are not familiar with the concepts of gradients and neural network training, it is recommended to read sections A.4 \"Automatic differentiation made easy\" and A.7 \"Typical training loop\" in Appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316b291",
   "metadata": {},
   "source": [
    "Now let's use the print_gradients function and apply it to the model without skip connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788c5bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.0002017411752603948\n",
      "layers.1.0.weight has gradient mean of 0.00012011770741082728\n",
      "layers.2.0.weight has gradient mean of 0.0007152437465265393\n",
      "layers.3.0.weight has gradient mean of 0.0013988513965159655\n",
      "layers.4.0.weight has gradient mean of 0.005049604922533035\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec213561",
   "metadata": {},
   "source": [
    "The output looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a524796",
   "metadata": {},
   "source": [
    "layers.0.0.weight has gradient mean of 0.00020173587836325169 \\\n",
    "layers.1.0.weight has gradient mean of 0.0001201116101583466 \\\n",
    "layers.2.0.weight has gradient mean of 0.0007152041653171182 \\\n",
    "layers.3.0.weight has gradient mean of 0.001398873864673078 \\\n",
    "layers.4.0.weight has gradient mean of 0.005049646366387606"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb6c8f",
   "metadata": {},
   "source": [
    "From the output of the print_gradients function, we can see that the gradient gradually decreases from the last layer (layers.4) to the first layer (layers.0). This phenomenon is called the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1a75d",
   "metadata": {},
   "source": [
    "Now, let’s instantiate a model with skip connections and see how it differs from the previous model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3d189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22186796367168427\n",
      "layers.1.0.weight has gradient mean of 0.207092747092247\n",
      "layers.2.0.weight has gradient mean of 0.32923877239227295\n",
      "layers.3.0.weight has gradient mean of 0.2667771875858307\n",
      "layers.4.0.weight has gradient mean of 1.3268063068389893\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c3c36",
   "metadata": {},
   "source": [
    "The output looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6e576",
   "metadata": {},
   "source": [
    "layers.0.0.weight has gradient mean of 0.22169792652130127 \\\n",
    "layers.1.0.weight has gradient mean of 0.20694105327129364 \\\n",
    "layers.2.0.weight has gradient mean of 0.32896995544433594 \\\n",
    "layers.3.0.weight has gradient mean of 0.2665732502937317 \\\n",
    "layers.4.0.weight has gradient mean of 1.3258541822433472 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa22ff7",
   "metadata": {},
   "source": [
    "As we can see, the last layer (layers.4) still has a larger gradient than the other layers according to the output.\n",
    "However, as we progress towards the first layer (layers.0), the gradient values ​​become more stable and do not shrink to extremely small values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028b8b5b",
   "metadata": {},
   "source": [
    "In summary, shortcut connections are important for overcoming the limitations imposed by the vanishing gradient problem in deep neural networks.\n",
    "Shortcut connections are a core building block of large models such as LLMs, and when we train the GPT model in the next chapter, they will help facilitate more efficient training by ensuring consistent gradient flow across layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114fddd3",
   "metadata": {},
   "source": [
    "Having introduced shortcut connections, we will now connect all the previously introduced concepts (layer normalization, GELU activation function, feed-forward modules, and shortcut connections) in the transformer module in the next section, which is the final building block we need to encode the GPT architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cell)",
   "language": "python",
   "name": "cell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
