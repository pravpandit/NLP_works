{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "The jupter notebook involved in this article is in the [Chapter 4 code base](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1). \n",
    "\n",
    "It is recommended to open this tutorial directly using google colab notebook to quickly download relevant datasets and models. \n",
    "If you are opening this notebook in google colab, you may need to install the Transformers and 🤗Datasets libraries. Uncomment the following commands to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MOsHUjgdIrIW"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tune the transformer model on the machine question answering task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nm8eowt6YBU6"
   },
   "source": [
    "In this notebook, we will learn how to fine-tune the transformer model of [🤗 Transformers](https://github.com/huggingface/transformers) to solve machine question answering tasks. This article mainly solves the extractive question answering task: given a question and a text, find the text fragment (span) that can answer the question from the text. By using the `Trainer` API and the dataset package, we will easily load the dataset and then fine-tune the transformer. The figure below gives a simple example\n",
    "![Widget inference representing the QA task](images/question_answering.png)\n",
    "\n",
    "**Note:** Note: The question answering task in this article is to extract answers from text, not to generate answers directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "The examples designed in this notebook can be used to solve any extractive question answering task similar to SQUAD 1 and SQUAD 2, and can use any model checkpoint in the [Model Hub](https://huggingface.co/models) as long as these models contain a token classification head and a fast tokenizer. For the correspondence between models and fast tokenizers, see: [this table](https://huggingface.co/transformers/index.html#bigtable).\n",
    "\n",
    "If your dataset is different from this notebook, you can use this notebook directly with only minor adjustments. Of course, depending on your hardware (computer memory, graphics card size), you need to adjust the batch size reasonably to avoid out-of-memory errors.\n",
    "Set those three parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "# squad_v2 equals True or False to use SQUAD v1 or SQUAD v2 respectively.\n",
    "# If you are using other datasets, True means that the model can answer \"unanswerable\" questions, that is, some questions do not give answers, while False means that all questions must be answered.\n",
    "squad_v2 = False\r\n",
    "model_checkpoint = \"distilbert-base-uncased\"\r\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download the data and get the evaluation indicators we need (compared with the benchmark). \n",
    "\n",
    "These two tasks can be easily completed using the functions `load_dataset` and `load_metric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKx2zKs5IrIq"
   },
   "source": [
    "As an example, we will use the [SQUAD dataset](https://rajpurkar.github.io/SQuAD-explorer/) in this notebook. Similarly, this notebook is also compatible with all question answering datasets provided by the dataset repository. \n",
    "\n",
    "If you are using your own dataset (json or csv format), please check out the [Datasets documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) to learn how to load your custom dataset. You may need to adjust the names used for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_AY1ATSIrIq"
   },
   "outputs": [],
   "source": [
    "# Download data (make sure there is network)\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you can also download the data from the [link](https://gas.graviti.cn/dataset/datawhale/SQuAD) we provided and decompress it, copy the two decompressed json files to the `docs/Chapter 4-Using Transformers to solve NLP tasks/datasets/squad` directory, and then load it with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "\r\n",
    "data_path = './dataset/squad/'\r\n",
    "path = os.path.join(data_path, 'squad.py')\r\n",
    "cache_dir = os.path.join(data_path, 'cache')\r\n",
    "data_files = {\"train\": os.path.join(data_path, \"train-v1.1.json\"), \"validation\": os.path.join(data_path, \"dev-v1.1.json\")}\r\n",
    "datasets = load_dataset(path, data_files=data_files, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "This `datasets` object is a [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict) structure, and training, validation, and testing correspond to a key in this dict respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the following datasets and their properties\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_Tr9XWDYBVA"
   },
   "source": [
    "Whether it is a training set, validation set or test set, each question-answering data sample will have three keys: \"context\", \"question\" and \"answers\".\n",
    "\n",
    "We can use a subscript to select a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'id': '5733be284776f41900661182',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'title': 'University_of_Notre_Dame'}"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]\r\n",
    "# answers represent answers\n",
    "# context represents a text fragment\n",
    "# question represents the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5GXOp9PYBVA"
   },
   "source": [
    "Note the annotation of answers. In addition to giving the answer text in the text snippet, answers also gives the position of the answer (counted from the beginning of the character, in the above example it is the 515th position).\n",
    "\n",
    "To further understand what the data looks like, the following function will randomly select a few examples from the dataset for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\r\n",
    "import random\r\n",
    "import pandas as pd\r\n",
    "from IPython.display import display, HTML\r\n",
    "\r\n",
    "def show_random_elements(dataset, num_examples=10):\r\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\r\n",
    "    picks = []\r\n",
    "    for _ in range(num_examples):\r\n",
    "        pick = random.randint(0, len(dataset)-1)\r\n",
    "        while pick in picks:\r\n",
    "            pick = random.randint(0, len(dataset)-1)\r\n",
    "        picks.append(pick)\r\n",
    "    \r\n",
    "    df = pd.DataFrame(dataset[picks])\r\n",
    "    for column, typ in dataset.features.items():\r\n",
    "        if isinstance(typ, ClassLabel):\r\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\r\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\r\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\r\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>context</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'answer_start': [185], 'text': ['diesel fuel']}</td>\n",
       "      <td>In Alberta, five bitumen upgraders produce synthetic crude oil and a variety of other products: The Suncor Energy upgrader near Fort McMurray, Alberta produces synthetic crude oil plus diesel fuel; the Syncrude Canada, Canadian Natural Resources, and Nexen upgraders near Fort McMurray produce synthetic crude oil; and the Shell Scotford Upgrader near Edmonton produces synthetic crude oil plus an intermediate feedstock for the nearby Shell Oil Refinery. A sixth upgrader, under construction in 2015 near Redwater, Alberta, will upgrade half of its crude bitumen directly to diesel fuel, with the remainder of the output being sold as feedstock to nearby oil refineries and petrochemical plants.</td>\n",
       "      <td>571b074c9499d21900609be3</td>\n",
       "      <td>Besides crude oil, what does the Suncor Energy plant produce?</td>\n",
       "      <td>Asphalt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'answer_start': [191], 'text': ['the GIOVE satellites for the Galileo system']}</td>\n",
       "      <td>Compass-M1 is an experimental satellite launched for signal testing and validation and for the frequency filing on 14 April 2007. The role of Compass-M1 for Compass is similar to the role of the GIOVE satellites for the Galileo system. The orbit of Compass-M1 is nearly circular, has an altitude of 21,150 km and an inclination of 55.5 degrees.</td>\n",
       "      <td>56e1161ccd28a01900c6757b</td>\n",
       "      <td>The purpose of the Compass-M1 satellite is similar to the purpose of what other satellite?</td>\n",
       "      <td>BeiDou_Navigation_Satellite_System</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"], num_examples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before feeding the data into the model, we need to preprocess the data. The preprocessing tool is called `Tokenizer`. `Tokenizer` first tokenizes the input, then converts the tokens into the corresponding token ID required in the pre-model, and then converts them into the input format required by the model.\n",
    "\n",
    "In order to achieve the purpose of data preprocessing, we use the `AutoTokenizer.from_pretrained` method to instantiate our tokenizer, which ensures:\n",
    "\n",
    "- We get a tokenizer that corresponds to the pre-trained model one by one.\n",
    "- When using the tokenizer corresponding to the specified model checkpoint, we also download the vocabulary required by the model, more precisely, the tokens vocabulary.\n",
    "\n",
    "This downloaded tokens vocabulary will be cached so that it will not be downloaded again when used again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\r\n",
    "    \r\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "The following code requires that the tokenizer must be of type transformers.PreTrainedTokenizerFast, because we need to use some special features of the fast tokenizer (such as multi-threaded fast tokenizer) during preprocessing.\n",
    "\n",
    "Almost all tokenizers corresponding to models have corresponding fast tokenizers. We can view the features of the tokenizers corresponding to all pre-trained models in the [Model Tokenizer Correspondence Table](https://huggingface.co/transformers/index.html#bigtable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0UUlLKzYBVD"
   },
   "outputs": [],
   "source": [
    "import transformers\r\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdHO2nryYBVE",
    "outputId": "af9ba744-7aa9-44a8-b0c8-2b60b21bf786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单个文本tokenize: ['what', 'is', 'your', 'name', '?']\n",
      "2个文本tokenize: ['[CLS]', 'my', 'name', 'is', 'sy', '##lva', '##in', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# If we want to see the text format after tokenizer preprocessing, we only use the tokenize method of tokenizer. add special tokens means adding special tokens required by the pre-training model.\n",
    "print(\"单个文本tokenize: {}\".format(tokenizer.tokenize(\"What is your name?\"), add_special_tokens=True))\r\n",
    "print(\"2个文本tokenize: {}\".format(tokenizer.tokenize(\"My name is Sylvain.\", add_special_tokens=True)))\r\n",
    "# The input format of the pre-trained model requires token IDs and an attetnion mask. You can use the following method to get the input required by the pre-trained model format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "The tokenizer can preprocess a single text or a pair of texts. The data obtained after tokenizer preprocessing meets the input format of the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E0gsIhoYBVF",
    "outputId": "e810df73-25ff-4fd9-edff-417bbe9679b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess a single text\n",
    "tokenizer(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the two texts. You can see that the tokenizer adds 101 token ID at the beginning, 102 token ID in the middle to distinguish the two texts, and ends with 102. These rules are designed by the pre-trained model.\n",
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0tivQQwYBVG"
   },
   "source": [
    "The token IDs or input_ids you see above generally vary with the names of the pre-trained models. The reason is that different pre-trained models set different rules during pre-training. But as long as the names of the tokenizer and the model are the same, the input format pre-processed by the tokenizer will meet the model requirements. For more information about pre-processing, please refer to [this tutorial](https://huggingface.co/transformers/preprocessing.html)\n",
    "\n",
    "Now we also need to think about how pre-trained machine question answering models handle very long texts. Generally speaking, there is a maximum length requirement for pre-trained model inputs, so we usually truncate overlong inputs. However, if we truncate the overlong context in the question and answer data triple <question, context, answer>, we may lose the answer (because we extract a small fragment from the context as the answer). To solve this problem, the following code finds an example that exceeds the length and then shows you how to handle it. We slice the overlong input into multiple shorter inputs, each of which must meet the model's maximum length input requirement. Since the answer may be in the same place as the slice, we need to allow intersections between adjacent slices, which is controlled by the `doc_stride` parameter in the code..\n",
    "\n",
    "Machine question answering pre-training models usually concatenate question and context as input, and then let the model find the answer from the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDHb6I5aYBVH"
   },
   "outputs": [],
   "source": [
    "max_length = 384 # 输入feature的最大长度，question和context拼接之后\r\n",
    "doc_stride = 128 # 2个切片之间的重合token数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVKI8CGNYBVH"
   },
   "source": [
    "The for loop iterates over the dataset, looking for an extremely long sample. The maximum input required by the notebook example model is 384 (512 is also commonly used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ma0M9v_YBVH"
   },
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\r\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\r\n",
    "        break\r\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A84Fzhp9YBVI"
   },
   "source": [
    "If not truncated, the input length is 396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WySoZrRwYBVI",
    "outputId": "53943502-8f19-4a8f-bf98-18fe0cf096a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaygYboUYBVI"
   },
   "source": [
    "Now if we truncate to the maximum length of 384, the information of the extra-long part will be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXb6ChXWYBVJ",
    "outputId": "fd940c60-69d9-44fc-d8ea-35c58bf54422"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "738aDe4aYBVJ"
   },
   "source": [
    "Note that, generally speaking, we only slice the context, not the question. Since the context is concatenated after the question and corresponds to the second text, we use `only_second` to control it. The tokenizer uses `doc_stride` to control the overlap length between slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLvoOEUIYBVJ"
   },
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\r\n",
    "    example[\"question\"],\r\n",
    "    example[\"context\"],\r\n",
    "    max_length=max_length,\r\n",
    "    truncation=\"only_second\",\r\n",
    "    return_overflowing_tokens=True,\r\n",
    "    stride=doc_stride\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOA0Y8dGYBVK"
   },
   "source": [
    "Since the overlong input is sliced, we get multiple inputs, the lengths of these input_ids are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEZwY2IzYBVK",
    "outputId": "d4eb8bd4-81fa-4aa5-aae2-bf90bd7b2bc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 157]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuVGMM2BYBVL"
   },
   "source": [
    "We can restore the preprocessed token IDs, input_ids to text format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBs66sJXYBVL",
    "outputId": "dca34307-56e2-45b3-bc21-e6a656c2f8d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切片: 0\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
      "切片: 1\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(tokenized_example[\"input_ids\"][:2]):\n",
    "    print(\"切片: {}\".format(i))\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9IYlTYgYBVL"
   },
   "source": [
    "Since we have sliced ​​the super long text, we need to find the answer position again (relative to the beginning of each piece of context). The machine question answering model will use the position of the answer (the starting and ending positions of the answer, start and end) as the training label (instead of the token IDS of the answer). Therefore, the slice needs to have a corresponding relationship with the original input, and the position of each token in the context after slicing corresponds to the position in the original super long context. In the tokenizer, you can use the `return_offsets_mapping` parameter to get the map of this correspondence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8PK9eS5YBVM",
    "outputId": "d9bcf780-2654-43dc-afd3-4b023a9e0229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418)]\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "# Print the correspondence between the position subscripts before and after the slice\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGeKX0oxYBVM"
   },
   "source": [
    "What is printed above is the position of the first 100 tokens of the 0th slice of tokenized_example in the original context slice. Note that the first token is `[CLS]` set to (0, 0) because this token is not part of the question or answer. The starting and ending positions corresponding to the second token are 0 and 3. We can convert the corresponding token according to the token id after slicing; then use the `offset_mapping` parameter to map back to the token position before slicing to find the tokens at the original position. Since the question is spliced ​​in front of the context, you can just find it from the question according to the subscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv9vD3r6YBVN",
    "outputId": "204f24b3-5feb-4106-f9b4-eb52b5e62fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how How\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SpfrDNzYBVN"
   },
   "source": [
    "Therefore, we get the position correspondence before and after the slice. We also need to use the `sequence_ids` parameter to distinguish between question and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRQPZUQPYBVN",
    "outputId": "94e9a2d8-fc4e-472f-9ddb-61be9641e38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCtYEPCsYBVO"
   },
   "source": [
    "`None` corresponds to special tokens, and 0 or 1 represents the first text and the second text respectively. Since we pass question in as the first input and context in as the second input, they correspond to question and context respectively. Finally, we can find the position of the annotated answer in the features after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZK2Modc7YBVO",
    "outputId": "ad67bfcb-ad7f-4df1-9696-eda61d49e499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_position: 23, end_position: 26\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# Find the Start token index of the current text.\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# Find the End token idnex of the current text.\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# Check if the answer is outside the text interval. In this case, it means that the data of the sample is marked at the CLS token position.\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "# Move token_start_index and token_end_index to both sides of the answer.\n",
    "# Note: The answer is in the boundary condition at the end.\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(\"start_position: {}, end_position: {}\".format(start_position, end_position))\n",
    "else:\n",
    "    print(\"The answer is not in this feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5H2Rv7PYBVP"
   },
   "source": [
    "We need to verify the location of the answer. The verification method is: use the answer location index, get the corresponding token ID, then convert it into text, and then compare it with the original answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdmegUoSYBVP",
    "outputId": "b5979cb1-9fbb-4863-cccc-ad0c5d5ce760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 1, 600\n",
      "over 1,600\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HKReju5YBVQ"
   },
   "source": [
    "Sometimes question is concatenated with context, and sometimes context is concatenated with question. Different models have different requirements, so we need to use the `padding_side` parameter to specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DryiuiaMYBVQ"
   },
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\" #context在右边"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BDIIhJmYBVR"
   },
   "source": [
    "Now, let's combine all the steps together. For the case where there is no answer in the context, we directly place the marked answer start and end positions at the subscript of CLS. If the parameter `allow_impossible_answers` is `False`, then these samples without answers will be thrown away. For the sake of simplicity, we will throw them away first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mObBCXnrYBVR"
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "# We need to perform truncation and padding on examples while retaining all information, so we need to use the slicing method.\n",
    "# Each super long text example will be sliced ​​into multiple inputs, and there will be intersections between two adjacent inputs.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "# We use the overflow_to_sample_mapping parameter to map the slice IDs to the original IDs.\n",
    "# For example, if 2 examples are cut into 4 slices, the corresponding slices are [0, 0, 1, 1], and the first two slices correspond to the original first example.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "# offset_mapping also corresponds to 4 slices\n",
    "# The offset_mapping parameter helps us map to the original input. Since the answer is marked on the original input, it helps us find the starting and ending positions of the answer.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "# Relabel the data\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "# Process each piece\n",
    "# Label the samples without answers on CLS\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "# Distinguishing between question and context\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "# Get the original example index.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "# If there is no answer, use the location of CLS as the answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "# The character level start/end position of the answer.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# Find the index start at the token level.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "# Find the token level index end.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "# Check if the answer exceeds the text length. If it exceeds, CLS index is also used as a marker.\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "# If it does not exceed, find the start and end positions of the answer token. .\n",
    "# Note: we could go after the last offset if the answer is the last word (edge ​​case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "The above preprocessing function can process one sample or multiple sample examples. If it processes multiple samples, it returns a list of the results of the preprocessing of multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-b70jh26IrJS"
   },
   "outputs": [],
   "source": [
    "features = prepare_train_features(datasets['train'][:5])\n",
    "# Process 5 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "Next, all samples in the dataset datasets are preprocessed by using the `map` function to apply the preprocessing function `prepare_train_features` to all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDtsaJeVIrJT"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "Even better, the returned results are automatically cached to avoid recalculation the next time they are processed (but be aware that if the input changes, it may be affected by the cache!). The datasets library function will detect the input parameters to determine if there are any changes. If there are no changes, the cached data will be used. If there are changes, the data will be reprocessed. However, if the input parameters do not change, it is best to clear the cache when you want to change the input. The way to clear it is to use the `load_from_cache_file=False` parameter. In addition, the `batched=True` parameter used above is a feature of the tokenizer, because it will use multiple threads to process the input in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "At present, we have preprocessed the data required for training/fine-tuning. Now we download the pre-trained model. Since we are doing machine question answering tasks, we use this class `AutoModelForQuestionAnswering`. Similar to tokenizer, the model is also loaded using the `from_pretrained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 268M/268M [00:46<00:00, 5.79MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\r\n",
    "\r\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "Since the task we are fine-tuning is the machine question answering task, and we are loading a pre-trained language model, the above will prompt us that some unmatched neural network parameters were thrown away when loading the model (the neural network head of the pre-trained language model was thrown away, and the neural network head of the machine question answering was randomly initialized).\n",
    "\n",
    "Because of these randomly initialized parameters, we need to re-fine-tune our model on the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "In order to get a `Trainer` training tool, we need 3 more elements, the most important of which is the training settings/parameters [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments). The training settings contain all the properties that can define the training process. At the same time, it requires a folder name. This folder will be used to save the model and other model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\r\n",
    "    f\"test-squad\",\r\n",
    "    evaluation_strategy = \"epoch\",\r\n",
    "    learning_rate=2e-5, #学习率\r\n",
    "    per_device_train_batch_size=batch_size,\r\n",
    "    per_device_eval_batch_size=batch_size,\r\n",
    "    num_train_epochs=3, # 训练的论次\r\n",
    "    weight_decay=0.01,\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "The `evaluation_strategy = \"epoch\"` parameter above tells the training code that we will do a validation evaluation once per epoch.\n",
    "\n",
    "The `batch_size` above is defined before this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G0oFbpTYBVX"
   },
   "source": [
    "We use a default_data_collator to feed the preprocessed data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nTsgJKRYBVX"
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\r\n",
    "\r\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "During training, we will only calculate the loss. Evaluating the model based on the evaluation indicators will be put in the next section.\n",
    "\n",
    "Just pass the model, training parameters, data, the previously used tokenizer, and the data delivery tool default_data_collator into Tranier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\r\n",
    "    model,\r\n",
    "    args,\r\n",
    "    train_dataset=tokenized_datasets[\"train\"],\r\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\r\n",
    "    data_collator=data_collator,\r\n",
    "    tokenizer=tokenizer,\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "Call the `train` method to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWZRjmU6r-RP"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_G79pAyYBVY"
   },
   "source": [
    "Since the training time is very long, if it is trained on a local Mac, each epcoh takes about 2 seconds to disappear, so save the following model after each training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYP7NNo3YBVZ"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"test-squad-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwFvugZTYBVZ"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fcMB_uGYBVZ"
   },
   "source": [
    "Model evaluation is a little more complicated. We need to post-process the model output into the text format we need. The model itself predicts the logits at the start/end position of the answer. If we feed the model a batch when evaluating, the output is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzkOFb3uYBVZ"
   },
   "outputs": [],
   "source": [
    "import torch\r\n",
    "\r\n",
    "for batch in trainer.get_eval_dataloader():\r\n",
    "    break\r\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\r\n",
    "with torch.no_grad():\r\n",
    "    output = trainer.model(**batch)\r\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4McnKTSYBVa"
   },
   "source": [
    "The output of the model is a dict-like data structure, which includes the loss (because the label is provided, so there is loss), the logits of the answer start and end. When we output the prediction results, we don’t need to look at the loss, we can just look at the logits directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNYbBFcvYBVa",
    "outputId": "7f752049-0965-4484-96b2-d2326c6f5dcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 384]), torch.Size([16, 384]))"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qQ4y1cwYBVa"
   },
   "source": [
    "Each token in each feature will have a logit. The simplest way to predict the answer is to choose the largest subscript in the start logits as the answer's starting position, and the largest subscript in the end logits as the answer's ending position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTZn5YtxYBVb",
    "outputId": "8818a3c5-1eda-4926-9d37-af2a0124b59f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 46,  57,  78,  43, 118,  15,  72,  35,  15,  34,  73,  41,  80,  91,\n",
       "         156,  35], device='cuda:0'),\n",
       " tensor([ 47,  58,  81,  55, 118, 110,  75,  37, 110,  36,  76,  53,  83,  94,\n",
       "         158,  35], device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po1DsUQdYBVb"
   },
   "source": [
    "The above strategy is good in most cases. However, if our input tells us that we can't find the answer: for example, the position of start is larger than the position of end, or the position of start and end points to the question.\n",
    "\n",
    "At this time, the simple way is that we need to continue to choose the second best prediction as our answer. If it doesn't work, look at the third best prediction, and so on.\n",
    "\n",
    "Since the above method is not easy to find a feasible answer, we need to think of a more reasonable method. We add the logits of start and end to get a new score, and then look at the best `n_best_size` start and end pairs. From the `n_best_size` start and end pairs, deduce the corresponding answer, then check whether the answer is valid, and finally sort them according to the score, and choose the highest score as the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBWVTvBTYBVb"
   },
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9BOWPEoYBVb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "start_logits = output.start_logits[0].cpu().numpy()\r\n",
    "end_logits = output.end_logits[0].cpu().numpy()\r\n",
    "# Collect the best start and end logits locations:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\r\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\r\n",
    "valid_answers = []\r\n",
    "for start_index in start_indexes:\r\n",
    "    for end_index in end_indexes:\r\n",
    "        if start_index <= end_index: # 如果start小雨end，那么合理的\r\n",
    "            valid_answers.append(\r\n",
    "                {\r\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\r\n",
    "                    \"text\": \"\" # 后续需要根据token的下标将答案找出来\r\n",
    "                }\r\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be7maAWsYBVc"
   },
   "source": [
    "Then we sort the valid_answers by score and find the best one. The last step is to check if the text corresponding to the start and end positions is in the context instead of the question.\n",
    "\n",
    "To do this, we need to add the following two pieces of information to the validation features:\n",
    "- The ID of the example that generated the feature. Since each example may generate multiple features, each feature/slice of features needs to know the example they correspond to.\n",
    "- Offset mapping: Map the position of the tokens of each slice to the original text based on the subscript position of the character.\n",
    "\n",
    "So we reprocessed the following validation set. It is slightly different from the `prepare_train_features` when processing training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJpleD_oYBVc"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "def prepare_validation_features(examples):\r\n",
    "# Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "# In one example it is possible to give several features when a context is long, each of those features having a\n",
    "# context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\r\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\r\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\r\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\r\n",
    "        max_length=max_length,\r\n",
    "        stride=doc_stride,\r\n",
    "        return_overflowing_tokens=True,\r\n",
    "        return_offsets_mapping=True,\r\n",
    "        padding=\"max_length\",\r\n",
    "    )\r\n",
    "\r\n",
    "# Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "# its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\r\n",
    "\r\n",
    "# We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\r\n",
    "\r\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\r\n",
    "# Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\r\n",
    "        context_index = 1 if pad_on_right else 0\r\n",
    "\r\n",
    "# One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\r\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\r\n",
    "\r\n",
    "# Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "# position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\r\n",
    "            (o if sequence_ids[k] == context_index else None)\r\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\r\n",
    "        ]\r\n",
    "\r\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmaTICcJYBVd"
   },
   "source": [
    "As before, apply the prepare_validation_features function to each sample in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "32ba04d6240149f49eb48c8d8b7f9aae"
     ]
    },
    "id": "xDfua4clYBVd",
    "outputId": "4789e3b2-52f0-4ca0-9d01-8c2b8e10a167"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ba04d6240149f49eb48c8d8b7f9aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validation_features = datasets[\"validation\"].map(\r\n",
    "    prepare_validation_features,\r\n",
    "    batched=True,\r\n",
    "    remove_columns=datasets[\"validation\"].column_names\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNECLWJLYBVe"
   },
   "source": [
    "Use the `Trainer.predict` method to get all prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTbFJ3FhYBVe"
   },
   "outputs": [],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlTxqJN3YBVe"
   },
   "source": [
    "This `Trainer` *hides* some attributes that are not used during model training (here are `example_id` and `offset_mapping`, which will be used during post-processing), so we need to set them back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyQsRyqGYBVe"
   },
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac0BahcRYBVf"
   },
   "source": [
    "When a token position corresponds to the question part, the `prepare_validation_features` function sets the offset mappings to `None`, so we can easily identify whether the token is in the context based on the offset mapping. We also avoid throwing away very long answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcqAk9GgYBVf"
   },
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wBRLoJoYBVf",
    "outputId": "6e889849-7d40-4003-8d8d-546b8c6eb6a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 16.706663, 'text': 'Denver Broncos'},\n",
       " {'score': 14.635585,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 13.234194, 'text': 'Carolina Panthers'},\n",
       " {'score': 12.468662, 'text': 'Broncos'},\n",
       " {'score': 11.709289, 'text': 'Denver'},\n",
       " {'score': 10.397583,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 10.104669,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 9.721636,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 9.007437,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10'},\n",
       " {'score': 8.834958,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
       " {'score': 8.38701,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
       " {'score': 8.143825,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.'},\n",
       " {'score': 8.03359,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 7.832466,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
       " {'score': 7.650557,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 7.6060467, 'text': 'Carolina Panthers 24–10'},\n",
       " {'score': 7.5795317,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
       " {'score': 7.433568, 'text': 'Carolina'},\n",
       " {'score': 6.742434,\n",
       "  'text': 'Carolina Panthers 24–10 to earn their third Super Bowl title.'},\n",
       " {'score': 6.71136,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\r\n",
    "end_logits = output.end_logits[0].cpu().numpy()\r\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\r\n",
    "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
    "# an example index\n",
    "context = datasets[\"validation\"][0][\"context\"]\r\n",
    "\r\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\r\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\r\n",
    "valid_answers = []\r\n",
    "for start_index in start_indexes:\r\n",
    "    for end_index in end_indexes:\r\n",
    "# Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "# to part of the input_ids that are not in the context.\n",
    "        if (\r\n",
    "            start_index >= len(offset_mapping)\r\n",
    "            or end_index >= len(offset_mapping)\r\n",
    "            or offset_mapping[start_index] is None\r\n",
    "            or offset_mapping[end_index] is None\r\n",
    "        ):\r\n",
    "            continue\r\n",
    "# Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\r\n",
    "            continue\r\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\r\n",
    "            start_char = offset_mapping[start_index][0]\r\n",
    "            end_char = offset_mapping[end_index][1]\r\n",
    "            valid_answers.append(\r\n",
    "                {\r\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\r\n",
    "                    \"text\": context[start_char: end_char]\r\n",
    "                }\r\n",
    "            )\r\n",
    "\r\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\r\n",
    "valid_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLHf5tsBYBVg"
   },
   "outputs": [],
   "source": [
    "将预测答案和真实答案进行比较即可:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPqG0pyiYBVg",
    "outputId": "fa1a7b51-09fb-4ce4-e858-456205dbdb31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_start': [177, 177, 177],\n",
       " 'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']}"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Vl2BpcVYBVh"
   },
   "source": [
    "You can see that the model did it right!\n",
    "\n",
    "As mentioned in the example above, since the first feature must come from the first example, it is relatively easy. For other fearures, we need a mapping between features and examples. Similarly, since an example may be sliced ​​into multiple features, we also need to collect all the answers in all features. The following code maps the subscript of example and the subscript of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgOFWhElYBVh"
   },
   "outputs": [],
   "source": [
    "import collections\r\n",
    "\r\n",
    "examples = datasets[\"validation\"]\r\n",
    "features = validation_features\r\n",
    "\r\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\r\n",
    "features_per_example = collections.defaultdict(list)\r\n",
    "for i, feature in enumerate(features):\r\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXJ-THjzYBVh"
   },
   "source": [
    "The post-processing process is basically complete. The last thing is how to solve the situation of no answer (when squad_v2=True). The above code only considers the answers in the context, so we also need to collect the prediction scores of no answers (the start and end of CLStoken corresponding to the prediction of no answers). If an example sample has multiple features, then we also need to predict whether there are no answers in multiple features. So the final score of no answer is the one with the smallest no answer score of all features.\n",
    "\n",
    "As long as the final score of no answer is higher than the scores of all other answers, then the question is unanswered.\n",
    "\n",
    "Combine everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00SHF2PzYBVh"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\r\n",
    "\r\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\r\n",
    "    all_start_logits, all_end_logits = raw_predictions\r\n",
    "# Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\r\n",
    "    features_per_example = collections.defaultdict(list)\r\n",
    "    for i, feature in enumerate(features):\r\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\r\n",
    "\r\n",
    "# The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\r\n",
    "\r\n",
    "# Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\r\n",
    "\r\n",
    "# Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\r\n",
    "# Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\r\n",
    "\r\n",
    "        min_null_score = None # Only used if squad_v2 is True.\r\n",
    "        valid_answers = []\r\n",
    "        \r\n",
    "        context = example[\"context\"]\r\n",
    "# Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\r\n",
    "# We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\r\n",
    "            end_logits = all_end_logits[feature_index]\r\n",
    "# This is what will allow us to map some of the positions in our logits to span of texts in the original\n",
    "# context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\r\n",
    "\r\n",
    "# Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\r\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\r\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\r\n",
    "                min_null_score = feature_null_score\r\n",
    "\r\n",
    "# Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\r\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\r\n",
    "            for start_index in start_indexes:\r\n",
    "                for end_index in end_indexes:\r\n",
    "# Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "# to part of the input_ids that are not in the context.\n",
    "                    if (\r\n",
    "                        start_index >= len(offset_mapping)\r\n",
    "                        or end_index >= len(offset_mapping)\r\n",
    "                        or offset_mapping[start_index] is None\r\n",
    "                        or offset_mapping[end_index] is None\r\n",
    "                    ):\r\n",
    "                        continue\r\n",
    "# Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\r\n",
    "                        continue\r\n",
    "\r\n",
    "                    start_char = offset_mapping[start_index][0]\r\n",
    "                    end_char = offset_mapping[end_index][1]\r\n",
    "                    valid_answers.append(\r\n",
    "                        {\r\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\r\n",
    "                            \"text\": context[start_char: end_char]\r\n",
    "                        }\r\n",
    "                    )\r\n",
    "        \r\n",
    "        if len(valid_answers) > 0:\r\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\r\n",
    "        else:\r\n",
    "# In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "# failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\r\n",
    "        \r\n",
    "# Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        if not squad_v2:\r\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\r\n",
    "        else:\r\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\r\n",
    "            predictions[example[\"id\"]] = answer\r\n",
    "\r\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfpxu9j3YBV-"
   },
   "source": [
    "Apply the postprocessing function to the original predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "347ebed36d3541388e4e821372e91aa4"
     ]
    },
    "id": "Df4vY9d1YBV_",
    "outputId": "026516bc-d0e4-439a-b77a-daf738f61aa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 10784 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347ebed36d3541388e4e821372e91aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoRhXyiPYBV_"
   },
   "source": [
    "Then we load the evaluation indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYNJLOAbYBV_"
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can also use the local script we provide to load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_path = './dataset/metrics/squad.py'\r\n",
    "metric = load_metric(metric_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HckepCtJYBWA"
   },
   "source": [
    "Then we calculate the evaluation index based on the predictions and annotations. In order to make a reasonable comparison, we need to format the predictions and annotations. For squad2, the evaluation index also requires the parameter `no_answer_probability` (since there is no answer, it is directly set to an empty string, so this parameter is directly set to 0.0 here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4y-LM_cYBWA",
    "outputId": "a122acf7-203c-4eb3-d26b-99b05b78a2df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 76.74550614947965, 'f1': 85.13412652023338}"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\r\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()]\r\n",
    "else:\r\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\r\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\r\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exnxfrEKYBWA"
   },
   "source": [
    "Finally, don’t forget to [see how to upload a model](https://huggingface.co/transformers/model_sharing.html) and upload the model to [🤗 Model Hub](https://huggingface.co/models). You can then use your model by name, just like at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAYJ1DnfYBWA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "4.3-问答任务-抽取式问答",
   "provenance": []
  },
  "interpreter": {
   "hash": "3bfce0b4c492a35815b5705a19fe374a7eea0baaa08b34d90450caf1fe9ce20b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "dea1d2a4b8017c12e0b65317b2730de6c54ea697ad6dca9df4eaf20de101624f"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
