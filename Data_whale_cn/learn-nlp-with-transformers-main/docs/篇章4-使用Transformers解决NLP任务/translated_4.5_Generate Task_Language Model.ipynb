{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTAeNLV3WdB0"
   },
   "source": [
    "The jupter notebook involved in this article is in the [Chapter 4 code base](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1).\n",
    "\n",
    "It is recommended to open this tutorial directly using google colab notebook to quickly download relevant datasets and models.\n",
    "If you are opening this notebook in google colab, you may need to install the Transformers and ðŸ¤—Datasets libraries. Uncomment the following commands to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAWdZQdTWdB3"
   },
   "outputs": [],
   "source": [
    "# !pip install datasets transformers\n",
    "# -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FC_ZTXsWdB3"
   },
   "source": [
    "If you are opening this jupyter notebook on your local machine, please make sure your environment has the latest versions of the above libraries installed.\n",
    "\n",
    "You can find the specific python script file for this jupyter notebook [here](https://github.com/huggingface/transformers/tree/master/examples/language-modeling) and you can also fine-tune your model in a distributed manner using multiple GPUs or TPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgQvrzh3WdB4"
   },
   "source": [
    "# Fine-tune the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqHxDcitWdB4"
   },
   "source": [
    "In this jupyter notebook, we will illustrate how to fine-tune any [ðŸ¤—Transformers](https://github.com/huggingface/transformers) model using the language modeling task. \n",
    "\n",
    "This tutorial will cover two types of language modeling tasks:\n",
    "\n",
    "+ Causal language modeling (CLM): The model needs to predict the character at the next position in the sentence (similar to the decoder and GPT of BERT-like models, where characters are input from left to right). To ensure that the model does not cheat, an attention mask is used to prevent the model from seeing the characters after. For example, when the model tries to predict the character at position i+1 in the sentence, this mask will prevent it from accessing characters after position i.\n",
    "\n",
    "![Inference Representation Causal Language Modeling Task Image](./images/causal_language_modeling.png)\n",
    "\n",
    "+ Masked language modeling (MLM): The model needs to recover some characters that were \"MASKed\" out of the input (pre-training task for BERT-like models). This way the model can see the entire sentence, so the model can predict the character before the \"\\[MASK\\]\" tag based on the characters before and after the \"\\[MASK\\]\" tag.\n",
    "\n",
    "![Widget inference representing the masked language modeling task](images/masked_language_modeling.png)\n",
    "\n",
    "Next, we will explain how to easily load and preprocess the datasets for each task, and how to fine-tune the model using the \"Trainer\" API.\n",
    "\n",
    "Of course, you can also run the python script version of this jupyter notebook directly on a distributed environment or TPU, which can be found in the [examples folder](https://github.com/huggingface/transformers/tree/master/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GobLIFiRWdB5"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tfkh562BWdB5"
   },
   "source": [
    "In the following tasks, we will use the [Wikitext 2](https://huggingface.co/datasets/wikitext#data-instances) dataset as an example. You can load this dataset through the ðŸ¤—Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vtDCQuMSWdB5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Users/niepig/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqMUxoJrWdB7"
   },
   "source": [
    "If you encounter the following error:\n",
    "![request Error](images/request_error.png)\n",
    "\n",
    "Solution:\n",
    "\n",
    "MAC users: Add a line ```199.232.68.133 raw.githubusercontent.com``` in the ```/etc/hosts``` file\n",
    "\n",
    "Windowso users: Add a line ```199.232.68.133 raw.githubusercontent.com``` in the ```C:\\Windows\\System32\\drivers\\etc\\hosts``` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjl5FpYDWdB7"
   },
   "source": [
    "Of course you can replace the dataset above with any dataset publicly available on [hub](https://huggingface.co/datasets), or use your own files. Simply uncomment the following cell and replace the path with the one that will lead to your file path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fncDchlaWdB7"
   },
   "outputs": [],
   "source": [
    "# datasets = load_dataset(\"text\", data_files={\"train\": path_to_train.txt, \"validation\": path_to_validation.txt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODIsscsTWdB8"
   },
   "source": [
    "You can also load datasets from csv or JSON files, see the [full documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) for more information.\n",
    "\n",
    "To access the actual element in a dataset, you need to first select a key and then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H887z9CbWdB8",
    "outputId": "f9d05402-f99b-40da-b672-887e6a8c5597"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3vbv6yHWdB8"
   },
   "source": [
    "To quickly understand the structure of the data, the function below will display some randomly selected examples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "II9ha_LmWdB9"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "LaCaYQyJWdB9",
    "outputId": "8fcf2a87-fa7c-46b1-bd03-26325ce69da9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On 3 March 1967 , parliament decided to build four short take @-@ off and landing airports along the Helgeland coast between Trondheim and BodÃ¸ . Braathens placed an order for a de Havilland Canada DHC @-@ 6 Twin Otter and planned to start the company Braathens STOL . It applied to operate the route without subsidies , but the concession was rejected and granted with subsidies to WiderÃ¸e , which had been operating the routes using seaplanes . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rao Ramesh was cast as a tantrik who helps Gill 's character in the present era . Mumaith Khan was selected for another item number , a remix version of the hit song \" Bangaru Kodipetta \" from Gharana Mogudu ( 1992 ) ; Gharana Mogudu 's music was also composed by M. M. Keeravani . Chiranjeevi made a special appearance after the song , making Magadheera the first film he appeared in after his entry into politics . When Rajamouli suggested the idea of a cameo appearance , Chiranjeevi was initially hesitant till the director narrated the complete sequence and the importance of the song . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>= = = Total Nonstop Action Wrestling ( 2015 â€“ present ) = = = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Daily Telegraph gave the visual novel the award for \" Best Script \" in its video game awards of 2011 , stating that \" Love 's layered narrative of a high school teacher embroiled in his student â€™ s worries goes places most mainstream video games wouldn 't dare . \" \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LH0Uk_OsWdB9"
   },
   "source": [
    "As we can see, some of the text are complete paragraphs of Wikipedia articles, while others are just titles or empty lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Nu5lPu8WdB-"
   },
   "source": [
    "## Causal Language Modeling (CLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7gOchUNWdB-"
   },
   "source": [
    "For the Causal Language Model (CLM), we first get all the texts in the dataset and concatenate them after they are tokenized. Then, we will split them in examples of specific sequence lengths. In this way, the model will receive continuous chunks of text like this:\n",
    "\n",
    "```\n",
    "text1\n",
    "```\n",
    "or\n",
    "```\n",
    "end of text1 [BOS_TOKEN] start of text2\n",
    "```\n",
    "\n",
    "depending on whether they span several original texts in the dataset. The labels will be the same as the input, but shifted to the left.\n",
    "\n",
    "In this example, we will use the [`distilgpt2`](https://huggingface.co/distilgpt2) model. You can also choose any of the checkpoints listed [here](https://huggingface.co/models?filter=causal-lm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z37txOiBWdB-"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk8BWvYWWdB-"
   },
   "source": [
    "In order to tokenize all text with the vocabulary used when training the model, we must download a pre-trained tokenizer. All of this can be done with the AutoTokenizer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mQwZ5UssWdB_"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAQJGvMxWdB_"
   },
   "source": [
    "We can now call the tokenizer on all of our text, which can be done simply using the map method from the Datasets library. First, we define a function that calls the tokenizer on our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wxhKKMYgWdB_"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM_kMpbCWdB_"
   },
   "source": [
    "We then apply it to the tokenization in the datasets object, using batch=True and 4 processes to speed up preprocessing. We don't need the text column afterwards, so we discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNb1U12YWdCA"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc2niRZJWdCA"
   },
   "source": [
    "If we now look at an element of the dataset, we see that the text has been replaced by the input_ids required by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgC4UWv8WdCA",
    "outputId": "e3257089-88b6-4b15-fbe1-45073073ad3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpKE1TJXWdCA"
   },
   "source": [
    "The next step is a little more difficult: we need to concatenate all the texts together and then split the result into small chunks of a specific `block_size`. To do this, we will use the `map` method again with the option `batch=True`. This option allows us to vary the number of examples in the dataset by returning a different number of samples. In this way, we can create new examples from a batch of examples.\n",
    "\n",
    "First, we need to get the maximum length used when pre-training the model. The maximum length is set to 128 here, in case your video memory explodes ðŸ’¥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uEnLI7LJWdCB"
   },
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwZu_FSjWdCB"
   },
   "source": [
    "We then write the preprocessing function to group our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OhhL0v2FWdCB"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "# Concatenate all text\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "# We remove the part corresponding to the remainder. But if the model supports it, you can add padding. You can customize this part as needed.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "# Split by max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpsALat7WdCC"
   },
   "source": [
    "First, notice that we duplicated the input for the labels.\n",
    "\n",
    "This is because the ðŸ¤— transformer library's models shift right by default, so we don't need to do that manually.\n",
    "\n",
    "Also note that by default, the `map` method will send a batch of 1,000 examples to be processed by the preprocessing function. So here, we're dropping the remainder, making the concatenated tokenized text a multiple of `block_size` every 1,000 examples. You can tweak this behavior by passing a higher batch size (which will of course be processed more slowly). You can also use `multiprocessing` to speed up the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmoi9YUZWdCC"
   },
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPkZvXnCWdCD"
   },
   "source": [
    "Now we can check if the dataset has changed: now the samples contain blocks of `block_size` consecutive characters, possibly spanning several original texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "49I25iXJWdCD",
    "outputId": "f7f0364e-7ac0-44e0-aed1-d483b1dda631"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' game and follows the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \". \\n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Oz'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVkDCaz5WdCD"
   },
   "source": [
    "Now that the data has been cleaned, we can instantiate our trainer. We will build a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "f9a94ec0a95a435b8f58cc67994099f7",
      "782ab18684dd4f36a403d180138e8f1d",
      "6d8e4de69163477891b6636d869f6e4e",
      "27a5fef432d24131b2678f9cf5906a4f",
      "36d6db8a50de4a3c886647f31b60b621",
      "969eb6b77105455cab015cb8574b1bd3",
      "a0d0fe9dec9b413fa80fa4d678dcb9c3",
      "7ecafb32516947aa9ace74da667d9665"
     ]
    },
    "id": "jm5DOjPOWdCD",
    "outputId": "5ec1e6e5-66ef-4033-fdc4-3ee3ee6e0dd9"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAFO2Y6_WdCE"
   },
   "source": [
    "Check the torch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11XlW2ogWdCE",
    "outputId": "d8e8c62e-dfc0-43d1-b377-2b1796a52f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import importlib.util\n",
    "# import importlib_metadata\n",
    "a = importlib.util.find_spec(\"torch\") is not None\n",
    "print(a)\n",
    "# _torch_version = importlib_metadata.version(\"torch\")\n",
    "# print(_torch_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8_qK5i3WdCE"
   },
   "source": [
    "and some `TrainingArguments`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WWhPVy82WdCE"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qGz6BxoOWdCF"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efdl12AIWdCF"
   },
   "source": [
    "We pass all of this to the `Trainer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1ukmE65zWdCF"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDn2o1gSWdCF"
   },
   "source": [
    "Then we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "a55CO2xGWdCF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 31/7002 [04:16<14:27:52,  7.47s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAFX3mCwWdCG"
   },
   "source": [
    "Once training is complete, we can evaluate our model and get its perplexity on the validation set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1A7eBP3WdCG"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJzcfmsVWdCG"
   },
   "source": [
    "## Mask Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr5UHTPjWdCG"
   },
   "source": [
    "Masked Language Model (MLM) We will use the same dataset preprocessing as before with one additional step:\n",
    "\n",
    "We will randomly \"mask\" some characters (replace them with \"[MASK]\") and adjust the labels to only contain labels at the \"[MASK]\" position (since we don't need to predict characters that are not \"masked\").\n",
    "\n",
    "In this example, we will use the [`distilroberta-base`](https://huggingface.co/distilroberta-base) model. You can also choose any of the checkpoints listed [here](https://huggingface.co/models?filter=causal-lm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2X4qJPeXWdCG"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilroberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMDHywzqWdCH"
   },
   "source": [
    "We can apply the same tokenizer function as before, we just need to update our tokenizer to use the checkpoint we just selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgIqOa4uWdCH"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4BALsgJWdCH"
   },
   "source": [
    "Like before, we group the texts together and split them into samples of length `block_size`. If your dataset consists of individual sentences, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4jJo5X2WdCH"
   },
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wbjmPZQWdCI"
   },
   "source": [
    "The rest is very similar to what we did before, with two exceptions. First we use a model suitable for masked language modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkJuOG4oWdCI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVKOscEdWdCI"
   },
   "source": [
    "Secondly, we use a special data_collator. A data_collator is a function that is responsible for taking samples and batching them into tensors.\n",
    "\n",
    "In the previous example, we didn't have anything special to do, so we just used the default value for this parameter. Here we are doing random \"MASK\".\n",
    "\n",
    "We could do this as a preprocessing step (`tokenizer`), but at each stage the characters are always masked in the same way. By doing this step in a data_collator, we can ensure that the random masking is done in a new way every time we examine the data.\n",
    "\n",
    "To implement masking, `Transformers` provides a `DataCollatorForLanguageModeling` for masked language models. We can adjust the probability of masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-k8wJK7WdCI"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m83JcPGyWdCI"
   },
   "source": [
    "Then we have to pass everything to the trainer and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I12S2ZQxWdCJ"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_4PHx1CWdCJ"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDKbrOmzWdCJ"
   },
   "source": [
    "Like before, we can evaluate our model on the validation set.\n",
    "\n",
    "The perplexity is much lower compared to the CLM objective because for the MLM objective we only need to make predictions on the hidden tokens (here 15% of the total) while having access to the rest of the tokens.\n",
    "\n",
    "Hence, it is an easier task for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60hUa-W5WdCJ"
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPa5UTWWWdCK"
   },
   "outputs": [],
   "source": [
    "ä¸è¦å¿˜è®°å°†ä½ çš„æ¨¡åž‹[ä¸Šä¼ ](https://huggingface.co/transformers/model_sharing.html)åˆ°[ðŸ¤— æ¨¡åž‹ä¸­å¿ƒ](https://huggingface.co/models)ã€‚"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4.5-ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡åž‹",
   "provenance": []
  },
  "interpreter": {
   "hash": "3bfce0b4c492a35815b5705a19fe374a7eea0baaa08b34d90450caf1fe9ce20b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "27a5fef432d24131b2678f9cf5906a4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ecafb32516947aa9ace74da667d9665",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a0d0fe9dec9b413fa80fa4d678dcb9c3",
      "value": " 353M/353M [00:11&lt;00:00, 31.6MB/s]"
     }
    },
    "36d6db8a50de4a3c886647f31b60b621": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6d8e4de69163477891b6636d869f6e4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_969eb6b77105455cab015cb8574b1bd3",
      "max": 352833716,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36d6db8a50de4a3c886647f31b60b621",
      "value": 352833716
     }
    },
    "782ab18684dd4f36a403d180138e8f1d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ecafb32516947aa9ace74da667d9665": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "969eb6b77105455cab015cb8574b1bd3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0d0fe9dec9b413fa80fa4d678dcb9c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9a94ec0a95a435b8f58cc67994099f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d8e4de69163477891b6636d869f6e4e",
       "IPY_MODEL_27a5fef432d24131b2678f9cf5906a4f"
      ],
      "layout": "IPY_MODEL_782ab18684dd4f36a403d180138e8f1d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
