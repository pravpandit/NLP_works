{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface\n",
    "This article contains a lot of source code and explanations. Each module is divided by paragraphs and horizontal lines. At the same time, the website is equipped with a sidebar to help everyone quickly jump between each section. I hope that everyone will have a deep understanding of BERT after reading it. At the same time, it is recommended to use tools such as pycharm and vscode to debug the bert source code step by step, debug to the corresponding module and then compare the explanation of this chapter.\n",
    "\n",
    "The jupyter involved can be found in [Code Base: Chapter 3-Write a Transformer Model: BERT, Download](https://github.com/datawhalechina/learn-nlp-with-transformers/tree/main/docs/%E7%AF%87%E7%AB%A03-%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AATransformer%E6%A8%A1%E5%9E%8B%EF%BC%9ABERT)\n",
    "\n",
    "This chapter will be based on H[HuggingFace/Transformers, 48.9k Star](https://github.com/huggingface/transformers) for learning. All the codes in this chapter are in [huggingface bert, please note that due to the rapid version update, it may beThere are differences, please refer to version 4.4.2](https://github.com/huggingface/transformers/tree/master/src/transformers/models/bert)HuggingFace is a chatbot startup service provider headquartered in New York. It caught the signal of the BERT trend very early and started to implement the BERT model based on pytorch. The project was originally named pytorch-pretrained-bert. While reproducing the original effect, it provides easy-to-use methods to facilitate various play and research based on this powerful model.\n",
    "\n",
    "As the number of users increased, the project also developed into a larger open source community, merging various pre-trained language models and adding Tensorflow implementations, and was renamed Transformers in the second half of 2019. As of the time of writing this article (March 30, 2021), the project has 43k+ stars. It can be said that Transformers has become a de facto basic NLP tool.\n",
    "\n",
    "## Main content of this section\n",
    "![Figure: BERT structure](./pictures/3-6-bert.png) Figure: BERT structure, source IrEne: Interpretable Energy Prediction for Transformers\n",
    "\n",
    "This article is based on the BERT-related code of the pytorch version of the Transformers version 4.4.2 (released on March 19, 2021) project, and analyzes it from the perspective of code structure, specific implementation and principle, and usage.\n",
    "Main contents:\n",
    "\n",
    "1. BERT Tokenization word segmentation model (BertTokenizer)\n",
    "\n",
    "2. BERT Model ontology model (BertModel)\n",
    "\n",
    "- BertEmbeddings\n",
    "\n",
    "- BertEncoder\n",
    "\n",
    "- BertLayer\n",
    "\n",
    "- BertAttention\n",
    "\n",
    "- BertIntermediate\n",
    "\n",
    "- BertOutput\n",
    "\n",
    "- BertPooler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## 1-Tokenization-BertTokenizer\n",
    "Tokenizers related to BERT are mainly written in [`models/bert/tokenization_bert.py`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import unicodedata\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"bert-base-uncased\": \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\",\n",
    "    }\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"bert-base-uncased\": 512,\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"bert-base-uncased\": {\"do_lower_case\": True},\n",
    "}\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\")\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class BertTokenizer(PreTrainedTokenizer):\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        do_lower_case=True,\n",
    "        do_basic_tokenize=True,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        tokenize_chinese_chars=True,\n",
    "        strip_accents=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            do_lower_case=do_lower_case,\n",
    "            do_basic_tokenize=do_basic_tokenize,\n",
    "            never_split=never_split,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            tokenize_chinese_chars=tokenize_chinese_chars,\n",
    "            strip_accents=strip_accents,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained \"\n",
    "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n",
    "            )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.do_basic_tokenize = do_basic_tokenize\n",
    "        if do_basic_tokenize:\n",
    "            self.basic_tokenizer = BasicTokenizer(\n",
    "                do_lower_case=do_lower_case,\n",
    "                never_split=never_split,\n",
    "                tokenize_chinese_chars=tokenize_chinese_chars,\n",
    "                strip_accents=strip_accents,\n",
    "            )\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n",
    "\n",
    "    @property\n",
    "    def do_lower_case(self):\n",
    "        return self.basic_tokenizer.do_lower_case\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.vocab, **self.added_tokens_encoder)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        if self.do_basic_tokenize:\n",
    "            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
    "\n",
    "# If the token is part of the never_split set\n",
    "                if token in self.basic_tokenizer.never_split:\n",
    "                    split_tokens.append(token)\n",
    "                else:\n",
    "                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n",
    "        else:\n",
    "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
    "        return split_tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "\"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "\"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n",
    "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
    "        adding special tokens. A BERT sequence has the following format:\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs to which the special tokens will be added.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "        Returns:\n",
    "            :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` method.\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "        Returns:\n",
    "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
    "            )\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n",
    "        pair mask has the following format:\n",
    "        ::\n",
    "            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "            | first sequence    | second sequence |\n",
    "        If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "        Returns:\n",
    "            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
    "            sequence(s).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
    "        index = 0\n",
    "        if os.path.isdir(save_directory):\n",
    "            vocab_file = os.path.join(\n",
    "                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
    "            )\n",
    "        else:\n",
    "            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\"\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "        return (vocab_file,)\n",
    "\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "\n",
    "    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True, strip_accents=None):\n",
    "        if never_split is None:\n",
    "            never_split = []\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.never_split = set(never_split)\n",
    "        self.tokenize_chinese_chars = tokenize_chinese_chars\n",
    "        self.strip_accents = strip_accents\n",
    "\n",
    "    def tokenize(self, text, never_split=None):\n",
    "        \"\"\"\n",
    "        Basic Tokenization of a piece of text. Split on \"white spaces\" only, for sub-word tokenization, see\n",
    "        WordPieceTokenizer.\n",
    "        Args:\n",
    "            **never_split**: (`optional`) list of str\n",
    "                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n",
    "                :func:`PreTrainedTokenizer.tokenize`) List of token not to split.\n",
    "        \"\"\"\n",
    "# union() returns a new set by concatenating the two sets.\n",
    "        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n",
    "        text = self._clean_text(text)\n",
    "\n",
    "# This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "# models. This is also applied to the English models now, but it doesn't\n",
    "# matter since the English models were not trained on any Chinese data\n",
    "# and generally don't have any Chinese data in them (there are Chinese\n",
    "# characters in the vocabulary because Wikipedia does have some Chinese\n",
    "# words in the English Wikipedia.).\n",
    "        if self.tokenize_chinese_chars:\n",
    "            text = self._tokenize_chinese_chars(text)\n",
    "        orig_tokens = whitespace_tokenize(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if token not in never_split:\n",
    "                if self.do_lower_case:\n",
    "                    token = token.lower()\n",
    "                    if self.strip_accents is not False:\n",
    "                        token = self._run_strip_accents(token)\n",
    "                elif self.strip_accents:\n",
    "                    token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_punc(token, never_split))\n",
    "\n",
    "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "\"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text, never_split=None):\n",
    "\"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        if never_split is not None and text in never_split:\n",
    "            return [text]\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    def _tokenize_chinese_chars(self, text):\n",
    "\"\"\"Adds whitespace around any CJK character.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if self._is_chinese_char(cp):\n",
    "                output.append(\" \")\n",
    "                output.append(char)\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _is_chinese_char(self, cp):\n",
    "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "# This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "# https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "        #\n",
    "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "# despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "# as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "# space-separated words, so they are not treated specially and handled\n",
    "# like the all of the other languages.\n",
    "        if (\n",
    "            (cp >= 0x4E00 and cp <= 0x9FFF)\n",
    "            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n",
    "            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n",
    "            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
    "            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n",
    "            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n",
    "            or (cp >= 0xF900 and cp <= 0xFAFF)\n",
    "            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n",
    "        ):  #\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cp = ord(char)\n",
    "            if cp == 0 or cp == 0xFFFD or _is_control(char):\n",
    "                continue\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "\n",
    "class WordpieceTokenizer(object):\n",
    "\"\"\"Runs WordPiece tokenization.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n",
    "        tokenization using the given vocabulary.\n",
    "        For example, :obj:`input = \"unaffable\"` wil return as output :obj:`[\"un\", \"##aff\", \"##able\"]`.\n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer`.\n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class BertTokenizer(PreTrainedTokenizer):\n",
    "\"\"\"\n",
    "Construct a BERT tokenizer. Based on WordPiece.\n",
    "\n",
    "This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.\n",
    "Users should refer to this superclass for more information regarding those methods.\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "`BertTokenizer` is a tokenizer based on `BasicTokenizer` and `WordPieceTokenizer`:\n",
    "- BasicTokenizer is responsible for the first step of processing - splitting sentences by punctuation, spaces, etc., and handling whether to unify lowercase, and cleaning illegal characters.\n",
    "- For Chinese characters, split by word through preprocessing (adding spaces);\n",
    "- At the same time, you can use never_split specifies that some words are not to be segmented;\n",
    "- This step is optional (executed by default).\n",
    "- WordPieceTokenizer further decomposes words into subwords based on words.\n",
    "- Subword is between char and word. It not only retains the meaning of words to a certain extent, but also takes into account the word list explosion caused by singular and plural, tense and OOV (Out-Of-Vocabulary) problems of unregistered words in English, and separates the root and tense affixes, thereby reducing the word list and the difficulty of training;\n",
    "- For example, the word tokenizer can be decomposed into two parts: \"token\" and \"##izer\". Note that the \"##\" of the latter word means that it is connected to the previous word.\n",
    "BertTokenizer has the following common methods:\n",
    "- from_pretrained: Initialize a tokenizer from a directory containing a vocabulary file (vocab.txt);\n",
    "- tokenize: Decompose a text (word or sentence) into a list of subwords;\n",
    "- convert_tokens_to_ids: Convert a list of subwords to a list of subword indexes;\n",
    "- convert_ids_to_tokens: The opposite of the previous one;\n",
    "- convert_tokens_to_string: Convert sThe ubword list is concatenated back to words or sentences by “##”;\n",
    "- encode: For a single sentence input, decompose the words and add special words to form a structure of “[CLS], x, [SEP]” and convert it into a list of subscripts corresponding to the word list; for two sentence inputs (only the first two for multiple sentences), decompose the words and add special words to form a structure of “[CLS], x1, [SEP], x2, [SEP]” and convert it into a list of subscripts;\n",
    "- decode: The output of the encode method can be turned into a complete sentence.\n",
    "And the class’s own methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 698kB/s]\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 11.1kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 863kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bt('I like natural language progressing!')\n",
    "# {'input_ids': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## 2-Model-BertModel\n",
    "The code related to the BERT model is mainly written in [`/models/bert/modeling_bert.py`](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py), which has more than a thousand lines of code, including the basic structure of the BERT model and the fine-tuning model based on it.\n",
    "\n",
    "Let's start with the analysis of the BERT model:\n",
    "```\n",
    "class BertModel(BertPreTrainedModel):\n",
    "\"\"\"\n",
    "\n",
    "The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
    "all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
    "set to :obj:`True`. To be used in a Seq2Seq model, the model needs to be initialized with both :obj:`is_decoder`\n",
    "argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
    "input to the forward pass.\n",
    "\"\"\" \n",
    "```\n",
    "BertModel is mainly a transformer encoder structure, which contains three parts:\n",
    "1. embeddings, that is, entities of the BertEmbeddings class, which obtain the corresponding vector representation according to the word symbol;\n",
    "2. encoder, that is, entities of the BertEncoder class;\n",
    "3. pooler, that is, entities of the BertPooler class, which is optional.\n",
    "\n",
    "**Note that BertModel can also be configured as a Decoder, but this part is not discussed below.**\n",
    "\n",
    "The following will introduce the meaning of each parameter and return value during the forward propagation of BertModel:\n",
    "```\n",
    "def forward(\n",
    "self,\n",
    "input_ids=None,\n",
    "attention_mask=None,\n",
    "token_type_ids=None,\n",
    "position_ids=None,head_mask=None,\n",
    "inputs_embeds=None,\n",
    "encoder_hidden_states=None,\n",
    "encoder_attention_mask=None,\n",
    "past_key_values=None,\n",
    "use_cache=None,\n",
    "output_attentions=None,\n",
    "output_hidden_states=None,\n",
    "return_dict=None,\n",
    "): ...\n",
    "```\n",
    "- input_ids: the subscript list corresponding to the subword after tokenizer segmentation;\n",
    "- attention_mask: in the process of self-attention, this mask is used to mark the difference between the sentence and padding of the subword, and fill the padding part with 0;\n",
    "- token_type_ids: mark the sentence in which the subword is currently located (first sentence/second sentence/padding);\n",
    "- position_ids: mark the current position of the subwordThe position index of the sentence where the previous word is located;\n",
    "- head_mask: used to invalidate some attention calculations of some layers;\n",
    "- inputs_embeds: if provided, then input_ids is not needed, and the embedding lookup process is skipped and directly entered into the encoder calculation as Embedding;\n",
    "- encoder_hidden_states: this part works when BertModel is configured as a decoder, and cross-attention will be performed instead of self-attention;\n",
    "- encoder_attention_mask: as above, used to mark the padding of the encoder input in cross-attention;\n",
    "- past_key_values: this parameter seems to pass in the pre-calculated K-V product to reduce the cost of cross-attention (because this part is originally a repeated calculation);\n",
    "- use_cache: save the previous parameter and pass it back to speed up decoding;\n",
    "- output_attentions: whether to return the attention output of each intermediate layer;\n",
    "- output_hidden_states: whether to return the output of each intermediate layer；\n",
    "- return_dict: whether to return the output in the form of key-value pairs (ModelOutput class, which can also be used as tuple), the default is true.\n",
    "\n",
    "**Note that the head_mask here invalidates the attention calculation, which is different from the attention head pruning mentioned below, and only multiplies the calculation results of some attention by this coefficient. **\n",
    "\n",
    "The output part is as follows:\n",
    "```\n",
    "# BertModel forward propagation return part\n",
    "if not return_dict:\n",
    "return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "last_hidden_state=sequence_output,\n",
    "pooler_output=pooled_output,\n",
    "past_key_values=encoder_outputs.past_key_values,\n",
    "hidden_state=sequence_output,\n",
    "pooler_output=pooled_output,\n",
    "past_key_values=encoder_outputs.past_key_values,n_states=encoder_outputs.hidden_states,\n",
    "attentions=encoder_outputs.attentions,\n",
    "cross_attentions=encoder_outputs.cross_attentions,\n",
    ")\n",
    "```\n",
    "It can be seen that the return value not only includes the output of encoder and pooler, but also includes other specified output parts (hidden_states and attention, etc., which are conveniently accessed in encoder_outputs[1:]):\n",
    "\n",
    "```\n",
    "# The forward propagation return part of BertEncoder, that is, the encoder_outputs above\n",
    "if not return_dict:\n",
    "return tuple(\n",
    "v\n",
    "for v in [\n",
    "hidden_states,\n",
    "next_decoder_cache,\n",
    "all_hidden_states,\n",
    "all_self_attentions,\n",
    "all_cross_attentions,\n",
    "]\n",
    "if v is not None\n",
    ")\n",
    "return BaseModelOutputWithPastAndCrossAttentions(\n",
    "last_hidden_state=hidden_states,\n",
    "past_key_values=next_decoder_cache,\n",
    "hidden_states=all_hidden_states,\n",
    "attentions=all_self_attentions,\n",
    "cross_attentions=all_cross_attentions,\n",
    ")\n",
    "```\n",
    "\n",
    "ThisIn addition, BertModel also has the following methods to facilitate BERT players to perform various operations:\n",
    "\n",
    "- get_input_embeddings: extract word_embeddings in embedding, i.e., the word vector part;\n",
    "\n",
    "- set_input_embeddings: assign values ​​to word_embeddings in embedding;\n",
    "\n",
    "- _prune_heads: provides a function for pruning attention heads, with the input as a dictionary of {layer_num: list of heads to prune in this layer}, which can prune some attention heads of the specified layer.\n",
    "\n",
    "** Pruning is a complex operation, which requires copying the weights of the retained attention head part Wq, Kq, Vq and the fully connected part after splicing to a new smaller weight matrix (note that grad must be disabled before copying), and recording the pruned heads in real time to prevent subscript errors. For details, refer to the prune_heads method in the BertAttention part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import *\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
    "    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
    "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
    "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
    "    input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            batch_size, seq_length = input_shape\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size, seq_length = input_shape\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "# past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "# ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "# If a 2D or 3D attention mask is provided for the cross-attention\n",
    "# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "# Prepare head mask if needed\n",
    "# 1.0 in head_mask indicate we keep the head\n",
    "# attention_probs has shape bsz x n_heads x N x N\n",
    "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.1-BertEmbeddings\n",
    "Contains three parts summed up:\n",
    "![Bert-embedding](./pictures/3-0-embedding.png) Figure: Bert-embedding\n",
    "\n",
    "1. word_embeddings, the embedding corresponding to the subword in the above text.\n",
    "2. token_type_embeddings, used to indicate the sentence where the current word is located, to help distinguish the difference between the sentence and the padding and sentence pairs.\n",
    "3. position_embeddings, the position embedding of each word in the sentence, used to distinguish the order of words. Unlike the design in the transformer paper, this part is trained instead of the fixed embedding calculated by the Sinusoidal function. It is generally believed that this implementation is not conducive to scalability (difficult to directly migrate to longer sentences).\n",
    "\n",
    "The three embeddings are added without weights and output after passing through a layer of LayerNorm+dropout, and its size is (batch_size, sequence_length, hidden_size).\n",
    "\n",
    "** [Why use LayerNorm+Dropout here? Why use LayerNorm instead of BatchNorm?rm? You can refer to a good answer: Why does transformer use layer normalization instead of other normalization methods? ](https://www.zhihu.com/question/395811291/answer/1260290120)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "\"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "# any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "# position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
    "            self.register_buffer(\n",
    "                \"token_type_ids\",\n",
    "                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n",
    "                persistent=False,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "# Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
    "# when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
    "#issue #5664\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "### 2.2-BertEncoder\n",
    "\n",
    "Contains multiple layers of BertLayer. There is nothing special to explain about this part, but there is a detail worth referring to: using gradient checkpointing technology to reduce the memory usage during training.\n",
    "\n",
    "**Gradient checkpointing is a gradient checkpoint. It compresses the model space by reducing the saved computational graph nodes, but it is necessary to recalculate the unstored values ​​when calculating the gradient. Refer to the paper \"Training Deep Nets with Sublinear Memory Cost\". The process is as follows**\n",
    "![gradient-checkpointing](./pictures/3-1-gradient-checkpointing.gif) Figure: gradient-checkpointing\n",
    "\n",
    "In BertEncoder, gradient checkpoint is implemented through torch.utils.checkpoint.checkpoint, which is convenient to use. You can refer to the document: torch.utils.checkpoint - PyTorch 1.8.1 documentation for the specific implementation of this mechanism.It is quite complicated, so I won’t expand on it here.\n",
    "\n",
    "Going deeper, we enter a layer of Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
    "                        \"`use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "#### 2.2.1.1 BertAttention\n",
    "\n",
    "I thought the implementation of attention was here, but I didn't expect to go to the next level... Among them, the self member is the implementation of multi-head attention, and the output member implements a series of operations after attention, including full connection + dropout + residual + LayerNorm.\n",
    "\n",
    "```\n",
    "class BertAttention(nn.Module):\n",
    "def __init__(self, config):\n",
    "super().__init__()\n",
    "self.self = BertSelfAttention(config)\n",
    "self.output = BertSelfOutput(config)\n",
    "self.pruned_heads = set()\n",
    "```\n",
    "First, let's go back to this layer. The pruning operation mentioned above appears here, namely the prune_heads method:\n",
    "```\n",
    "def prune_heads(self, heads):\n",
    "if len(heads) == 0:\n",
    "returnn\n",
    "heads, index = find_pruneable_heads_and_indices(\n",
    "heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    ")\n",
    "\n",
    "# Prune linear layers\n",
    "self.self.query = prune_linear_layer(self.self.query, index)\n",
    "self.self.key = prune_linear_layer(self.self.key, index)\n",
    "self.self.value = prune_linear_layer(self.self.value, index)\n",
    "self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "# Updatehyper params and store pruned heads\n",
    "self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "self.pruned_heads = self.pruned_heads.union(heads) \n",
    "```\n",
    "The specific implementation here is summarized as follows:\n",
    "- `find_pruneable_heads_and_indices` locates the head to be pruned and the dimension index to be retained;\n",
    "\n",
    "- `prune_linear_layer` is responsible for transferring the dimensions that have not been pruned in the Wk/Wq/Wv weight matrix (together with bias) according to the index to the new matrix.\n",
    "Next, let's get to the highlight - the specific implementation of Self-Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "# Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "# Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "##### 2.2.1.1.1 BertSelfAttention\n",
    "\n",
    "**Warning: This is the core area of ​​the model and the only part that involves formulas, so a lot of code will be posted. **\n",
    "\n",
    "Initialization part:\n",
    "```\n",
    "class BertSelfAttention(nn.Module):\n",
    "def __init__(self, config):\n",
    "super().__init__()\n",
    "if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "raise ValueError(\n",
    "\"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "\"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
    "\n",
    "self.num_attention_heads = config.num_attention_heads\n",
    "self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "self.max_position_embeddings = config.max_position_embeddings\n",
    "self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "self.is_decoder = config.is_decoder\n",
    "```\n",
    "- Remove the familiar query, key, value threeweights and a dropout, there is also a mysterious position_embedding_type, and decoder tag;\n",
    "- Note that hidden_size and all_head_size are the same at the beginning. As for why it seems unnecessary to set this variable-obviously, it is because of the pruning function above. After cutting off a few attention heads, all_head_size will naturally be smaller;\n",
    "\n",
    "- Hidden_size must be an integer multiple of num_attention_heads. Taking bert-base as an example, each attention contains 12 heads, and hidden_size is 768, so the size of each head is attention_head_size=768/12=64;\n",
    "\n",
    "- What is position_embedding_type? You will know if you continue to read on.\n",
    "\n",
    "Then the key point is the forward propagation process.\n",
    "\n",
    "First, let's review the basic formula of multi-head self-attention:\n",
    "\n",
    "$$MHA(Q, K, V) = Concat(head_1, ..., head_h)W^O$$\n",
    "$$head_i = SDPA(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "$$SDPA(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt(d_k)})V$$\n",
    "\n",
    "As we all know, these attention heads are calculated in parallel, so the three weights of query, key, and value above are unique - this does not mean that all heads share weights, but are \"spliced\" together.\n",
    "\n",
    "**[The reason for multi-head in the original paper is that Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. Another more reliable analysis is: Why does Transformer need Multi-head Attention? ](https://www.zhihu.com/question/341222779/answer/814111138)**\n",
    "\n",
    "Look at forwardMethod: \n",
    "```\n",
    "def transpose_for_scores(self, x):\n",
    "new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "x = x.view(*new_x_shape)\n",
    "return x.permute(0, 2, 1, 3)\n",
    "\n",
    "def forward(\n",
    "self,\n",
    "hidden_states,\n",
    "attention_mask=None,\n",
    "head_mask=None,\n",
    "encoder_hidden_states=None,\n",
    "encoder_attention_mask=None,\n",
    "past_key_value=None,\n",
    "output_attentions=False,\n",
    "):\n",
    "mixed_query_layer = self.query(hidden_states, head_mask=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False,\n",
    "):\n",
    "mixed_query_layer = self.query(hidden_states, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False,_states)\n",
    "\n",
    "# Omit some cross-attention calculations\n",
    "key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "# ...\n",
    "```\n",
    "Here `transpose_for_scores` is used to split `hidden_size` into multiple head outputs, and transpose the middle two dimensions for matrix multiplication;\n",
    "\n",
    "Here the shape of `key_layer/value_layer/query_layer`r is: (batch_size, num_attention_heads, sequence_length, attention_head_size);\n",
    "Here the shape of `attention_scores` is: (batch_size, num_attention_heads, sequence_length, sequence_length), which conforms to the shape of the attention map obtained by calculating multiple heads separately.\n",
    "\n",
    "Here we have implemented the multiplication of K and Q to obtain the raw attention scores. According to the formula, the next step should be to scale by $d_k$ and perform softmax operations. However, what first appears in front of us is a strange positional_embedding, and a bunch of Einstein sums:\n",
    "\n",
    "```\n",
    "# ...\n",
    "if self.position_embedding_type == \"relative_key\" or self.position_embedding_ttype == \"relative_key_query\":\n",
    "seq_length = hidden_states.size()[1]\n",
    "position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "distance = position_ids_l - position_ids_r\n",
    "positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "positional_embedding = positionpositional_embedding.to(dtype=query_layer.dtype) # fp16 compatibility\n",
    "\n",
    "if self.position_embedding_type == \"relative_key\":\n",
    "relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "attention_scores = attention_scores + relative_position_scores\n",
    "elif self.position_embedding_type == \"relative_key_query\":\n",
    "relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)dding)\n",
    "relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "# ...\n",
    "```\n",
    "**[For Einstein summation convention, refer to the following document: torch.einsum - PyTorch 1.8.1 documentation](https://pytorch.org/docs/stable/generated/torch.einsum.html)**\n",
    "\n",
    "For different positional_embedding_type, there are three operations:\n",
    "\n",
    "- absolute: default value, this part does not need to be processed;\n",
    "- relative_key: process key_layer and compare it with positional_embedd hereing and key matrices are multiplied as key-related position encodings;\n",
    "- relative_key_query: multiply both key and value as position encodings.\n",
    "\n",
    "Back to the normal attention process:\n",
    "```\n",
    "# ...\n",
    "attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "if attention_mask is not None:\n",
    "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "attention_scores = attention_scores + attention_mask # Why is it + instead of * here?\n",
    "\n",
    "# Normalize the attention scores to probabilities.\n",
    "attention_probs =nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "# This is actually dropping out entire tokens to attend to, which might\n",
    "# seem a bit unusual, but is taken from the original Transformer paper.\n",
    "attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "# Mask heads if we want to\n",
    "if head_mask is not None:\n",
    "attention_probs = attention_probs * head_mask\n",
    "\n",
    "context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "context_layer = context_layer.permute(0,2, 1, 3).contiguous()\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "context_layer = context_layer.view(*new_context_layer_shape)\n",
    "outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "# Omit the decoder return value part...\n",
    "return outputs\n",
    "```\n",
    "\n",
    "Major question: What is attention_scores = attention_scores + attention_mask doing here? Shouldn't it be multiplied by mask?\n",
    "- Because the attention_mask here has been [tampered with], the part that was originally 1 becomes 0, and the part that was originally 0 (i.e. padding) becomes a larger negative number, so the addition results in a largerNegative value:\n",
    "- Why use a larger negative number? Because after the softmax operation, this item will become a decimal close to 0.\n",
    "\n",
    "```\n",
    "(Pdb) attention_mask\n",
    "tensor([[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],\n",
    "[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],\n",
    "[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],\n",
    "...,\n",
    "[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],\n",
    "[[[ -0., -0., -0., ..., -10000., -10000., -10000.]]],\n",
    "[[[ -0., -0.,-0., ..., -10000., -10000., -10000.]]]],\n",
    "device='cuda:0')\n",
    "```\n",
    "\n",
    "So, where is this step executed?\n",
    "I didn't find the answer in modeling_bert.py, but I found a special class in modeling_utils.py: class ModuleUtilsMixin, and found a clue in its get_extended_attention_mask method:\n",
    "\n",
    "```\n",
    "def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: Tuple[int], device: device) -> Tensor:\n",
    "\"\"\"\n",
    "Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "Arguments:\n",
    "attention_mask (:obj:`torch.Tensor`):\n",
    "Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "input_shape (:obj:`Tuple[int]`):\n",
    "The shape of the input to the model.\n",
    "device: (:obj:`torch.device`):\n",
    "The device of the input to the model.\n",
    "\n",
    "Returns:\n",
    ":obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
    "\"\"\"\n",
    "# Omit part...\n",
    "\n",
    "# Since attention_mask is 1.0for positions we want to attend and 0.0 for\n",
    "# masked positions, this operation will create a tensor which is 0.0 for\n",
    "# positions we want to attend and -10000.0 for masked positions.\n",
    "# Since we are adding it to the raw scores before the softmax, this is\n",
    "# effectively the same as removing these entirely.\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=self.dtype) # fp16 compatibility\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "return extended_attention_mask\n",
    "```\n",
    "\n",
    "So, when is this function called? What is the relationship with BertModel?\n",
    "\n",
    "OK, here are the inheritance details of `BertModel`: `BertModel` inherits from `BertPreTrainedModel`, which inherits from `PreTrainedModel`, and `PreTrainedModel` inherits from three base classes [nn.Module, ModuleUtilsMixin, GenerationMixin]. ——What a complex encapsulation!\n",
    "\n",
    "This means that BertModel must have called get_extended_attention_mask on the original attention_mask at some intermediate step, causing the attention_mask to change from the original [1, 0] to [0, -1e4].\n",
    "\n",
    "Finally, this call was found in the forward propagation process of BertModel (line 944):\n",
    "\n",
    "```\n",
    "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "# ourselves in which case we just need to make it broadcastable to all heads.\n",
    "extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "```\n",
    "Problem solved: This method not only changes the value of the mask, but also broadcasts it to a shape that can be directly added to the attention map.\n",
    "You are worthy of you, HuggingFace.\n",
    "\n",
    "In addition, the details worth noting are:\n",
    "\n",
    "- Scaling according to the dimension of each head, for bert-base it is the square root of 64, that is, 8;\n",
    "- attention_probs not only does softmax, but also uses dropout once. Is this because it is worried that the attention matrix is ​​too dense... It is also mentioned here that it is very unusual, but the original Transformer paper does this;\n",
    "- head_mask is the mask for multi-head calculation mentioned earlier. If it is not set, it will be all 1 by default and will not work here; \n",
    "- context_layer is the product of the attention matrix and the value matrix. The original size is: (batch_size, num_attention_heads, sequence_length, attention_head_size); \n",
    "- After the context_layer is transposed and viewed, the shape is restored to (batch_size, sequence_length, hidden_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "# If this is instantiated as a cross-attention module, the keys\n",
    "# and values ​​come from an encoder; the attention mask needs to be\n",
    "# such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "# reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        if self.is_decoder:\n",
    "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "# Further calls to cross_attention layer can then reuse all cross-attention\n",
    "# key/value_states (first \"if\" case)\n",
    "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "# all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "# if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "# Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "# This is actually dropping out entire tokens to attend to, which might\n",
    "# seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "# Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "##### 2.2.1.1.2 BertSelfOutput\n",
    "```\n",
    "class BertSelfOutput(nn.Module):\n",
    "def __init__(self, config):\n",
    "super().__init__()\n",
    "self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "def forward(self, hidden_states, input_tensor):\n",
    "hidden_states = self.dense(hidden_states)\n",
    "hidden_states = self.dropout(hidden_states)tes)\n",
    "hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "return hidden_states\n",
    "```\n",
    "\n",
    "**Here we have the combination of LayerNorm and Dropout, but here we first use Dropout, then perform residual connection and then LayerNorm. As for why we need to do residual connection, the most direct purpose is to reduce the difficulty of training caused by too deep network layers, and to be more sensitive to the original input~**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "#### 2.2.1.2 BertIntermediate\n",
    "\n",
    "After reading BertAttention, there is a fully connected + activated operation after Attention:\n",
    "```\n",
    "class BertIntermediate(nn.Module):\n",
    "def __init__(self, config):\n",
    "super().__init__()\n",
    "self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "if isinstance(config.hidden_act, str):\n",
    "self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "else:\n",
    "self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "def forward(self, hidden_states):\n",
    "hidden_states = self.dense(hidden_states)\n",
    "hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "return hidden_states\n",
    "```\n",
    "\n",
    "- The full connection here is expanded. Taking bert-base as an example, the expanded dimension is 3072, which is 4 times the original dimension of 768;\n",
    "- The activation function here is implemented by default as gelu (Gaussian Error Linerar Units (GELUS)). Of course, it cannot be calculated directly, but can be approximated by an expression containing tanh (omitted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "#### 2.2.1.3 BertOutput\n",
    "\n",
    "Here is another full connection + dropout + LayerNorm, and a residual connection residual connect:\n",
    "```\n",
    "class BertOutput(nn.Module):\n",
    "def __init__(self, config):\n",
    "super().__init__()\n",
    "self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "def forward(self, hidden_states, input_tensor):\n",
    "hidden_states = self.dense(hidden_states)\n",
    "hidden_states = self.dropout(hidden_states)\n",
    "hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "return hidden_states\n",
    "```\n",
    "\n",
    "The operation here is not unrelated to BertSelfOutput, but it is exactly the same... two components that are very easy to confuse.\n",
    "The following content also includes BERT-based application models, as well as BERT-related optimizers and usage, which will be introduced in detail in the next article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "### 2.2.3 BertPooler\n",
    "This layer simply takes the first token of the sentence, the vector corresponding to `[CLS]`, and then passes it through a fully connected layer and an activation function to output: (This part is optional because pooling has many different operations)\n",
    "\n",
    "```\n",
    "class BertPooler(nn.Module):\n",
    "def __init__(self, config):\n",
    "super().__init__()\n",
    "self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "self.activation = nn.Tanh()\n",
    "\n",
    "def forward(self, hidden_states):\n",
    "# We \"pool\" the model by simply taking the hidden state corresponding\n",
    "# to the first token.\n",
    "first_token_tensor = hidden_states[:, 0]\n",
    "pooled_output = self.dense(first_token_tensor)\n",
    "pooled_output = self.activation(pooled_output)\n",
    "return pooled_output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to bert pooler size: 768\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "# We \"pool\" the model by simply taking the hidden state corresponding\n",
    "# to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "from transformers.models.bert.configuration_bert import *\n",
    "import torch\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "bert_pooler = BertPooler(config=config)\n",
    "print(\"input to bert pooler size: {}\".format(config.hidden_size))\n",
    "batch_size = 1\n",
    "seq_len = 2\n",
    "hidden_size = 768\n",
    "x = torch.rand(batch_size, seq_len, hidden_size)\n",
    "y = bert_pooler(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This section analyzes and studies the implementation of the Bert model. I hope that readers can have a more detailed understanding of the Bert implementation.\n",
    "\n",
    "It is worth noting that in the Bert model implemented by HuggingFace, a variety of memory-saving technologies are used:\n",
    "\n",
    "- Gradient checkpoint, which does not retain the forward propagation nodes and only calculates them when needed; apply_chunking_to_forward, which calculates the FFN part according to multiple small batches and low dimensions\n",
    "- BertModel contains complex encapsulation and many components. Taking bert-base as an example, the main components are as follows:\n",
    "- A total of Dropout appears 1+(1+1+1)x12=37 times;\n",
    "- A total of LayerNorm appears 1+(1+1)x12=25 times;\n",
    "BertModel has a very large number of parameters. Taking bert-base as an example, its parameter volume is 109M.\n",
    "\n",
    "## Acknowledgements\n",
    "This article was mainly written by Li Luoqiu of Zhejiang University, and the students of this project were responsible for sorting and summarizing."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bfce0b4c492a35815b5705a19fe374a7eea0baaa08b34d90450caf1fe9ce20b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
