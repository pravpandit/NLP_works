{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Sentence Sliding Window Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "âœ… In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key = utils.get_openai_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"data/äººå·¥æ™ºèƒ½.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "7 \n",
      "\n",
      "<class 'llama_index.schema.Document'>\n",
      "Doc ID: b03a0e50-2e8a-49bf-82f0-4a3909364809\n",
      "Text: 2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ç»´åŸºç™¾ç§‘ï¼Œâ¾ƒç”±çš„ç™¾ç§‘å…¨ä¹¦\n",
      "https://zh.wikipedia.org/wiki/ â¼ˆâ¼¯æ™ºèƒ½ 2/13â€œâ¼ˆâ¼¯æ™ºèƒ½â€çš„å„åœ°å¸¸â½¤åç§° ä¸­å›½â¼¤é™†â¼ˆâ¼¯æ™ºèƒ½ å°æ¹¾â¼ˆâ¼¯æ™ºæ…§\n",
      "æ¸¯æ¾³â¼ˆâ¼¯æ™ºèƒ½ æ–°â»¢â¼ˆâ¼¯æ™ºèƒ½ã€â¼ˆâ¼¯æ™ºæ…§ â½‡éŸ©â¼ˆâ¼¯çŸ¥èƒ½ è¶Šå—æ™ºæ…§â¼ˆé€  [å±•å¼€] [å±•å¼€] [å±•å¼€] [å±•å¼€] [å±•å¼€] [å±•å¼€]â¼ˆâ¼¯æ™ºèƒ½ç³»åˆ—å†…å®¹\n",
      "ä¸»è¦â½¬æ ‡ å®ç°â½…å¼ â¼ˆâ¼¯æ™ºèƒ½å“²å­¦ å†å² æŠ€æœ¯ æœ¯è¯­â¼ˆâ¼¯æ™ºèƒ½ï¼ˆè‹±è¯­ï¼šartiï¬cial intelligence ï¼Œç¼©å†™ä¸º\n",
      "AIï¼‰äº¦ç§°æœºå™¨æ™ºèƒ½ï¼ŒæŒ‡ç”±â¼ˆåˆ¶é€ å‡ºæ¥çš„æœºå™¨æ‰€è¡¨ç°å‡ºæ¥çš„æ™ºèƒ½ã€‚é€šå¸¸â¼ˆâ¼¯\n",
      "æ™ºèƒ½æ˜¯æŒ‡â½¤æ™®é€šè®¡ç®—æœºç¨‹åºæ¥å‘ˆç°â¼ˆç±»æ™ºèƒ½çš„æŠ€æœ¯ã€‚è¯¥è¯ä¹ŸæŒ‡å‡ºç ”ç©¶è¿™æ ·çš„æ™ºèƒ½ç³»ç»Ÿæ˜¯å¦èƒ½å¤Ÿå®ç°ï¼Œä»¥åŠå¦‚ä½•å®ç°ã€‚åŒ æ—¶ï¼Œé€šè¿‡ åŒ»å­¦ ã€ç¥ç»ç§‘å­¦\n",
      "ã€æœºå™¨â¼ˆå­¦ åŠ...\n"
     ]
    }
   ],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents_en = SimpleDirectoryReader(\n",
    "    input_files=[\"data/eBook-How-to-Build-a-Career-in-AI.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      "\n",
      "41 \n",
      "\n",
      "<class 'llama_index.schema.Document'>\n",
      "Doc ID: 5ab262c9-a207-4f2d-9513-fc4d5c350cf5\n",
      "Text: PAGE 1Founder, DeepLearning.AICollected Insights from Andrew Ng\n",
      "How to  Build Your Career in AIA Simple Guide\n"
     ]
    }
   ],
   "source": [
    "print(type(documents_en), \"\\n\")\n",
    "print(len(documents_en), \"\\n\")\n",
    "print(type(documents_en[0]))\n",
    "print(documents_en[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the text of each document in documents is concatenated into a string, and then a Document instance is created, which represents the entire document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
    "document_en = Document(text=\"\\n\\n\".join([doc.text for doc in documents_en]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Chinese punctuation marks with English punctuation marks for easy subsequent processing\n",
    "# If it is an English document, you can skip this step\n",
    "# If not processed, it will lead to the inability to correctly segment Chinese sentences, which will affect the size of the subsequent sentence_window and cause the input length to exceed the maximum limit of gpt-3.5-turbo\n",
    "document.text=document.text.replace('ã€‚','. ')\n",
    "document.text=document.text.replace('ï¼','! ')\n",
    "document.text=document.text.replace('ï¼Ÿ','? ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentence sliding window search settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parser object named node_parser is created, the window size is specified as 3, and the original text metadata key is set to ``original_text``. The parser created in this way can be used to extract nodes from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# create the sentence window node parser w/ default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Chinese text string\n",
    "Use the get_nodes_from_documents method of node_parser to extract nodes from the provided text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ä½ å¥½. ä½ æ€ä¹ˆæ ·? æˆ‘å¾ˆå¥½!  \"\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([Document(text=text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = \"hello. how are you? I am fine!  \"\n",
    "\n",
    "nodes_en = node_parser.get_nodes_from_documents([Document(text=text_en)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each individual word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ä½ å¥½. ', 'ä½ æ€ä¹ˆæ ·? ', 'æˆ‘å¾ˆå¥½!  ']\n",
      "['hello. ', 'how are you? ', 'I am fine!  ']\n"
     ]
    }
   ],
   "source": [
    "print([x.text for x in nodes])\n",
    "print([x.text for x in nodes_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½.  ä½ æ€ä¹ˆæ ·?  æˆ‘å¾ˆå¥½!  \n",
      "hello.  how are you?  I am fine!  \n"
     ]
    }
   ],
   "source": [
    "print(nodes[1].metadata[\"window\"])\n",
    "print(nodes_en[1].metadata[\"window\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ä½ å¥½. å§å°. çŒ«ç‹—. è€é¼ \"\n",
    "text_en2 = 'hello. bar. cat. dog. mouse.'\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([Document(text=text)])\n",
    "nodes_en2 = node_parser.get_nodes_from_documents([Document(text=text_en2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ä½ å¥½. ', 'å§å°. ', 'çŒ«ç‹—. ', 'è€é¼ ']\n",
      "['hello. ', 'how are you? ', 'I am fine!  ']\n"
     ]
    }
   ],
   "source": [
    "print([x.text for x in nodes])\n",
    "print([x.text for x in nodes_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½.  å§å°.  çŒ«ç‹—. \n",
      "hello.  bar.  cat. \n"
     ]
    }
   ],
   "source": [
    "print(nodes[0].metadata[\"window\"])\n",
    "print(nodes_en2[0].metadata[\"window\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating an Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of a language model is created using the `GPT-3.5-turbo` model from `OpenAI`, with the temperature parameter set to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `ServiceContext` object is created using the `ServiceContext.from_defaults` method, which contains service-related context information for index construction, including language model, embedding model, and node parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "sentence_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=\"local:BAAI/bge-small-zh-v1.5\",\n",
    "    node_parser=node_parser,\n",
    ")\n",
    "\n",
    "sentence_context_en = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    node_parser=node_parser,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `VectorStoreIndex` object is created using the `VectorStoreIndex.from_documents` method, which is used to store and retrieve vector information related to documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "sentence_index = VectorStoreIndex.from_documents(\n",
    "    [document], service_context=sentence_context\n",
    ")\n",
    "\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "sentence_index_en = VectorStoreIndex.from_documents(\n",
    "    [document_en], service_context=sentence_context_en\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist the created index to the specified directory (\"./sentence_index\") . This allows the index to be reloaded in subsequent runs without having to rebuild it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")\n",
    "sentence_index_en.storage_context.persist(persist_dir=\"./sentence_index_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks if the index file exists and rebuilds it if it does not. If it does exist, it will use the `load_index_from_storage` method to load the index from the existing index file instead of rebuilding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code is optional to check\n",
    "# if an index file exists, then it will load it\n",
    "# if not, it will rebuild it\n",
    "\n",
    "import os\n",
    "from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "if not os.path.exists(\"./sentence_index\"):\n",
    "    sentence_index = VectorStoreIndex.from_documents(\n",
    "        [document], service_context=sentence_context\n",
    "    )\n",
    "\n",
    "    sentence_index.storage_context.persist(persist_dir=\"./sentence_index\")\n",
    "else:\n",
    "    sentence_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./sentence_index\"),\n",
    "        service_context=sentence_context\n",
    "    )\n",
    "\n",
    "if not os.path.exists(\"./sentence_index_en\"):\n",
    "    sentence_index_en = VectorStoreIndex.from_documents(\n",
    "        [document_en], service_context=sentence_context_en\n",
    "    )\n",
    "\n",
    "    sentence_index_en.storage_context.persist(persist_dir=\"./sentence_index_en\")\n",
    "else:\n",
    "    sentence_index_en = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./sentence_index_en\"),\n",
    "        service_context=sentence_context_en\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Post-creation processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A post-processor instance is created using the `MetadataReplacementPostProcessor` class, and the target metadata key is set to `window`. The function of this post-processor is to replace the content of the target metadata key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "postproc = MetadataReplacementPostProcessor(\n",
    "    target_metadata_key=\"window\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `NodeWithScore` class to associate a score with each node in the original node list to form a node list with scores. \n",
    "Use the `deepcopy` function to create a deep copy of the original node list for subsequent comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import NodeWithScore\n",
    "from copy import deepcopy\n",
    "\n",
    "scored_nodes = [NodeWithScore(node=x, score=1.0) for x in nodes]\n",
    "nodes_old = [deepcopy(n) for n in nodes]\n",
    "\n",
    "scored_nodes_en = [NodeWithScore(node=x, score=1.0) for x in nodes_en2]\n",
    "nodes_old_en = [deepcopy(n) for n in nodes_en2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å§å°. \n",
      "bar. \n"
     ]
    }
   ],
   "source": [
    "print(nodes_old[1].text)\n",
    "print(nodes_old_en[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `postprocess_nodes` method of the postprocessor, the contents of the target metadata key in the list of nodes with scores are replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_nodes = postproc.postprocess_nodes(scored_nodes)\n",
    "replaced_nodes_en = postproc.postprocess_nodes(scored_nodes_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½.  å§å°.  çŒ«ç‹—.  è€é¼ \n",
      "hello.  bar.  cat.  dog. \n"
     ]
    }
   ],
   "source": [
    "print(replaced_nodes[1].text)\n",
    "print(replaced_nodes_en[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Adding a reordering block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A post-processor instance is created using the `SentenceTransformerRerank` class, the parameter `top_n` is set to 2, and the model used is \"BAAI/bge-reranker-base\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# BAAI/bge-reranker-base\n",
    "# link: https://huggingface.co/BAAI/bge-reranker-base\n",
    "rerank = SentenceTransformerRerank(\n",
    "    top_n=2, model=\"BAAI/bge-reranker-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `QueryBundle` object is created containing the query text \"I want a dog.\". \n",
    "A list of two nodes with scores is created, representing text nodes containing the text \"This is a cat\" and \"This is a dog\", with scores of 0.6 and 0.4 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "from llama_index.schema import TextNode, NodeWithScore\n",
    "\n",
    "query = QueryBundle(\"æˆ‘æƒ³è¦åªç‹—.\")\n",
    "\n",
    "scored_nodes = [\n",
    "    NodeWithScore(node=TextNode(text=\"è¿™æ˜¯åªçŒ«\"), score=0.6),\n",
    "    NodeWithScore(node=TextNode(text=\"è¿™æ˜¯åªç‹—\"), score=0.4),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "from llama_index.schema import TextNode, NodeWithScore\n",
    "\n",
    "query_en = QueryBundle(\"I want a dog.\")\n",
    "\n",
    "scored_nodes_en = [\n",
    "    NodeWithScore(node=TextNode(text=\"This is a cat\"), score=0.6),\n",
    "    NodeWithScore(node=TextNode(text=\"This is a dog\"), score=0.4),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `postprocess_nodes` method of the `SentenceTransformerRerank` class, rerank the list of nodes with scores, taking into account the query text. The reranked nodes will be based on the pre-trained sentence transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_nodes = rerank.postprocess_nodes(\n",
    "    scored_nodes, query_bundle=query\n",
    ")\n",
    "\n",
    "reranked_nodes_en = rerank.postprocess_nodes(\n",
    "    scored_nodes_en, query_bundle=query_en\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text and scores in the re-ranked node list are output. This shows the effect of the sentence transformation model on node re-ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('è¿™æ˜¯åªç‹—', 0.9660425), ('è¿™æ˜¯åªçŒ«', 0.06396222)]\n",
      "[('This is a dog', 0.9182736), ('This is a cat', 0.0014040753)]\n"
     ]
    }
   ],
   "source": [
    "print([(x.text, x.score) for x in reranked_nodes])\n",
    "print([(x.text, x.score) for x in reranked_nodes_en])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Running the Index Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `as_query_engine` method to convert `sentence_index` to a query engine object `sentence_window_engine`. \n",
    "Here, the `top k` of `similarity` is set to 6, and the `node_postprocessors` parameter is passed in, which contains the `postproc` and `rerank` postprocessors created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=6, node_postprocessors=[postproc, rerank]\n",
    ")\n",
    "\n",
    "sentence_window_engine_en = sentence_index_en.as_query_engine(\n",
    "    similarity_top_k=6, node_postprocessors=[postproc, rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query engine's `query` method executes a query, \"What is the key to success in the field of artificial intelligence?\" The query engine will use the previously set post-processor to post-process the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_response = sentence_window_engine.query(\n",
    "    \"åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå»ºåŠŸç«‹ä¸šçš„å…³é”®æ˜¯ä»€ä¹ˆ?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_response_en = sentence_window_engine_en.query(\n",
    "    \"What are the keys to building a career in AI?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `display_response` function provided by the `LLAMA` framework displays the response results of the query. This usually includes a set of nodes that match the query, as well as their text, scores, and other information. \n",
    "This way, the results of the query can be better visualized and understood in the `Notebook` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå»ºåŠŸç«‹ä¸šçš„å…³é”®æ˜¯ç³»ç»Ÿèƒ½å¤Ÿæ­£ç¡®è§£é‡Šå¤–éƒ¨æ•°æ®ï¼Œä»ä¸­å­¦ä¹ ï¼Œå¹¶åˆ©ç”¨è¿™äº›çŸ¥è¯†é€šè¿‡çµæ´»é€‚åº”å®ç°ç‰¹å®šç›®æ ‡å’Œä»»åŠ¡çš„èƒ½åŠ›ã€‚"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "display_response(window_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Learning foundational technical skills, working on projects, finding a job, and being part of a supportive community are the keys to building a career in AI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "display_response(window_response_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge the above operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`documents`: list of documents to build index for. \n",
    "`llm`: OpenAI language model instance. \n",
    "`embed_model`: name or path to embedding model. \n",
    "`sentence_window_size`: size of sentence window. \n",
    "`save_dir`: directory to persist index. \n",
    "\n",
    "Create a node_parser for sentence window. \n",
    "Create a ServiceContext with context information such as language model and node_parser. \n",
    "If index does not exist in the specified directory, create a VectorStoreIndex based on the provided documents and persist it to the specified directory. \n",
    "If index file already exists in the directory, load index from file. \n",
    "Return the constructed sentence window index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-zh-v1.5\",\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index\",\n",
    "):\n",
    "# create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            documents, service_context=sentence_context\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "def build_sentence_window_index_en(\n",
    "    documents_en,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index_en\",\n",
    "):\n",
    "# create the sentence window node parser w/ default settings\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        sentence_index_en = VectorStoreIndex.from_documents(\n",
    "            documents_en, service_context=sentence_context\n",
    "        )\n",
    "        sentence_index_en.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        sentence_index_en = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir),\n",
    "            service_context=sentence_context,\n",
    "        )\n",
    "\n",
    "    return sentence_index_en\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sentence_index`: constructed sentence window index. \n",
    "`similarity_top_k`: top k for similarity query. \n",
    "`rerank_top_n`: reranked top n. \n",
    "\n",
    "Defines two postprocessors: `postproc` for replacing metadata keys, `rerank` for reranking nodes using sentence transformation model. \n",
    "Creates a query engine `sentence_window_engine`, converts sentence window index to query engine, and uses defined postprocessors. \n",
    "Returns the constructed query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_window_query_engine(\n",
    "    sentence_index, similarity_top_k=6, rerank_top_n=2\n",
    "):\n",
    "# define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine\n",
    "\n",
    "def get_sentence_window_query_engine_en(\n",
    "    sentence_index_en, similarity_top_k=6, rerank_top_n=2\n",
    "):\n",
    "# define postprocessors\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine_en = sentence_index_en.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the previously defined `build_sentence_window_index` function, passing in the document list, language model instance, and save directory to build the sentence window index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "index = build_sentence_window_index(\n",
    "    [document],\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    save_dir=\"./sentence_index\",\n",
    ")\n",
    "\n",
    "index_en = build_sentence_window_index_en(\n",
    "    [document_en],\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    save_dir=\"./sentence_index_en\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the previously defined `get_sentence_window_query_engine` function, passing in the constructed sentence window index and similarity `top k` to get the query engine for the sentence window. \n",
    "Here, `similarity_top_k` is set to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_sentence_window_query_engine(index, similarity_top_k=6)\n",
    "query_engine_en = get_sentence_window_query_engine(index_en, similarity_top_k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TruLens Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the generated questions from the file named 'generated_questions.text' and store them in the `eval_questions` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open('data/generated_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "# Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)\n",
    "\n",
    "eval_questions_en = []\n",
    "with open('data/generated_questions_en.txt', 'r') as file:\n",
    "    for line in file:\n",
    "# Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function `run_evals` is defined that accepts the generated list of questions, the `TruLens` recorder, and the query engine as parameters. For each question, a recording is started using the `TruLens` recorder, and then the query is executed using the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "def run_evals(eval_questions, tru_recorder, query_engine):\n",
    "    for question in eval_questions:\n",
    "        with tru_recorder as recording:\n",
    "            response = query_engine.query(question)\n",
    "\n",
    "\n",
    "def run_evals_en(eval_questions_en, tru_recorder, query_engine):\n",
    "    for question in eval_questions_en:\n",
    "        with tru_recorder as recording:\n",
    "            response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the `TruLens` database using the `reset_database` method of the Tru class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ğŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from utils import get_prebuilt_trulens_recorder\n",
    "\n",
    "from trulens_eval import Tru\n",
    "\n",
    "tru = Tru()\n",
    "\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set the sliding window size to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the previously defined functions `build_sentence_window_index` and `get_sentence_window_query_engine` to build the sentence window index and query engine respectively. Here, the window size is set to 1, and the save directory is specified as \"sentence_index_1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index_1 = build_sentence_window_index(\n",
    "    documents,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-small-zh-v1.5\",  # \"local:BAAI/bge-small-en-v1.5\" for english\n",
    "    sentence_window_size=1,\n",
    "    save_dir=\"sentence_index_1\",\n",
    ")\n",
    "sentence_window_engine_1 = get_sentence_window_query_engine(\n",
    "    sentence_index_1\n",
    ")\n",
    "tru_recorder_1 = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_1,\n",
    "    app_id='sentence window engine 1'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index_1_en = build_sentence_window_index_en(\n",
    "    documents_en,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",  # \"local:BAAI/bge-small-en-v1.5\" for english\n",
    "    sentence_window_size=1,\n",
    "    save_dir=\"sentence_index_1_en\",\n",
    ")\n",
    "sentence_window_engine_1_en = get_sentence_window_query_engine(\n",
    "    sentence_index_1_en\n",
    ")\n",
    "tru_recorder_1_en = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_1_en,\n",
    "    app_id='sentence window engine 1_en'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the previously defined evaluation function `run_evals`, pass in the generated question list, `TruLens` recorder `tru_recorder_1` and the constructed query engine `sentence_window_engine_1`, and run the evaluation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-me3Y2JVoMQFvYW4UUurcFXXM on tokens per min (TPM): Limit 60000, Used 58250, Requested 1999. Please try again in 249ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-me3Y2JVoMQFvYW4UUurcFXXM on tokens per min (TPM): Limit 60000, Used 59832, Requested 502. Please try again in 334ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-me3Y2JVoMQFvYW4UUurcFXXM on tokens per min (TPM): Limit 60000, Used 58502, Requested 1922. Please try again in 424ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-me3Y2JVoMQFvYW4UUurcFXXM on tokens per min (TPM): Limit 60000, Used 59610, Requested 1723. Please try again in 1.333s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n"
     ]
    }
   ],
   "source": [
    "run_evals(eval_questions, tru_recorder_1, sentence_window_engine_1)\n",
    "run_evals(eval_questions_en, tru_recorder_1, sentence_window_engine_1_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder_1 = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_1,\n",
    "    app_id='sentence window engine 1'\n",
    ")\n",
    "\n",
    "tru_recorder_1_en = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_1_en,\n",
    "    app_id='sentence window engine 1_en'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_json</th>\n",
       "      <th>type</th>\n",
       "      <th>record_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tags</th>\n",
       "      <th>record_json</th>\n",
       "      <th>cost_json</th>\n",
       "      <th>perf_json</th>\n",
       "      <th>ts</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Answer Relevance_calls</th>\n",
       "      <th>Context Relevance_calls</th>\n",
       "      <th>Groundedness_calls</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence window engine 1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_87b8d0d554e7d74fa19c16f4692d69cf</td>\n",
       "      <td>\"\\u4eba\\u5de5\\u667a\\u80fd\\u4e2d\\u7684\\u5148\\u9...</td>\n",
       "      <td>\"\\u5148\\u9a8c\\u77e5\\u8bc6\\u5728\\u4eba\\u5de5\\u6...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_87b8d0d554e7d74fa19...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2024-03-12T10:30:49.113462\", \"...</td>\n",
       "      <td>2024-03-12T10:31:02.269094</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½ä¸­çš„å…ˆéªŒçŸ¥è¯†æ˜¯å¦‚ä½•è¢«å­˜å‚¨çš„ï¼Ÿ', 're...</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½ä¸­çš„å…ˆéªŒçŸ¥è¯†æ˜¯å¦‚ä½•è¢«å­˜å‚¨çš„ï¼Ÿ', 're...</td>\n",
       "      <td>[{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence window engine 1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_b9425d9aa02130eec6c73f7cc6f700f8</td>\n",
       "      <td>\"\\u4eba\\u5de5\\u667a\\u80fd\\u7684\\u81ea\\u6211\\u6...</td>\n",
       "      <td>\"The self-updating and self-improving capabili...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_b9425d9aa02130eec6c...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2024-03-12T10:31:02.914647\", \"...</td>\n",
       "      <td>2024-03-12T10:31:09.293573</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½çš„è‡ªæˆ‘æ›´æ–°å’Œè‡ªæˆ‘æå‡æ˜¯å¦å¯èƒ½å¯¼è‡´å…¶è„±ç¦»äºº...</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½çš„è‡ªæˆ‘æ›´æ–°å’Œè‡ªæˆ‘æå‡æ˜¯å¦å¯èƒ½å¯¼è‡´å…¶è„±ç¦»äºº...</td>\n",
       "      <td>[{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence window engine 1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_8c266922b3864f14c5aa3fd4fc98923c</td>\n",
       "      <td>\"\\u7ba1\\u7406\\u8005\\u5982\\u4f55\\u7ba1\\u7406AI\\...</td>\n",
       "      <td>\"Management should consider adjusting their wo...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_8c266922b3864f14c5a...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2024-03-12T10:31:09.505494\", \"...</td>\n",
       "      <td>2024-03-12T10:31:12.333074</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[{'args': {'prompt': 'ç®¡ç†è€…å¦‚ä½•ç®¡ç†AIï¼Ÿ', 'response':...</td>\n",
       "      <td>[{'args': {'prompt': 'ç®¡ç†è€…å¦‚ä½•ç®¡ç†AIï¼Ÿ', 'response':...</td>\n",
       "      <td>[{'args': {'source': 'ä»»ä½•çš„ç§‘æŠ€éƒ½ä¼šæœ‰ç“¶é¢ˆï¼Œ æ‘©å°”å®šå¾‹ åˆ°â½¬å‰ä¹Ÿé‡åˆ°ç›¸...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence window engine 1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_48957156666710cd55d5059c9ed69b56</td>\n",
       "      <td>\"\\u5f3a\\u4eba\\u5de5\\u667a\\u80fd\\u662f\\u4ec0\\u4...</td>\n",
       "      <td>\"\\u5f3a\\u4eba\\u5de5\\u667a\\u80fd\\u662f\\u4e00\\u7...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_48957156666710cd55d...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2024-03-12T10:31:12.872050\", \"...</td>\n",
       "      <td>2024-03-12T10:31:21.471832</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[{'args': {'prompt': 'å¼ºäººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ', 'response': ...</td>\n",
       "      <td>[{'args': {'prompt': 'å¼ºäººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ', 'response': ...</td>\n",
       "      <td>[{'args': {'source': 'The Behavioral and Brain...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence window engine 1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_c3d563072ae3ef443e6e102c53e1677e</td>\n",
       "      <td>\"\\u4eba\\u5de5\\u667a\\u80fd\\u88ab\\u6ee5\\u7528\\u5...</td>\n",
       "      <td>\"The misuse of artificial intelligence can lea...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_c3d563072ae3ef443e6...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2024-03-12T10:31:21.670386\", \"...</td>\n",
       "      <td>2024-03-12T10:31:28.646002</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½è¢«æ»¥ç”¨å¸¦æ¥çš„å±å®³ï¼Ÿ', 'respons...</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½è¢«æ»¥ç”¨å¸¦æ¥çš„å±å®³ï¼Ÿ', 'respons...</td>\n",
       "      <td>[{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     app_id  \\\n",
       "0  sentence window engine 1   \n",
       "1  sentence window engine 1   \n",
       "2  sentence window engine 1   \n",
       "3  sentence window engine 1   \n",
       "4  sentence window engine 1   \n",
       "\n",
       "                                            app_json  \\\n",
       "0  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "1  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "2  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "3  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "4  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "\n",
       "                                                type  \\\n",
       "0  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "1  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "2  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "3  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "4  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "\n",
       "                                      record_id  \\\n",
       "0  record_hash_87b8d0d554e7d74fa19c16f4692d69cf   \n",
       "1  record_hash_b9425d9aa02130eec6c73f7cc6f700f8   \n",
       "2  record_hash_8c266922b3864f14c5aa3fd4fc98923c   \n",
       "3  record_hash_48957156666710cd55d5059c9ed69b56   \n",
       "4  record_hash_c3d563072ae3ef443e6e102c53e1677e   \n",
       "\n",
       "                                               input  \\\n",
       "0  \"\\u4eba\\u5de5\\u667a\\u80fd\\u4e2d\\u7684\\u5148\\u9...   \n",
       "1  \"\\u4eba\\u5de5\\u667a\\u80fd\\u7684\\u81ea\\u6211\\u6...   \n",
       "2  \"\\u7ba1\\u7406\\u8005\\u5982\\u4f55\\u7ba1\\u7406AI\\...   \n",
       "3  \"\\u5f3a\\u4eba\\u5de5\\u667a\\u80fd\\u662f\\u4ec0\\u4...   \n",
       "4  \"\\u4eba\\u5de5\\u667a\\u80fd\\u88ab\\u6ee5\\u7528\\u5...   \n",
       "\n",
       "                                              output tags  \\\n",
       "0  \"\\u5148\\u9a8c\\u77e5\\u8bc6\\u5728\\u4eba\\u5de5\\u6...    -   \n",
       "1  \"The self-updating and self-improving capabili...    -   \n",
       "2  \"Management should consider adjusting their wo...    -   \n",
       "3  \"\\u5f3a\\u4eba\\u5de5\\u667a\\u80fd\\u662f\\u4e00\\u7...    -   \n",
       "4  \"The misuse of artificial intelligence can lea...    -   \n",
       "\n",
       "                                         record_json  \\\n",
       "0  {\"record_id\": \"record_hash_87b8d0d554e7d74fa19...   \n",
       "1  {\"record_id\": \"record_hash_b9425d9aa02130eec6c...   \n",
       "2  {\"record_id\": \"record_hash_8c266922b3864f14c5a...   \n",
       "3  {\"record_id\": \"record_hash_48957156666710cd55d...   \n",
       "4  {\"record_id\": \"record_hash_c3d563072ae3ef443e6...   \n",
       "\n",
       "                                           cost_json  \\\n",
       "0  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "1  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "2  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "3  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "4  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "\n",
       "                                           perf_json  \\\n",
       "0  {\"start_time\": \"2024-03-12T10:30:49.113462\", \"...   \n",
       "1  {\"start_time\": \"2024-03-12T10:31:02.914647\", \"...   \n",
       "2  {\"start_time\": \"2024-03-12T10:31:09.505494\", \"...   \n",
       "3  {\"start_time\": \"2024-03-12T10:31:12.872050\", \"...   \n",
       "4  {\"start_time\": \"2024-03-12T10:31:21.670386\", \"...   \n",
       "\n",
       "                           ts  Answer Relevance  Context Relevance  \\\n",
       "0  2024-03-12T10:31:02.269094               0.9               0.40   \n",
       "1  2024-03-12T10:31:09.293573               1.0               0.60   \n",
       "2  2024-03-12T10:31:12.333074               0.8               0.25   \n",
       "3  2024-03-12T10:31:21.471832               0.8               0.80   \n",
       "4  2024-03-12T10:31:28.646002               0.9               0.50   \n",
       "\n",
       "   Groundedness                             Answer Relevance_calls  \\\n",
       "0      1.000000  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½ä¸­çš„å…ˆéªŒçŸ¥è¯†æ˜¯å¦‚ä½•è¢«å­˜å‚¨çš„ï¼Ÿ', 're...   \n",
       "1      0.800000  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½çš„è‡ªæˆ‘æ›´æ–°å’Œè‡ªæˆ‘æå‡æ˜¯å¦å¯èƒ½å¯¼è‡´å…¶è„±ç¦»äºº...   \n",
       "2      1.000000  [{'args': {'prompt': 'ç®¡ç†è€…å¦‚ä½•ç®¡ç†AIï¼Ÿ', 'response':...   \n",
       "3      0.333333  [{'args': {'prompt': 'å¼ºäººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ', 'response': ...   \n",
       "4      1.000000  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½è¢«æ»¥ç”¨å¸¦æ¥çš„å±å®³ï¼Ÿ', 'respons...   \n",
       "\n",
       "                             Context Relevance_calls  \\\n",
       "0  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½ä¸­çš„å…ˆéªŒçŸ¥è¯†æ˜¯å¦‚ä½•è¢«å­˜å‚¨çš„ï¼Ÿ', 're...   \n",
       "1  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½çš„è‡ªæˆ‘æ›´æ–°å’Œè‡ªæˆ‘æå‡æ˜¯å¦å¯èƒ½å¯¼è‡´å…¶è„±ç¦»äºº...   \n",
       "2  [{'args': {'prompt': 'ç®¡ç†è€…å¦‚ä½•ç®¡ç†AIï¼Ÿ', 'response':...   \n",
       "3  [{'args': {'prompt': 'å¼ºäººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ', 'response': ...   \n",
       "4  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½è¢«æ»¥ç”¨å¸¦æ¥çš„å±å®³ï¼Ÿ', 'respons...   \n",
       "\n",
       "                                  Groundedness_calls  latency  total_tokens  \\\n",
       "0  [{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...       13             0   \n",
       "1  [{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...        6             0   \n",
       "2  [{'args': {'source': 'ä»»ä½•çš„ç§‘æŠ€éƒ½ä¼šæœ‰ç“¶é¢ˆï¼Œ æ‘©å°”å®šå¾‹ åˆ°â½¬å‰ä¹Ÿé‡åˆ°ç›¸...        2             0   \n",
       "3  [{'args': {'source': 'The Behavioral and Brain...        8             0   \n",
       "4  [{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...        6             0   \n",
       "\n",
       "   total_cost  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Set the sliding window size to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously defined function is called to build the sentence window index, query engine, and `TruLens` recorder. The window size is set to 3, and the save directory is specified as \"sentence_index_3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index_3 = build_sentence_window_index(\n",
    "    documents,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-small-zh-v1.5\",  # \"local:BAAI/bge-small-en-v1.5\" for english\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index_3\",\n",
    ")\n",
    "sentence_window_engine_3 = get_sentence_window_query_engine(\n",
    "    sentence_index_3\n",
    ")\n",
    "\n",
    "tru_recorder_3 = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_3,\n",
    "    app_id='sentence window engine 3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index_3_en = build_sentence_window_index_en(\n",
    "    documents_en,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",  # \"local:BAAI/bge-small-en-v1.5\" for english\n",
    "    sentence_window_size=3,\n",
    "    save_dir=\"sentence_index_3_en\",\n",
    ")\n",
    "sentence_window_engine_3_en = get_sentence_window_query_engine(\n",
    "    sentence_index_3_en\n",
    ")\n",
    "\n",
    "tru_recorder_3_en = get_prebuilt_trulens_recorder(\n",
    "    sentence_window_engine_3_en,\n",
    "    app_id='sentence window engine 3_en'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `run_evals` function, passing in the generated question list `eval_questions`, the `TruLens` recorder `tru_recorder_3`, and the constructed query engine `sentence_window_engine_3` to run the evaluation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evals(eval_questions, tru_recorder_3, sentence_window_engine_3)\n",
    "run_evals(eval_questions_en, tru_recorder_3_en, sentence_window_engine_3_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_json</th>\n",
       "      <th>type</th>\n",
       "      <th>record_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tags</th>\n",
       "      <th>record_json</th>\n",
       "      <th>cost_json</th>\n",
       "      <th>perf_json</th>\n",
       "      <th>ts</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Answer Relevance_calls</th>\n",
       "      <th>Context Relevance_calls</th>\n",
       "      <th>Groundedness_calls</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence window engine 1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_e9815da1c66c0943f4d155a06f94e9c4</td>\n",
       "      <td>\"\\u4eba\\u5de5\\u667a\\u80fd\\u4e2d\\u7684\\u5148\\u9...</td>\n",
       "      <td>\"\\u5148\\u9a8c\\u77e5\\u8bc6\\u5728\\u4eba\\u5de5\\u6...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_e9815da1c66c0943f4d...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2024-03-10T22:55:51.162029\", \"...</td>\n",
       "      <td>2024-03-10T22:56:03.820730</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½ä¸­çš„å…ˆéªŒçŸ¥è¯†æ˜¯å¦‚ä½•è¢«å­˜å‚¨çš„ï¼Ÿ', 're...</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½ä¸­çš„å…ˆéªŒçŸ¥è¯†æ˜¯å¦‚ä½•è¢«å­˜å‚¨çš„ï¼Ÿ', 're...</td>\n",
       "      <td>[{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence window engine 1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_140932cb020d95e0554ecf0489eb42f2</td>\n",
       "      <td>\"\\u4eba\\u5de5\\u667a\\u80fd\\u7684\\u81ea\\u6211\\u6...</td>\n",
       "      <td>\"The self-updating and self-improving capabili...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_140932cb020d95e0554...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2024-03-10T22:56:04.085254\", \"...</td>\n",
       "      <td>2024-03-10T22:56:10.348414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½çš„è‡ªæˆ‘æ›´æ–°å’Œè‡ªæˆ‘æå‡æ˜¯å¦å¯èƒ½å¯¼è‡´å…¶è„±ç¦»äºº...</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½çš„è‡ªæˆ‘æ›´æ–°å’Œè‡ªæˆ‘æå‡æ˜¯å¦å¯èƒ½å¯¼è‡´å…¶è„±ç¦»äºº...</td>\n",
       "      <td>[{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence window engine 1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_5bd9fa7d1b1997c136b9d1d4ce3d2684</td>\n",
       "      <td>\"\\u7ba1\\u7406\\u8005\\u5982\\u4f55\\u7ba1\\u7406AI\\...</td>\n",
       "      <td>\"Management should consider adjusting their wo...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_5bd9fa7d1b1997c136b...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2024-03-10T22:56:10.552787\", \"...</td>\n",
       "      <td>2024-03-10T22:56:13.694339</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>[{'args': {'prompt': 'ç®¡ç†è€…å¦‚ä½•ç®¡ç†AIï¼Ÿ', 'response':...</td>\n",
       "      <td>[{'args': {'prompt': 'ç®¡ç†è€…å¦‚ä½•ç®¡ç†AIï¼Ÿ', 'response':...</td>\n",
       "      <td>[{'args': {'source': 'ä»»ä½•çš„ç§‘æŠ€éƒ½ä¼šæœ‰ç“¶é¢ˆï¼Œ æ‘©å°”å®šå¾‹ åˆ°â½¬å‰ä¹Ÿé‡åˆ°ç›¸...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence window engine 1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_cd9a2aa2bdf60288d1b04cdbeac630f3</td>\n",
       "      <td>\"\\u5f3a\\u4eba\\u5de5\\u667a\\u80fd\\u662f\\u4ec0\\u4...</td>\n",
       "      <td>\"\\u5f3a\\u4eba\\u5de5\\u667a\\u80fd\\u662f\\u4e00\\u7...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_cd9a2aa2bdf60288d1b...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2024-03-10T22:56:13.882046\", \"...</td>\n",
       "      <td>2024-03-10T22:56:21.948222</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'args': {'prompt': 'å¼ºäººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ', 'response': ...</td>\n",
       "      <td>[{'args': {'prompt': 'å¼ºäººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ', 'response': ...</td>\n",
       "      <td>[{'args': {'source': 'The Behavioral and Brain...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence window engine 1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_736205619e133e5333d36a19a29293fe</td>\n",
       "      <td>\"\\u4eba\\u5de5\\u667a\\u80fd\\u88ab\\u6ee5\\u7528\\u5...</td>\n",
       "      <td>\"The misuse of artificial intelligence can lea...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_736205619e133e5333d...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2024-03-10T22:56:22.140156\", \"...</td>\n",
       "      <td>2024-03-10T22:56:28.835570</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½è¢«æ»¥ç”¨å¸¦æ¥çš„å±å®³ï¼Ÿ', 'respons...</td>\n",
       "      <td>[{'args': {'prompt': 'äººå·¥æ™ºèƒ½è¢«æ»¥ç”¨å¸¦æ¥çš„å±å®³ï¼Ÿ', 'respons...</td>\n",
       "      <td>[{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     app_id  \\\n",
       "0  sentence window engine 1   \n",
       "1  sentence window engine 1   \n",
       "2  sentence window engine 1   \n",
       "3  sentence window engine 1   \n",
       "4  sentence window engine 1   \n",
       "\n",
       "                                            app_json  \\\n",
       "0  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "1  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "2  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "3  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "4  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "\n",
       "                                                type  \\\n",
       "0  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "1  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "2  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "3  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "4  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "\n",
       "                                      record_id  \\\n",
       "0  record_hash_e9815da1c66c0943f4d155a06f94e9c4   \n",
       "1  record_hash_140932cb020d95e0554ecf0489eb42f2   \n",
       "2  record_hash_5bd9fa7d1b1997c136b9d1d4ce3d2684   \n",
       "3  record_hash_cd9a2aa2bdf60288d1b04cdbeac630f3   \n",
       "4  record_hash_736205619e133e5333d36a19a29293fe   \n",
       "\n",
       "                                               input  \\\n",
       "0  \"\\u4eba\\u5de5\\u667a\\u80fd\\u4e2d\\u7684\\u5148\\u9...   \n",
       "1  \"\\u4eba\\u5de5\\u667a\\u80fd\\u7684\\u81ea\\u6211\\u6...   \n",
       "2  \"\\u7ba1\\u7406\\u8005\\u5982\\u4f55\\u7ba1\\u7406AI\\...   \n",
       "3  \"\\u5f3a\\u4eba\\u5de5\\u667a\\u80fd\\u662f\\u4ec0\\u4...   \n",
       "4  \"\\u4eba\\u5de5\\u667a\\u80fd\\u88ab\\u6ee5\\u7528\\u5...   \n",
       "\n",
       "                                              output tags  \\\n",
       "0  \"\\u5148\\u9a8c\\u77e5\\u8bc6\\u5728\\u4eba\\u5de5\\u6...    -   \n",
       "1  \"The self-updating and self-improving capabili...    -   \n",
       "2  \"Management should consider adjusting their wo...    -   \n",
       "3  \"\\u5f3a\\u4eba\\u5de5\\u667a\\u80fd\\u662f\\u4e00\\u7...    -   \n",
       "4  \"The misuse of artificial intelligence can lea...    -   \n",
       "\n",
       "                                         record_json  \\\n",
       "0  {\"record_id\": \"record_hash_e9815da1c66c0943f4d...   \n",
       "1  {\"record_id\": \"record_hash_140932cb020d95e0554...   \n",
       "2  {\"record_id\": \"record_hash_5bd9fa7d1b1997c136b...   \n",
       "3  {\"record_id\": \"record_hash_cd9a2aa2bdf60288d1b...   \n",
       "4  {\"record_id\": \"record_hash_736205619e133e5333d...   \n",
       "\n",
       "                                           cost_json  \\\n",
       "0  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "1  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "2  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "3  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "4  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "\n",
       "                                           perf_json  \\\n",
       "0  {\"start_time\": \"2024-03-10T22:55:51.162029\", \"...   \n",
       "1  {\"start_time\": \"2024-03-10T22:56:04.085254\", \"...   \n",
       "2  {\"start_time\": \"2024-03-10T22:56:10.552787\", \"...   \n",
       "3  {\"start_time\": \"2024-03-10T22:56:13.882046\", \"...   \n",
       "4  {\"start_time\": \"2024-03-10T22:56:22.140156\", \"...   \n",
       "\n",
       "                           ts  Answer Relevance  Context Relevance  \\\n",
       "0  2024-03-10T22:56:03.820730               0.9                0.4   \n",
       "1  2024-03-10T22:56:10.348414               1.0                0.5   \n",
       "2  2024-03-10T22:56:13.694339               0.8                0.3   \n",
       "3  2024-03-10T22:56:21.948222               0.9                0.7   \n",
       "4  2024-03-10T22:56:28.835570               0.9                0.3   \n",
       "\n",
       "   Groundedness                             Answer Relevance_calls  \\\n",
       "0           1.0  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½ä¸­çš„å…ˆéªŒçŸ¥è¯†æ˜¯å¦‚ä½•è¢«å­˜å‚¨çš„ï¼Ÿ', 're...   \n",
       "1           0.8  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½çš„è‡ªæˆ‘æ›´æ–°å’Œè‡ªæˆ‘æå‡æ˜¯å¦å¯èƒ½å¯¼è‡´å…¶è„±ç¦»äºº...   \n",
       "2           0.9  [{'args': {'prompt': 'ç®¡ç†è€…å¦‚ä½•ç®¡ç†AIï¼Ÿ', 'response':...   \n",
       "3           1.0  [{'args': {'prompt': 'å¼ºäººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ', 'response': ...   \n",
       "4           0.0  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½è¢«æ»¥ç”¨å¸¦æ¥çš„å±å®³ï¼Ÿ', 'respons...   \n",
       "\n",
       "                             Context Relevance_calls  \\\n",
       "0  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½ä¸­çš„å…ˆéªŒçŸ¥è¯†æ˜¯å¦‚ä½•è¢«å­˜å‚¨çš„ï¼Ÿ', 're...   \n",
       "1  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½çš„è‡ªæˆ‘æ›´æ–°å’Œè‡ªæˆ‘æå‡æ˜¯å¦å¯èƒ½å¯¼è‡´å…¶è„±ç¦»äºº...   \n",
       "2  [{'args': {'prompt': 'ç®¡ç†è€…å¦‚ä½•ç®¡ç†AIï¼Ÿ', 'response':...   \n",
       "3  [{'args': {'prompt': 'å¼ºäººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ', 'response': ...   \n",
       "4  [{'args': {'prompt': 'äººå·¥æ™ºèƒ½è¢«æ»¥ç”¨å¸¦æ¥çš„å±å®³ï¼Ÿ', 'respons...   \n",
       "\n",
       "                                  Groundedness_calls  latency  total_tokens  \\\n",
       "0  [{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...       12             0   \n",
       "1  [{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...        6             0   \n",
       "2  [{'args': {'source': 'ä»»ä½•çš„ç§‘æŠ€éƒ½ä¼šæœ‰ç“¶é¢ˆï¼Œ æ‘©å°”å®šå¾‹ åˆ°â½¬å‰ä¹Ÿé‡åˆ°ç›¸...        3             0   \n",
       "3  [{'args': {'source': 'The Behavioral and Brain...        8             0   \n",
       "4  [{'args': {'source': '2/2/24, 2:43 PM â¼ˆâ¼¯æ™ºèƒ½  - ...        6             0   \n",
       "\n",
       "   total_cost  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
