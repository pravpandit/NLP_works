{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a786c77c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter 3 Storage\n",
    "\n",
    "- [1. Set OpenAI API Key](#1. Set OpenAI-API-Key)\n",
    "- [2. Dialogue Cache Storage](#2. Dialogue Cache Storage)\n",
    "- [2.1 English Version](#2.1-English Version)\n",
    "- [2.1.1 Initialize Dialogue Model](#2.1.1-Initialize Dialogue Model)\n",
    "- [2.1.2 First Round Dialogue](#2.1.2-First Round Dialogue)\n",
    "- [2.1.3 Second Round Dialogue](#2.1.3-Second Round Dialogue)\n",
    "- [2.1.4 Third Round Dialogue](#2.1.4-Third Round Dialogue)\n",
    "- [2.1.5 View Storage Cache](#2.1.5-View Storage Cache)\n",
    "- [2.1.6 Add Content Directly to Storage Cache](#2.1.6-Add Content Directly to Storage Cache)\n",
    "- [2.2 Chinese Version](#2.2-Chinese Version)\n",
    "- [2.1.1 Initialize the dialogue model](#2.1.1- Initialize the dialogue model)\n",
    "- [2.1.2 First round of dialogue](#2.1.2-First round of dialogue)\n",
    "- [2.1.3 Second round of dialogue](#2.1.3-Second round of dialogue)\n",
    "- [2.1.4 Third round of dialogue](#2.1.4-Third round of dialogue)\n",
    "- [2.1.5 View storage cache](#2.1.5-View storage cache)\n",
    "- [2.1.6 Add content directly to storage cache](#2.1.6-Add content directly to storage cache)\n",
    "- [2.3 Summary](#2.3-Summary)\n",
    "- [Three, dialogue cache window storage](#Three, dialogue cache window storage)\n",
    "- [3.1 English version](#3.1-English version)\n",
    "- [3.1.1 Add two rounds of dialogue to window storage](#3.1.1-Add two rounds of dialogue to window storage)\n",
    "- [3.1.2 Apply window storage in dialogue chain](#3.1.2-Apply window storage in dialogue chain)\n",
    "- [3.2 Chinese version](#3.2-Chinese version)\n",
    "- [3.1.1 Add two rounds of conversations to window storage](#3.1.1-Add two rounds of conversations to window storage)\n",
    "- [3.1.2 Apply window storage in conversation chain](#3.1.2-Apply window storage in conversation chain)\n",
    "- [IV. Conversation token cache storage](#IV. Conversation token cache storage)\n",
    "- [4.1 English version](#4.1-English version)\n",
    "- [4.2 Chinese version](#4.2-Chinese version)\n",
    "- [4.3 Supplement](#4.3-Supplement)\n",
    "- [V. Conversation summary cache storage](#V. Conversation summary cache storage)\n",
    "- [5.1 English version](#5.1-English version)\n",
    "- [5.1.1 Use conversation summary cache storage](#5.1.1-Use conversation summary cache storage)\n",
    "- [5.1.2 Conversation chain based on conversation summary cache storage](#5.1.2-Conversation chain based on conversation summary cache storage)\n",
    "- [5.2 Chinese version](#5.2-Chinese version)\n",
    "- [5.2.1 Use conversation summary cache storage](#5.2.1-Use conversation summary cache storage)\n",
    "- [5.1.2 Conversation chain based on conversation summary cache storage](#5.1.2-Conversation chain based on conversation summary cache storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10db6f",
   "metadata": {},
   "source": [
    "When you interact with those language models, they don't remember what you had communicated with them before, which is a big problem when we build some applications (such as chatbots) -- it doesn't seem smart enough! Therefore, in this section we will introduce the storage module in LangChain, that is, how to embed previous conversations into the language model so that it has the ability of continuous conversation.\n",
    "\n",
    "When using the storage (Memory) module in LangChain, it can help save and manage historical chat messages, as well as build knowledge about specific entities. These components can store information across multiple rounds of conversations and allow tracking of specific information and context during the conversation.\n",
    "\n",
    "LangChain provides multiple storage types. Among them, buffer storage allows the most recent chat messages to be retained, and summary storage provides a summary of the entire conversation. Entity storage allows information about specific entities to be retained in multiple rounds of conversations. These memory components are modular and can be used in combination with other components to enhance the robot's conversation management capabilities. The storage module can be accessed and updated through simple API calls, allowing developers to more easily manage and maintain conversation history.\n",
    "\n",
    "This course mainly introduces four of the storage modules, and other modules can be viewed in the document for learning.\n",
    "- Conversation Buffer Memory\n",
    "- Conversation Buffer Window MemoryonBufferWindowMemory）\n",
    "- Conversation Token Buffer Memory\n",
    "- Conversation Summary Buffer Memory\n",
    "\n",
    "In LangChain, storage refers to the short-term memory of the large language model (LLM). Why is it short-term memory? That's because after the LLM is trained (some long-term memory is obtained), its parameters will not change due to user input. When the user talks to the trained LLM, the LLM will temporarily remember the user's input and the output it has generated in order to predict the subsequent output, and after the model output is completed, it will \"forget\" the previous user's input and its output. Therefore, this information can only be called the short-term memory of the LLM. \n",
    "\n",
    "In order to extend the retention time of the LLM short-term memory, it is necessary to use some external storage methods to memorize it, so that when the user talks to the LLM, the LLM can know as much as possible about the historical conversation information between the user and it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca56e6b-1e07-4405-a1ca-f4237f20fa75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Set OpenAI API Key\n",
    "\n",
    "Log in to [OpenAI account](https://platform.openai.com/account/api-keys) to get API Key, and then set it as an environment variable.\n",
    "\n",
    "- If you want to set it as a global environment variable, you can refer to [Zhihu article](https://zhuanlan.zhihu.com/p/627665725).\n",
    "\n",
    "- If you want to set it as a local/project environment variable, create a `.env` file in this file directory, open the file and enter the following content.\n",
    "\n",
    "<p style=\"font-family:verdana; font-size:12px;color:green\">\n",
    "OPENAI_API_KEY=\"your_api_key\" \n",
    "</p>\n",
    "\n",
    "Replace \"your_api_key\" with your own API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932bd47-c6d5-4794-8102-a12b84412a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the required packages python-dotenv and openai\n",
    "# If you need to view the installation process log, you can remove -q\n",
    "!pip install -q python-dotenv\n",
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10446712-9fa6-4d71-94ce-2ea4cf197e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# Read local/project environment variables.\n",
    "\n",
    "# find_dotenv() finds and locates the path of the .env file\n",
    "# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment\n",
    "# If you set a global environment variable, this line of code will have no effect.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get the environment variable OPENAI_API_KEY\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dcd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Conversation Cache Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8cd26-45c5-4bae-90b2-b07d23bf3bb2",
   "metadata": {},
   "source": [
    "### 2.1 English version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e77a3d-7aaa-48c1-b219-19bd6f4eb674",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.1 Initialize the dialogue model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20ad6fe2",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88bdf13d",
   "metadata": {
    "height": 133
   },
   "outputs": [],
   "source": [
    "# Here we set the parameter temperature to 0.0 to reduce the randomness of the generated answers.\n",
    "# If you want to get different and innovative answers every time, you can try increasing this parameter.\n",
    "llm = ChatOpenAI(temperature=0.0)  \n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a new ConversationChain Class instance\n",
    "# When the verbose parameter is set to True, the program will output more detailed information to provide more debugging or runtime information.\n",
    "# On the contrary, when the verbose parameter is set to False, the program will run in a more concise way and only output key information.\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea83837",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.2 First round of dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b4c42",
   "metadata": {},
   "source": [
    "When we ran the prediction (<font style=\"font-family:verdana; font-size:13.5px;color:#0055AA\">predict</font>), some prompts were generated, as you can see below, he said \"The following is a friendly conversation between a human and an AI, the AI ​​is talkative\" and so on, this is actually the prompt generated by LangChain to make the system have a hopeful and friendly conversation, and the conversation must be saved, and the prompt for the current completed model chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db24677d",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71564ad",
   "metadata": {},
   "source": [
    "#### 2.1.3 Second round of dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d006bd",
   "metadata": {},
   "source": [
    "When we have the second round of conversation, it will keep the above prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc3ef937",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb734b",
   "metadata": {},
   "source": [
    "#### 2.1.4 The third round of dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393df3d",
   "metadata": {},
   "source": [
    "In order to verify whether he remembered the previous conversation, we asked him to answer what he had said before (my name). We can see that he did output the correct name, so this conversation chain will become longer and longer as it goes on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "acf3339a",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Andrew.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96a8d9",
   "metadata": {},
   "source": [
    "#### 2.1.5 View storage cache\n",
    "\n",
    "Storage cache (<font style=\"font-family:verdana; font-size:13.5px\">memory.</font><font style=\"font-family:verdana; font-size:13.5px;color:#0055AA\">buffer</font>)\n",
    "Stores all conversation information so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2529400d",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Andrew.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5de846",
   "metadata": {},
   "source": [
    "It is also possible to print the history of messages in the cache with load_memory_variables({}) . The {} here is an empty dictionary, there are some more advanced features that allow the user to use more complex input, but we won't be discussing them in this short course, so don't worry about why there is an empty curly brace here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5018cb0a",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Andrew.\"}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2e892",
   "metadata": {},
   "source": [
    "#### 2.1.6 Add content directly to the storage cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14219b70",
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()  # 新建一个空的对话缓存记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a36e9905",
   "metadata": {
    "height": 48
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})  # 向缓存区添加指定对话的输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61631b1f",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)  # 查看缓存区结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2fdf9ec",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Human: Hi\\nAI: What's up\"}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))# 再次加载记忆变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac544f2",
   "metadata": {},
   "source": [
    "Continue to add new content, and the conversation history will be saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ca79256",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "890a4497",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eddd91a-bf1c-4b82-b99c-c585420e4ecb",
   "metadata": {},
   "source": [
    "### 2.2 Chinese version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3e4e9-7a6a-4a09-9ac3-0096a67849c7",
   "metadata": {},
   "source": [
    "#### 2.1.1 Initialize the dialogue model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3577aaff-7edb-40b0-866a-e407e63d55e0",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a77d37ab-1f75-4ae8-8d7c-5066773ead81",
   "metadata": {
    "height": 133
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)  \n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747cb539-abc4-4e47-8cb9-1ee608ab07fc",
   "metadata": {},
   "source": [
    "#### 2.1.2 First round of dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e48462-7a92-4842-bdaa-2a478ba2252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"你好, 我叫皮皮鲁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d320b-6b20-4722-99db-c48a43711d6c",
   "metadata": {},
   "source": [
    "#### 2.1.3 Second round of dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ed6e97a-d7ea-4188-a6d7-f91d2a29d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？\n",
      "Human: 1+1等于多少？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1等于2。'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"1+1等于多少？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fd531-216e-42d8-b226-839747ad7dd3",
   "metadata": {},
   "source": [
    "#### 2.1.4 The third round of dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfe488-2758-42c7-9c20-e483b4c22ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？\n",
      "Human: 1+1等于多少？\n",
      "AI: 1+1等于2。\n",
      "Human: What is my name?\n",
      "AI: 你的名字是皮皮鲁。\n",
      "Human: 我叫什么名字？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你叫皮皮鲁。'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"我叫什么名字？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9822e-943d-4905-a1f8-a0d28c215d60",
   "metadata": {},
   "source": [
    "#### 2.1.5 View storage cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0795580f-b6b6-47e0-8882-26fe204560bd",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？\n",
      "Human: 1+1等于多少？\n",
      "AI: 1+1等于2。\n",
      "Human: What is my name?\n",
      "AI: 你的名字是皮皮鲁。\n",
      "Human: 我叫什么名字？\n",
      "AI: 你叫皮皮鲁。\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfe7824f-bd6e-4b95-92e2-1c85c62a92e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？\n",
      "Human: 1+1等于多少？\n",
      "AI: 1+1等于2。\n",
      "Human: What is my name?\n",
      "AI: 你的名字是皮皮鲁。\n",
      "Human: 我叫什么名字？\n",
      "AI: 你叫皮皮鲁。\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48942759-8afb-4aed-80c5-a48952a2b0c0",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你好, 我叫皮皮鲁\\nAI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？\\nHuman: 1+1等于多少？\\nAI: 1+1等于2。\\nHuman: What is my name?\\nAI: 你的名字是皮皮鲁。\\nHuman: 我叫什么名字？\\nAI: 你叫皮皮鲁。'}\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d0c4625-e928-45dc-b8da-4ab865ac5f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 你好, 我叫皮皮鲁\\nAI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？\\nHuman: 1+1等于多少？\\nAI: 1+1等于2。\\nHuman: What is my name?\\nAI: 你的名字是皮皮鲁。\\nHuman: 我叫什么名字？\\nAI: 你叫皮皮鲁。'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3778e-5fbf-43ed-9df1-d57d98ec6fb0",
   "metadata": {},
   "source": [
    "#### 2.1.6 Add content directly to the storage cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "147b2c30-1662-4b49-aaf8-c228428e5cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 你好，我叫皮皮鲁\\nAI: 你好啊，我叫鲁西西'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"你好，我叫皮皮鲁\"}, {\"output\": \"你好啊，我叫鲁西西\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b5e27f0-dad7-41b6-9326-bebf6299638f",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfa7c555-06ab-4906-b3dc-906f789e08f5",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 你好，我叫皮皮鲁\\nAI: 你好啊，我叫鲁西西\\nHuman: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "deb33de8-37ea-4180-a73e-0fc456b14eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 你好，我叫皮皮鲁\\nAI: 你好啊，我叫鲁西西\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: 很高兴和你成为朋友！\\nAI: 是的，让我们一起去冒险吧！'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"很高兴和你成为朋友！\"}, {\"output\": \"是的，让我们一起去冒险吧！\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10146f1a-0114-4902-8122-d19ae6f7c461",
   "metadata": {},
   "source": [
    "### 2.3 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759b6bc-edb2-4cfe-b0f8-1bf6c4d796f9",
   "metadata": {},
   "source": [
    "When we use a large language model for a chat conversation, the large language model itself is actually stateless. The language model itself does not remember the historical conversation so far. Each call to the API node is independent. The memory can store all the terms or conversations so far and input or attach context to the LLM to generate output. It looks like it remembers what was said before when it is in the next round of conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98e9ff",
   "metadata": {},
   "source": [
    "## 3. Conversation Cache Window Storage\n",
    "\n",
    "As conversations become longer, the amount of memory required becomes very long. The cost of sending a large number of tokens to LLM will also become more expensive, which is why the API call fee is usually charged based on the number of tokens it needs to process.\n",
    "\n",
    "In response to the above problems, LangChain also provides several convenient storage methods to save historical conversations. Among them, the conversation cache window storage only retains a window size of conversations. It only uses the most recent n interactions. This can be used to keep a sliding window of recent interactions so that the buffer is not too large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c9061-9916-4524-b497-93a0aa2b7d06",
   "metadata": {},
   "source": [
    "### 3.1 English version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641477a4",
   "metadata": {},
   "source": [
    "#### 3.1.1 Add two rounds of dialogue to window storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ea6233e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# k is the window parameter, k=1 means only one conversation memory is kept\n",
    "memory = ConversationBufferWindowMemory(k=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc4553fb",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# Add two rounds of dialogue to memory\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6a788403",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And view the current record of the memory variable\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bda148",
   "metadata": {},
   "source": [
    "#### 3.1.2 Using window storage in a dialog chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4087bc87",
   "metadata": {
    "height": 133
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=False  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d661e3",
   "metadata": {},
   "source": [
    "Note here! Since a window of memory is used here, only one round of historical messages can be saved. Therefore, the AI ​​cannot know the name you mentioned in the first round of conversation. It can only remember the conversation information of the previous round (the second round) at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4faaa952",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb20ddaa",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 is equal to 2.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "489b2194",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information.\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88837e7c-cf4b-469e-b820-bbfc49ba876c",
   "metadata": {},
   "source": [
    "### 3.2 Chinese version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ec3ad-6959-4a36-b1b5-4fcafe8088ad",
   "metadata": {},
   "source": [
    "#### 3.1.1 Add two rounds of dialogue to window storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68a2907c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: 很高兴和你成为朋友！\\nAI: 是的，让我们一起去冒险吧！'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# k=1 means only one conversation memory is kept\n",
    "memory = ConversationBufferWindowMemory(k=1)  \n",
    "memory.save_context({\"input\": \"你好，我叫皮皮鲁\"}, {\"output\": \"你好啊，我叫鲁西西\"})\n",
    "memory.save_context({\"input\": \"很高兴和你成为朋友！\"}, {\"output\": \"是的，让我们一起去冒险吧！\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabf017-5bfd-4904-9f06-388f994eddc9",
   "metadata": {},
   "source": [
    "#### 3.1.2 Using window storage in a dialog chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ee854d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？\n",
      "1+1等于2。\n",
      "很抱歉，我无法知道您的名字。\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=False  )\n",
    "print(conversation.predict(input=\"你好, 我叫皮皮鲁\"))\n",
    "print(conversation.predict(input=\"1+1等于多少？\"))\n",
    "print(conversation.predict(input=\"我叫什么名字？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2931b92",
   "metadata": {},
   "source": [
    "## 4. Conversation token cache storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5b4c7",
   "metadata": {},
   "source": [
    "With conversation token cache memoization, the memory will limit the number of tokens saved. If the number of tokens exceeds the specified number, it will cut off the earlier part of the conversation to keep the number of tokens corresponding to the most recent communication, but not exceeding the token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f6d063c",
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": [
    "!pip install -q tiktoken            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fb9020ed",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a84112",
   "metadata": {},
   "source": [
    "### 4.1 English version\n",
    "Added dialog to Token cache storage, limited the number of tokens, and tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43582ee6",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
    "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "284288e1",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62b2e1",
   "metadata": {},
   "source": [
    "You can see that the tokens that exceeded the limit have been discarded!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6be43",
   "metadata": {},
   "source": [
    "### 4.2 Chinese version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9191020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: 轻舟已过万重山。'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
    "memory.save_context({\"input\": \"朝辞白帝彩云间，\"}, {\"output\": \"千里江陵一日还。\"})\n",
    "memory.save_context({\"input\": \"两岸猿声啼不住，\"}, {\"output\": \"轻舟已过万重山。\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08ef4a-876f-422a-81f9-4805288e5955",
   "metadata": {},
   "source": [
    "### 4.3 Supplement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d918b",
   "metadata": {},
   "source": [
    "ChatGPT uses a Byte Pair Encoding (BPE)-based method for tokenization (splitting input text into tokens). BPE is a common tokenization technique that splits input text into smaller subword units. \n",
    "\n",
    "OpenAI has released a new open source Python library [tiktoken](https://github.com/openai/tiktoken) on its official GitHub. This library is mainly used to calculate the number of tokens. Compared with HuggingFace's tokenizer, its speed has increased several times. \n",
    "\n",
    "For specific token calculation methods, especially the difference between Chinese characters and English words, please refer to [Zhihu article](https://www.zhihu.com/question/594159910) for specific lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55d5d",
   "metadata": {},
   "source": [
    "## 5. Conversation summary cache storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d39b83a",
   "metadata": {},
   "source": [
    "Conversation summary cache storage, **use LLM to write a summary of the historical conversation so far**, and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72dcf8b1",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b213e-ce17-46a0-8652-03658ca58dd8",
   "metadata": {},
   "source": [
    "### 5.1 English version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572ef39",
   "metadata": {},
   "source": [
    "#### 5.1.1 Using the conversation summary cache\n",
    "\n",
    "Create a long string containing someone's schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a5b238f",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "# Create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "# Using conversation summary cache memory\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100) \n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.save_context(\n",
    "    {\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15226a41-ab36-43a0-93f7-c03c6b374936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb97b6",
   "metadata": {},
   "source": [
    "#### 5.1.2 Conversation chain based on conversation summary cache storage\n",
    "Based on the above conversation summary cache storage, create a new conversation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6728edba",
   "metadata": {
    "height": 99
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9a221b1d",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A good demo to show during the lunch meeting with the customer interested in AI would be the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, you can demonstrate the capabilities of AI in natural language processing and how it can be applied to various industries and use cases.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb582617",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting. A good demo to show during the lunch meeting with the customer interested in AI would be the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, you can demonstrate the capabilities of AI in natural language processing and how it can be applied to various industries and use cases.\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba827aa",
   "metadata": {
    "height": 31
   },
   "source": [
    "### 5.2 Chinese version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64898f33-c538-4e68-b008-7123870b692b",
   "metadata": {},
   "source": [
    "#### 5.2.1 Using the conversation summary cache\n",
    "\n",
    "Create a long string containing someone's schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2c07922b",
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": [
    "# Create a long string\n",
    "schedule = \"在八点你和你的产品团队有一个会议。 \\\n",
    "你需要做一个PPT。 \\\n",
    "上午9点到12点你需要忙于LangChain。\\\n",
    "Langchain是一个有用的工具，因此你的项目进展的非常快。\\\n",
    "中午，在意大利餐厅与一位开车来的顾客共进午餐 \\\n",
    "走了一个多小时的路程与你见面，只为了解最新的 AI。 \\\n",
    "确保你带了笔记本电脑可以展示最新的 LLM 样例.\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"你好，我叫皮皮鲁\"}, {\"output\": \"你好啊，我叫鲁西西\"})\n",
    "memory.save_context({\"input\": \"很高兴和你成为朋友！\"}, {\"output\": \"是的，让我们一起去冒险吧！\"})\n",
    "memory.save_context({\"input\": \"今天的日程安排是什么？\"}, {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "17424a12-430f-4529-9067-300978c6169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human introduces themselves as Pipilu and the AI introduces themselves as Luxixi. They express happiness at becoming friends and decide to go on an adventure together. The human asks about the schedule for the day. The AI informs them that they have a meeting with their product team at 8 o'clock and need to prepare a PowerPoint presentation. From 9 am to 12 pm, they will be busy with LangChain, a useful tool that helps their project progress quickly. At noon, they will have lunch with a customer who has driven for over an hour just to learn about the latest AI. The AI advises the human to bring their laptop to showcase the latest LLM samples.\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e29a956-607a-4247-9eb5-01285a370991",
   "metadata": {},
   "source": [
    "#### 5.1.2 Conversation chain based on conversation summary cache storage\n",
    "Based on the above conversation summary cache storage, create a new conversation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "52696c8c",
   "metadata": {
    "height": 31
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "48690d13",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human introduces themselves as Pipilu and the AI introduces themselves as Luxixi. They express happiness at becoming friends and decide to go on an adventure together. The human asks about the schedule for the day. The AI informs them that they have a meeting with their product team at 8 o'clock and need to prepare a PowerPoint presentation. From 9 am to 12 pm, they will be busy with LangChain, a useful tool that helps their project progress quickly. At noon, they will have lunch with a customer who has driven for over an hour just to learn about the latest AI. The AI advises the human to bring their laptop to showcase the latest LLM samples.\n",
      "Human: 展示什么样的样例最好呢？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'展示一些具有多样性和创新性的样例可能是最好的选择。你可以选择展示一些基于图像识别的样例，比如人脸识别、物体识别等。另外，你也可以展示一些自然语言处理方面的样例，比如文本生成、情感分析等。最重要的是选择那些能够展示出你们团队的技术实力和创造力的样例。'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"展示什么样的样例最好呢？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "85bba1f8",
   "metadata": {
    "height": 31
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human introduces themselves as Pipilu and the AI introduces themselves as Luxixi. They express happiness at becoming friends and decide to go on an adventure together. The human asks about the schedule for the day. The AI informs them that they have a meeting with their product team at 8 o'clock and need to prepare a PowerPoint presentation. From 9 am to 12 pm, they will be busy with LangChain, a useful tool that helps their project progress quickly. At noon, they will have lunch with a customer who has driven for over an hour just to learn about the latest AI. The AI advises the human to bring their laptop to showcase the latest LLM samples. The human asks what kind of samples would be best to showcase. The AI suggests that showcasing diverse and innovative samples would be the best choice. They recommend selecting samples based on image recognition, such as face recognition and object recognition. Additionally, they suggest showcasing samples related to natural language processing, such as text generation and sentiment analysis. The AI emphasizes the importance of choosing samples that demonstrate the team's technical expertise and creativity.\"}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})  # 摘要记录更新了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
