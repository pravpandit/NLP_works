{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# Chapter 4 Model Chain\n",
    "\n",
    "- [1. Set OpenAI API Key](#1. Set OpenAI-API-Key)\n",
    "- [2. Large Language Model Chain](#2. Large Language Model Chain)\n",
    "- [2.1 Import Data](#2.1-Import Data)\n",
    "- [2.2 Initialize Language Model](#2.2-Initialize Language Model)\n",
    "- [2.3 Initialize Prompt Template](#2.3-Initialize Prompt Template)\n",
    "- [2.4 Build Large Language Model Chain](#2.4-Build Large Language Model Chain)\n",
    "- [2.5 Run Large Language Model Chain](#2.5-Run Large Language Model Chain)\n",
    "- [2.6 Chinese Prompt](#2.6-Chinese Prompt)\n",
    "- [3. Sequential Chain](#3. Sequential Chain)\n",
    "- [3.1 Simple Sequential Chain](#3.1-Simple Sequential Chain)\n",
    "- [3.1.1 Create Two Subchains](#3.1.1-Create Two Subchains)\n",
    "- [3.1.2 Build a simple sequential chain](#3.1.2-Build a simple sequential chain)\n",
    "- [3.1.3 Run a simple sequential chain](#3.1.3-Run a simple sequential chain)\n",
    "- [3.1.4 Chinese prompts](#3.1.4-Chinese prompts)- [3.2 Sequence chain](#3.2-Sequence chain)\n",
    "- [3.2.1 Create four subchains](#3.2.1-Create four subchains)\n",
    "- [3.2.2 Combine four subchains](#3.2.2-Combining four subchains)\n",
    "- [3.2.3 Chinese prompts](#3.2.3-Chinese prompts)\n",
    "- [IV. Routing chain](#IV.-Routing chain)\n",
    "- [4.1 Define prompt template](#4.1-Define prompt template)\n",
    "- [4.2 Name and describe prompt template](#4.2-Name and describe prompt template)\n",
    "- [4.3 Create corresponding target chain based on prompt template information](#4.3-Create corresponding target chain based on prompt template information--)\n",
    "- [4.4 Create default target chain](#4.4-Create default target chain)\n",
    "- [4.5 Define routing templates between different chains](#4.5-Define routing templates between different chains)\n",
    "- [4.6 Build routing chains](#4.6-Build routing chains)\n",
    "- [4.7 Create overall links](#4.7-Create overall links)\n",
    "- [4.8 Ask questions](#4.8-Ask questions)\n",
    "- [4.9 Chinese prompts](#4.9-Chinese promptsText Tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54810ef7",
   "metadata": {},
   "source": [
    "Chains typically combine a large language model (LLM) with a prompt, based on which we can perform a series of operations on text or data.\n",
    "\n",
    "Chains can accept multiple inputs at once\n",
    "\n",
    "For example, we can create a chain that accepts user input, formats it using a prompt template, and then passes the formatted response to the LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21009bf6-49bd-466e-8177-74c23533d938",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Set up OpenAI API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5993b-f28d-4be4-984f-2d05be9f4579",
   "metadata": {},
   "source": [
    "Log in to your [OpenAI account](https://platform.openai.com/account/api-keys) to get your API Key, and then set it as an environment variable.\n",
    "\n",
    "- If you want to set it as a global environment variable, you can refer to [Zhihu article](https://zhuanlan.zhihu.com/p/627665725).\n",
    "- If you want to set it as a local/project environment variable, create a `.env` file in this file directory, open the file and enter the following content.\n",
    "\n",
    "<p style=\"font-family:verdana; font-size:12px;color:green\">\n",
    "OPENAI_API_KEY=\"your_api_key\" \n",
    "</p>\n",
    "\n",
    "Replace \"your_api_key\" with your own API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc3519c-4d12-4011-9223-2f3cb3c42b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the required packages python-dotenv and openai\n",
    "# If you need to view the installation process log, you can remove -q\n",
    "!pip install -q python-dotenv\n",
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ad53241-bef6-42b8-894b-bcbbc8c64df7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Read local/project environment variables.\n",
    "\n",
    "# find_dotenv() finds and locates the path of the .env file\n",
    "# load_dotenv() reads the .env file and loads the environment variables in it into the current running environment\n",
    "# If you set a global environment variable, this line of code will have no effect.\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Get the environment variable OPENAI_API_KEY\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940ce7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Large Language Model Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000bd16",
   "metadata": {},
   "source": [
    "The Large Language Model Chain (LLMChain) is a simple but very powerful chain, and is the basis for many of the chains we will introduce later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e93b9-52ba-4f86-a953-e4661b895a3d",
   "metadata": {},
   "source": [
    "### 2.1 Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b84e441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a09c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                                             Review\n",
       "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
       "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
       "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
       "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
       "4  Milk Frother Handheld\\n  Â I loved this product. But they only seem to l..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa83d10-04ee-4355-abe3-ab699c9eaca9",
   "metadata": {},
   "source": [
    "### 2.2 Initialize the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92dff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.prompts import ChatPromptTemplate  \n",
    "from langchain.chains import LLMChain   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "943237a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the parameter temperature to 0.0 to reduce the randomness of the generated answers.\n",
    "# If you want to get different and innovative answers every time, you can try adjusting this parameter.\n",
    "llm = ChatOpenAI(temperature=0.0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81887434",
   "metadata": {},
   "source": [
    "### 2.3 Initialize prompt template\n",
    "Initialize prompt, this prompt will accept a variable called product. This prompt will ask LLM to generate the best name to describe the company that makes the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdcdb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(   \n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22cb13",
   "metadata": {},
   "source": [
    "### 2.4 Building a Large Language Model Chain\n",
    "\n",
    "Combine the Large Language Model (LLM) and Prompt into a chain. This large language model chain is very simple and allows us to run the prompt in a sequential manner and combine it into the large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7abc20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d5ff6",
   "metadata": {},
   "source": [
    "### 2.5 Running a Large Language Model Chain\n",
    "So if we have a product called \"Queen Size Sheet Set\", we can run it through this chain by using chain.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad44d1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Royal Linens.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ede1c",
   "metadata": {},
   "source": [
    "You can enter any product description and see what results the chain will output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab31583-5cdf-4885-94d7-59c0d3b90b2e",
   "metadata": {},
   "source": [
    "### 2.6 Chinese prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2181be10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"è±ªååºçºº\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(   \n",
    "    \"æè¿°å¶é {product}çä¸ä¸ªå¬å¸çæä½³åç§°æ¯ä»ä¹?\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "product = \"å¤§å·åºåå¥è£\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49158430",
   "metadata": {},
   "source": [
    "## 3. Sequence Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03469",
   "metadata": {},
   "source": [
    "### 3.1 Simple Sequential Chains\n",
    "\n",
    "Sequential Chains are chains that execute their links in a predefined order. Specifically, we will use SimpleSequentialChain, which is the simplest type of sequential chain, where each step has an input/output and the output of one step is the input of the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "febee243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d019d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e732589",
   "metadata": {},
   "source": [
    "#### 3.1.1 Create two subchains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f31aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template 1: This prompt will accept a product and return the best name to describe the company\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f5d5b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt Template 2: Accepts a company name and outputs a 20-word description of the company\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1991f4",
   "metadata": {},
   "source": [
    "#### 3.1.2 Building a Simple Sequential Chain\n",
    "Now we can combine the two LLMChains so that we can create the company name and description in one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c1eb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122f26a",
   "metadata": {},
   "source": [
    "Give an input and run the above chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80d102-da6b-49f4-8abd-7bff97211232",
   "metadata": {},
   "source": [
    "#### 3.1.3 Running a simple sequence chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78458efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mRoyal Rest Linens\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mRoyal Rest Linens provides high-quality and luxurious linens for a comfortable and restful night's sleep.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Royal Rest Linens provides high-quality and luxurious linens for a comfortable and restful night's sleep.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983eb68-adf0-4c18-beed-ff6fc09cd78b",
   "metadata": {},
   "source": [
    "#### 3.1.4 Chinese prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c32997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"å°ºå¯¸çåºåæéå¬å¸\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3må°ºå¯¸çåºåæéå¬å¸æ¯ä¸å®¶ä¸æ³¨äºåºä¸ç¨åçäº§çå¬å¸ã\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å°ºå¯¸çåºåæéå¬å¸æ¯ä¸å®¶ä¸æ³¨äºåºä¸ç¨åçäº§çå¬å¸ã'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(   \n",
    "    \"æè¿°å¶é {product}çä¸ä¸ªå¬å¸çæå¥½çåç§°æ¯ä»ä¹\"\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(   \n",
    "    \"åä¸ä¸ª20å­çæè¿°å¯¹äºä¸é¢è¿ä¸ª\\\n",
    "    å¬å¸ï¼{company_name}ç\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],verbose=True)\n",
    "product = \"å¤§å·åºåå¥è£\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ce18c",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69f4c0",
   "metadata": {},
   "source": [
    "When there is only one input and one output, a simple sequential chain (SimpleSequentialChain) can be implemented. When there are multiple inputs or multiple outputs, we need to use a sequential chain (SequentialChain) to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c129ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI    #å¯¼å¥OpenAIæ¨¡å\n",
    "from langchain.prompts import ChatPromptTemplate   #å¯¼å¥èå¤©æç¤ºæ¨¡æ¿\n",
    "from langchain.chains import LLMChain    #å¯¼å¥LLMé¾ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03a8e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811445c",
   "metadata": {},
   "source": [
    "Next we will create a series of chains and then use them one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e87ec-b99a-4130-a5b5-cdce064dc6ca",
   "metadata": {},
   "source": [
    "#### 3.2.1 Create four subchains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "016187ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subchain 1\n",
    "\n",
    "# prompt template 1: Translate into English (translate the following review into English)\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: Input: Review Output: English Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fb0730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subchain 2\n",
    "\n",
    "# prompt template 2: Summarize the following review in one sentence\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: Input: English Review Output: Summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6accf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subchain 3\n",
    "\n",
    "# prompt template 3: What language is used in the following review\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: Input: Review Output: Language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7a46121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subchain 4\n",
    "\n",
    "# prompt template 4: Write a follow-up response to the following summary using specific language\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: Input: summary, language Output: follow-up reply\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff435291-5d20-4c3e-9ed7-76a1140f96d2",
   "metadata": {},
   "source": [
    "#### 3.2.2 Combining four subchains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89603117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: review\n",
    "#Output: English review, summary, follow-up reply\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509de01",
   "metadata": {},
   "source": [
    "Let's select a review and pass it through the entire chain. We can see that the original review is in French, the English review can be seen as a translation, followed by a summary based on the English review, and the final output is a continuation of the original French information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51b04f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goÃ»t mÃ©diocre. La mousse ne tient pas, c'est bizarre. J'achÃ¨te les mÃªmes dans le commerce et le goÃ»t est bien meilleur...\\nVieux lot ou contrefaÃ§on !?\",\n",
       " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in the store and the taste is much better...\\nOld batch or counterfeit!?\",\n",
       " 'summary': 'The reviewer is disappointed with the taste and consistency of the product, suspecting that either an old batch or counterfeit version is the cause.',\n",
       " 'followup_message': \"RÃ©ponse de suivi :\\n\\nCher(e) critique,\\n\\nNous sommes dÃ©solÃ©s d'apprendre que notre produit ne rÃ©pond pas Ã  vos attentes. Nous prenons trÃ¨s au sÃ©rieux les commentaires de nos clients et nous tenons Ã  vous rassurer que nous attachons une grande importance Ã  la qualitÃ© de nos produits.\\n\\nNous vÃ©rifions constamment nos processus de production afin de nous assurer que nos produits sont toujours frais et authentiques. Si vous pensez avoir reÃ§u un produit contrefait ou un lot pÃ©rimÃ©, nous vous prions de nous contacter directement afin de rÃ©soudre ce problÃ¨me au plus vite. Nous avons un service client dÃ©diÃ© qui sera ravi de vous aider avec votre prÃ©occupation.\\n\\nNous tenons Ã  nous excuser pour toute dÃ©ception ou dÃ©sagrÃ©ment que cela aurait pu causer. Votre satisfaction est notre prioritÃ© absolue et nous ferons tout notre possible pour remÃ©dier Ã  la situation et regagner votre confiance.\\n\\nNous vous remercions de votre comprÃ©hension et de votre patience. Votre avis est important pour nous et nous espÃ©rons avoir l'occasion de nous rattraper Ã  l'avenir.\\n\\nCordialement,\\nL'Ã©quipe de [Nom de l'entreprise]\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d72f1d-8a1e-4eb6-b00f-1963b3cf3adb",
   "metadata": {},
   "source": [
    "#### 3.2.3 Chinese prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31624a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goÃ»t mÃ©diocre. La mousse ne tient pas, c'est bizarre. J'achÃ¨te les mÃªmes dans le commerce et le goÃ»t est bien meilleur...\\nVieux lot ou contrefaÃ§on !?\",\n",
       " 'English_Review': \"I find the taste mediocre. The foam doesn't last, it's weird. I buy the same ones from the store and the taste is much better...\\nOld batch or counterfeit!?\",\n",
       " 'summary': \"The taste is mediocre, the foam doesn't last, and there is suspicion of old batch or counterfeit.\",\n",
       " 'followup_message': \"åå¤: Je suis dÃ©solÃ© de vous entendre dire que le goÃ»t est moyen et que la mousse ne dure pas longtemps. Nous prenons ces problÃ¨mes trÃ¨s au sÃ©rieux. Nous allons enquÃªter pour vÃ©rifier s'il s'agit d'un ancien lot ou d'une contrefaÃ§on. Votre satisfaction est notre prioritÃ© et nous ferons de notre mieux pour rÃ©soudre ce problÃ¨me. Merci de nous avoir informÃ©s.\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "\n",
    "#Subchain 1\n",
    "# prompt template 1: Translate into English (translate the following review into English)\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"æä¸é¢çè¯è®ºreviewç¿»è¯æè±æ:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: Input: Review Output: English Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n",
    "\n",
    "#Subchain 2\n",
    "# prompt template 2: Summarize the following review in one sentence\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"è¯·ä½ ç¨ä¸å¥è¯æ¥æ»ç»ä¸é¢çè¯è®ºreview:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: Input: English Review Output: Summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n",
    "\n",
    "\n",
    "#Subchain 3\n",
    "# prompt template 3: What language is used in the following review\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"ä¸é¢çè¯è®ºreviewä½¿ç¨çä»ä¹è¯­è¨:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: Input: Review Output: Language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
    "\n",
    "\n",
    "#Subchain 4\n",
    "# prompt template 4: Write a follow-up response to the following summary using specific language\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"ä½¿ç¨ç¹å®çè¯­è¨å¯¹ä¸é¢çæ»ç»åä¸ä¸ªåç»­åå¤:\"\n",
    "    \"\\n\\næ»ç»: {summary}\\n\\nè¯­è¨: {language}\"\n",
    ")\n",
    "# chain 4: Input: summary, language Output: follow-up reply\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n",
    "\n",
    "\n",
    "# Combine the four subchains\n",
    "#Input: review Output: English review, summary, follow-up response\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041ea4c",
   "metadata": {},
   "source": [
    "## 4. Routing Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c32f97",
   "metadata": {},
   "source": [
    "So far, we've learned about large language model chains and sequential chains. But what if we want to do something more complex?\n",
    "\n",
    "A fairly common but basic operation is to take an input and route it to a chain, depending on what exactly that input is. If you have multiple child chains, each specialized for a specific type of input, you can make up a routing chain that first decides which child chain to pass it to, and then passes it to that chain.\n",
    "\n",
    "A router consists of two components:\n",
    "\n",
    "- Router Chain: The router chain itself, which is responsible for selecting the next chain to call\n",
    "\n",
    "- destination_chains: The chains that the router chain can route to\n",
    "\n",
    "For a concrete example, let's look at where we are routing between different types of chains, where we have different prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31b06fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain  #å¯¼å¥å¤æç¤ºé¾\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3f50bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b4708",
   "metadata": {},
   "source": [
    "### 4.1 Defining prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b85285-c736-4a5d-bd14-d9b5025ca29b",
   "metadata": {},
   "source": [
    "First, we define prompt templates that are suitable for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ade83f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first hint is suitable for answering physics questions\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The second prompt is suitable for answering math problems\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The third one is suitable for answering historical questions\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The fourth one is suitable for answering computer questions\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b749b-0d3c-46f2-afab-b1213f208b13",
   "metadata": {},
   "source": [
    "### 4.2 Naming and describing the prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922b35e",
   "metadata": {},
   "source": [
    "After defining these prompt templates, we can name each template and give a specific description. For example, the first physics description is suitable for answering questions about physics, and this information will be passed to the routing chain, which will then decide when to use this sub-chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "141a3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795cd42",
   "metadata": {},
   "source": [
    "LLMRouterChain (This chain uses the LLM to determine how to route things)\n",
    "\n",
    "Here we need a **multi-tip chain**. This is a specific type of chain that is used to route between multiple different tip templates. But this is just one type of routing, we can also route between any type of chain.\n",
    "\n",
    "A couple of the classes we are going to implement here is the large model router chain. This class itself uses the language model to route between different child chains. This is where the description and name provided above will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46633b43",
   "metadata": {},
   "source": [
    "### 4.3 Create the corresponding target chain based on the prompt template information \n",
    "The target chain is the chain called by the routing chain, and each target chain is a language model chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8eefec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba115de",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.4 Create a default target chain\n",
    "In addition to the target chain, we also need a default target chain. This is a chain that is called when the router cannot decide which child chain to use. In the example above, it might be called when the input question has nothing to do with physics, mathematics, history, or computer science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f98018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948700c4",
   "metadata": {},
   "source": [
    "### 4.5 Define routing templates between different chains\n",
    "\n",
    "This includes a description of the task to be completed and the specific format the output should be in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f30c2c",
   "metadata": {},
   "source": [
    "Note: An example is added here based on the original tutorial, mainly because the \"gpt-3.5-turbo\" model is not well adapted to understand the meaning of the template, and using \"text-davinci-003\" or \"gpt-4-0613\" can work well, so more example prompts are added here to make it better to learn.\n",
    "eg:\n",
    "<< INPUT >>\n",
    "\"What is black body radiation?\"\n",
    "<< OUTPUT >>\n",
    "```json\n",
    "{{{{\n",
    "\"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "\"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11b2e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\n",
    "\n",
    "eg:\n",
    "<< INPUT >>\n",
    "\"What is black body radiation?\"\n",
    "<< OUTPUT >>\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c46d0",
   "metadata": {},
   "source": [
    "### 4.6 Building the Routing Chain\n",
    "First, we create the complete router template by formatting the target defined above. This template can be used for many different types of targets.\n",
    "So here, instead of just physics, math, history, and computer science, you could add a different subject like English or Latin.\n",
    "\n",
    "Next, we create the prompt template from this template\n",
    "\n",
    "Finally, we create the routing chain by passing in the llm and the entire routing prompt. It is important to note that there is routing output parsing here, which is important because it will help this link decide which sub-links to route between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1387109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e92355c",
   "metadata": {},
   "source": [
    "### 4.7 Create the overall link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fb7d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple prompt chains\n",
    "chain = MultiPromptChain(router_chain=router_chain,    #lè·¯ç±é¾è·¯\n",
    "                         destination_chains=destination_chains,   #ç®æ é¾è·¯\n",
    "                         default_chain=default_chain,      #é»è®¤é¾è·¯\n",
    "                         verbose=True   \n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086503f7",
   "metadata": {},
   "source": [
    "### 4.8 Asking questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969cd878",
   "metadata": {},
   "source": [
    "If we ask a physical question, we expect to see it routed to the physical link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Black body radiation is the electromagnetic radiation emitted by a perfect black body, which absorbs all incident radiation and reflects none. It is characterized by a continuous spectrum of radiated energy that is dependent on the temperature of the body, with higher temperatures leading to more intense and shorter wavelength radiation. This phenomenon is an important concept in thermal physics and has numerous applications, ranging from understanding stellar spectra to designing artificial light sources.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question: What is blackbody radiation?\n",
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c5ca9",
   "metadata": {},
   "source": [
    "If we ask a math question, we would like to see it routed to the math link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b717379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I can answer this question. The answer to 2 + 2 is 4.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question: What is 2+2?\n",
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186a2b9",
   "metadata": {},
   "source": [
    "What happens if we pass it a question that has nothing to do with any of the sub-links?\n",
    "\n",
    "Here, we asked a question about biology, and we can see that the link it chose is None. This means it will be passed to the default link, which itself is just a generic call to the language model. The language model luckily knows a lot about biology, so it can help us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29e5be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Every cell in our body contains DNA because DNA carries the genetic information that determines the characteristics and functions of each cell. DNA contains the instructions for the synthesis of proteins, which are essential for the structure and function of cells. Additionally, DNA is responsible for the transmission of genetic information from one generation to the next. Therefore, every cell in our body needs DNA to carry out its specific functions and to maintain the integrity of the organism as a whole.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question: Why does every cell in our body contain DNA?\n",
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b5f1d-b010-4f33-a928-d3b9e85bf289",
   "metadata": {},
   "source": [
    "### 4.9 Chinese prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7fade7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "#The first hint is suitable for answering physics questions\n",
    "physics_template = \"\"\"ä½ æ¯ä¸ä¸ªéå¸¸èªæçç©çä¸å®¶ã \\\n",
    "ä½ æé¿ç¨ä¸ç§ç®æ´å¹¶ä¸æäºçè§£çæ¹å¼å»åç­é®é¢ã\\\n",
    "å½ä½ ä¸ç¥éé®é¢çç­æ¡æ¶ï¼ä½ æ¿è®¤\\\n",
    "ä½ ä¸ç¥é.\n",
    "\n",
    "è¿æ¯ä¸ä¸ªé®é¢:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The second hint is suitable for answering math questions\n",
    "math_template = \"\"\"ä½ æ¯ä¸ä¸ªéå¸¸ä¼ç§çæ°å­¦å®¶ã \\\n",
    "ä½ æé¿åç­æ°å­¦é®é¢ã \\\n",
    "ä½ ä¹æä»¥å¦æ­¤ä¼ç§ï¼ \\\n",
    "æ¯å ä¸ºä½ è½å¤å°æ£æçé®é¢åè§£ä¸ºç»æé¨åï¼\\\n",
    "åç­ç»æé¨åï¼ç¶åå°å®ä»¬ç»åå¨ä¸èµ·ï¼åç­æ´å¹¿æ³çé®é¢ã\n",
    "\n",
    "è¿æ¯ä¸ä¸ªé®é¢ï¼\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The third one is suitable for answering historical questions\n",
    "history_template = \"\"\"ä½ æ¯ä»¥ä¸ºéå¸¸ä¼ç§çåå²å­¦å®¶ã \\\n",
    "ä½ å¯¹ä¸ç³»ååå²æ¶æçäººç©ãäºä»¶åèæ¯æçæå¥½çå­¦è¯åçè§£\\\n",
    "ä½ æè½åæèãåæãè¾©è¯ãè®¨è®ºåè¯ä¼°è¿å»ã\\\n",
    "ä½ å°éåå²è¯æ®ï¼å¹¶æè½åå©ç¨å®æ¥æ¯æä½ çè§£éåå¤æ­ã\n",
    "\n",
    "è¿æ¯ä¸ä¸ªé®é¢:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "#The fourth one is suitable for answering computer questions\n",
    "computerscience_template = \"\"\" ä½ æ¯ä¸ä¸ªæåçè®¡ç®æºç§å­¦ä¸å®¶ã\\\n",
    "ä½ æåé åãåä½ç²¾ç¥ã\\\n",
    "åç»æ§æç»´ãèªä¿¡ãè§£å³é®é¢çè½åã\\\n",
    "å¯¹çè®ºåç®æ³ççè§£ä»¥ååºè²çæ²éæå·§ã\\\n",
    "ä½ éå¸¸æé¿åç­ç¼ç¨é®é¢ã\\\n",
    "ä½ ä¹æä»¥å¦æ­¤ä¼ç§ï¼æ¯å ä¸ºä½ ç¥é  \\\n",
    "å¦ä½éè¿ä»¥æºå¨å¯ä»¥è½»æ¾è§£éçå½ä»¤å¼æ­¥éª¤æè¿°è§£å³æ¹æ¡æ¥è§£å³é®é¢ï¼\\\n",
    "å¹¶ä¸ä½ ç¥éå¦ä½éæ©å¨æ¶é´å¤ææ§åç©ºé´å¤ææ§ä¹é´åå¾è¯å¥½å¹³è¡¡çè§£å³æ¹æ¡ã\n",
    "\n",
    "è¿è¿æ¯ä¸ä¸ªè¾å¥ï¼\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "deb8aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"åå­\": \"ç©çå­¦\", \n",
    "        \"æè¿°\": \"æé¿åç­å³äºç©çå­¦çé®é¢\", \n",
    "        \"æç¤ºæ¨¡æ¿\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"åå­\": \"æ°å­¦\", \n",
    "        \"æè¿°\": \"æé¿åç­æ°å­¦é®é¢\", \n",
    "        \"æç¤ºæ¨¡æ¿\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"åå­\": \"åå²\", \n",
    "        \"æè¿°\": \"æé¿åç­åå²é®é¢\", \n",
    "        \"æç¤ºæ¨¡æ¿\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"åå­\": \"è®¡ç®æºç§å­¦\", \n",
    "        \"æè¿°\": \"æé¿åç­è®¡ç®æºç§å­¦é®é¢\", \n",
    "        \"æç¤ºæ¨¡æ¿\": computerscience_template\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6eb641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"åå­\"]\n",
    "    prompt_template = p_info[\"æç¤ºæ¨¡æ¿\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['åå­']}: {p['æè¿°']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af088d6b-a70b-4cd7-bc6e-4ad7b32e6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aae035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "\n",
    "# Multiple Prompt Routing Template\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"ç»è¯­è¨æ¨¡åä¸ä¸ªåå§ææ¬è¾å¥ï¼\\\n",
    "è®©å¶éæ©æéåè¾å¥çæ¨¡åæç¤ºã\\\n",
    "ç³»ç»å°ä¸ºæ¨æä¾å¯ç¨æç¤ºçåç§°ä»¥åæéåæ¹æç¤ºçæè¿°ã\\\n",
    "å¦æä½ è®¤ä¸ºä¿®æ¹åå§è¾å¥æç»ä¼å¯¼è´è¯­è¨æ¨¡åååºæ´å¥½çååºï¼\\\n",
    "ä½ ä¹å¯ä»¥ä¿®æ¹åå§è¾å¥ã\n",
    "\n",
    "\n",
    "<< æ ¼å¼ >>\n",
    "è¿åä¸ä¸ªå¸¦æJSONå¯¹è±¡çmarkdownä»£ç çæ®µï¼è¯¥JSONå¯¹è±¡çæ ¼å¼å¦ä¸ï¼\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": å­ç¬¦ä¸² \\ ä½¿ç¨çæç¤ºåå­æèä½¿ç¨ \"DEFAULT\"\n",
    "    \"next_inputs\": å­ç¬¦ä¸² \\ åå§è¾å¥çæ¹è¿çæ¬\n",
    "}}}}\n",
    "```\n",
    "\n",
    "\n",
    "è®°ä½ï¼âdestinationâå¿é¡»æ¯ä¸é¢æå®çåéæç¤ºåç§°ä¹ä¸ï¼\\\n",
    "æèå¦æè¾å¥ä¸å¤ªéåä»»ä½åéæç¤ºï¼\\\n",
    "åå¯ä»¥æ¯ âDEFAULTâ ã\n",
    "è®°ä½ï¼å¦ææ¨è®¤ä¸ºä¸éè¦ä»»ä½ä¿®æ¹ï¼\\\n",
    "å ânext_inputsâ å¯ä»¥åªæ¯åå§è¾å¥ã\n",
    "\n",
    "<< åéæç¤º >>\n",
    "{destinations}\n",
    "\n",
    "<< è¾å¥ >>\n",
    "{{input}}\n",
    "\n",
    "<< è¾åº (è®°å¾è¦åå« ```json)>>\n",
    "\n",
    "æ ·ä¾:\n",
    "<< è¾å¥ >>\n",
    "\"ä»ä¹æ¯é»ä½è¾å°?\"\n",
    "<< è¾åº >>\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": å­ç¬¦ä¸² \\ ä½¿ç¨çæç¤ºåå­æèä½¿ç¨ \"DEFAULT\"\n",
    "    \"next_inputs\": å­ç¬¦ä¸² \\ åå§è¾å¥çæ¹è¿çæ¬\n",
    "}}}}\n",
    "```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470b25c-ef82-496c-a6e0-e04b99b08e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple prompt chains\n",
    "chain = MultiPromptChain(router_chain=router_chain,    #lè·¯ç±é¾è·¯\n",
    "                         destination_chains=destination_chains,   #ç®æ é¾è·¯\n",
    "                         default_chain=default_chain,      #é»è®¤é¾è·¯\n",
    "                         verbose=True   \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4446724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'ä»ä¹æ¯é»ä½è¾å°ï¼'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'é»ä½è¾å°æ¯æä¸ä¸ªçæ³åçç©ä½ï¼å®è½å¤å®å¨å¸æ¶ææå¥å°å°å®è¡¨é¢çè¾å°è½éï¼å¹¶ä»¥ç­è¾å°çå½¢å¼éæ°åå°åºæ¥ãé»ä½è¾å°çç¹ç¹æ¯å¶è¾å°è½éçåå¸ä¸æ¸©åº¦æå³ï¼éçæ¸©åº¦çåé«ï¼è¾å°è½éçå³°å¼ä¼åæ´ç­çæ³¢é¿æ¹åç§»å¨ãè¿ä¸ªç°è±¡è¢«ç§°ä¸ºé»ä½è¾å°è°±çä½ç§»å®å¾ï¼ç±æ®æåå¨20ä¸çºªåæåºãé»ä½è¾å°å¨ç ç©¶ç­åå­¦ãéå­åå­¦åå®å®å­¦ç­é¢åä¸­å·æéè¦çåºç¨ã'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chinese\n",
    "chain.run(\"ä»ä¹æ¯é»ä½è¾å°ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef81eda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "History: {'input': 'ä½ ç¥éæç½æ¯è°å?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'æç½æ¯åææ¶æçä¸ä½èåè¯äººãä»çè¯æ­ä»¥è±ªæ¾ãå¥æ¾ãèªç±çé£æ ¼èç§°ï¼è¢«èªä¸ºâè¯ä»âãä»çä½åæ¶åå¹¿æ³ï¼åæ¬å±±æ°´ç°å­ãåå²ä¼ è¯´ãå²çæèç­å¤ä¸ªæ¹é¢ï¼å¯¹ä¸­å½å¤å¸æå­¦çåå±äº§çäºæ·±è¿çå½±åã'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "chain.run(\"ä½ ç¥éæç½æ¯è°å?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "795bea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': '2 + 2 ç­äºå¤å°'}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2 + 2 ç­äº 4ã'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "chain.run(\"2 + 2 ç­äºå¤å°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a64d0759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'ä¸ºä»ä¹æä»¬èº«ä½éçæ¯ä¸ªç»èé½åå«DNAï¼'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'æä»¬èº«ä½éçæ¯ä¸ªç»èé½åå«DNAï¼æ¯å ä¸ºDNAæ¯éä¼ ä¿¡æ¯çè½½ä½ãDNAæ¯ç±åç§ç¢±åºï¼èºåå¤ãé¸åå¤ãè¸èºå§å¶åé³åå¤ï¼ç»æçé¿é¾ç¶åå­ï¼å®å­å¨äºçç©ä½çéä¼ ä¿¡æ¯ï¼åæ¬ä¸ªä½çç¹å¾ãçé¿åè²ãä»£è°¢åè½ç­ãæ¯ä¸ªç»èé½éè¦è¿äºéä¼ ä¿¡æ¯æ¥æ§è¡å¶ç¹å®çåè½åä»»å¡ï¼å æ­¤æ¯ä¸ªç»èé½éè¦åå«DNAãæ­¤å¤ï¼DNAè¿è½éè¿å¤å¶åä¼ éç»ä¸ä¸ä»£ç»èåä¸ªä½ï¼ä»¥ä¿è¯éä¼ ä¿¡æ¯çä¼ æ¿ã'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chinese\n",
    "chain.run(\"ä¸ºä»ä¹æä»¬èº«ä½éçæ¯ä¸ªç»èé½åå«DNAï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
