{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46a7a6a-3182-421d-a208-f45eafd1c4ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Appendix 1 Comparison between ChatGPT and ChatGLM\n",
    "\n",
    "There are many large domestic models, such as Wenxin Yiyan, Tongyi Qianwen, Xinghuo, MOSS and ChatGLM, etc., but only MOSS and ChatGLM can be deployed locally and open APIs. MOSS requires too much GPU memory (it needs ```80GB``` without quantization, and multiple rounds of dialogue will still burst the memory), but ChatGLM can be deployed on a laptop (```int4``` version only needs ```6GB``` video memory). So this article uses ChatGLM and ChatGPT to compare and see the advantages and disadvantages of domestic models.\n",
    "\n",
    "This article will select various aspects of this tutorial for comparison, and finally summarize the advantages and disadvantages of ChatGPT and ChatGLM. In addition, this article is also suitable for readers who do not have OpenAI api key. After deploying ``` ChatGLM-6B```, you can also learn the entire course using the functions introduced later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5debf6b",
   "metadata": {},
   "source": [
    "<div class=\"toc\">\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#1 Environment Configuration\" data-toc-modified-id=\"1. Environment Configuration\">1. Environment Configuration</a></span></li>\n",
    "<li><span><a href=\"#2 Text Comprehension\" data-toc-modified-id=\"2. Text Comprehension\">2. Text Comprehension</a></span></li>\n",
    "<li><span><a href=\"#3 Structured Output\" data-toc-modified-id=\"3. Structured Output\">3. Structured Output</a></span></li>\n",
    "<li><span><a href=\"#4 Translation and Conversion\" data-toc-modified-id=\"4. Translation and Conversion\">4. Translation and Conversion</a></span></li>\n",
    "<li><span><a href=\"#5 Logical Reasoning\" data-toc-modified-id=\"5. Logical Reasoning\">5. Logical Reasoning</a></span></li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be808ea-5284-4399-b832-5205c2745d13",
   "metadata": {},
   "source": [
    "## 1. Environment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553056f",
   "metadata": {},
   "source": [
    "### 1.1 ChatGLM environment configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd3834-1cf9-47d5-a94f-351d3645e0f6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;For the configuration of ChatGLM environment, please refer to this article of DataWhale: [ChatGLM-6B Local Deployment Guide! ](https://mp.weixin.qq.com/s/545Z4DTB78q_sLqBq6dC1A)\n",
    "\n",
    "&emsp;&emsp;After deployment, run the `api.py` file in the project. The following is a `get_completion` function similar to ChatGPT that uses ChatGLM's api encapsulation. You only need to pass in the prompt parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a6ab28-e60a-4a67-bbde-4062cc844894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe69c47-ccc4-47db-a0f5-21e273b35fcb",
   "metadata": {},
   "source": [
    "If you don't have the OpenAI key, after deploying ChatGLM-6B, you can also use this function to complete the entire course. Come on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37dfda9c-768c-4c1b-921b-d5e2e03f8663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_completion(prompt):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\"prompt\": prompt, \"history\": []}\n",
    "    response = requests.post(url='http://127.0.0.1:8000', headers=headers, data=json.dumps(data))\n",
    "    return response.json()['response']\n",
    "\n",
    "get_completion(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597ca502-d339-4667-9f7c-4da30ac1f410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_completion_glm(prompt):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\"prompt\": prompt, \"history\": []}\n",
    "    response = requests.post(url='http://127.0.0.1:8000', headers=headers, data=json.dumps(data))\n",
    "    return response.json()['response']\n",
    "\n",
    "get_completion_glm(\"ä½ å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c608de-2293-48df-bb0e-491686e427af",
   "metadata": {},
   "source": [
    "### 1.2 ChatGPT environment configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262731f-870d-4810-8b63-67da58a6a7b8",
   "metadata": {},
   "source": [
    "Import the OpenAI package, load the API key, and define the getCompletion function.\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcff373b-c85d-4f64-b8eb-0042d646de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# Import third-party libraries\n",
    "\n",
    "openai.api_key = \"sk-...\"\n",
    "# Set API_KEY, please replace it with your own API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc891b0-cebd-4235-81d6-6b1fb4cc65b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_completion_gpt(prompt, model=\"gpt-3.5-turbo\", temperature=0): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # å€¼è¶Šä½åˆ™è¾“å‡ºæ–‡æœ¬éšæœºæ€§è¶Šä½\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "get_completion_gpt('ä½ å¥½')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f99c24",
   "metadata": {},
   "source": [
    "## 2. Text comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807a1f5-bdf2-46ab-a77f-59985374e647",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Text Summarization and Conditional Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1925a9e-54d9-4f75-a625-a1698b95e268",
   "metadata": {},
   "source": [
    "#### 2.1.1 Text with steps (meeting input conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b1bc6e-cd39-44dd-b2a5-0f20ce53fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text with steps\n",
    "text_1 = f\"\"\"\n",
    "æ³¡ä¸€æ¯èŒ¶å¾ˆå®¹æ˜“ã€‚é¦–å…ˆï¼Œéœ€è¦æŠŠæ°´çƒ§å¼€ã€‚\\\n",
    "åœ¨ç­‰å¾…æœŸé—´ï¼Œæ‹¿ä¸€ä¸ªæ¯å­å¹¶æŠŠèŒ¶åŒ…æ”¾è¿›å»ã€‚\\\n",
    "ä¸€æ—¦æ°´è¶³å¤Ÿçƒ­ï¼Œå°±æŠŠå®ƒå€’åœ¨èŒ¶åŒ…ä¸Šã€‚\\\n",
    "ç­‰å¾…ä¸€ä¼šå„¿ï¼Œè®©èŒ¶å¶æµ¸æ³¡ã€‚å‡ åˆ†é’Ÿåï¼Œå–å‡ºèŒ¶åŒ…ã€‚\\\n",
    "å¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥åŠ ä¸€äº›ç³–æˆ–ç‰›å¥¶è°ƒå‘³ã€‚\\\n",
    "å°±è¿™æ ·ï¼Œä½ å¯ä»¥äº«å—ä¸€æ¯ç¾å‘³çš„èŒ¶äº†ã€‚\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "æ‚¨å°†è·å¾—ç”±ä¸‰ä¸ªå¼•å·æ‹¬èµ·æ¥çš„æ–‡æœ¬ã€‚\\\n",
    "å¦‚æœå®ƒåŒ…å«ä¸€ç³»åˆ—çš„æŒ‡ä»¤ï¼Œåˆ™éœ€è¦æŒ‰ç…§ä»¥ä¸‹æ ¼å¼é‡æ–°ç¼–å†™è¿™äº›æŒ‡ä»¤ï¼š\n",
    "\n",
    "ç¬¬ä¸€æ­¥ - ...\n",
    "ç¬¬äºŒæ­¥ - â€¦\n",
    "â€¦\n",
    "ç¬¬Næ­¥ - â€¦\n",
    "\n",
    "å¦‚æœæ–‡æœ¬ä¸­ä¸åŒ…å«ä¸€ç³»åˆ—çš„æŒ‡ä»¤ï¼Œåˆ™ç›´æ¥å†™â€œæœªæä¾›æ­¥éª¤â€ã€‚\"\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad49ba0c-38ae-4137-bfdc-207037ea7d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸€æ­¥: æŠŠæ°´çƒ§å¼€\n",
      "ç¬¬äºŒæ­¥: æŠŠèŒ¶åŒ…æ”¾å…¥æ¯å­ä¸­\n",
      "ç¬¬ä¸‰æ­¥: ç­‰å¾…æ°´è¶³å¤Ÿçƒ­\n",
      "ç¬¬å››æ­¥: å€’åœ¨èŒ¶åŒ…ä¸Š\n",
      "ç¬¬äº”æ­¥: ç­‰å¾…èŒ¶å¶æµ¸æ³¡\n",
      "ç¬¬å…­æ­¥: å–å‡ºèŒ¶åŒ…\n",
      "ç¬¬ä¸ƒæ­¥: å¦‚æœå–œæ¬¢,å¯ä»¥åŠ ä¸€äº›ç³–æˆ–ç‰›å¥¶\n",
      "ç¬¬å…«æ­¥: æ…æ‹Œå‡åŒ€\n",
      "ç¬¬ä¹æ­¥: äº«ç”¨ç¾å‘³çš„èŒ¶\n",
      "\n",
      "æœªæä¾›æ­¥éª¤\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd825a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸€æ­¥ - æŠŠæ°´çƒ§å¼€ã€‚\n",
      "ç¬¬äºŒæ­¥ - æ‹¿ä¸€ä¸ªæ¯å­å¹¶æŠŠèŒ¶åŒ…æ”¾è¿›å»ã€‚\n",
      "ç¬¬ä¸‰æ­¥ - æŠŠçƒ­æ°´å€’åœ¨èŒ¶åŒ…ä¸Šã€‚\n",
      "ç¬¬å››æ­¥ - ç­‰å¾…å‡ åˆ†é’Ÿï¼Œè®©èŒ¶å¶æµ¸æ³¡ã€‚\n",
      "ç¬¬äº”æ­¥ - å–å‡ºèŒ¶åŒ…ã€‚\n",
      "ç¬¬å…­æ­¥ - å¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥åŠ ä¸€äº›ç³–æˆ–ç‰›å¥¶è°ƒå‘³ã€‚\n",
      "ç¬¬ä¸ƒæ­¥ - å°±è¿™æ ·ï¼Œä½ å¯ä»¥äº«å—ä¸€æ¯ç¾å‘³çš„èŒ¶äº†ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43edb5-37b5-4d43-9e16-d2a9b558ef73",
   "metadata": {},
   "source": [
    "**Note**: It can be seen here that the text provided is a text with steps. ChatGLM provides the steps, but says \"no steps provided\" at the end. However, ChatGPT provides the steps, and the steps are more complete than ChatGLM, and the answer is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37727f9a",
   "metadata": {},
   "source": [
    "#### 2.1.2 Text without steps (does not meet input conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a4b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text without steps\n",
    "text_2 = f\"\"\"\n",
    "ä»Šå¤©é˜³å…‰æ˜åªšï¼Œé¸Ÿå„¿åœ¨æ­Œå”±ã€‚\\\n",
    "è¿™æ˜¯ä¸€ä¸ªå»å…¬å›­æ•£æ­¥çš„ç¾å¥½æ—¥å­ã€‚\\\n",
    "é²œèŠ±ç››å¼€ï¼Œæ ‘æåœ¨å¾®é£ä¸­è½»è½»æ‘‡æ›³ã€‚\\\n",
    "äººä»¬å¤–å‡ºäº«å—ç€è¿™ç¾å¥½çš„å¤©æ°”ï¼Œæœ‰äº›äººåœ¨é‡é¤ï¼Œæœ‰äº›äººåœ¨ç©æ¸¸æˆæˆ–è€…åœ¨è‰åœ°ä¸Šæ”¾æ¾ã€‚\\\n",
    "è¿™æ˜¯ä¸€ä¸ªå®Œç¾çš„æ—¥å­ï¼Œå¯ä»¥åœ¨æˆ·å¤–åº¦è¿‡å¹¶æ¬£èµå¤§è‡ªç„¶çš„ç¾æ™¯ã€‚\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "æ‚¨å°†è·å¾—ç”±ä¸‰ä¸ªå¼•å·æ‹¬èµ·æ¥çš„æ–‡æœ¬ã€‚\\\n",
    "å¦‚æœå®ƒåŒ…å«ä¸€ç³»åˆ—çš„æŒ‡ä»¤ï¼Œåˆ™éœ€è¦æŒ‰ç…§ä»¥ä¸‹æ ¼å¼é‡æ–°ç¼–å†™è¿™äº›æŒ‡ä»¤ï¼š\n",
    "\n",
    "ç¬¬ä¸€æ­¥ - ...\n",
    "ç¬¬äºŒæ­¥ - â€¦\n",
    "â€¦\n",
    "ç¬¬Næ­¥ - â€¦\n",
    "\n",
    "å¦‚æœæ–‡æœ¬ä¸­ä¸åŒ…å«ä¸€ç³»åˆ—çš„æŒ‡ä»¤ï¼Œåˆ™ç›´æ¥å†™â€œæœªæä¾›æ­¥éª¤â€ã€‚\"\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0d5ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸€æ­¥: å»å…¬å›­æ•£æ­¥\n",
      "ç¬¬äºŒæ­¥: æ¬£èµé²œèŠ±å’Œæ ‘æ\n",
      "ç¬¬ä¸‰æ­¥: æ„Ÿå—å¾®é£\n",
      "ç¬¬å››æ­¥: æ”¾æ¾èº«å¿ƒ\n",
      "ç¬¬äº”æ­¥: é‡é¤æˆ–ç©æ¸¸æˆ\n",
      "ç¬¬å…­æ­¥: æ„Ÿå—å¤§è‡ªç„¶çš„ç¾ä¸½\n",
      "\n",
      "æœªæä¾›æ­¥éª¤ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a58b936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœªæä¾›æ­¥éª¤ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a5d02-0284-48fb-a22e-19b9d343ef65",
   "metadata": {},
   "source": [
    "**Note:** A step-free text was provided, but ChatGLM answered with a step and said no step at the end, which is almost the same as the step-based text answer above. ChatCPT directly gave the answer of \"no step provided\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f0fb0",
   "metadata": {},
   "source": [
    "### 2.2 Provide a few examples of text continuation (Few-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7371e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "ä½ çš„ä»»åŠ¡æ˜¯ä»¥ä¸€è‡´çš„é£æ ¼å›ç­”é—®é¢˜ã€‚\n",
    "\n",
    "<å­©å­>: æ•™æˆ‘è€å¿ƒã€‚\n",
    "\n",
    "<ç¥–çˆ¶æ¯>: æŒ–å‡ºæœ€æ·±å³¡è°·çš„æ²³æµæºäºä¸€å¤„ä¸èµ·çœ¼çš„æ³‰çœ¼ï¼›æœ€å®ä¼Ÿçš„äº¤å“ä¹ä»å•ä¸€çš„éŸ³ç¬¦å¼€å§‹ï¼›æœ€å¤æ‚çš„æŒ‚æ¯¯ä»¥ä¸€æ ¹å­¤ç‹¬çš„çº¿å¼€å§‹ç¼–ç»‡ã€‚\n",
    "\n",
    "<å­©å­>: æ•™æˆ‘éŸ§æ€§ã€‚\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b02cefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "éŸ§æ€§æ˜¯ä¸€ç§å¼ºå¤§çš„å“è´¨,å¯ä»¥å¸®åŠ©äººä»¬åœ¨æŒ‘æˆ˜ä¸­ä¿æŒå†·é™å’ŒåšéŸ§ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åŸ¹å…»éŸ§æ€§çš„æ–¹æ³•:\n",
      "\n",
      "1. æ¥å—æŒ‘æˆ˜å¹¶åšå¥½å‡†å¤‡:éŸ§æ€§éœ€è¦é¢å¯¹æŒ‘æˆ˜å¹¶å…‹æœå›°éš¾ã€‚è¦åŸ¹å…»éŸ§æ€§,éœ€è¦å…ˆå‡†å¤‡å¥½æ¥å—æŒ‘æˆ˜,å¹¶é‡‡å–å¿…è¦çš„æªæ–½æ¥è§£å†³é—®é¢˜ã€‚\n",
      "\n",
      "2. å­¦ä¼šæ”¾æ¾:å½“æˆ‘ä»¬æ„Ÿåˆ°ç´§å¼ å’Œç„¦è™‘æ—¶,å¯èƒ½ä¼šå¤±å»éŸ§æ€§ã€‚å­¦ä¼šæ”¾æ¾è‡ªå·±,å¯ä»¥å¸®åŠ©ç¼“è§£å‹åŠ›å’Œç„¦è™‘,ä»è€Œæé«˜éŸ§æ€§ã€‚\n",
      "\n",
      "3. åŸ¹å…»è‡ªæˆ‘æ„è¯†:éŸ§æ€§éœ€è¦äº†è§£è‡ªå·±çš„å¼±ç‚¹å’Œä¼˜ç‚¹,å¹¶ç§¯ææ”¹å–„è‡ªå·±ã€‚å­¦ä¼šå…³æ³¨è‡ªå·±çš„æƒ…ç»ªå’Œè¡Œä¸º,å¹¶åŠªåŠ›æ”¹å–„è‡ªå·±çš„éŸ§æ€§ã€‚\n",
      "\n",
      "4. å­¦ä¹ æ²Ÿé€šæŠ€å·§:æ²Ÿé€šæ˜¯å½±å“éŸ§æ€§çš„é‡è¦å› ç´ ã€‚å­¦ä¹ æœ‰æ•ˆçš„æ²Ÿé€šæŠ€å·§,å¯ä»¥å¸®åŠ©æ›´å¥½åœ°è¡¨è¾¾è‡ªå·±çš„æ„è§å’Œæƒ³æ³•,åŒæ—¶ä¹Ÿå¯ä»¥å¸®åŠ©æ›´å¥½åœ°ç†è§£ä»–äººçš„è§‚ç‚¹ã€‚\n",
      "\n",
      "5. åšæŒé”»ç‚¼å’Œå¥åº·é¥®é£Ÿ:é”»ç‚¼å’Œå¥åº·é¥®é£Ÿå¯ä»¥å¸®åŠ©ä¿æŒèº«ä½“å¥åº·å’Œç²¾ç¥å¥åº·,ä»è€Œæé«˜éŸ§æ€§ã€‚\n",
      "\n",
      "å¸Œæœ›è¿™äº›æ–¹æ³•å¯ä»¥å¸®åŠ©åŸ¹å…»éŸ§æ€§ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc62b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ç¥–çˆ¶æ¯>: éŸ§æ€§å°±åƒæ˜¯ä¸€æ£µæ ‘ï¼Œå®ƒéœ€è¦ç»å†é£å¹é›¨æ‰“ã€æ—¥æ™’é£å¹²ï¼Œæ‰èƒ½æˆé•¿å¾—æ›´åŠ åšå¼ºã€‚æ‰€ä»¥ï¼Œå½“ä½ é‡åˆ°æŒ«æŠ˜å’Œå›°éš¾æ—¶ï¼Œä¸è¦è½»æ˜“æ”¾å¼ƒï¼Œè¦åšæŒä¸‹å»ï¼Œç›¸ä¿¡è‡ªå·±çš„èƒ½åŠ›å’Œæ½œåŠ›ï¼Œæœ€ç»ˆä½ ä¼šæˆä¸ºä¸€æ£µèŒå£®æˆé•¿çš„å¤§æ ‘ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d968f-41da-4f68-beef-a50800944254",
   "metadata": {},
   "source": [
    "**Note:** I asked you to imitate, not to surpass! It can be seen that ChatGLM's answer has almost nothing to do with the small amount of sample text provided, while ChatGPT continues to write according to the provided sample model. ChatGPT is a thin piece of ChatGLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdbe63f",
   "metadata": {},
   "source": [
    "### 2.3 Focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73948cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_review_zh = \"\"\"\n",
    "è¿™ä¸ªç†ŠçŒ«å…¬ä»”æ˜¯æˆ‘ç»™å¥³å„¿çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œå¥¹å¾ˆå–œæ¬¢ï¼Œå»å“ªéƒ½å¸¦ç€ã€‚\n",
    "å…¬ä»”å¾ˆè½¯ï¼Œè¶…çº§å¯çˆ±ï¼Œé¢éƒ¨è¡¨æƒ…ä¹Ÿå¾ˆå’Œå–„ã€‚ä½†æ˜¯ç›¸æ¯”äºä»·é’±æ¥è¯´ï¼Œ\n",
    "å®ƒæœ‰ç‚¹å°ï¼Œæˆ‘æ„Ÿè§‰åœ¨åˆ«çš„åœ°æ–¹ç”¨åŒæ ·çš„ä»·é’±èƒ½ä¹°åˆ°æ›´å¤§çš„ã€‚\n",
    "å¿«é€’æ¯”é¢„æœŸæå‰äº†ä¸€å¤©åˆ°è´§ï¼Œæ‰€ä»¥åœ¨é€ç»™å¥³å„¿ä¹‹å‰ï¼Œæˆ‘è‡ªå·±ç©äº†ä¼šã€‚\n",
    "\"\"\"\n",
    "# Focus on transportation\n",
    "prompt = f\"\"\"\n",
    "ä½ çš„ä»»åŠ¡æ˜¯ä»ç”µå­å•†åŠ¡ç½‘ç«™ä¸Šç”Ÿæˆä¸€ä¸ªäº§å“è¯„è®ºçš„ç®€çŸ­æ‘˜è¦ã€‚\n",
    "\n",
    "è¯·å¯¹ä¸‰ä¸ªåå¼•å·ä¹‹é—´çš„è¯„è®ºæ–‡æœ¬è¿›è¡Œæ¦‚æ‹¬ï¼Œæœ€å¤š30ä¸ªè¯æ±‡ï¼Œå¹¶ä¸”èšç„¦åœ¨äº§å“è¿è¾“ä¸Šã€‚\n",
    "\n",
    "è¯„è®º: ```{prod_review_zh}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a151b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯„è®º: ç†ŠçŒ«å…¬ä»”é€ç»™å¥³å„¿ç”Ÿæ—¥ç¤¼ç‰©,å¥¹å¾ˆå–œæ¬¢ã€‚å…¬ä»”è½¯ã€å¯çˆ±,é¢éƒ¨è¡¨æƒ…å’Œå–„ã€‚ä½†å°,æ„Ÿè§‰åœ¨å…¶ä»–åœ°æ–¹ç”¨åŒæ ·çš„ä»·é’±èƒ½ä¹°åˆ°æ›´å¤§ã€‚å¿«é€’æ¯”é¢„æœŸæå‰äº†ä¸€å¤©åˆ°è´§,è‡ªå·±ç©äº†ä¼šã€‚\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d9715b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¿«é€’æå‰ä¸€å¤©åˆ°è´§ï¼Œç†ŠçŒ«å…¬ä»”å¾ˆè½¯å¯çˆ±ï¼Œä½†æœ‰ç‚¹å°ï¼Œä»·é’±ä¸å¤ªåˆ’ç®—ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe00c2a-f8e6-4531-8077-33b50de7dba7",
   "metadata": {},
   "source": [
    "**Note:** Let it focus on transportation. ChatGLM even puts the content of transportation at the end of the answer, while ChatGPT puts the transportation part at the front to show emphasis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b64ec6e",
   "metadata": {},
   "source": [
    "### 2.4 Extracting key information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8244e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "ä½ çš„ä»»åŠ¡æ˜¯ä»ç”µå­å•†åŠ¡ç½‘ç«™ä¸Šçš„äº§å“è¯„è®ºä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚\n",
    "\n",
    "è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªåå¼•å·ä¹‹é—´çš„è¯„è®ºæ–‡æœ¬ä¸­æå–äº§å“è¿è¾“ç›¸å…³çš„ä¿¡æ¯ï¼Œæœ€å¤š30ä¸ªè¯æ±‡ã€‚\n",
    "\n",
    "è¯„è®º: ```{prod_review_zh}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ee29821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¿è¾“æ—¶é—´:å¿«é€’æ¯”é¢„æœŸæå‰äº†ä¸€å¤©åˆ°è´§,ç†ŠçŒ«å…¬ä»”å°å·§,å¿«é€’è¿‡ç¨‹å®‰å…¨é¡ºåˆ©ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17fa8390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¿«é€’æ¯”é¢„æœŸæå‰äº†ä¸€å¤©åˆ°è´§ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc52af4-bf0e-4592-9292-ed238233a195",
   "metadata": {},
   "source": [
    "**Note:** Yes, yes, both ChatGLM and ChatGPT extract the shipping information, and ChatGLM even goes a little further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07bb807-0a6e-43e6-b8a8-a597c42a6753",
   "metadata": {},
   "source": [
    "### 2.5 Summary\n",
    "\n",
    "- In terms of text comprehension, the gap between ChatGLM and ChatGPT is a bit large. First of all, for step text, ChatGLM gives steps regardless of whether the text provided has steps or not. ChatGPT gives steps if there are steps, and does not give steps if there are no steps.\n",
    "\n",
    "- In terms of sample text continuation, ChatGLM directly abandons the few examples provided and lets itself go, based on the principle of imitating and surpassing. ChatGPT gives answers based on the few examples provided.\n",
    "\n",
    "- In terms of key information extraction, ChatGLM performs poorly and is not very good. ChatGPT meets my requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852532a-d1fb-44eb-87d5-8f95aa3e1606",
   "metadata": {},
   "source": [
    "## 3. Structured output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9370dc",
   "metadata": {},
   "source": [
    "### 3.1 Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0106c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "åœ¨ä¸€ä¸ªè¿·äººçš„æ‘åº„é‡Œï¼Œå…„å¦¹æ°å…‹å’Œå‰å°”å‡ºå‘å»ä¸€ä¸ªå±±é¡¶äº•é‡Œæ‰“æ°´ã€‚\\\n",
    "ä»–ä»¬ä¸€è¾¹å”±ç€æ¬¢ä¹çš„æ­Œï¼Œä¸€è¾¹å¾€ä¸Šçˆ¬ï¼Œ\\\n",
    "ç„¶è€Œä¸å¹¸é™ä¸´â€”â€”æ°å…‹ç»Šäº†ä¸€å—çŸ³å¤´ï¼Œä»å±±ä¸Šæ»šäº†ä¸‹æ¥ï¼Œå‰å°”ç´§éšå…¶åã€‚\\\n",
    "è™½ç„¶ç•¥æœ‰äº›æ‘”ä¼¤ï¼Œä½†ä»–ä»¬è¿˜æ˜¯å›åˆ°äº†æ¸©é¦¨çš„å®¶ä¸­ã€‚\\\n",
    "å°½ç®¡å‡ºäº†è¿™æ ·çš„æ„å¤–ï¼Œä»–ä»¬çš„å†’é™©ç²¾ç¥ä¾ç„¶æ²¡æœ‰å‡å¼±ï¼Œç»§ç»­å……æ»¡æ„‰æ‚¦åœ°æ¢ç´¢ã€‚\n",
    "\"\"\"\n",
    "# example\n",
    "prompt = f\"\"\"\n",
    "1-ç”¨ä¸€å¥è¯æ¦‚æ‹¬ä¸‹é¢ç”¨<>æ‹¬èµ·æ¥çš„æ–‡æœ¬ã€‚\n",
    "2-å°†æ‘˜è¦ç¿»è¯‘æˆè‹±è¯­ã€‚\n",
    "3-åœ¨è‹±è¯­æ‘˜è¦ä¸­åˆ—å‡ºæ¯ä¸ªåç§°ã€‚\n",
    "4-è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸‹é”®ï¼šEnglish_summaryï¼Œnum_namesã€‚\n",
    "\n",
    "è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š\n",
    "æ–‡æœ¬ï¼š<è¦æ€»ç»“çš„æ–‡æœ¬>\n",
    "æ‘˜è¦ï¼š<æ‘˜è¦>\n",
    "ç¿»è¯‘ï¼š<æ‘˜è¦çš„ç¿»è¯‘>\n",
    "åç§°ï¼š<è‹±è¯­æ‘˜è¦ä¸­çš„åç§°åˆ—è¡¨>\n",
    "è¾“å‡º JSONï¼š<å¸¦æœ‰ English_summary å’Œ num_names çš„ JSON>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c5c885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‘˜è¦:\n",
      "\n",
      "åœ¨ä¸€ä¸ªè¿·äººçš„æ‘åº„é‡Œ,å…„å¦¹æ°å…‹å’Œå‰å°”å‡ºå‘å»ä¸€ä¸ªå±±é¡¶äº•é‡Œæ‰“æ°´ã€‚ä»–ä»¬ä¸€è¾¹å”±ç€æ¬¢ä¹çš„æ­Œ,ä¸€è¾¹å¾€ä¸Šçˆ¬,ç„¶è€Œä¸å¹¸é™ä¸´â€”â€”æ°å…‹ç»Šäº†ä¸€å—çŸ³å¤´,ä»å±±ä¸Šæ»šäº†ä¸‹æ¥,å‰å°”ç´§éšå…¶åã€‚è™½ç„¶ç•¥æœ‰äº›æ‘”ä¼¤,ä½†ä»–ä»¬è¿˜æ˜¯å›åˆ°äº†æ¸©é¦¨çš„å®¶ä¸­ã€‚å°½ç®¡å‡ºäº†è¿™æ ·çš„æ„å¤–,ä»–ä»¬çš„å†’é™©ç²¾ç¥ä¾ç„¶æ²¡æœ‰å‡å¼±,ç»§ç»­å……æ»¡æ„‰æ‚¦åœ°æ¢ç´¢ã€‚\n",
      "\n",
      "ç¿»è¯‘:\n",
      "\n",
      "In a charming village,å…„å¦¹æ°å…‹ andå‰å°” went to aå±±é¡¶ well to fetch water. They were singing joyfully as they reached the top, but something went wrong -æ°å…‹ stumbled and fell from the mountain, whileå‰å°” followed closely. Although they were slightly injured, they returned home feeling brave andæ„‰æ‚¦. Although they encountered such an unexpected situation, their spirit of adventure did not dim, and they continued to explore with joy.\n",
      "\n",
      "åç§°:\n",
      "\n",
      "The names in the English summary are: Jack,å‰å°”, and their mother.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0090af5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‘˜è¦ï¼šå…„å¦¹æ°å…‹å’Œå‰å°”åœ¨è¿·äººçš„æ‘åº„é‡Œå†’é™©ï¼Œä¸å¹¸æ‘”ä¼¤åå›åˆ°å®¶ä¸­ï¼Œä½†ä»ç„¶å……æ»¡å†’é™©ç²¾ç¥ã€‚\n",
      "ç¿»è¯‘ï¼šIn a charming village, siblings Jack and Jill set out to fetch water from a mountaintop well. While climbing and singing, Jack trips on a stone and tumbles down the mountain, with Jill following closely behind. Despite some bruises, they make it back home safely. Their adventurous spirit remains undiminished as they continue to explore with joy.\n",
      "åç§°ï¼šJackï¼ŒJill\n",
      "è¾“å‡º JSONï¼š{\"English_summary\": \"In a charming village, siblings Jack and Jill set out to fetch water from a mountaintop well. While climbing and singing, Jack trips on a stone and tumbles down the mountain, with Jill following closely behind. Despite some bruises, they make it back home safely. Their adventurous spirit remains undiminished as they continue to explore with joy.\", \"num_names\": 2}\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6f8c4-e649-4dd5-9b1c-46d724f92f7b",
   "metadata": {},
   "source": [
    "**Note:** It can be seen that ChatGLM completely ignores the `output json object` in the prompt, and the summary of this output is like repeating it again, and some Chinese translations are not fully translated. ChatGPT's answer meets the requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7c59f",
   "metadata": {},
   "source": [
    "### 3.2 Extracting structured information from customer reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6372802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "lamp_review_zh = \"\"\"\n",
    "æˆ‘éœ€è¦ä¸€ç›æ¼‚äº®çš„å§å®¤ç¯ï¼Œè¿™æ¬¾ç¯å…·æœ‰é¢å¤–çš„å‚¨ç‰©åŠŸèƒ½ï¼Œä»·æ ¼ä¹Ÿä¸ç®—å¤ªé«˜ã€‚\\\n",
    "æˆ‘å¾ˆå¿«å°±æ”¶åˆ°äº†å®ƒã€‚åœ¨è¿è¾“è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„ç¯ç»³æ–­äº†ï¼Œä½†æ˜¯å…¬å¸å¾ˆä¹æ„å¯„é€äº†ä¸€ä¸ªæ–°çš„ã€‚\\\n",
    "å‡ å¤©åå°±æ”¶åˆ°äº†ã€‚è¿™æ¬¾ç¯å¾ˆå®¹æ˜“ç»„è£…ã€‚æˆ‘å‘ç°å°‘äº†ä¸€ä¸ªé›¶ä»¶ï¼Œäºæ˜¯è”ç³»äº†ä»–ä»¬çš„å®¢æœï¼Œä»–ä»¬å¾ˆå¿«å°±ç»™æˆ‘å¯„æ¥äº†ç¼ºå¤±çš„é›¶ä»¶ï¼\\\n",
    "åœ¨æˆ‘çœ‹æ¥ï¼ŒLumina æ˜¯ä¸€å®¶éå¸¸å…³å¿ƒé¡¾å®¢å’Œäº§å“çš„ä¼˜ç§€å…¬å¸ï¼\n",
    "\"\"\"\n",
    "# Chinese\n",
    "prompt = f\"\"\"\n",
    "ä»è¯„è®ºæ–‡æœ¬ä¸­è¯†åˆ«ä»¥ä¸‹é¡¹ç›®ï¼š\n",
    "- è¯„è®ºè€…è´­ä¹°çš„ç‰©å“\n",
    "- åˆ¶é€ è¯¥ç‰©å“çš„å…¬å¸\n",
    "\n",
    "è¯„è®ºæ–‡æœ¬ç”¨ä¸‰ä¸ªåå¼•å·åˆ†éš”ã€‚å°†ä½ çš„å“åº”æ ¼å¼åŒ–ä¸ºä»¥ â€œç‰©å“â€ å’Œ â€œå“ç‰Œâ€ ä¸ºé”®çš„ JSON å¯¹è±¡ã€‚\n",
    "å¦‚æœä¿¡æ¯ä¸å­˜åœ¨ï¼Œè¯·ä½¿ç”¨ â€œæœªçŸ¥â€ ä½œä¸ºå€¼ã€‚\n",
    "è®©ä½ çš„å›åº”å°½å¯èƒ½ç®€çŸ­ã€‚\n",
    "  \n",
    "è¯„è®ºæ–‡æœ¬: ```{lamp_review_zh}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd30e290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç‰©å“:å§å®¤ç¯\n",
      "å“ç‰Œ:Lumina\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef715b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ç‰©å“\": \"å§å®¤ç¯\",\n",
      "  \"å“ç‰Œ\": \"Lumina\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd113e-2ffb-4828-a03a-a7d1c78b82d8",
   "metadata": {},
   "source": [
    "**Note:** ChatGLM successfully extracted information! Verbally praised once, but did not output according to the json object, verbally criticized once. ChatGPT did a good job, praised once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edd035",
   "metadata": {},
   "source": [
    "### 3.3 Extract multiple pieces of information at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec787579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese\n",
    "prompt = f\"\"\"\n",
    "ä»è¯„è®ºæ–‡æœ¬ä¸­è¯†åˆ«ä»¥ä¸‹é¡¹ç›®ï¼š\n",
    "- æƒ…ç»ªï¼ˆæ­£é¢æˆ–è´Ÿé¢ï¼‰\n",
    "- å®¡ç¨¿äººæ˜¯å¦è¡¨è¾¾äº†æ„¤æ€’ï¼Ÿï¼ˆæ˜¯æˆ–å¦ï¼‰\n",
    "- è¯„è®ºè€…è´­ä¹°çš„ç‰©å“\n",
    "- åˆ¶é€ è¯¥ç‰©å“çš„å…¬å¸\n",
    "\n",
    "è¯„è®ºç”¨ä¸‰ä¸ªåå¼•å·åˆ†éš”ã€‚å°†æ‚¨çš„å“åº”æ ¼å¼åŒ–ä¸º JSON å¯¹è±¡ï¼Œä»¥ â€œSentimentâ€ã€â€œAngerâ€ã€â€œItemâ€ å’Œ â€œBrandâ€ ä½œä¸ºé”®ã€‚\n",
    "å¦‚æœä¿¡æ¯ä¸å­˜åœ¨ï¼Œè¯·ä½¿ç”¨ â€œæœªçŸ¥â€ ä½œä¸ºå€¼ã€‚\n",
    "è®©ä½ çš„å›åº”å°½å¯èƒ½ç®€çŸ­ã€‚\n",
    "å°† Anger å€¼æ ¼å¼åŒ–ä¸ºå¸ƒå°”å€¼ã€‚\n",
    "\n",
    "è¯„è®ºæ–‡æœ¬: ```{lamp_review_zh}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c008c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: æ­£é¢\n",
      "Anger: å¦\n",
      "Item: å§å®¤ç¯\n",
      "Brand:mina\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5f91d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Sentiment\": \"æ­£é¢\",\n",
      "  \"Anger\": false,\n",
      "  \"Item\": \"å§å®¤ç¯\",\n",
      "  \"Brand\": \"Lumina\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff470ae-7110-4e97-8e8b-45835af17df4",
   "metadata": {},
   "source": [
    "**Note:** ChatGLM does extract information without any problem, but it still does not convert it into json object output. And `Anger` does not give a boolean value, which is a deduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f5442-9b64-4e0a-a370-b34f51067c3a",
   "metadata": {},
   "source": [
    "### 3.4 Summary\n",
    "\n",
    "Extract information + structured output, ChatGLM can basically only extract information, and does not implement output json objects. The ability needs to be improved. I wonder how the ChatGLM -130B version is? I hope it can be better, come on~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0085689-c1f1-4cfa-ae1c-714731c02a3a",
   "metadata": {},
   "source": [
    "## 4. Translation and conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b817b",
   "metadata": {},
   "source": [
    "### 4.1 Multilingual Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1db0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "è¯·å°†ä»¥ä¸‹æ–‡æœ¬åˆ†åˆ«ç¿»è¯‘æˆä¸­æ–‡ã€è‹±æ–‡ã€æ³•è¯­å’Œè¥¿ç­ç‰™è¯­: \n",
    "```I want to order a basketball.```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df001eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```æˆ‘æƒ³è®¢è´­ä¸€æŠŠç¯®çƒã€‚```\n",
      "\n",
      "ä¸­æ–‡:æˆ‘æƒ³è®¢è´­ä¸€æŠŠç¯®çƒã€‚\n",
      "è‹±æ–‡:I want to order a basketball.\n",
      "æ³•è¯­:Je veux orderir un basketball.\n",
      "è¥¿ç­ç‰™è¯­:Vuelgo a intentar orderar un basketball.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24b29ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸­æ–‡ï¼šæˆ‘æƒ³è®¢è´­ä¸€ä¸ªç¯®çƒã€‚\n",
      "è‹±æ–‡ï¼šI want to order a basketball.\n",
      "æ³•è¯­ï¼šJe veux commander un ballon de basket.\n",
      "è¥¿ç­ç‰™è¯­ï¼šQuiero pedir una pelota de baloncesto.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422cb54-6153-4bf5-bdbe-c87d0780cfb6",
   "metadata": {},
   "source": [
    "**Note:** I have limited knowledge, and the French and Spanish translations were checked using Youdao Translate. ChatGLM and ChatGPT's translations are both correct. Big win!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb18fc",
   "metadata": {},
   "source": [
    "### 4.2 Translation + Formal Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aac408fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼Œåˆ†åˆ«å±•ç¤ºæˆæ­£å¼ä¸éæ­£å¼ä¸¤ç§è¯­æ°”: \n",
    "```Would you like to order a pillow?```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a760260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``` Would you like to order a pillow?```\n",
      "\n",
      "æ­£å¼è¯­æ°”:\n",
      "\n",
      "æˆ‘æƒ³è®¢è´­ä¸€å¼ æ•å¤´ã€‚\n",
      "\n",
      "éæ­£å¼è¯­æ°”:\n",
      "\n",
      "å˜¿,ä½ æƒ³é¢„è®¢ä¸€å¼ æ•å¤´å—?\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "729355da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£å¼è¯­æ°”ï¼šè¯·é—®æ‚¨éœ€è¦è®¢è´­æ•å¤´å—ï¼Ÿ\n",
      "éæ­£å¼è¯­æ°”ï¼šä½ è¦ä¸è¦è®¢ä¸€ä¸ªæ•å¤´ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc886170-3b7d-484a-b79c-e7cad453109d",
   "metadata": {},
   "source": [
    "**Note:** Both tones, ChatGLM and ChatGPT, gave good answers and both received extra points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fd232-34fa-4c04-80db-ac6698740f20",
   "metadata": {},
   "source": [
    "### 4.3 Summary\n",
    "\n",
    "In terms of translation, ChatGLM is almost the same as ChatGPT, and can even be said to be a little better. From another perspective, it is not bad to deploy a ChatGLM-int4 locally for translation. At least the locally deployed API is free!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a0225",
   "metadata": {},
   "source": [
    "## 5. Logical Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cae8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "è¯·åˆ¤æ–­å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆæ˜¯å¦æ­£ç¡®ï¼Œè¯·é€šè¿‡å¦‚ä¸‹æ­¥éª¤è§£å†³è¿™ä¸ªé—®é¢˜ï¼š\n",
    "\n",
    "æ­¥éª¤ï¼š\n",
    "\n",
    "    é¦–å…ˆï¼Œè‡ªå·±è§£å†³é—®é¢˜ã€‚\n",
    "    ç„¶åå°†ä½ çš„è§£å†³æ–¹æ¡ˆä¸å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯„ä¼°å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚åœ¨è‡ªå·±å®Œæˆé—®é¢˜ä¹‹å‰ï¼Œè¯·å‹¿å†³å®šå­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆæ˜¯å¦æ­£ç¡®ã€‚\n",
    "\n",
    "ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š\n",
    "\n",
    "    é—®é¢˜ï¼šé—®é¢˜æ–‡æœ¬\n",
    "    å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆï¼šå­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆæ–‡æœ¬\n",
    "    å®é™…è§£å†³æ–¹æ¡ˆå’Œæ­¥éª¤ï¼šå®é™…è§£å†³æ–¹æ¡ˆå’Œæ­¥éª¤æ–‡æœ¬\n",
    "    å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆå’Œå®é™…è§£å†³æ–¹æ¡ˆæ˜¯å¦ç›¸åŒï¼šæ˜¯æˆ–å¦\n",
    "    å­¦ç”Ÿçš„æˆç»©ï¼šæ­£ç¡®æˆ–ä¸æ­£ç¡®\n",
    "\n",
    "é—®é¢˜ï¼š\n",
    "\n",
    "    æˆ‘æ­£åœ¨å»ºé€ ä¸€ä¸ªå¤ªé˜³èƒ½å‘ç”µç«™ï¼Œéœ€è¦å¸®åŠ©è®¡ç®—è´¢åŠ¡ã€‚ \n",
    "    - åœŸåœ°è´¹ç”¨ä¸ºæ¯å¹³æ–¹è‹±å°º100ç¾å…ƒ\n",
    "    - æˆ‘å¯ä»¥ä»¥æ¯å¹³æ–¹è‹±å°º250ç¾å…ƒçš„ä»·æ ¼è´­ä¹°å¤ªé˜³èƒ½ç”µæ± æ¿\n",
    "    - æˆ‘å·²ç»è°ˆåˆ¤å¥½äº†ç»´æŠ¤åˆåŒï¼Œæ¯å¹´éœ€è¦æ”¯ä»˜å›ºå®šçš„10ä¸‡ç¾å…ƒï¼Œå¹¶é¢å¤–æ”¯ä»˜æ¯å¹³æ–¹è‹±å°º10ç¾å…ƒ\n",
    "    ä½œä¸ºå¹³æ–¹è‹±å°ºæ•°çš„å‡½æ•°ï¼Œé¦–å¹´è¿è¥çš„æ€»è´¹ç”¨æ˜¯å¤šå°‘ã€‚\n",
    "\n",
    "å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆï¼š\n",
    "\n",
    "    è®¾xä¸ºå‘ç”µç«™çš„å¤§å°ï¼Œå•ä½ä¸ºå¹³æ–¹è‹±å°ºã€‚\n",
    "    è´¹ç”¨ï¼š\n",
    "    1. åœŸåœ°è´¹ç”¨ï¼š100x\n",
    "    2. å¤ªé˜³èƒ½ç”µæ± æ¿è´¹ç”¨ï¼š250x\n",
    "    3. ç»´æŠ¤è´¹ç”¨ï¼š100,000+100x\n",
    "    æ€»è´¹ç”¨ï¼š100x+250x+100,000+100x=450x+100,000\n",
    "\n",
    "å®é™…è§£å†³æ–¹æ¡ˆå’Œæ­¥éª¤ï¼š\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b5d7427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®é™…è§£å†³æ–¹æ¡ˆå’Œæ­¥éª¤ä¸å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆç›¸åŒã€‚\n",
      "\n",
      "å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆå¯ä»¥ç®€åŒ–ä¸º:\n",
      "\n",
      "æ€»è´¹ç”¨ = åœŸåœ°è´¹ç”¨ + å¤ªé˜³èƒ½ç”µæ± æ¿è´¹ç”¨ + ç»´æŠ¤è´¹ç”¨\n",
      "\n",
      "æ ¹æ®é¢˜ç›®ä¸­çš„æ•°æ®,åœŸåœ°è´¹ç”¨ä¸ºæ¯å¹³æ–¹è‹±å°º100ç¾å…ƒ,å¤ªé˜³èƒ½ç”µæ± æ¿è´¹ç”¨ä¸ºæ¯å¹³æ–¹è‹±å°º250ç¾å…ƒ,ç»´æŠ¤è´¹ç”¨ä¸ºæ¯å¹´10ä¸‡ç¾å…ƒå’Œæ¯å¹³æ–¹è‹±å°º10ç¾å…ƒ,å› æ­¤:\n",
      "\n",
      "æ€»è´¹ç”¨ = 100x + 250x + 100,000 + 10x\n",
      "\n",
      "å°†xçš„è¡¨è¾¾å¼ä»£å…¥æ€»è´¹ç”¨å…¬å¼ä¸­,å¾—åˆ°:\n",
      "\n",
      "æ€»è´¹ç”¨ = 100x + 250x + 100,000 + 10x = 450x + 100,000\n",
      "\n",
      "å› æ­¤,å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆå’Œå®é™…è§£å†³æ–¹æ¡ˆç›¸åŒã€‚\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_glm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71c23747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆå’Œæ­¥éª¤ï¼š\n",
      "    1. è®¡ç®—åœŸåœ°è´¹ç”¨ï¼š100ç¾å…ƒ/å¹³æ–¹è‹±å°º * xå¹³æ–¹è‹±å°º = 100xç¾å…ƒ\n",
      "    2. è®¡ç®—å¤ªé˜³èƒ½ç”µæ± æ¿è´¹ç”¨ï¼š250ç¾å…ƒ/å¹³æ–¹è‹±å°º * xå¹³æ–¹è‹±å°º = 250xç¾å…ƒ\n",
      "    3. è®¡ç®—ç»´æŠ¤è´¹ç”¨ï¼š10ä¸‡ç¾å…ƒ + 10ç¾å…ƒ/å¹³æ–¹è‹±å°º * xå¹³æ–¹è‹±å°º = 10ä¸‡ç¾å…ƒ + 10xç¾å…ƒ\n",
      "    4. è®¡ç®—æ€»è´¹ç”¨ï¼š100xç¾å…ƒ + 250xç¾å…ƒ + 10ä¸‡ç¾å…ƒ + 10xç¾å…ƒ = 360x + 10ä¸‡ç¾å…ƒ\n",
      "\n",
      "å­¦ç”Ÿçš„è§£å†³æ–¹æ¡ˆå’Œå®é™…è§£å†³æ–¹æ¡ˆæ˜¯å¦ç›¸åŒï¼šå¦\n",
      "\n",
      "å­¦ç”Ÿçš„æˆç»©ï¼šä¸æ­£ç¡®\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a313cd9-647e-4639-aa06-e28dd2df7827",
   "metadata": {},
   "source": [
    "**Note:** Actually the studentâ€™s solution is incorrect. The maintenance cost is $10 per square foot. The student mistakenly wrote it as $100 in his solution. ChatGLM found this mistake, but it did not point out the error in the studentâ€™s solution. Instead, ChatGPT found the error and gave the correct solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
