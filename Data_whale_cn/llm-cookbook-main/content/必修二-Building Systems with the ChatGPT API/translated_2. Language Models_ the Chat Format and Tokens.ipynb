{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630",
   "metadata": {},
   "source": [
    "# Chapter 2 Language Model, Questioning Paradigm and Token\n",
    "\n",
    "- [I. Environment Configuration](#I. Environment Configuration)\n",
    "- [1.1 Loading API key and some Python libraries. ](#1.1-Loading-API-key-and-some-Python-libraries.)\n",
    "- [1.2 Helper function auxiliary function](#1.2-Helper-function-auxiliary function)\n",
    "- [II. Try to ask questions to the model and get results](#II. Try to ask questions to the model and get results)\n",
    "- [III. Tokens](#III. Tokens)\n",
    "- [IV. Helper function auxiliary function (questioning paradigm)](#IV. Helper-function-auxiliary function-(questioning paradigm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf0c21",
   "metadata": {},
   "source": [
    "In this chapter, we will share with you how large language models (LLMs) work, how they are trained, and how details such as tokenizers affect LLM output. We will also introduce LLM's chat format, a way to specify system messages and user messages, and show you how to take advantage of this capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f",
   "metadata": {},
   "source": [
    "## 1. Environment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33004b0",
   "metadata": {},
   "source": [
    "### 1.1 Loading the API key and some Python libraries.\n",
    "In this course, you are provided with some code to load the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19cd4e96",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "# import tiktoken This is not used later. If you are interested in its use, you can refer to this article for related content: https://zhuanlan.zhihu.com/p/629776230\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # Read the local .env environment file. (It is recommended to use this method in the future and put the key in the .env file to protect your own key)\n",
    "\n",
    "openai.api_key  = 'sk-***' # 更换成您自己的key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c",
   "metadata": {},
   "source": [
    "### 1.2 Helper function\n",
    "If you have taken the \"ChatGPT Prompt Engineering for Developers\" course before, you will be relatively familiar with this.\n",
    "Call this function and enter Prompt, and it will give the corresponding Completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed96988",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "# Official document writing https://platform.openai.com/overview\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的模型生成聊天回复。\n",
    "\n",
    "    参数:\n",
    "    prompt: 用户的输入，即聊天的提示。\n",
    "    model: 使用的模型，默认为\"gpt-3.5-turbo\"。\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"] # 模型生成的回复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10a390-2461-447d-bf8b-8498db404c44",
   "metadata": {},
   "source": [
    "## 2. Try to ask the model questions and get results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50317bec",
   "metadata": {},
   "source": [
    "LLMs can be built using supervised learning, where they learn by repeatedly predicting the next word. And, given a large training set, tens of billions of words or more, you can create a large-scale training set where you start with a sentence or part of a text and repeatedly ask the language model to learn to predict what the next word is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325afca0",
   "metadata": {},
   "source": [
    "There are two main types of LLMs: Base LLMs and Instruction Tuned LLMs, which are becoming increasingly popular. Base LLMs are trained by repeatedly predicting the next word, so if we give it a prompt like \"Once upon a time there was a unicorn,\" it might be able to complete a story about the unicorn's life in a magical forest with his other unicorn friends by predicting word by word.\n",
    "\n",
    "However, the downside of this approach is that if you give it a prompt like \"What is the capital of China?\", it's likely that it has a list of quiz questions about China from the Internet in its data. At this point, it might answer the question with \"What is the largest city in China? What is the population of China?\" and so on. But in reality, you just want to know what the capital of China is, not to list all these questions. However, an Instruction Tuned LLM will try to follow the prompt and give the answer \"The capital of China is Beijing.\"\n",
    "\n",
    "So how do you turn a Base LLM into an Instruction Tuned LLM? This is the process of training an Instruction Tuned LLM (such as ChatGPT). First, you need to train the Base LLM on a lot of data, so you need hundreds of billions of words, or even more. This process can take months on large supercomputing systems. After training the base language model, you train it further on a small set of examples to make the model’s output match the input.For example, you can ask a contractor to help you write many examples of instructions and train the model on the correct responses to those instructions. This creates a training set for fine-tuning, where the model learns to predict the next word when following instructions.\n",
    "\n",
    "Later, to improve the quality of the language model output, a common approach is to have humans rate many different outputs, such as useful, true, harmless, etc. You can then further tune the language model to increase the probability of generating highly rated outputs. This is often done using reinforcement learning human feedback (RLHF) techniques. Compared to the months it may take to train a base language model, the transition from a base language model to an instruction-fine-tuned language model may only take days, using smaller datasets and computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1cc57b2",
   "metadata": {
    "height": 72
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of China is Beijing.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"What is the capital of China?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10f34f3b",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国的首都是北京。\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"中国的首都是哪里？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76233527",
   "metadata": {},
   "source": [
    "In the description of LLM so far, we have described it as predicting one word at a time, but there is actually a more important technical detail. That is, **`LLM does not actually repeatedly predict the next word, but repeatedly predicts the next token`**. When LLM receives input, it converts it into a series of tokens, where each token represents a common sequence of characters. For example, for the sentence \"Learning new things is fun!\", each word is converted into a token, and for less common words such as \"Prompting as powerful developer tool\", the word \"prompting\" is split into three tokens, namely \"prom\", \"pt\", and \"ing\".\n",
    "\n",
    "When you ask ChatGPT to reverse the letters of \"lollipop\", ChatGPT has difficulty outputting the correct order of the letters because the tokenizer breaks \"lollipop\" into three tokens, namely \"l\", \"oll\", \"ipop\". You can help ChatGPT better recognize each letter in a word and output them correctly by adding hyphens or spaces between letters so that the tokenizer breaks each letter into separate tokens.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc2d9e40",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reversed letters of \"lollipop\" are \"pillipol\".\n"
     ]
    }
   ],
   "source": [
    "# In order to better display the effect, there is no prompt translated into Chinese here\n",
    "# Note that the letter flipping here is wrong. Andrew Ng uses this example to explain how tokens are calculated.\n",
    "response = get_completion(\"Take the letters in lollipop \\\n",
    "and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f",
   "metadata": {},
   "source": [
    "\"lollipop\" in reverse should be \"popillol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37cab84f",
   "metadata": {
    "height": 88
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-o-p-i-l-l-o-l\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6cb95",
   "metadata": {},
   "source": [
    "![Tokens.png](../../figures/Tokens.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46bc72",
   "metadata": {},
   "source": [
    "For English input, a token generally corresponds to 4 characters or three quarters of a word; for Chinese input, a token generally corresponds to one or half a word.\n",
    "\n",
    "Different models have different token limits. It should be noted that the token limit here is the sum of the input prompt and the output completion tokens. Therefore, the longer the input prompt, the lower the upper limit of the completion that can be output.\n",
    "\n",
    "The token upper limit of ChatGPT3.5-turbo is 4096."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd",
   "metadata": {},
   "source": [
    "## 4. Helper function (questioning paradigm)\n",
    "The following are the helper functions used in the course.\n",
    "The following figure is a questioning paradigm provided by OpenAI. Next, Mr. Andrew Ng demonstrates how to use this paradigm to ask better questions\n",
    "![Chat-format.png](../../figures/Chat-format.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b6b3d",
   "metadata": {},
   "source": [
    "System information is used to specify the rules of the model, such as settings and answer criteria, while assistant information is the specific instructions for the model to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f89efad",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    '''\n",
    "    封装一个支持更多参数的自定义访问 OpenAI GPT3.5 的函数\n",
    "\n",
    "    参数: \n",
    "    messages: 这是一个消息列表，每个消息都是一个字典，包含 role(角色）和 content(内容)。角色可以是'system'、'user' 或 'assistant’，内容是角色的消息。\n",
    "    model: 调用的模型，默认为 gpt-3.5-turbo(ChatGPT)，有内测资格的用户可以选择 gpt-4\n",
    "    temperature: 这决定模型输出的随机程度，默认为0，表示输出将非常确定。增加温度会使输出更随机。\n",
    "    max_tokens: 这决定模型输出的最大的 token 数。\n",
    "    '''\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # 这决定模型输出的随机程度\n",
    "        max_tokens=max_tokens, # 这决定模型输出的最大的 token 数\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28c3424",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, a carrot would sprout,\n",
      "With a cheery orange hue, without a doubt.\n",
      "With a leafy green top, it danced in the breeze,\n",
      "A happy carrot, so eager to please.\n",
      "\n",
      "It grew in the soil, oh so deep and grand,\n",
      "Stretching its roots, reaching far and expand.\n",
      "With a joyful smile, it soaked up the sun,\n",
      "Growing tall and strong, its journey begun.\n",
      "\n",
      "Days turned to weeks, as it grew day and night,\n",
      "Round and plump, it was quite a delight.\n",
      "With every raindrop that fell from above,\n",
      "The carrot grew sweeter, spreading more love.\n",
      "\n",
      "At last, the day came when it was time to eat,\n",
      "With a grin on my face, I took a seat.\n",
      "I chopped and I sliced, so grateful, you see,\n",
      "For this happy carrot, bringing joy to me.\n",
      "\n",
      "So let us remember, when times may get tough,\n",
      "A happy carrot's journey, it's enough.\n",
      "For even in darkness, there's always delight,\n",
      "Just like a carrot, shining so bright.\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d0ef08f",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在海洋的深处，有一只小鲸鱼，\n",
      "她快乐又聪明，灵感从不匮乏。\n",
      "她游遍五大洲，探索未知的秘密，\n",
      "用歌声传递喜悦，令人心旷神怡。\n",
      "\n",
      "她跃出海面，高高的飞翔，\n",
      "尾巴抽空着水花，像梦幻般的画。\n",
      "她和海豚一起，跳跃在太阳下，\n",
      "与海洋中的生命，在欢乐中共舞。\n",
      "\n",
      "她喜欢和海龟一起，缓缓漫游，\n",
      "看美丽的珊瑚，和色彩鲜艳的鱼群。\n",
      "她欢迎每个新朋友，无论大或小，\n",
      "因为在她眼中，每个人都独特而珍贵。\n",
      "\n",
      "她知道快乐是如此简单，如此宝贵，\n",
      "在每个时刻中，她都努力传达幸福的表情。\n",
      "所以当你感到疲惫，沮丧或者低落，\n",
      "想起小鲸鱼的快乐，让你心中再次充满鲜活。\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答。'},    \n",
    "{'role':'user', \n",
    " 'content':'就快乐的小鲸鱼为主题给我写一首短诗'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56c6978d",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a cheerful carrot named Charlie who always brightened everyone's day with his vibrant orange color and contagious laughter.\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e34c399e",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在追随波浪的起伏中，小鲸鱼快乐地跳跃着，因为它知道游泳的真正乐趣不仅仅在目的地，而是在于享受整个旅程。\n"
     ]
    }
   ],
   "source": [
    "# Length Control\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'你的所有答复只能是一句话'},    \n",
    "{'role':'user',\n",
    " 'content':'写一个关于快乐的小鲸鱼的故事'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14fd6331",
   "metadata": {
    "height": 217
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a carrot so happy and bright, it danced and sang from morning till night.\n"
     ]
    }
   ],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who \\\n",
    "responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "response = get_completion_from_messages(messages, \n",
    "                                        temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ca678de",
   "metadata": {
    "height": 181
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在蓝色的大海里，有一只小鲸鱼，无忧无虑，快乐游泳，一切因快乐而变得光辉。\n"
     ]
    }
   ],
   "source": [
    "# Combination of the above\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答，只回答一句话'},    \n",
    "{'role':'user',\n",
    " 'content':'写一个关于快乐的小鲸鱼的故事'},\n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89a70c79",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"gpt-3.5-turbo\", \n",
    "                                   temperature=0, \n",
    "                                   max_tokens=500):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的 GPT-3 模型生成聊天回复，并返回生成的回复内容以及使用的 token 数量。\n",
    "\n",
    "    参数:\n",
    "    messages: 聊天消息列表。\n",
    "    model: 使用的模型名称。默认为\"gpt-3.5-turbo\"。\n",
    "    temperature: 控制生成回复的随机性。值越大，生成的回复越随机。默认为 0。\n",
    "    max_tokens: 生成回复的最大 token 数量。默认为 500。\n",
    "\n",
    "    返回:\n",
    "    content: 生成的回复内容。\n",
    "    token_dict: 包含'prompt_tokens'、'completion_tokens'和'total_tokens'的字典，分别表示提示的 token 数量、生成的回复的 token 数量和总的 token 数量。\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message[\"content\"]\n",
    "    \n",
    "    token_dict = {\n",
    "'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "'completion_tokens':response['usage']['completion_tokens'],\n",
    "'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a64cf3c6",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, with colors so cheery,\n",
      "There lived a carrot, oh so merry!\n",
      "With a vibrant orange hue, and a leafy green top,\n",
      "This happy carrot just couldn't stop.\n",
      "\n",
      "It danced in the breeze, with a joyful sway,\n",
      "Spreading happiness throughout the day.\n",
      "With a smile so wide, and eyes full of glee,\n",
      "This carrot was as happy as can be.\n",
      "\n",
      "It loved the sunshine, and the rain's gentle touch,\n",
      "Growing tall and strong, oh so much!\n",
      "From the earth it sprouted, reaching for the sky,\n",
      "A happy carrot, oh my, oh my!\n",
      "\n",
      "So if you're feeling down, just remember this tale,\n",
      "Of a carrot so happy, it'll never fail.\n",
      "Find joy in the little things, and let your heart sing,\n",
      "Just like that carrot, oh what joy it will bring!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\ \n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfd8fbd4",
   "metadata": {
    "height": 146
   },
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答。'},    \n",
    "{'role':'user', \n",
    " 'content':'就快乐的小鲸鱼为主题给我写一首短诗'},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "352ad320",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 37, 'completion_tokens': 173, 'total_tokens': 210}\n"
     ]
    }
   ],
   "source": [
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f65685",
   "metadata": {},
   "source": [
    "Finally, we believe that the revolutionary impact of Prompt on AI application development is still underappreciated. In the traditional supervised machine learning workflow, if you want to build a classifier that can classify restaurant reviews as positive or negative, you first need to get a large amount of labeled data, maybe hundreds of them, and this process may take weeks or even a month. Then you need to train a model on this data, find a suitable open source model, and do model tuning and evaluation, which may take days, weeks, or even months. Finally, you may need to use cloud services to deploy the model, upload the model to the cloud, and let it run before you can finally call your model. The whole process usually takes a team months to complete.\n",
    "\n",
    "In contrast, with the prompt-based machine learning approach, when you have a text application, you just provide a simple prompt. This process may only take a few minutes, or at most a few hours if it takes multiple iterations to get a valid prompt. Within a few days (although in practice it is usually a few hours), you can run the model through API calls and start using it. Once you reach this step, it only takes a few minutes or hours to start calling the model for inference. As a result, applications that previously might have taken six months or even a year to build can now be built in minutes, hours, or at most days.time, you can build it using Prompt. This approach is dramatically changing how AI applications are built quickly.\n",
    "\n",
    "It is important to note that this approach works for many unstructured data applications, especially text applications, and increasingly vision applications, although current vision technology is still developing. It does not work for structured data applications, that is, machine learning applications that process large numbers of values ​​in Excel spreadsheets. However, for applications that are suitable for this approach, AI components can be built quickly and are changing the construction workflow of the entire system. Building the entire system may still take days, weeks, or longer, but at least this part can be done faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe248d6",
   "metadata": {},
   "source": [
    "In the next chapter, we'll show how to use these components to evaluate the input of a customer service assistant.\n",
    "This will be part of a more complete example in this course of building a customer service assistant for an online retailer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
