{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190c69e4-1a17-46bf-a6ea-79760b4ef4ca",
   "metadata": {},
   "source": [
    "# Tagging and Extraction\n",
    "\n",
    "- [I. Set OpenAI API Key](#I. Set OpenAI-API-Key)\n",
    "- [II. Tagging](#II. Tagging)\n",
    "- [2.1 Create Tagging Function](#2.1-Create Tagging Function)\n",
    "- [2.2 Implement Tagging through LangChain](#2.2-Implement Tagging through LangChain)\n",
    "- [2.3 Structured Parsing Tagging Results](#2.3-Structural Parsing Tagging Results)\n",
    "- [III. Extraction](#III. Extraction)\n",
    "- [3.1 Create Extraction Function](#3.1-Create Extraction Function)\n",
    "- [3.2 Implement Extraction Function through LangChain](#3.2-Implement Extraction Function through LangChain)\n",
    "- [3.3 Structured Parsing Extraction Results](#3.3-Structural Parsing Extraction Results)\n",
    "- [IV. Application Cases](#IV. Application Cases)\n",
    "- [4.1 Loading data](#4.1-Loading data)- [4.2 Extract article overview](#4.2-Extract article overview)\n",
    "- [4.3 Extract article information](#4.3-Extract article information)\n",
    "- [4.4 Block text extraction](#4.4-Block text extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54f3b7-ff99-453c-a5cd-e6fb47569a50",
   "metadata": {},
   "source": [
    "# 1. Set OpenAI-API-Key\n",
    "\n",
    "For details, see `Set OpenAI_API_KEY.ipynb` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf894094-198d-429d-96da-f7092cd7c44e",
   "metadata": {},
   "source": [
    "# 2. Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2fb4b-3678-48e2-bd81-edb485dfee05",
   "metadata": {},
   "source": [
    "What is `Tagging`:\n",
    "- LLM given a function description, generates a structured output by selecting arguments from the input text, forming a function call\n",
    "- More generally, LLM can evaluate input text and generate **structured output**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4ebb3",
   "metadata": {},
   "source": [
    "## 2.1 Create Tagging function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ba8f1",
   "metadata": {},
   "source": [
    "We define a `Tagging`, which inherits from Pydantic's BaseModel class, so the `Tagging` class also has strict data type verification. The `Tagging` class contains two member variables: `sentiment` and `language`:\n",
    "- `sentiment`: used to determine the sentiment of user information, including pos (positive), neg (negative), neutral (neutral).\n",
    "- `language`: used to determine which country the user uses, and must comply with the ISO 639-1 encoding standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from typing import List  \n",
    "from pydantic import BaseModel, Field  \n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tagging class\n",
    "# This table is based on the input text to mark the text sentiment as `pos` (positive), `neg` (negative) or `neutral` (neutral)\n",
    "class Tagging(BaseModel):\n",
    "\"\"\"Mark this text with specific information.\"\"\"\n",
    "# The sentiment label of the text, optional values ​​are `pos` (positive), `neg` (negative) or `neutral` (neutral)\n",
    "    sentiment: str = Field(description=\"文本的情绪，请从“正面”、“负面”或“中立”中选择\")\n",
    "# The language tag of the text should be the ISO 639-1 standard code\n",
    "    language: str = Field(description=\"文本语言(应采用ISO 639-1代码)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74150c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Tagging',\n",
       " 'description': '用特定信息标记这段文本。',\n",
       " 'parameters': {'title': 'Tagging',\n",
       "  'description': '用特定信息标记这段文本。',\n",
       "  'type': 'object',\n",
       "  'properties': {'sentiment': {'title': 'Sentiment',\n",
       "    'description': '文本的情绪，请从“正面”、“负面”或“中立”中选择',\n",
       "    'type': 'string'},\n",
       "   'language': {'title': 'Language',\n",
       "    'description': '文本语言(应采用ISO 639-1代码)',\n",
       "    'type': 'string'}},\n",
       "  'required': ['sentiment', 'language']}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert Tagging data model to OpenAI function\n",
    "convert_pydantic_to_openai_function(Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4440590",
   "metadata": {},
   "source": [
    "## 2.2 Tagging through LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245ef62",
   "metadata": {},
   "source": [
    "Next we need to convert the `Tagging` class into a function description object that OpenAI can recognize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab35dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from langchain.prompts import ChatPromptTemplate \n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6795dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatOpenAI model instance with a temperature of 0\n",
    "model = ChatOpenAI(temperature=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Tagging\n",
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d8550",
   "metadata": {},
   "source": [
    "With the function description variable, we use the `LCEL` syntax to create a chain. Before that, we need to create prompt, model, bind the function description variable, and finally create the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ba9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the from_messages method of ChatPromptTemplate to create a chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"仔细思考，然后按指示标记文本\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3061eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the model to the function and specify the name of the function call\n",
    "model_with_functions = model.bind(\n",
    "    functions=tagging_functions,\n",
    "    function_call={\"name\": \"Tagging\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label chain, combining the prompt template and the model\n",
    "tagging_chain = prompt | model_with_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f070b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'Tagging', 'arguments': '{\\n  \"sentiment\": \"正面\",\\n  \"language\": \"zh\"\\n}'}}, example=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the tag chain and pass in the input text\n",
    "tagging_chain.invoke({\"input\": \"我爱langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3a755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'Tagging', 'arguments': '{\\n  \"sentiment\": \"中立\",\\n  \"language\": \"zh\"\\n}'}}, example=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the label chain again and pass in another input text\n",
    "tagging_chain.invoke({\"input\": \"我想要问的不是这些问题\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcc4fd",
   "metadata": {},
   "source": [
    "## 2.3 Structured parsing of Tagging results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30edaa",
   "metadata": {},
   "source": [
    "The above output is the result of AIMessage format given by LLM. We can use the `LCEL` syntax to add a json output parser when creating a chain to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import JsonOutputFunctionsParser from the langchain.output_parsers.openai_functions module\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tag chain, combining the prompt template, model and JsonOutputFunctionsParser parser\n",
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea84888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': '正面', 'language': 'zh'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the tag chain and pass in the input text\n",
    "tagging_chain.invoke({\"input\": \"我爱langchain\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a85c10",
   "metadata": {},
   "source": [
    "# 3. Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79dbfe",
   "metadata": {},
   "source": [
    "What is Extraction:\n",
    "- Extraction is similar to Tagging, but is used to extract multiple pieces of information.\n",
    "- When given an input Json pattern, LLM has been fine-tuned to find and fill in the parameters of that pattern.\n",
    "- This feature is not limited to function mode and can be used for general purpose extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b77f8",
   "metadata": {},
   "source": [
    "## 3.1 Create Extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ebd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from typing import Optional  \n",
    "from pydantic import BaseModel, Field  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81783873",
   "metadata": {},
   "source": [
    "Two classes, `Person` and `Information`, are defined:\n",
    "- The `person` class contains two members, name and age, where age is optional.\n",
    "- The `Information` class contains a people member, which is a collection (List) of persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9838da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Person class\n",
    "class Person(BaseModel):\n",
    "\"\"\"personal information\"\"\"\n",
    "    name: str = Field(description=\"人的名字\")  # 人的名字\n",
    "    age: Optional[int] = Field(description=\"人的年龄\")  # 人的年龄，可选字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Information category\n",
    "class Information(BaseModel):\n",
    "\"\"\"Information to extract\"\"\"\n",
    "    people: List[Person] = Field(description=\"关于人的信息列表\")  # 关于人的信息列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5dd9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Information',\n",
       " 'description': '要提取的信息',\n",
       " 'parameters': {'title': 'Information',\n",
       "  'description': '要提取的信息',\n",
       "  'type': 'object',\n",
       "  'properties': {'people': {'title': 'People',\n",
       "    'description': '关于人的信息列表',\n",
       "    'type': 'array',\n",
       "    'items': {'title': 'Person',\n",
       "     'description': '个人信息',\n",
       "     'type': 'object',\n",
       "     'properties': {'name': {'title': 'Name',\n",
       "       'description': '人的名字',\n",
       "       'type': 'string'},\n",
       "      'age': {'title': 'Age', 'description': '人的年龄', 'type': 'integer'}},\n",
       "     'required': ['name']}}},\n",
       "  'required': ['people']}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the Information data model to an OpenAI function\n",
    "convert_pydantic_to_openai_function(Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of extraction features and bind the extraction features to the model\n",
    "extraction_functions = [convert_pydantic_to_openai_function(Information)]  \n",
    "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68313aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'Information', 'arguments': '{\\n  \"people\": [\\n    {\\n      \"name\": \"乔\",\\n      \"age\": 30\\n    },\\n    {\\n      \"name\": \"玛莎\",\\n      \"age\": 0\\n    }\\n  ]\\n}'}}, example=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Call the extraction model and pass in text information\n",
    "extraction_model.invoke(\"乔30岁，他妈妈叫玛莎\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d007db",
   "metadata": {},
   "source": [
    "## 3.2 Creating an Extraction Function through LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252aa292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ChatPromptTemplate to create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"提取相关信息，如果没有明确提供不要猜测。可以提取部分信息\"), \n",
    "    (\"human\", \"{input}\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e99c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an extraction chain, combining the prompt template and the extraction model\n",
    "extraction_chain = prompt | extraction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3fa31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'Information', 'arguments': '{\\n  \"people\": [\\n    {\\n      \"name\": \"乔\",\\n      \"age\": 30\\n    },\\n    {\\n      \"name\": \"玛莎\"\\n    }\\n  ]\\n}'}}, example=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the extraction chain and pass in the input text\n",
    "extraction_chain.invoke({\"input\": \"乔30岁，他妈妈叫玛莎\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new extraction chain and add JsonOutputFunctionsParser to parse the output\n",
    "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b986c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': [{'name': '乔', 'age': 30}, {'name': '玛莎'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the extraction chain again\n",
    "extraction_chain.invoke({\"input\": \"乔30岁，他妈妈叫玛莎\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b57f8",
   "metadata": {},
   "source": [
    "## 3.3 Structured Parsing Extraction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an extraction chain and specify the keyword \"name\" to parse the output\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd2d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': '乔', 'age': 30}, {'name': '玛莎'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the extraction chain and pass in the input text\n",
    "extraction_chain.invoke({\"input\": \"乔30岁，他妈妈叫玛莎\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c15c6b",
   "metadata": {},
   "source": [
    "# 4. Application Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc0bd6",
   "metadata": {},
   "source": [
    "We can apply tagging to a larger body of text. For example, load a blog post and extract tagging information from a subset of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb6951",
   "metadata": {},
   "source": [
    "## 4.1 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading documents using WebBaseLoader\n",
    "from langchain.document_loaders import WebBaseLoader  \n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\") \n",
    "documents = loader.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first document\n",
    "doc = documents[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 10,000 characters of the page content\n",
    "page_content = doc.page_content[:10000]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3c7f4",
   "metadata": {},
   "source": [
    "## 4.2 Extract article overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7008c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BaseModel and Field from pydantic to create data models\n",
    "from pydantic import BaseModel, Field  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010fc64",
   "metadata": {},
   "source": [
    "Define a Pydantic class `Overview`\n",
    "- `summary`: represents the summary of the article content\n",
    "- `language`: represents the language used in the article\n",
    "- `keyword`: represents the keywords in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ed0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Overview category\n",
    "class Overview(BaseModel):\n",
    "\"\"\"An overview of a text\"\"\"\n",
    "    summary: str = Field(description=\"提供内容的简明总结。\")  # 内容摘要\n",
    "    language: str = Field(description=\"提供编写内容所用的语言。\")  # 内容语言\n",
    "    keywords: str = Field(description=\"提供与内容相关的关键字。\")  # 关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9173caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Overview data model to OpenAI function\n",
    "overview_tagging_function = [\n",
    "    convert_pydantic_to_openai_function(Overview)\n",
    "]\n",
    "tagging_model = model.bind(\n",
    "    functions=overview_tagging_function,\n",
    "    function_call={\"name\":\"Overview\"}  # 绑定函数调用\n",
    ")\n",
    "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()  # 创建标注链并加入解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4c06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'LLM Powered Autonomous Agents is a concept of building agents with LLM (large language model) as its core controller. It involves several key components such as planning, memory, and tool use. The agent breaks down tasks into smaller subgoals, utilizes short-term and long-term memory, and learns to call external APIs for additional information. Self-reflection is also an important aspect for agents to improve iteratively. There are various techniques and frameworks, such as Chain of Thought, ReAct, Reflexion, and Chain of Hindsight, that enable agents to plan, reflect, and improve their performance.',\n",
       " 'language': 'English',\n",
       " 'keywords': 'LLM, autonomous agents, planning, memory, tool use, self-reflection, Chain of Thought, ReAct, Reflexion, Chain of Hindsight'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calling the annotation chain\n",
    "tagging_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d884b6",
   "metadata": {},
   "source": [
    "## 4.3 Extracting article information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9305294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Paper class for title and author\n",
    "class Paper(BaseModel):\n",
    "\"\"\"Information about the mentioned paper.\"\"\"\n",
    "    title: str  # 论文标题\n",
    "    author: Optional[str]  # 作者，可选字段\n",
    "\n",
    "# Create Info, user extracts paper information list\n",
    "class Info(BaseModel):\n",
    "\"\"\"Information to extract\"\"\"\n",
    "    papers: List[Paper] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07444034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Info data model to OpenAI function\n",
    "paper_extraction_function = [\n",
    "    convert_pydantic_to_openai_function(Info)\n",
    "]\n",
    "extraction_model = model.bind(\n",
    "    functions=paper_extraction_function, \n",
    "    function_call={\"name\":\"Info\"}  # 绑定函数调用\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b2adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an extraction chain and add a parser\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3151bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LLM Powered Autonomous Agents', 'author': 'Lilian Weng'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the extraction chain and find that the name of the paper itself is extracted. Therefore, we can improve it in combination with prompt\n",
    "extraction_chain.invoke({\"input\": page_content})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "A article will be passed to you. Extract from it all papers that are mentioned by this article. \n",
    "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\n",
    "\"\"\"\n",
    "\n",
    "template_chinese = \"\"\"\n",
    "一篇文章将转交给你。把这篇文章中提到的所有论文都摘录出来。\n",
    "不要提取文章本身的名称。如果没有提到论文，那很好——你不需要提取任何论文!只返回一个空列表。\n",
    "不要编造或猜测任何额外的信息。只提取文本中的内容。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat prompt using a custom prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template_chinese),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5df8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the extraction chain\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7f9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': ''},\n",
       " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': ''},\n",
       " {'title': 'LLM+P (Liu et al. 2023)', 'author': ''},\n",
       " {'title': 'ReAct (Yao et al. 2023)', 'author': ''},\n",
       " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': ''},\n",
       " {'title': 'Chain of Hindsight (CoH; Liu et al. 2023)', 'author': ''},\n",
       " {'title': 'Algorithm Distillation (AD; Laskin et al. 2023)', 'author': ''}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the extraction chain again\n",
    "extraction_chain.invoke({\"input\": page_content})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304665d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calling the extraction chain with irrelevant input will not return valid information\n",
    "extraction_chain.invoke({\"input\": \"hi\"})  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53459cd8",
   "metadata": {},
   "source": [
    "## 4.4 Chunk text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f32ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "# Instantiate the text segmenter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f8598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split document content, text_splitter can split long text into multiple short texts\n",
    "splits = text_splitter.split_text(doc.page_content)  \n",
    "\n",
    "# Get the number of segmented paragraphs\n",
    "len(splits)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to flatten a list\n",
    "def flatten(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51306c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example calling the flatten function\n",
    "flatten([[1, 2], [3, 4]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n"
     ]
    }
   ],
   "source": [
    "# Print the last thousand characters of the first split text block\n",
    "print(splits[0][-1000:])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c780252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from langchain.schema.runnable import RunnableLambda  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Lambda function to preprocess text\n",
    "prep = RunnableLambda(\n",
    "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9706f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'hi'}]\n",
      "1\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# Test prep\n",
    "print(prep.invoke(\"hi\"))\n",
    "print(len(prep.invoke(\"hi\")))\n",
    "\n",
    "# Put a long text in and it will be split into multiple short texts\n",
    "print(len(prep.invoke(doc.page_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4bac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain calls, including preprocessing, mapping extraction\n",
    "# Use extraction_chain to extract multiple short texts separately, and flatten the resulting list together using the flatten function\n",
    "chain = prep | extraction_chain.map() | flatten  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6e4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'AutoGPT', 'author': ''},\n",
       " {'title': 'GPT-Engineer', 'author': ''},\n",
       " {'title': 'BabyAGI', 'author': ''},\n",
       " {'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': ''},\n",
       " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': ''},\n",
       " {'title': 'LLM+P (Liu et al. 2023)', 'author': ''},\n",
       " {'title': 'ReAct (Yao et al. 2023)', 'author': ''},\n",
       " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': ''},\n",
       " {'title': 'Reflexion: A Framework for Self-Reflection in Reinforcement Learning',\n",
       "  'author': 'Shinn & Labash'},\n",
       " {'title': 'Chain of Hindsight: Improving Reinforcement Learning with Sequential Feedback',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'Algorithm Distillation: Learning Process of Reinforcement Learning',\n",
       "  'author': 'Laskin et al.'},\n",
       " {'title': 'Algorithm Distillation', 'author': 'Laskin et al. 2023'},\n",
       " {'title': 'ED (expert distillation)', 'author': ''},\n",
       " {'title': 'RL^2', 'author': 'Duan et al. 2017'},\n",
       " {'title': 'Maximum Inner Product Search (MIPS)', 'author': ''},\n",
       " {'title': 'LSH (Locality-Sensitive Hashing)', 'author': ''},\n",
       " {'title': 'ANNOY (Approximate Nearest Neighbors Oh Yeah)', 'author': ''},\n",
       " {'title': 'HNSW (Hierarchical Navigable Small World)', 'author': ''},\n",
       " {'title': 'FAISS (Facebook AI Similarity Search)', 'author': ''},\n",
       " {'title': 'ScaNN (Scalable Nearest Neighbors)', 'author': ''},\n",
       " {'title': 'MRKL: Modular Reasoning, Knowledge and Language',\n",
       "  'author': 'Karpas et al. 2022'},\n",
       " {'title': 'TALM: Tool Augmented Language Models',\n",
       "  'author': 'Parisi et al. 2022'},\n",
       " {'title': 'Toolformer', 'author': 'Schick et al. 2023'},\n",
       " {'title': 'HuggingGPT', 'author': 'Shen et al. 2023'},\n",
       " {'title': 'API-Bank: A Benchmark for Evaluating Tool-Augmented Language Models',\n",
       "  'author': 'Li et al. 2023'},\n",
       " {'title': 'ChemCrow: A Domain-Specific Example of Tool-Augmented Language Models',\n",
       "  'author': 'Bran et al. 2023'},\n",
       " {'title': 'LLM-based evaluation of GPT-4 and ChemCrow', 'author': 'Unknown'},\n",
       " {'title': 'LLM-empowered agents for scientific discovery',\n",
       "  'author': 'Boiko et al. (2023)'},\n",
       " {'title': 'Generative Agents Simulation', 'author': 'Park, et al. (2023)'},\n",
       " {'title': 'Park et al. 2023', 'author': ''},\n",
       " {'title': 'Super Mario: A Classic Platform Game', 'author': 'John Smith'},\n",
       " {'title': 'MVC Architecture in Python', 'author': 'Jane Doe'},\n",
       " {'title': 'Keyboard Control in Python Games', 'author': 'David Johnson'},\n",
       " {'title': 'A Study on Machine Learning Algorithms', 'author': 'John Smith'},\n",
       " {'title': 'Deep Learning Techniques for Image Recognition',\n",
       "  'author': 'Jane Doe'},\n",
       " {'title': 'Natural Language Processing: A Comprehensive Review',\n",
       "  'author': 'David Johnson'},\n",
       " {'title': 'Chain of thought prompting elicits reasoning in large language models.',\n",
       "  'author': 'Wei et al.'},\n",
       " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models.',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Chain of Hindsight Aligns Language Models with Feedback',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'ReAct: Synergizing reasoning and acting in language models.',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
       "  'author': 'Shinn & Labash'},\n",
       " {'title': 'In-context Reinforcement Learning with Algorithm Distillation',\n",
       "  'author': 'Laskin et al.'},\n",
       " {'title': 'MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.',\n",
       "  'author': 'Karpas et al.'},\n",
       " {'title': 'API-Bank: A Benchmark for Tool-Augmented LLMs',\n",
       "  'author': 'Li et al.'},\n",
       " {'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace',\n",
       "  'author': 'Shen et al.'},\n",
       " {'title': 'ChemCrow: Augmenting large-language models with chemistry tools.',\n",
       "  'author': 'Bran et al.'},\n",
       " {'title': 'Emergent autonomous scientific research capabilities of large language models.',\n",
       "  'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents: Interactive Simulacra of Human Behavior.',\n",
       "  'author': 'Joon Sung Park, et al.'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain.invoke(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027e4c9",
   "metadata": {},
   "source": [
    "# 5. English version template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92ad4e",
   "metadata": {},
   "source": [
    "**2.1 Create Tagging Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a19e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagging(BaseModel):\n",
    "\"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab576c2",
   "metadata": {},
   "source": [
    "**2.2 Tagging through LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7353b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca92a1",
   "metadata": {},
   "source": [
    "**3.1 Create Extraction Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "\"\"\"Information about a person.\"\"\"\n",
    "    name: str = Field(description=\"person's name\")  \n",
    "    age: Optional[int] = Field(description=\"person's age\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5703cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Information(BaseModel):\n",
    "\"\"\"Information to extract.\"\"\"\n",
    "    people: List[Person] = Field(description=\"List of info about people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc2f8c",
   "metadata": {},
   "source": [
    "**3.2 Creating Extraction Functions through LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203804b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"), \n",
    "    (\"human\", \"{input}\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612cbbc",
   "metadata": {},
   "source": [
    "**4.2 Extracting Article Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8557237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Overview(BaseModel):\n",
    "\"\"\"Overview of a section of text.\"\"\"\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\") \n",
    "    language: str = Field(description=\"Provide the language that the content is written in.\") \n",
    "    keywords: str = Field(description=\"Provide keywords related to the content.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30203abe",
   "metadata": {},
   "source": [
    "**4.3 Extracting article information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper(BaseModel):\n",
    "\"\"\"Information about papers mentioned.\"\"\"\n",
    "    title: str  \n",
    "    author: Optional[str]  \n",
    "\n",
    "class Info(BaseModel):\n",
    "\"\"\"Information to extract\"\"\"\n",
    "    papers: List[Paper] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498e7dcd",
   "metadata": {},
   "source": [
    "prompt using `template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5899a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "A article will be passed to you. Extract from it all papers that are mentioned by this article. \n",
    "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
