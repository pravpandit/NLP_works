{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f7fce3",
   "metadata": {
    "id": "c5f7fce3"
   },
   "source": [
    "# Chapter 3 LangChain Expression Language LangChain Expression Language\n",
    "\n",
    "In this chapter, we will introduce LangChain Expression Language (or LCEL), which is called Langchain's expression language. LCEL is a new syntax and an important addition to the LangChain toolkit. It has many advantages that make it easier and more convenient for us to handle LangChain and agents.\n",
    "\n",
    "1. LCEL provides asynchronous, batch, and stream processing support, which makes the code versatile and can be quickly applied and run in different servers.\n",
    "\n",
    "- Asynchronous: The program can execute multiple tasks at the same time, rather than one after another in sequence\n",
    "\n",
    "- Batch processing: It is a method of processing a group of tasks or data as a batch, rather than one by one\n",
    "\n",
    "- Streaming processing: It is a method of processing data continuously. Data will continuously enter the system and be processed. Streaming processing can process data immediately when it arrives, and can process data in a continuous and low-latency manner.\n",
    "\n",
    "2. LCEL has fallbacks, also known as fallback safety mechanism. Sometimes the results obtained by LLM are uncontrollable. In this case, you can roll back the results and even attach them to the entire chain.\n",
    "\n",
    "3. LCEL adds LLM parallelism, LLM operation is usually time-consuming, parallelism can speed up the speed of getting results.\n",
    "\n",
    "4. LCEL has built-in logging to record the operation of the agent. Even if the agent is complex, the log helps to understand the operation of complex chains and agents.\n",
    "\n",
    "In the previous course, we know that LangChain provides a component chain (chain) that can combine components to play a more powerful role in LLM, but the syntax is very complex. Here, LCEL provides a pipeline syntax that makes it easy to build complex chains from basic components. We can use LangChain to complete the combination of `Chain = prompt | LLM | OutputParser`. We will discuss the specific use in the following content. Chains usually combine large language models (LLM) with prompts, based on which we can perform a series of operations on text or data.\n",
    "\n",
    "![image.png](../../figures/LCEL.png)\n",
    "\n",
    "- [1. Simple Chain](#1. Simple Chain-Simple-Chain)\n",
    "- [2. More complex chain](#2. More complex chain-More-complex-chain)\n",
    "- [1.1 Building a simple vector database](#1.1-Build a simple vector database)\n",
    "- [1.2 Use RunnableMap](#1.2-Use RunnableMap)\n",
    "- [Three, Bind Bind](#Three, Bind-Bind)\n",
    "- [3.1 Single function binding](#3.1-Single function binding)\n",
    "- [3.2 Multiple function binding](#3.2-Multiple function binding)\n",
    "- [Four, Fallbacks](#Four, Fallbacks)\n",
    "- [4.1 Use the early model to format the output](#4.1-Use the early model to format the output)\n",
    "- [4.2 Use the new model to format the output](#4.2-Use the new model to format the output)\n",
    "- [4.3 fallbacks method](#4.3-fallbacks method)\n",
    "- [Five, Interface Interface](#Five, Interface-Interface)\n",
    "- [5.1 invoke interface](#5.1-invoke interface)\n",
    "- [5.2 batch interface](#5.2-batch interface)\n",
    "- [5.3 stream interface](#5.3-stream interface)\n",
    "- [5.4 asynchronous interface](#5.4-asynchronous interface)\n",
    "- [VI. English version tips](#VI. English version tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631e459",
   "metadata": {},
   "source": [
    "## 1. Simple Chain\n",
    "\n",
    "Next we will still use OpenAI's API, so first we need to initialize our API_Key, the method is the same as the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb572a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cacb572a",
    "outputId": "1acef39b-494e-445e-bde0-2e8f83a2a84c"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install openai==0.28\n",
    "# !pip install \"langchain[docarray]\"\n",
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52952e6",
   "metadata": {
    "id": "c52952e6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR_API_KEY\"\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f48e9",
   "metadata": {
    "id": "0a7f48e9"
   },
   "source": [
    "Next, we first import the LangChain library and define a simple chain, which includes a prompt template, a large language model, and an output parser. We can see that the result of the large language model is successfully output, completing a simple chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ac70fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "81ac70fe",
    "outputId": "00302a8a-42eb-4eaa-ff0f-8540245932cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'ä¸ºä»€ä¹ˆç†Šä¸å–œæ¬¢ç©æ‰‘å…‹ç‰Œï¼Ÿå› ä¸ºä»–æ€»æ˜¯æŠŠä¸¤ä¸ªç†ŠæŒéƒ½éœ²å‡ºæ¥ï¼'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import modules required by LangChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Use ChatPromptTemplate to create a prompt from a template. The {topic} in the template will be replaced with the actual topic in subsequent code.\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äº{topic}çš„çŸ­ç¬‘è¯\"\n",
    ")\n",
    "\n",
    "# Create a ChatOpenAI model instance, using the gpt-3.5-turbo model by default\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a StrOutputParser instance to parse the output\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create a chain call to connect prompt, model and output_parser together\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "#Call chain call and pass in parameters\n",
    "chain.invoke({\"topic\": \"ç†Š\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563f727",
   "metadata": {
    "id": "7563f727"
   },
   "source": [
    "If we look at the output of `Chain`, we will find that it is the same as what we defined, and it consists of three parts, namely `Chain = prompt | LLM |OutputParser`. The `|` symbol is similar to the unix pipe operator, which links different components together and provides the output of one component as input to the next component. In this chain, the user input is passed to the prompt template, then the prompt template output is passed to the model, and then the model output is passed to the output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a351d14a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a351d14a",
    "outputId": "e9a7f195-fdfb-4523-eb34-a708ce0e0b24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äº{topic}çš„çŸ­ç¬‘è¯'))])\n",
       "| ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, openai_api_key='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', openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the value of Chain\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8c80e",
   "metadata": {
    "id": "35f8c80e"
   },
   "source": [
    "## 2. More complex chain\n",
    "\n",
    "Next, we will create a more complex chain. In the previous course, we have touched upon how to perform retrieval enhancement generation. So next we use LCEL to repeat the previous process, combining the user's question with the vector database retrieval results, and use RunnableMap to build a more complex chain.\n",
    "\n",
    "### 2.1 Build a simple vector database\n",
    "First, we build a vector database. This simple vector database contains only two sentences. We use OpenAI's Embedding as the embedding model, and then we create a retriever through `vector store.as_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308eab74",
   "metadata": {
    "id": "308eab74"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# Create a DocArrayInMemorySearch object to store and search document vectors\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"å“ˆé‡Œæ£®åœ¨è‚¯è‚–å·¥ä½œ\", \"ç†Šå–œæ¬¢åƒèœ‚èœœ\"],\n",
    "    embedding=OpenAIEmbeddings() # ä½¿ç”¨OpenAIçš„Embedding\n",
    ")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c1f68",
   "metadata": {
    "id": "768c1f68"
   },
   "source": [
    "As we learned earlier, if we call `retriever.get_relevant_documents`, we will get the relevant retrieved documents. First, we ask \"Where does Harrison work?\" We will find that a list of documents is returned. It will return a list of documents sorted by similarity, so the most relevant ones are placed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83e9118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f83e9118",
    "outputId": "4b3e4b1f-c378-43ec-8a2e-4b6a6a0d88b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='å“ˆé‡Œæ£®åœ¨è‚¯è‚–å·¥ä½œ'), Document(page_content='ç†Šå–œæ¬¢åƒèœ‚èœœ')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get documents related to the question \"Where does Harrison work?\"\n",
    "retriever.get_relevant_documents(\"å“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5eacb",
   "metadata": {
    "id": "a4b5eacb"
   },
   "source": [
    "If we change the question to, say, \"What do bears like to eat?\", we can see that the order of the questions changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d76aaf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d76aaf5",
    "outputId": "8e79e2d9-8572-44e0-edf7-70cd254e0252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ç†Šå–œæ¬¢åƒèœ‚èœœ'), Document(page_content='å“ˆé‡Œæ£®åœ¨è‚¯è‚–å·¥ä½œ')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get documents related to the question \"What do bears like to eat?\"\n",
    "retriever.get_relevant_documents(\"ç†Šå–œæ¬¢åƒä»€ä¹ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34293a",
   "metadata": {
    "id": "fb34293a"
   },
   "source": [
    "### 3.2 Using RunnableMap\n",
    "\n",
    "The above example returns two results because there are only two document lists, which is completely applicable to more documents. Next, we will add `RunnableMap`. In this `RunnableMap`, there are not only user questions, but also document lists corresponding to the questions. This is equivalent to adding context to the documents of the big model, so that retrieval enhancement can be completed. If we ask a question normally, we can see that the big model correctly returns the results in the document and obtains the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a9b652",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e8a9b652",
    "outputId": "d615b86a-a14b-4917-833f-e70c91730477"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'è‚¯è‚–'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "# Define a template string template\n",
    "template = \"\"\"ä»…æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n",
    "{context}\n",
    "\n",
    "é—®é¢˜ï¼š{question}\n",
    "\"\"\"\n",
    "\n",
    "# Use template as a template\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create a processing chain chain, including RunnableMap, prompt, model and output_parser components\n",
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser\n",
    "\n",
    "# Call the invoke method of chain\n",
    "chain.invoke({\"question\": \"å“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394af88e",
   "metadata": {
    "id": "394af88e"
   },
   "source": [
    "If we want to dig deeper into the working mechanism behind the scenes, we can look at `RunnableMap`, which we created as an input and operated in the same way. We can see that in it, `RunnableMap` provides two variables, `context` and `question`, one is the list of queried documents, and the other is the corresponding question. This large model can summarize and answer the corresponding questions based on the provided documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759d2af6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "759d2af6",
    "outputId": "fa16fa6c-dcb0-46e9-e538-428e711db372"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='å“ˆé‡Œæ£®åœ¨è‚¯è‚–å·¥ä½œ'),\n",
       "  Document(page_content='ç†Šå–œæ¬¢åƒèœ‚èœœ')],\n",
       " 'question': 'å“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œ?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RunnableMap object containing two key-value pairs\n",
    "# The key \"context\" corresponds to a lambda function, which is used to retrieve relevant documents. The function input parameter is x, which is the input dictionary. The function return value is retriever.get_relevant_documents(x[\"question\"])\n",
    "# The key \"question\" corresponds to a lambda function, which is used to get the question. The function input parameter is x, which is the input dictionary. The function return value is x[\"question\"]\n",
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})\n",
    "\n",
    "# Call the invoke method of inputs and pass it a dictionary as a parameter. The dictionary contains a key-value pair, the key is \"question\" and the value is \"Where does Harrison work?\"\n",
    "inputs.invoke({\"question\": \"å“ˆé‡Œæ£®åœ¨å“ªé‡Œå·¥ä½œ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01b704",
   "metadata": {
    "id": "2c01b704"
   },
   "source": [
    "## 3. Bind\n",
    "\n",
    "In the previous chapter, we introduced the call of OpenAI function. The new `function` parameter can automatically determine whether to use the tool function. If necessary, it will return the parameters needed. Next, we also use LangChain to implement the new function of OpenAI function call. First, we need a function description information and define the function. The function here still uses the `get_current_weather` function in the previous chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b00f651",
   "metadata": {
    "id": "1b00f651"
   },
   "outputs": [],
   "source": [
    "# Define a function\n",
    "functions = [\n",
    "  {\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"è·å–æŒ‡å®šä½ç½®çš„å½“å‰å¤©æ°”æƒ…å†µ\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"location\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"åŸå¸‚å’Œçœä»½ï¼Œä¾‹å¦‚ï¼šåŒ—äº¬ï¼ŒåŒ—äº¬å¸‚\",\n",
    "        },\n",
    "        \"unit\": {\"type\": \"string\", \"enum\": [\"æ‘„æ°åº¦\", \"åæ°åº¦\"]},\n",
    "      },\n",
    "      \"required\": [\"location\"],\n",
    "    },\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ff71d",
   "metadata": {
    "id": "5d2ff71d"
   },
   "source": [
    "### 3.1 Single function binding\n",
    "\n",
    "Next, we use the `bind` method to bind the tool function to the large model and build a simple chain. After calling, we can see that an `AIMessage` is returned, in which the returned `content` is empty, but the parameters we need to call the tool function are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f693ff9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f693ff9",
    "outputId": "9ebdd754-906f-4178-a31a-899ed3c7d304"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"åŒ—äº¬ï¼ŒåŒ—äº¬å¸‚\"\\n}'}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a ChatPromptTemplate object using the ChatPromptTemplate.from_messages method\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use the bind method to bind functions parameters\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "\n",
    "runnable = prompt | model\n",
    "\n",
    "# Call the invoke method\n",
    "runnable.invoke({\"input\": \"åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5fb09",
   "metadata": {
    "id": "89a5fb09"
   },
   "source": [
    "### 3.2 Multiple function bindings\n",
    "\n",
    "At the same time, we can also define multiple `function`s, and the big model can automatically determine which function to use during the conversation. Here we define two functions. The first function is similar to the previous `weather_search`, which searches for the weather of a given airport. Then we also define a sports news search `sports_search`. The weather query function `weather_search` accepts the parameter airport_code, which is the airport code, and the sports news search function `sports_search` accepts the parameter team_name, which is the sports team name. Since we don't need to run these functions here, the big model automatically determines whether to call these functions by asking questions and returns the parameters, and will not call them directly for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b95f4029",
   "metadata": {
    "id": "b95f4029"
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"weather_search\",\n",
    "        \"description\": \"æœç´¢ç»™å®šæœºåœºä»£ç çš„å¤©æ°”\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"airport_code\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"è¦è·å–å¤©æ°”çš„æœºåœºä»£ç \"\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"airport_code\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sports_search\",\n",
    "        \"description\": \"æœç´¢æœ€è¿‘ä½“è‚²èµ›äº‹çš„æ–°é—»\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"team_name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"è¦æœç´¢çš„ä½“è‚²é˜Ÿå\"\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"team_name\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a77d53",
   "metadata": {
    "id": "d4a77d53"
   },
   "source": [
    "Then we can use the function to bind the big model and define a simple chain. We can see that after we ask relevant questions, the big model can automatically judge and correctly return the parameters and know that the function needs to be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "519e761d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "519e761d",
    "outputId": "7ad43d65-bd62-4339-c8f8-e5c74a414dac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'sports_search', 'arguments': '{\\n  \"team_name\": \"çˆ±å›½è€…é˜Ÿ\"\\n}'}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bind large model\n",
    "model = model.bind(functions=functions)\n",
    "runnable = prompt | model\n",
    "\n",
    "runnable.invoke({\"input\": \"çˆ±å›½è€…é˜Ÿæ˜¨å¤©è¡¨ç°çš„æ€ä¹ˆæ ·?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b988820",
   "metadata": {
    "id": "0b988820"
   },
   "source": [
    "## 4. Fallbacks\n",
    "\n",
    "When using early OpenAI models such as \"text-davinci-001\", these models do not support formatted output results during the conversation, that is, they all output results in the form of strings, which sometimes brings some trouble to us when we need to parse the output of LLM. For example, the following example uses the early model \"text-davinci-001\" to answer user questions. We hope that llm can output results in json format.\n",
    "\n",
    "We define the OpenAI model and create a simple chain to add json to output results in json format. We let simple_model write three poems and output them in josn format. Each poem must contain: `title, author and first sentence of the poem`. We will find that the result is only a string, and the content in the specified format cannot be output. Although there are some `[` in it, it is essentially a large string, which makes it impossible for us to parse the output.\n",
    "\n",
    "> Since OpenAI retired the model text-davinci-001 on January 4, 2024, you will use the replacement model gpt-3.5-turbo-instruct recommended by OpenAI.\n",
    "\n",
    "When using language models, you may often encounter problems from the underlying API, whether these problems are rate limits or downtime. Therefore, when you move your LLM application into a real production environment, it becomes increasingly important to guard against these problems. That's why we introduced the concept of `Fallbacks`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d93a1",
   "metadata": {},
   "source": [
    "### 4.1 Formatting output using early models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11e2b2e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "11e2b2e2",
    "outputId": "1294676f-7fbf-4185-ac16-49fa5d0512f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\n{\\n  \"title\": \"æ˜¥é£\",\\n  \"author\": \"æç™½\",\\n  \"first_line\": \"æ˜¥é£åˆç»¿æ±Ÿå—å²¸\",\\n  \"content\": [\\n    \"æ˜¥é£åˆç»¿æ±Ÿå—å²¸\",\\n    \"èŠ±å¼€æ»¡æ ‘æŸ³å¦‚ä¸\",\\n    \"é¸Ÿå„¿æ¬¢å”±å¤©åœ°å®½\",\\n    \"äººé—´æ˜¥è‰²æœ€å®œäºº\"\\n  ]\\n}\\n\\n{\\n  \"title\": \"å¤œé›¨\",\\n  \"author\": \"æœç”«\",\\n  \"first_line\": \"å¤œé›¨æ½‡æ½‡\",\\n  \"content\": [\\n    \"å¤œé›¨æ½‡æ½‡\",\\n    \"å­¤ç¯ç…§æ—§\",\\n    \"æ€å¿µå¦‚æ½®\",\\n    \"æ³›æ»¥å¿ƒå¤´\"\\n  ]\\n}\\n\\n{\\n  \"title\": \"å±±è¡Œ\",\\n  \"author\": \"ç‹ç»´\",\\n  \"first_line\": \"è¿œä¸Šå¯’å±±çŸ³å¾„æ–œ\",\\n  \"content\": [\\n    \"è¿œä¸Šå¯’å±±çŸ³å¾„æ–œ\",\\n    \"ç™½äº‘ç”Ÿå¤„æœ‰äººå®¶\",\\n    \"åœè½¦åçˆ±æ«æ—æ™š\",\\n    \"éœœå¶çº¢äºäºŒæœˆèŠ±\"\\n  ]\\n}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "# Using an early OpenAI model\n",
    "simple_model = OpenAI(\n",
    "    temperature=0,\n",
    "    max_tokens=1000,\n",
    "    model=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "simple_chain = simple_model | json.loads\n",
    "\n",
    "challenge = \"å†™ä¸‰é¦–è¯—ï¼Œå¹¶ä»¥josnæ ¼å¼è¾“å‡ºï¼Œæ¯é¦–è¯—å¿…é¡»åŒ…å«:æ ‡é¢˜ï¼Œä½œè€…å’Œè¯—çš„ç¬¬ä¸€å¥ã€‚\"\n",
    "\n",
    "simple_model.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22ae8e",
   "metadata": {
    "id": "6a22ae8e"
   },
   "source": [
    "If we use `simple_chain` to run, we will find that there is a json decoding error, because the returned result is a string and cannot be parsed, so the following code will report an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ee6ba5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "08ee6ba5",
    "outputId": "2ef7f71f-4398-47ed-a8d8-bcaa75bd91d8"
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 15 column 1 (char 147)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7b2363c45b31>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchallenge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2051\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   2054\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3505\u001b[0m         \u001b[0;34m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3507\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m   3508\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m             output = cast(\n\u001b[1;32m   1245\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1247\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3381\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3383\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m   3384\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3385\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 15 column 1 (char 147)"
     ]
    }
   ],
   "source": [
    "simple_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c534366",
   "metadata": {
    "id": "1c534366"
   },
   "source": [
    "### 4.2 Formatting output using the new model\n",
    "\n",
    "So we will find that the early version of the OpenAI model does not support formatted output, so even if LangChain is used and `json.load` is added, errors will still occur, but if we use the new `gpt-3.5-turbo` model, this problem will not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e34ed7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0e34ed7",
    "outputId": "f3ae66a2-b0c7-4557-8914-687ac20b2a08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'æ˜¥é£',\n",
       "  'author': 'æç™½',\n",
       "  'first_line': 'æ˜¥é£åˆç»¿æ±Ÿå—å²¸ã€‚',\n",
       "  'content': 'æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚'},\n",
       " 'poem2': {'title': 'é™å¤œæ€',\n",
       "  'author': 'æœç”«',\n",
       "  'first_line': 'åºŠå‰æ˜æœˆå…‰ï¼Œ',\n",
       "  'content': 'åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚'},\n",
       " 'poem3': {'title': 'ç™»é¹³é›€æ¥¼',\n",
       "  'author': 'ç‹ä¹‹æ¶£',\n",
       "  'first_line': 'ç™½æ—¥ä¾å±±å°½ï¼Œ',\n",
       "  'content': 'ç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the new model by default\n",
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads\n",
    "\n",
    "chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a2605",
   "metadata": {
    "id": "603a2605"
   },
   "source": [
    "### 4.3 Fallbacks method\n",
    "\n",
    "At this point, you may wonder if there is any way to enable the early model to achieve the formatted output effect without changing too much code, rather than writing complex formatted output code to operate on the results. At this time, we can use the `fallbacks` method to give the early model such formatting capabilities. From the results, we can also see that we have successfully used `fallbacks` to give the simple model the ability to format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b1aede1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b1aede1",
    "outputId": "3823946c-a875-4807-9b0d-ba2910fe584d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'æ˜¥é£',\n",
       "  'author': 'æç™½',\n",
       "  'first_line': 'æ˜¥é£åˆç»¿æ±Ÿå—å²¸ã€‚',\n",
       "  'content': 'æ˜¥é£åˆç»¿æ±Ÿå—å²¸ï¼Œæ˜æœˆä½•æ—¶ç…§æˆ‘è¿˜ã€‚'},\n",
       " 'poem2': {'title': 'é™å¤œæ€',\n",
       "  'author': 'æœç”«',\n",
       "  'first_line': 'åºŠå‰æ˜æœˆå…‰ï¼Œ',\n",
       "  'content': 'åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚'},\n",
       " 'poem3': {'title': 'ç™»é¹³é›€æ¥¼',\n",
       "  'author': 'ç‹ä¹‹æ¶£',\n",
       "  'first_line': 'ç™½æ—¥ä¾å±±å°½ï¼Œ',\n",
       "  'content': 'ç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using with_fallbacks mechanism\n",
    "final_chain = simple_chain.with_fallbacks([chain])\n",
    "\n",
    "# Call the invoke method of final_chain and pass the challenge parameter\n",
    "final_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f0aac",
   "metadata": {},
   "source": [
    "### 4.4 How are fallbacks implemented?\n",
    "\n",
    "When we call LLM, it is often impossible to run LLM successfully due to underlying API problems, rate problems, or network problems. In this case, we can use fallback to solve this problem. Specifically, it uses another LLM to replace the original non-operable LLM to produce results. See the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_models.openai import ChatOpenAI\n",
    "from langchain_core.chat_models.anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic().with_fallbacks([ChatOpenAI()])\n",
    "model.invoke('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8fb2aa",
   "metadata": {},
   "source": [
    "In this case, ChatAnthropic will usually be used to answer first, but if calling ChatAnthropic fails, it will fall back to using the ChatOpenAI model to generate the response. If both LLMs fail, it will fall back to a hardcoded response. Hardcoded default responses are used to handle unusual situations or provide a fallback option when the required information cannot be obtained from external resources, such as \"Looks like our LLM providers are down. Here's a nice ğŸ¦œï¸ emoji for you instead.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e1841",
   "metadata": {},
   "source": [
    "If you want to learn more about fallbacks, please refer to the [official documentation](https://python.langchain.com/docs/guides/fallbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e3229d",
   "metadata": {
    "id": "96e3229d"
   },
   "source": [
    "## 5. Interface\n",
    "\n",
    "When using LangChain, there are many interfaces, among which the public standard interfaces include:\n",
    "\n",
    "- stream: stream returns output content\n",
    "- invoke: input calls chain\n",
    "- batch: call chain in parallel in the input list\n",
    "\n",
    "These also have corresponding asynchronous methods:\n",
    "\n",
    "- astream: asynchronous stream returns output content\n",
    "- ainvoke: asynchronously call chain on input\n",
    "- abatch: asynchronously call chain in parallel in the input list\n",
    "\n",
    "First, we define a simple prompt template, that is, \"Tell me a short joke about {topic}\", and then define a simple chain `Chain = prompt | LLM | OutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e228da",
   "metadata": {
    "id": "72e228da"
   },
   "outputs": [],
   "source": [
    "# Create a ChatPromptTemplate object, using the template \"Tell me a short joke about {topic}\"\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{topic}çš„çŸ­ç¬‘è¯\"\n",
    ")\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a StrOutputParser object\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create a chain to connect prompt, model and output_parser\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b02924",
   "metadata": {
    "id": "e8b02924"
   },
   "source": [
    "### 5.1 Invoke interface\n",
    "\n",
    "Next, we use the corresponding interfaces respectively. For example, we first use the conventional `invoke` call, which is also the method shown above, and we get the corresponding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e339d019",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "e339d019",
    "outputId": "6893312f-8473-411f-d175-c351532e5646"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'å½“ç†Šåœ¨æ£®æ—é‡Œé‡åˆ°ä¸€åªå…”å­æ—¶ï¼Œä»–é—®ï¼šâ€œå…”å­å…ˆç”Ÿï¼Œä½ æœ‰æ²¡æœ‰é—®é¢˜ï¼Ÿâ€å…”å­å›ç­”é“ï¼šâ€œå½“ç„¶ï¼Œå…ˆç”Ÿç†Šï¼Œæˆ‘æœ‰ä¸€ä¸ªé—®é¢˜ã€‚ä½ æ€ä¹ˆä¼šæ‹‰è¿™ä¹ˆé•¿çš„å°¾å·´ï¼Ÿâ€ç†Šå¬åç¬‘äº†èµ·æ¥ï¼šâ€œå…”å­å…ˆç”Ÿï¼Œè¿™ä¸æ˜¯å°¾å·´ï¼Œè¿™æ˜¯æˆ‘çš„é¢†å¸¦ï¼â€'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"ç†Š\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520f449",
   "metadata": {
    "id": "2520f449"
   },
   "source": [
    "### 5.2 batch interface\n",
    "\n",
    "Letâ€™s try to use the `batch` interface again. We will find that the large model can return the answers to two questions. We will give the chain an input list, which can contain multiple questions, and finally return the answers to multiple questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d549ac8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d549ac8a",
    "outputId": "fbd34c84-26b2-4576-d7ce-aa73be98d755"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºç†Šçš„çŸ­ç¬‘è¯ï¼š\\n\\næœ‰ä¸€å¤©ï¼Œä¸€åªç†Šèµ°è¿›äº†ä¸€å®¶é…’å§ã€‚ä»–èµ°åˆ°å§å°å‰ï¼Œå¯¹é…’ä¿è¯´ï¼šâ€œè¯·ç»™æˆ‘ä¸€æ¯â€¦â€¦èœ‚èœœå•¤é…’ã€‚â€\\n\\né…’ä¿ç–‘æƒ‘åœ°çœ‹ç€ç†Šï¼Œè¯´ï¼šâ€œå¯¹ä¸èµ·ï¼Œæˆ‘ä»¬è¿™é‡Œæ²¡æœ‰èœ‚èœœå•¤é…’ã€‚â€\\n\\nç†Šæœ‰äº›å¤±æœ›åœ°å¹äº†å£æ°”ï¼Œç„¶åè¯´ï¼šâ€œå¥½å§ï¼Œé‚£å°±ç»™æˆ‘æ¥ä¸€æ¯â€¦â€¦è‰è“é…’å§ã€‚â€\\n\\né…’ä¿æ‘‡æ‘‡å¤´ï¼Œè¯´ï¼šâ€œæŠ±æ­‰ï¼Œæˆ‘ä»¬ä¹Ÿæ²¡æœ‰è‰è“é…’ã€‚â€\\n\\nç†Šåˆå¹äº†å£æ°”ï¼Œç„¶åè¯´ï¼šâ€œé‚£è¯·ç»™æˆ‘æ¥ä¸€æ¯â€¦â€¦èœœç³–çº¢é…’å§ã€‚â€\\n\\né…’ä¿å®åœ¨æ— æ³•å¿å—äº†ï¼Œä»–å¯¹ç†Šè¯´ï¼šâ€œå¯¹ä¸èµ·ï¼Œæˆ‘ä»¬è¿™é‡Œæ²¡æœ‰è¿™äº›å¥‡æ€ªçš„é…’ï¼Œä½ æ˜¯ç†Šï¼Œä½ åº”è¯¥çŸ¥é“ç†Šåªèƒ½å–èœ‚èœœã€‚â€\\n\\nç†Šå¬åä¸€æ„£ï¼Œç„¶åè„¸è‰²ä¸€å˜ï¼Œè¯´ï¼šâ€œåŸæ¥ä½ ä»¬è¿™é‡Œæ²¡æœ‰èœ‚èœœå•¤é…’ï¼Œè‰è“é…’å’Œèœœç³–çº¢é…’ï¼Ÿé‚£è¯·ç»™æˆ‘æ¥ä¸€æ¯â€¦â€¦ç™½å¼€æ°´å§ã€‚â€',\n",
       " 'æœ‰ä¸€å¤©ï¼Œä¸€åªç‹ç‹¸åœ¨æ£®æ—é‡Œé‡åˆ°äº†ä¸€åªå…”å­ã€‚ç‹ç‹¸ç¬‘å˜»å˜»åœ°å¯¹å…”å­è¯´ï¼šâ€œå–‚ï¼Œå…”å­ï¼Œæˆ‘æœ‰ä¸€ä¸ªå¥½æ¶ˆæ¯å’Œä¸€ä¸ªåæ¶ˆæ¯ï¼Œä½ æƒ³å…ˆå¬å“ªä¸ªï¼Ÿâ€å…”å­æœ‰äº›å¥½å¥‡åœ°é—®ï¼šâ€œé‚£å°±å…ˆå‘Šè¯‰æˆ‘å¥½æ¶ˆæ¯å§ã€‚â€ç‹ç‹¸çœ¯èµ·çœ¼ç›è¯´ï¼šâ€œå¥½æ¶ˆæ¯æ˜¯ï¼Œä½ çš„æ™ºå•†æ¯”æˆ‘é«˜ã€‚â€å…”å­é«˜å…´åœ°è·³äº†èµ·æ¥ï¼šâ€œå¤ªå¥½äº†ï¼Œé‚£åæ¶ˆæ¯æ˜¯ä»€ä¹ˆï¼Ÿâ€ç‹ç‹¸ä¸€è„¸è½»æ¾åœ°è¯´ï¼šâ€œåæ¶ˆæ¯æ˜¯ï¼Œä½ çš„æ™ºå•†è¿˜æ¯”ä¸è¿‡èƒ¡èåœã€‚â€']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"ç†Š\"}, {\"topic\": \"ç‹ç‹¸\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a41d2",
   "metadata": {
    "id": "307a41d2"
   },
   "source": [
    "### 5.3 stream interface\n",
    "\n",
    "Next, let's take a look at the `stream` interface, which is streaming output content. This function is very necessary. Sometimes it can save users the trouble of waiting and let users see words pop up one by one instead of an empty screen, which will bring a better user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f934e46d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f934e46d",
    "outputId": "872109e2-47cb-4da6-a439-eb010ed3530d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¥½\n",
      "çš„\n",
      "ï¼Œ\n",
      "è¿™\n",
      "æ˜¯\n",
      "ä¸€ä¸ª\n",
      "å…³\n",
      "äº\n",
      "ç†Š\n",
      "çš„\n",
      "çŸ­\n",
      "ç¬‘\n",
      "è¯\n",
      "ï¼š\n",
      "\n",
      "\n",
      "æœ‰\n",
      "ä¸€\n",
      "å¤©\n",
      "ï¼Œ\n",
      "ä¸€\n",
      "åª\n",
      "å°\n",
      "ç†Š\n",
      "èµ°\n",
      "è¿›\n",
      "äº†\n",
      "ä¸€\n",
      "å®¶\n",
      "é…’\n",
      "å§\n",
      "ã€‚\n",
      "ä»–\n",
      "èµ°\n",
      "åˆ°\n",
      "å§\n",
      "å°\n",
      "å‰\n",
      "ï¼Œ\n",
      "å¯¹\n",
      "é…’\n",
      "ä¿\n",
      "è¯´\n",
      "ï¼šâ€œ\n",
      "é…’\n",
      "ä¿\n",
      "ï¼Œ\n",
      "ç»™\n",
      "æˆ‘\n",
      "ä¸€\n",
      "æ¯\n",
      "ç‰›\n",
      "å¥¶\n",
      "ã€‚â€\n",
      "\n",
      "\n",
      "é…’\n",
      "ä¿\n",
      "æƒŠ\n",
      "è®¶\n",
      "åœ°\n",
      "é—®\n",
      "é“\n",
      "ï¼šâ€œ\n",
      "å°\n",
      "ç†Š\n",
      "ï¼Œ\n",
      "ä½ \n",
      "æ€\n",
      "ä¹ˆ\n",
      "ä¼š\n",
      "æ¥\n",
      "è¿™\n",
      "é‡Œ\n",
      "å–\n",
      "ç‰›\n",
      "å¥¶\n",
      "ï¼Ÿ\n",
      "â€\n",
      "\n",
      "\n",
      "å°\n",
      "ç†Š\n",
      "æ·±\n",
      "æƒ…\n",
      "åœ°\n",
      "å›\n",
      "ç­”\n",
      "ï¼šâ€œ\n",
      "å› \n",
      "ä¸º\n",
      "æˆ‘çš„\n",
      "å¦ˆ\n",
      "å¦ˆ\n",
      "è¯´\n",
      "ï¼Œ\n",
      "æ¯\n",
      "å½“\n",
      "æˆ‘\n",
      "å–\n",
      "é…’\n",
      "çš„\n",
      "æ—¶\n",
      "å€™\n",
      "ï¼Œ\n",
      "æˆ‘\n",
      "éƒ½\n",
      "ä¼š\n",
      "å˜\n",
      "å¾—\n",
      "ç†Š\n",
      "æ ·\n",
      "ï¼â€\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in chain.stream({\"topic\": \"ç†Š\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda98771",
   "metadata": {
    "id": "fda98771"
   },
   "source": [
    "### 5.4 Asynchronous interface\n",
    "\n",
    "We can also try to call asynchronously, using `ainvoke`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd372fec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "cd372fec",
    "outputId": "1232ac0b-7900-493c-aec6-7b99abb928d8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªå…³äºç†Šçš„çŸ­ç¬‘è¯ï¼š\\n\\næœ‰ä¸€åªç†Šèµ°è¿›äº†ä¸€å®¶é¤å…ï¼Œä»–èµ°åˆ°æŸœå°å‰ï¼Œå¯¹ç€æœåŠ¡å‘˜è¯´ï¼šâ€œæˆ‘æƒ³è¦ä¸€æ¯å’–å•¡å’Œ......å—¯ï¼Œä¸€å—...ç‰›è‚‰ä¸‰æ˜æ²»ã€‚â€\\næœåŠ¡å‘˜ç–‘æƒ‘åœ°çœ‹ç€ç†Šï¼Œç„¶åé—®é“ï¼šâ€œå¯¹ä¸èµ·ï¼Œå…ˆç”Ÿï¼Œä½ æ˜¯çœŸçš„æƒ³è¦ä¸€å—ç‰›è‚‰ä¸‰æ˜æ²»å—ï¼Ÿâ€\\nç†Šç‚¹äº†ç‚¹å¤´ã€‚\\næœåŠ¡å‘˜åˆé—®ï¼šâ€œé‚£è¯·é—®ä¸ºä»€ä¹ˆä½ è¦æ¥è¿™é‡Œç‚¹é¤å‘¢ï¼Ÿâ€\\nç†Šå›ç­”é“ï¼šâ€œå› ä¸ºæˆ‘æ˜¯ä¸ªç†Šå•Šï¼â€'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await chain.ainvoke({\"topic\": \"ç†Š\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c23d0",
   "metadata": {
    "id": "926c23d0"
   },
   "source": [
    "## 6. English prompt words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nf-C5Y-9TR2d",
   "metadata": {
    "id": "nf-C5Y-9TR2d"
   },
   "source": [
    "**1. Build a simple chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "urymAcXkTOQ2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "urymAcXkTOQ2",
    "outputId": "97d1aee3-86a0-44cb-c534-6db4ffb6a11b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Why did the bear bring a flashlight to the party?\\n\\nBecause he wanted to be the \"light\" of the bearbecue!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b6bd6",
   "metadata": {},
   "source": [
    "**2.1 Building a simple document database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "J-IpScccTHtL",
   "metadata": {
    "id": "J-IpScccTHtL"
   },
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4nWE8nLjTJuT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nWE8nLjTJuT",
    "outputId": "65742853-5146-4a1c-fb0b-d1e456532b95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='harrison worked at kensho'),\n",
       " Document(page_content='bears like to eat honey')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hygSW50bTXzV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hygSW50bTXzV",
    "outputId": "2f88f2d7-781d-41d0-c50e-fa18e0420c3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='bears like to eat honey'),\n",
       " Document(page_content='harrison worked at kensho')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"what do bears like to eat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "-6iG51i5TZGJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-6iG51i5TZGJ",
    "outputId": "29de378b-2151-43fc-d2fc-f6a1b8d16896"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabda125",
   "metadata": {},
   "source": [
    "**3.2 Using RunnableMap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "JoLBmwfETg1L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoLBmwfETg1L",
    "outputId": "73127639-c19e-400c-e26e-8f68dfd93635"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='bears like to eat honey')],\n",
       " 'question': 'where did harrison work?'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})\n",
    "\n",
    "inputs.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HqjgfGT8Tj2Z",
   "metadata": {
    "id": "HqjgfGT8Tj2Z"
   },
   "source": [
    "**3.1 Single function binding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "_XZPKC1_Th6S",
   "metadata": {
    "id": "_XZPKC1_Th6S"
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0UPE0nWDTm3T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UPE0nWDTm3T",
    "outputId": "a4762623-c42f-4950-a21e-da73f7efe0c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'weather_search', 'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}'}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "\n",
    "runnable = prompt | model\n",
    "\n",
    "runnable.invoke({\"input\": \"what is the weather in sf\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a3fd8",
   "metadata": {},
   "source": [
    "**3.2 Multiple function binding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "U2R9nKncTpy6",
   "metadata": {
    "id": "U2R9nKncTpy6"
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "        {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "i9yES-wwTrYO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9yES-wwTrYO",
    "outputId": "c4e1ae94-d087-454b-a8c1-7fab9df2b790"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'sports_search', 'arguments': '{\\n  \"team_name\": \"patriots\"\\n}'}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.bind(functions=functions)\n",
    "\n",
    "runnable = prompt | model\n",
    "\n",
    "runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a9405",
   "metadata": {},
   "source": [
    "**4.1 Formatting output using early models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4lECkukwT4ua",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "4lECkukwT4ua",
    "outputId": "88c67d19-6d26-406a-9d26-08827794dc4e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\n{\\n    \"title\": \"Autumn Leaves\",\\n    \"author\": \"Emily Dickinson\",\\n    \"first_line\": \"The leaves are falling, one by one\"\\n}\\n\\n{\\n    \"title\": \"The Ocean\\'s Song\",\\n    \"author\": \"Pablo Neruda\",\\n    \"first_line\": \"I hear the ocean\\'s song, a symphony of waves\"\\n}\\n\\n{\\n    \"title\": \"A Winter\\'s Night\",\\n    \"author\": \"Robert Frost\",\\n    \"first_line\": \"The snow falls softly, covering the ground\"\\n}'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model = OpenAI(\n",
    "    temperature=0,\n",
    "    max_tokens=1000,\n",
    "    model=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "simple_chain = simple_model | json.loads\n",
    "\n",
    "challenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
    "\n",
    "simple_model.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c60800",
   "metadata": {},
   "source": [
    "**Early models are not supported and will result in decoding errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aX0oo-25T9hE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "aX0oo-25T9hE",
    "outputId": "8425f5f7-5891-4448-d67d-be89208e05a9"
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 9 column 1 (char 125)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-7b2363c45b31>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchallenge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2051\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   2054\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3505\u001b[0m         \u001b[0;34m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3507\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m   3508\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m             output = cast(\n\u001b[1;32m   1245\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1247\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3381\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3383\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m   3384\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3385\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 9 column 1 (char 125)"
     ]
    }
   ],
   "source": [
    "simple_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a60b7",
   "metadata": {},
   "source": [
    "**4.2 Newer models can format output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "oLFppvMMT-w7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLFppvMMT-w7",
    "outputId": "3fa0a0c1-3e9b-44ca-acaa-4b5fe5915c43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'Whispers of the Wind',\n",
       "  'author': 'Emily Rivers',\n",
       "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
       " 'poem2': {'title': 'Silent Serenade',\n",
       "  'author': 'Jacob Moore',\n",
       "  'first_line': 'In the stillness of night, a silent serenade'},\n",
       " 'poem3': {'title': 'Dancing Shadows',\n",
       "  'author': 'Sophia Anderson',\n",
       "  'first_line': 'Shadows dance upon the walls, a secret ballet'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads\n",
    "\n",
    "chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fe472",
   "metadata": {},
   "source": [
    "**4.3 Fallback mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "mWQ6qEwSUA0y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWQ6qEwSUA0y",
    "outputId": "1ca395fa-976f-4f5d-c0f4-f2bcbd0ba325"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'Whispers of the Wind',\n",
       "  'author': 'Emily Rivers',\n",
       "  'first_line': 'Softly it comes, the whisper of the wind'},\n",
       " 'poem2': {'title': 'Silent Serenade',\n",
       "  'author': 'Jacob Moore',\n",
       "  'first_line': 'In the stillness of night, a silent serenade'},\n",
       " 'poem3': {'title': 'Dancing Shadows',\n",
       "  'author': 'Sophia Anderson',\n",
       "  'first_line': 'Shadows dance upon the moonlit floor'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = simple_chain.with_fallbacks([chain])\n",
    "\n",
    "final_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd87734",
   "metadata": {},
   "source": [
    "**V. Interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "oenACTHsUFlm",
   "metadata": {
    "id": "oenACTHsUFlm"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5feb063",
   "metadata": {},
   "source": [
    "**5.1 invoke interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "uhpMBbexUHK2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "uhpMBbexUHK2",
    "outputId": "bd63b7f7-7d11-474e-bdd7-ad59a45323ff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6e2de",
   "metadata": {},
   "source": [
    "**5.2 Batch interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ptSp8UhQUILV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptSp8UhQUILV",
    "outputId": "b3125d7e-c35b-4ea3-999f-75543f36ac34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.language_models.llms:Retrying langchain_community.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-GXqDV5Kltm74g4hGPYEjLdb0 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\",\n",
       " 'Why did the frog take the bus to work?\\n\\nBecause his car got toad away!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b9ed7",
   "metadata": {},
   "source": [
    "**5.3 Stream Interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "Ehj9MzGPUI_I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ehj9MzGPUI_I",
    "outputId": "239b158c-6283-4892-d6f5-bcbe69329cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " don\n",
      "'t\n",
      " bears\n",
      " wear\n",
      " shoes\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " they\n",
      " have\n",
      " bear\n",
      " feet\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2858691",
   "metadata": {},
   "source": [
    "**5.4 Asynchronous Interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "xucFn07uUJ7Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xucFn07uUJ7Z",
    "outputId": "54545335-949b-463d-b8e2-41bb7c1076c1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
