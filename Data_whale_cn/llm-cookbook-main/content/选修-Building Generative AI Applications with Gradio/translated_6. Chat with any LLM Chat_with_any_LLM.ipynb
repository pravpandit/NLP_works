{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cf53b5",
   "metadata": {
    "height": 30
   },
   "source": [
    "# Chapter 6 Chatting with any big language model! ğŸ’¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7893c",
   "metadata": {
    "height": 30
   },
   "source": [
    "In this final lesson, we will build an application that chats with an open source LLM. We will use one of the best open source models, the Falcon 40B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d0ff1d",
   "metadata": {
    "height": 30
   },
   "source": [
    "We will build a chatbot application using the open source LLM. You may have used ChatGPT to chat, but it is expensive to run and the interaction patterns are rigid. A custom LLM can be run locally, fine-tuned on your own data, or run on the cloud at a lower cost. In this lesson, we will use the inference endpoint running \"falcon-40B-Instruct\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062996ff",
   "metadata": {
    "height": 30
   },
   "source": [
    "It's convenient to run locally using the Text Generation Inference Library. Of course, you can also use Gradio to create interfaces. Gradio is an API-based LLM, so it supports not only open source LLMs. But in this course, we will focus on the open source LLM Falcon 40B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce5955",
   "metadata": {
    "height": 30
   },
   "source": [
    "Loading the HF API key and related Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa6fa00-6bd1-4839-bcaf-8bae9267ee79",
   "metadata": {
    "height": 199
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os                # ç”¨äºæ“ä½œç³»ç»Ÿç›¸å…³çš„æ“ä½œï¼Œä¾‹å¦‚è¯»å–ç¯å¢ƒå˜é‡\n",
    "import io                # ç”¨äºå¤„ç†æµå¼æ•°æ®ï¼ˆä¾‹å¦‚æ–‡ä»¶æµï¼‰\n",
    "import IPython.display   # ç”¨äºåœ¨IPythonç¯å¢ƒä¸­æ˜¾ç¤ºæ•°æ®ï¼Œä¾‹å¦‚å›¾ç‰‡\n",
    "from PIL import Image    # ç”¨äºå¤„ç†å›¾åƒæ•°æ®\n",
    "import base64            # ç”¨äºå¤„ç†base64ç¼–ç ï¼Œé€šå¸¸ç”¨äºç¼–ç å›¾åƒæ•°æ®\n",
    "import requests          # ç”¨äºè¿›è¡ŒHTTPè¯·æ±‚ï¼Œä¾‹å¦‚GETå’ŒPOSTè¯·æ±‚\n",
    "\n",
    "# Set the default request timeout to 60 seconds\n",
    "requests.adapters.DEFAULT_TIMEOUT = 60\n",
    "\n",
    "# Import the function of dotenv library\n",
    "# dotenv allows you to read environment variables from a .env file\n",
    "# This is particularly useful when developing to avoid hard-coding sensitive information (such as API keys) into your code\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Find the .env file and load its contents\n",
    "# This allows you to use os.environ to read environment variables set in a .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Read 'HF_API_KEY' from the environment variable and store it in the hf_api_key variable\n",
    "hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f627a",
   "metadata": {
    "height": 30
   },
   "source": [
    "We set up our token and our helper functions here. You can see that we're using different libraries here. We're using the Text Generation Library, which is a stripped-down library for working with open source LLMs that allows you to both load an API (like we're doing here) and also run your own LLM locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095da8fe-24aa-4dc7-8e08-aa2f949ae21f",
   "metadata": {
    "height": 131
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Import necessary libraries\n",
    "import requests        # ç”¨äºè¿›è¡ŒHTTPè¯·æ±‚\n",
    "import json            # ç”¨äºå¤„ç†JSONæ•°æ®\n",
    "\n",
    "# Import the Client class in the custom module\n",
    "# Assume this class is used to communicate with the FalcomLM-instruct endpoint\n",
    "from text_generation import Client\n",
    "\n",
    "# Use os.environ to get the value of 'HF_API_FALCOM_BASE' from the environment variable, this should be the base URL of FalcomLM\n",
    "# Create a client instance using hf_api_key as part of the authentication\n",
    "# Set the request timeout to 120 seconds\n",
    "client = Client(os.environ['HF_API_FALCOM_BASE'], \n",
    "                headers={\"Authorization\": f\"Basic {hf_api_key}\"}, \n",
    "                timeout=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d034a95",
   "metadata": {
    "height": 30
   },
   "source": [
    "## Build an app to chat with any LLM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c335e24",
   "metadata": {
    "height": 30
   },
   "source": [
    "Here we will use an [inference endpoint](https://huggingface.co/inference-endpoints) to call `falcon-40b-instruct`, which is ranked high on the [ğŸ¤— open source LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
    "\n",
    "If you want to call it locally, you can use the [Transformers library](https://huggingface.co/docs/transformers/index) or the [text generation inference library](https://github.com/huggingface/text-generation-inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14f6261",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\næ•°å­¦æ˜¯è¢«å‘ç°çš„ï¼Œå› ä¸ºå®ƒæ˜¯è‡ªç„¶ç•Œçš„åŸºæœ¬è§„å¾‹å’Œæ¨¡å¼ã€‚äººç±»åªæ˜¯å‘ç°äº†è¿™äº›è§„å¾‹å’Œæ¨¡å¼ï¼Œå¹¶å°†å®ƒä»¬ç”¨ç¬¦å·å’Œå…¬å¼è¡¨è¾¾å‡ºæ¥ã€‚'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a Chinese prompt, which will be sent to the model for generating text\n",
    "prompt_chinese = \"æ•°å­¦æ˜¯è¢«å‘æ˜è¿˜æ˜¯è¢«å‘ç°çš„ï¼Ÿ\"\n",
    "\n",
    "#Call the client's generate method\n",
    "# Use the previously set Chinese prompt prompt_chinese\n",
    "# max_new_tokens parameter is used to limit the length of the generated text\n",
    "# Then get the generated text from the returned result\n",
    "client.generate(prompt_chinese, max_new_tokens=256).generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7065860-3c0b-490d-9e7c-22e5b79fc004",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMath has been both invented and discovered. It is a human invention in the sense that it is a system of rules and concepts that we have created to help us understand the world around us. However, it is also a discovery in the sense that it is a fundamental aspect of the universe that we have uncovered through our observations and experiments.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Has math been invented or discovered?\"\n",
    "client.generate(prompt, max_new_tokens=256).generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85eeeba",
   "metadata": {
    "height": 30
   },
   "source": [
    "In Lesson 2, we used a very simple Gradio interface with a textbox input and an output. Here, we can chat with LLM in a similar way. Duplicate our prompt again. Here, we can decide how many tokens we want. That's how you can ask LLM a question very simply. But we still can't chat because it won't understand or retain the context if you ask a follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb659e-b71b-46da-b9d2-6ee62498995f",
   "metadata": {
    "height": 182
   },
   "outputs": [],
   "source": [
    "# Back to Lesson 2, time flies!\n",
    "# Import required libraries\n",
    "import gradio as gr  # ç”¨äºåˆ›å»ºWebç•Œé¢\n",
    "import os  # ç”¨äºä¸æ“ä½œç³»ç»Ÿäº¤äº’ï¼Œå¦‚è¯»å–ç¯å¢ƒå˜é‡\n",
    "\n",
    "# Define a function to generate text based on input\n",
    "def generate(input, slider):\n",
    "# Use the predefined client object's generate method to generate text from the input\n",
    "# The slider value limits the number of tokens generated\n",
    "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
    "    return output  # è¿”å›ç”Ÿæˆçš„æ–‡æœ¬\n",
    "\n",
    "# Create a web interface\n",
    "# Input: a text box and a slider\n",
    "# Output: A text box displays the generated text\n",
    "demo = gr.Interface(\n",
    "    fn=generate, \n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Prompt\"),  # æ–‡æœ¬è¾“å…¥æ¡†\n",
    "        gr.Slider(label=\"Max new tokens\", value=20,  maximum=1024, minimum=1)  # æ»‘å—ç”¨äºé€‰æ‹©ç”Ÿæˆçš„tokençš„æœ€å¤§æ•°é‡\n",
    "    ], \n",
    "    outputs=[gr.Textbox(label=\"Completion\")]  # æ˜¾ç¤ºç”Ÿæˆæ–‡æœ¬çš„æ–‡æœ¬æ¡†\n",
    ")\n",
    "\n",
    "# Close any previous gradio instances that may have been started\n",
    "gr.close_all()\n",
    "\n",
    "# Start the Web interface\n",
    "# Use the environment variable PORT1 as the server port number\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5b428",
   "metadata": {
    "height": 30
   },
   "source": [
    "So basically what we do is, we send the model our previous question, its own answer, and the follow-up question. But setting all of this up is a bit cumbersome. That's where the Gradio chatbot component comes in, as it allows us to simplify the process of sending our conversation history to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc48b74",
   "metadata": {
    "height": 30
   },
   "source": [
    "Therefore, we want to solve this problem. To do this, we will introduce a new Gradio component - Gradio Chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1943fd-213a-48bb-966e-e84b9ae255b1",
   "metadata": {},
   "source": [
    "![math](images/ch06_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33547d43",
   "metadata": {
    "height": 30
   },
   "source": [
    "## Use `gr.Chatbot()` to help!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83360647",
   "metadata": {
    "height": 30
   },
   "source": [
    "Let's get started with the Gradio Chatbot component. Here we instantiate a Gradio ChatBot component with a text box prompt and a submit button, which is a very simple user interface. But we are not chatting with LLM yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c70dcae",
   "metadata": {
    "height": 30
   },
   "source": [
    "Just randomly select three canned responses and then add my message and the bot's message to the chat. So here, you can see I can say anything and it will basically randomly look at those three responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43beebb7-40a6-4af5-a701-882821b6ed36",
   "metadata": {
    "height": 369
   },
   "outputs": [],
   "source": [
    "# Import the random library to randomly select messages\n",
    "import random\n",
    "import gradio as gr  # ç”¨äºåˆ›å»ºWebç•Œé¢\n",
    "import os  # ç”¨äºä¸æ“ä½œç³»ç»Ÿäº¤äº’ï¼Œå¦‚è¯»å–ç¯å¢ƒå˜é‡\n",
    "\n",
    "# Define the robot's response function\n",
    "def respond(message, chat_history):\n",
    "# Randomly select a Chinese reply\n",
    "    bot_message_chinese = random.choice([\"å‘Šè¯‰æˆ‘æ›´å¤šä¿¡æ¯\", \n",
    "                                         \"é…·ï¼Œä½†æˆ‘ä¸æ„Ÿå…´è¶£\", \n",
    "                                         \"å—¯ï¼Œé‚£å¥½å§\"])\n",
    "    \n",
    "# Randomly select an English reply (this reply is not used in the interface)\n",
    "    bot_message = random.choice([\"Tell me more about it\", \n",
    "                                 \"Cool, but I'm not interested\", \n",
    "                                 \"Hmmmm, ok then\"]) \n",
    "    \n",
    "# Add the user's message and the robot's Chinese reply to the chat history\n",
    "    chat_history.append((message, bot_message_chinese))\n",
    "    \n",
    "# Returns an empty string and the updated chat history\n",
    "    return \"\", chat_history\n",
    "\n",
    "# Use the gr.Blocks() context manager to define the user interface\n",
    "with gr.Blocks() as demo:\n",
    "# Define a chatbot window with a height of 240\n",
    "    chatbot = gr.Chatbot(height=240) \n",
    "# Define a text box for user input messages\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "# Define a submit button\n",
    "    btn = gr.Button(\"Submit\")\n",
    "# Define a button to clear the text box and chat window contents\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "# When the user clicks the submit button, the response function is called\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "# When the user presses the Enter key in the text box, the response function is also called\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "\n",
    "# Close any previous gradio instances that may have been started\n",
    "gr.close_all()\n",
    "\n",
    "# Start the Web interface\n",
    "# Use the environment variable PORT2 as the server port number\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179df18-bad5-430d-a91b-1fb0c972d3ca",
   "metadata": {},
   "source": [
    "![math_with_template](images/ch06_math_with_template.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d932fde-da5e-47f1-959b-86b053bb9a42",
   "metadata": {},
   "source": [
    "We have to format the chat prompt. This format chat prompt function is being defined here.\n",
    "Here, all we want to do is make it include the chat history so that LLM knows the context.\n",
    "But that's not enough. We also need to tell it which information is from the user and which information is from LLM itself, which is the assistant we are calling.\n",
    "So we set up the format chat prompt function, and in each turn of the chat history, include a user information and an assistant information so that our model can accurately answer follow-up questions.\n",
    "Now, we want to pass the formatted prompt to our API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bae99d-7a63-4a40-bab7-de7d10b8ab1b",
   "metadata": {
    "height": 471
   },
   "outputs": [],
   "source": [
    "# Define a function to format the chat prompt.\n",
    "def format_chat_prompt(message, chat_history):\n",
    "# Initialize an empty string to store the formatted chat prompt.\n",
    "    prompt = \"\"\n",
    "# Traverse the chat history.\n",
    "    for turn in chat_history:\n",
    "# Extract user and bot messages from the chat log.\n",
    "        user_message, bot_message = turn\n",
    "# Updated prompts to include messages for users and robots.\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "# Add the current user message to the prompt and reserve a place for the robot's reply.\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "# Returns the formatted prompt.\n",
    "    return prompt\n",
    "\n",
    "# Define a function to generate the robot's reply.\n",
    "def respond(message, chat_history):\n",
    "# Call the above function to format the user's message and chat history into a prompt.\n",
    "    formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "# Use the generate method of the client object to generate the robot's reply (note: the client object is not defined in this code).\n",
    "    bot_message = client.generate(formatted_prompt,\n",
    "                                  max_new_tokens=1024,\n",
    "                                  stop_sequences=[\"\\nUser:\", \"\"]).generated_text\n",
    "# Add the user's message and the robot's reply to the chat history.\n",
    "    chat_history.append((message, bot_message))\n",
    "# Returns an empty string and the updated chat history (the empty string here can be replaced with a real robot reply and displayed on the interface if necessary).\n",
    "    return \"\", chat_history\n",
    "\n",
    "# The following code is the part that sets up the Gradio interface.\n",
    "\n",
    "# Define a code block using Gradio's Blocks feature.\n",
    "with gr.Blocks() as demo:\n",
    "# Create a Gradio chatbot component and set its height to 240.\n",
    "    chatbot = gr.Chatbot(height=240) \n",
    "# Create a text box component for input prompts.\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "# Create a submit button.\n",
    "    btn = gr.Button(\"Submit\")\n",
    "# Create a clear button that clears the contents of the text box and chatbot components.\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "# Set the button's click event. When clicked, call the respond function defined above, pass in the user's message and chat history, and then update the text box and chatbot components.\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "# Set the submit event of the text box (when the Enter key is pressed). The function is the same as the button click event above.\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) \n",
    "\n",
    "# Close all existing Gradio instances.\n",
    "gr.close_all()\n",
    "# Start a new Gradio application, set the sharing function to True, and use the environment variable PORT3 to specify the server port.\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45875a-8ca4-4124-982e-f2ee6c6e597a",
   "metadata": {},
   "source": [
    "![animal](images/ch06_animal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022bb649-868d-453d-95e4-fef9cb1feadd",
   "metadata": {},
   "source": [
    "![animal_in_context](images/ch06_animal_in_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae2ad9",
   "metadata": {
    "height": 30
   },
   "source": [
    "Now our chatbot should be able to answer follow-up questions.\n",
    "We can see that we sent it context. We sent it information and then asked it to complete. Once we enter another iteration loop, we send it our entire context and then ask it to complete. This is cool. However, if we keep iterating like this, then the model will reach a limit on the amount of information it can take in in a single conversation because we are always giving it more and more of the previous conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250fcfff",
   "metadata": {
    "height": 46
   },
   "source": [
    "To maximize the model's performance, we can set the maximum number of tokens `max_new_tokens` to 1024. This is the maximum value that the model can accept under the hardware conditions we run in the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b9d8d",
   "metadata": {
    "height": 64
   },
   "source": [
    "Try the following prompts:\n",
    "1. Which animals live in the savannah?\n",
    "2. Which of these animals is the strongest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded928f",
   "metadata": {
    "height": 30
   },
   "source": [
    "Here, we have created a simple but powerful user interface for chatting with LLM. If we need to go further with the best that Gradio has to offer, we can create a user interface with more features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b69830",
   "metadata": {
    "height": 30
   },
   "source": [
    "### Adding other advanced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09873dfd-5b6c-41d6-9479-12e8c8894295",
   "metadata": {
    "height": 828
   },
   "outputs": [],
   "source": [
    "# Define a function to format the chat prompt.\n",
    "def format_chat_prompt(message, chat_history, instruction):\n",
    "# Initialize prompts and add system commands.\n",
    "    prompt = f\"System:{instruction}\"\n",
    "# Traverse the chat history.\n",
    "    for turn in chat_history:\n",
    "# Extract user and bot messages from the chat log.\n",
    "        user_message, bot_message = turn\n",
    "# Updated prompts to include messages for users and robots.\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "# Add the current user message to the prompt and reserve a place for the robot's reply.\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "# Returns the formatted prompt.\n",
    "    return prompt\n",
    "\n",
    "# Define a function to generate the robot's reply.\n",
    "def respond(message, chat_history, instruction, temperature=0.7):\n",
    "# Call the above function to format the user's message, chat history, and system commands into a prompt.\n",
    "    prompt = format_chat_prompt(message, chat_history, instruction)\n",
    "# Update the chat history, adding the user's message first (the robot's reply part is empty at first).\n",
    "    chat_history = chat_history + [[message, \"\"]]\n",
    "# Generate the robot's reply using the generate_stream method of the client object (note: the client object is not defined in this code).\n",
    "    stream = client.generate_stream(prompt,\n",
    "                                    max_new_tokens=1024,\n",
    "                                    stop_sequences=[\"\\nUser:\", \"\"], \n",
    "                                    temperature=temperature)  # è®¾ç½®ç”Ÿæˆå›å¤çš„æ¸©åº¦ï¼Œå†³å®šå›å¤çš„éšæœºæ€§ã€‚\n",
    "    acc_text = \"\"\n",
    "# Get the bot's response using streaming.\n",
    "    for idx, response in enumerate(stream):\n",
    "        text_token = response.token.text\n",
    "\n",
    "# If there is any detailed information, return it directly.\n",
    "        if response.details:\n",
    "            return\n",
    "\n",
    "# If it is the first token and it starts with whitespace, remove the whitespace.\n",
    "        if idx == 0 and text_token.startswith(\" \"):\n",
    "            text_token = text_token[1:]\n",
    "\n",
    "# Cumulative generated text.\n",
    "        acc_text += text_token\n",
    "# Update the last round of chat history.\n",
    "        last_turn = list(chat_history.pop(-1))\n",
    "        last_turn[-1] += acc_text\n",
    "        chat_history = chat_history + [last_turn]\n",
    "        yield \"\", chat_history\n",
    "        acc_text = \"\"\n",
    "\n",
    "# Set up the Gradio interface part.\n",
    "with gr.Blocks() as demo:\n",
    "# Create a Gradio chatbot component and set its height.\n",
    "    chatbot = gr.Chatbot(height=240)\n",
    "# Create a text box component for input prompts.\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "# Create an accordion component to display advanced options.\n",
    "    with gr.Accordion(label=\"Advanced options\", open=False):\n",
    "# Create a text box inside the accordion component for entering system messages.\n",
    "        system = gr.Textbox(label=\"System message\", lines=2, value=\"ä¸€æ®µç”¨æˆ·å’ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ³•å¾‹åŠ©æ‰‹çš„å¯¹è¯. åŠ©æ‰‹ä¼šç»™å‡ºçœŸå®ä¸”æœ‰å¸®åŠ©çš„å›ç­”.\")\n",
    "# Create a slider to adjust the temperature of the response.\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
    "# Create a submit button.\n",
    "    btn = gr.Button(\"Submit\")\n",
    "# Create a clear button that clears the contents of the text box and chatbot components.\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "# Set the button's click event. When clicked, call the respond function defined above, pass in the user's message, chat history, and system message, and then update the text box and chatbot components.\n",
    "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
    "# Set the submit event of the text box (when the Enter key is pressed). The function is the same as the button click event above.\n",
    "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf44cb6-c55c-45eb-8aca-c60f08aa1e0f",
   "metadata": {},
   "source": [
    "![law_1](images/ch06_law_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70256e38-a97f-4375-9083-720571c6cc2c",
   "metadata": {},
   "source": [
    "![law_2](images/ch06_law_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e821c9-f568-4cf5-92b9-dee9f19a45d8",
   "metadata": {},
   "source": [
    "![law_3](images/ch06_law_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a928b-1620-4bb0-ab80-0c78ad47b96d",
   "metadata": {},
   "source": [
    "Here we have advanced options, including system messaging, which sets the mode in which LLM chats with you.\n",
    "So in system messaging, you can say, for example, you're a helpful assistant, or you can give it a specific tone, a specific intonation,\n",
    "you want it to be a little more playful, a little more serious, and you can really play around with system messaging and see what effect it has on your messages.\n",
    "\n",
    "Some people might even want to give LLM a persona, like you're a lawyer giving legal advice, or you're a doctor giving medical advice,\n",
    "but be aware that LLM has been known to give false information in a way that sounds real.\n",
    "So while it can be fun to experiment and explore with the Falcon 40B, in real-world scenarios, further safeguards must be put in place for use cases like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0e822-ec15-4cfe-9bfe-e90250fbd85e",
   "metadata": {},
   "source": [
    "There are other advanced parameters like the temperature here.\n",
    "The temperature is basically how much you want the model to vary. So if you set the temperature to zero, the model will tend to always respond the same way to the same inputs.\n",
    "So same question, same answer. The higher the temperature, the more information varies. But if the temperature is too high, it starts to give nonsense answers.\n",
    "So 0.7 is a good default parameter, but we encourage you to experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d7964",
   "metadata": {
    "height": 30
   },
   "source": [
    "Apart from that, this UI also allows us to stream responses.\n",
    "It is sent token by token and we can see it getting done in real time. So, we don't need to wait until the entire answer is ready. Here we can see how it is done. Don't worry if you don't understand everything here because our intention is to end the course with a very complete UI and provide all the features in LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979cc4b6",
   "metadata": {
    "height": 30
   },
   "source": [
    "In the format chat prompt, which is the function that we used before, we added a new element, which is the system directive. So before we start the user assistant conversation, we add a directive at the top of the system. So basically at the beginning of every message that is sent to the model, there will be the system message that we set. Here, we call the `generate_stream` function of the text generation library. And the `generate_stream` function is what it does is it generates the response tokens one by one. So in this loop, what happens is that it generates the response tokens token by token, adds it to the chat log, and then returns it to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ec80a-39ad-4f58-b79e-4f413c5074c0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Close any previous gradio instances that may have been started\n",
    "gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
