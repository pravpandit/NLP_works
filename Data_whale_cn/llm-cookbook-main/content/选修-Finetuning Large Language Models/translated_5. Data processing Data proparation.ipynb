{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfae479-9399-492d-acaa-d9751615ee86",
   "metadata": {
    "id": "1dfae479-9399-492d-acaa-d9751615ee86",
    "tags": []
   },
   "source": [
    "# Chapter 5 Data Processing\n",
    "\n",
    "- [I. Important factors in preparing training data](#I.-Important factors in preparing training data)\n",
    "- [II. Data processing steps](#II.-Data processing steps)\n",
    "- [2.1 Text Tokenization](#2.1-Text-Tokenization)\n",
    "- [2.1.1 Tokenize a text](#2.1.1-Tokenize a text)\n",
    "- [2.1.2 Tokenize multiple texts at a time](#2.1.2-Tokenize multiple texts at a time)\n",
    "- [2.1.3 Padding and truncation](#2.1.3-Padding and truncation)\n",
    "- [2.1.4 Prepare instruction data set](#2.1.4-Prepare instruction data set)\n",
    "- [2.1.5 Tokenize a sample](#2.1.5-Tokenize a sample)\n",
    "- [2.1.6 Token [Tokenized instruction dataset](#2.1.6-Tokenized instruction dataset)\n",
    "- [2.2 Splitting of test/training dataset](#2.2-Splitting of test/training dataset)\n",
    "- [2.2.1 Some datasets for you to try](#2.2.1-Some datasets for you to try)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80133889",
   "metadata": {
    "id": "80133889"
   },
   "source": [
    "In this course, you will learn how to prepare training data to provide a solid foundation for your machine learning models. We will start with data collection and guide you step by step through the process of data preprocessing, tokenization, and model training. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YeVJVkc-MbjI",
   "metadata": {
    "id": "YeVJVkc-MbjI"
   },
   "source": [
    "## 1. Important factors in preparing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e987c",
   "metadata": {
    "id": "b32e987c"
   },
   "source": [
    "**1. Data Quality**\n",
    "Data quality is the primary concern in data preparation. High-quality data can significantly improve model performance during fine-tuning and training. When preparing data, make sure to provide high-quality and accurate inputs. There is a famous saying: Garbage in, Garbage out\n",
    "\n",
    "**2. Data Diversity**\n",
    "Data diversity is another crucial factor. If the training data is too monotonous, the model may over-memorize and repeat outputs in similar situations. To avoid this, make sure the training data covers a variety of use cases and scenarios. Diverse datasets help models better understand different inputs and make more accurate predictions.\n",
    "\n",
    "**3. Data Authenticity**\n",
    "Although the generated data approach is feasible in some scenarios, in most cases, real data is very important for training and fine-tuning models. Generated data tends to have fixed patterns, which can limit the creativity and adaptability of the model. In most cases, having real data is more effective and helpful, especially for applications such as writing tasks. There are certain patterns in the generated data. Some services try to detect whether content is generated by a generative model by looking for patterns and regularities in the generated data.\n",
    "\n",
    "**4. Data quantity**\n",
    "The amount of data is indeed very important for training models. More data can usually help models generalize better and adapt to different situations. The introduction of pre-trained models can alleviate this problem to a certain extent.The problem of data quantity is that it has already established a certain foundational understanding by pre-training on a large amount of data on the Internet. So having more data helps the model, but not as important as the top three, and definitely not as important as quality.\n",
    "\n",
    "In summary, data quality, diversity, authenticity, and quantity need to be considered when preparing training data. These factors will jointly affect the performance of the model and the quality of the output. Next, let's take a deep dive into how to effectively prepare training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83a838",
   "metadata": {
    "id": "8d83a838"
   },
   "source": [
    "## 2. Data processing steps\n",
    "\n",
    "**1. Collect command-response pairs**\n",
    "Collect question-answer pairs or command-response pairs from different sources. This can be conversation records from users, existing question-answer datasets, etc.\n",
    "\n",
    "**2. Merge command pairs (add prompt templates if necessary)**\n",
    "Merge the collected command and response pairs to form a whole dataset. In this step, you can add some prompt templates to each command or response as needed to help the model better understand the context.\n",
    "\n",
    "**3. Tokenization**\n",
    "Convert text data into digital representation. This step is done by using a tokenizer, which divides the text into words or subwords and assigns a unique identifier, i.e., a token, to each word or subword. This converts the text into a form that can be understood by the machine and is ready for subsequent processing. During the tokenization process, padding or truncation operations are also required to ensure that all texts are of the same length for easy processing by the model.\n",
    "\n",
    "**4. Dataset division**\n",
    "Divide the processed dataset into a training set and a test set. The training set is used to train the parameters of the model, while the test set is used to evaluate the performance and generalization ability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a22be",
   "metadata": {
    "id": "bb6a22be"
   },
   "source": [
    "`ing` is a very common character. Most gerunds have this character. For example, finetun**ing** and tokeniz**ing** both have `ing`. In this example, `ing` is encoded as `278`.\n",
    "\n",
    "When you decode the token using the same tokenizer, it is restored to the original text.\n",
    "\n",
    "Each model is associated with a specific tokenizer and trained on it. If the wrong tokenizer is chosen, it will cause the model to think that different numbers represent different sets of letters and words, resulting in confusion and wrong results. Therefore, it is crucial to use the right tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af894033",
   "metadata": {
    "id": "af894033"
   },
   "source": [
    "![tokenizing.png](../../figures/tokenizing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c333aabd",
   "metadata": {
    "id": "c333aabd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_0kBBKjNsrML",
   "metadata": {
    "id": "_0kBBKjNsrML"
   },
   "source": [
    "The HuggingFace Transformers library is a very powerful and popular natural language processing tool library. You just need to specify the model and name you want, and it will automatically help you find the appropriate tokenizer and match it to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a30cf",
   "metadata": {
    "id": "795a30cf"
   },
   "source": [
    "### 2.1 Text Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JcZOWow4L6s_",
   "metadata": {
    "id": "JcZOWow4L6s_"
   },
   "source": [
    "Here we use the word segmenter corresponding to the 70m pythia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb2911fb",
   "metadata": {
    "id": "eb2911fb"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E0NrDzuIu2G7",
   "metadata": {
    "id": "E0NrDzuIu2G7"
   },
   "source": [
    "#### 2.1.1 Tokenizing a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0778167a",
   "metadata": {
    "id": "0778167a"
   },
   "outputs": [],
   "source": [
    "text = \"Hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emHUBlIHtq44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emHUBlIHtq44",
    "outputId": "dd690913-59c6-4ecc-8a24-1b3d9ac065fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12764, 13, 849, 403, 368, 32]\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "y8Q_Kfa1tivF",
   "metadata": {
    "id": "y8Q_Kfa1tivF"
   },
   "outputs": [],
   "source": [
    "text = \"嗨你好么\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b608b9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b608b9e",
    "outputId": "b876c673-e330-4a77-9cc9-7a2503702b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161, 234, 103, 24553, 34439, 43244]\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5qBAe0P5tEtn",
   "metadata": {
    "id": "5qBAe0P5tEtn"
   },
   "source": [
    "The tokenizer encodes this string of text into different numbers.\n",
    "The tokenizer outputs a dictionary containing `input_ids` representing tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cciQi04evAHI",
   "metadata": {
    "id": "cciQi04evAHI"
   },
   "source": [
    "Now decode it back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f9226a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75f9226a",
    "outputId": "573422b9-f031-4ac6-e8ec-fdaa3f71d43d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokens back into text:  Hi, how are you?\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "Jpfzf6v0uWs1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jpfzf6v0uWs1",
    "outputId": "82c66f19-60ac-453a-fe41-d4eac84cfe1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将 token 解码为文本:  嗨你好么\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"将 token 解码为文本: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EIKNZk6OvG8s",
   "metadata": {
    "id": "EIKNZk6OvG8s"
   },
   "source": [
    "It can be seen that the decoded text is consistent with the original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2602c1",
   "metadata": {
    "id": "de2602c1"
   },
   "source": [
    "#### 2.1.2 Tokenizing multiple texts at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JtSAmPiBvXIU",
   "metadata": {
    "id": "JtSAmPiBvXIU"
   },
   "source": [
    "Sometimes we need to tokenize multiple texts at once.\n",
    "\n",
    "When processing batch input, we can concatenate lists of texts as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5be6e7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5be6e7c",
    "outputId": "f9fb86e9-2137-489a-b710-309b56835711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
    "encoded_texts = tokenizer(list_texts)\n",
    "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "VuOAAO1Sv7xg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuOAAO1Sv7xg",
    "outputId": "984c2a4e-6a37-495e-d722-16c5b0ba6c93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码多个文本:  [[161, 234, 103, 24553, 34439, 43244], [15367, 45091, 34439], [12105]]\n"
     ]
    }
   ],
   "source": [
    "list_texts = [\"嗨你好么\", \"我很好\", \"是\"]\n",
    "encoded_texts = tokenizer(list_texts)\n",
    "print(\"编码多个文本: \", encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8jj6ZtLqv6iF",
   "metadata": {
    "id": "8jj6ZtLqv6iF"
   },
   "source": [
    "It can be seen that the length returned by the word segmenter is different for texts of different lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cbf826",
   "metadata": {
    "id": "31cbf826"
   },
   "source": [
    "#### 2.1.3 Padding and truncation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u9ULVPdj2Mzu",
   "metadata": {
    "id": "u9ULVPdj2Mzu"
   },
   "source": [
    "The model needs to process fixed-size tensors, so everything in a batch must be the same length.\n",
    "Padding is a strategy for dealing with these variable-length encoded texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gfbXF0eh2owJ",
   "metadata": {
    "id": "gfbXF0eh2owJ"
   },
   "source": [
    "When filling, you need to choose a specific number as the filling token to represent the filling. Usually `0` is used to fill, which is also the end token of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_ZKvpmIo2off",
   "metadata": {
    "id": "_ZKvpmIo2off"
   },
   "source": [
    "So when we run `padding = true` through the tokenizer, you can see that the `Yes` string has a lot of `0` on the right to make sure it is the same length as `Hi, how are you?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88d03447",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88d03447",
    "outputId": "b67909d5-d6a5-45ab-a76e-d3882f8d8627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using padding:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175, 0, 0, 0], [4374, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
    "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "qrDWBemnQzKU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrDWBemnQzKU",
    "outputId": "a9b12cdf-618b-4e14-d75e-193534471490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用填充后的结果:  [[161, 234, 103, 24553, 34439, 43244], [15367, 45091, 34439, 0, 0, 0], [12105, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
    "print(\"使用填充后的结果: \", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g4F5tVfN4BPx",
   "metadata": {
    "id": "g4F5tVfN4BPx"
   },
   "source": [
    "The model has a maximum length limit, which is the length of text that the model can process and accommodate. Therefore, it is not suitable for processing text data of arbitrary length.\n",
    "\n",
    "You may have noticed that there is a length limit when using Prompt before. This is the model's truncation strategy, which is used to truncate the encoded text to fit the actual acceptable model input.\n",
    "\n",
    "Through truncation, we can trim the overly long text to a length that is suitable for the model to process. This helps to shorten the processing time, because long text may take longer to process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WOetO0R75HPq",
   "metadata": {
    "id": "WOetO0R75HPq"
   },
   "source": [
    "Here we set the maximum length to 3 and set truncation (`truncation = True`). You can see that `Hi, how are you?` is much shorter, removing all the content on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e1ecfe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70e1ecfe",
    "outputId": "34382252-0750-421e-8798-d63dc820a0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using truncation:  [[12764, 13, 849], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bQiZ-xekQ7Qv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQiZ-xekQ7Qv",
    "outputId": "765e6d68-00db-4550-8aae-ccf8252c766f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用截断:  [[24553, 34439, 43244], [15367, 45091, 34439], [12105]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"使用截断: \", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9AbmHBC65qeI",
   "metadata": {
    "id": "9AbmHBC65qeI"
   },
   "source": [
    "In practical applications, for example, if you are writing an article, you may be given some prompts at a certain position, and there are many important contents to consider at the same time. It may be more important to keep the right side, which saves important information from the previous text to maintain the coherence of the context. At this time, it may be more appropriate to mark the truncated side as the left side. So, the final decision depends on the specific problem you are solving and the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7de95f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7de95f5",
    "outputId": "b8a7e8e5-803b-4512-b5ad-7c809b427c9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using left-side truncation:  [[403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "pCMTiaKhRAXn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCMTiaKhRAXn",
    "outputId": "73fbb564-0aaa-4947-ccab-f175d8768068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用左侧截断:  [[24553, 34439, 43244], [15367, 45091, 34439], [12105]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"使用左侧截断: \", encoded_texts_truncation_left[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QvfMy2026_KM",
   "metadata": {
    "id": "QvfMy2026_KM"
   },
   "source": [
    "In fact, we often use both padding and truncation when processing input. We set the parameters of truncation and padding to True. You can see that the content filled with 0 is truncated to three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dd95990",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dd95990",
    "outputId": "9f365518-00be-43f1-bc4d-c85b5f6ce15c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using both padding and truncation:  [[403, 368, 32], [42, 1353, 1175], [4374, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
    "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "YGsCLjHpRF9y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGsCLjHpRF9y",
    "outputId": "c3b6de3b-df3f-4a60-d0f3-5bbbadecefdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "同时使用填充和截断:  [[24553, 34439, 43244], [15367, 45091, 34439], [12105, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
    "print(\"同时使用填充和截断: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591152e5",
   "metadata": {
    "id": "591152e5"
   },
   "source": [
    "#### 2.1.4 Prepare instruction data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3cac5",
   "metadata": {
    "id": "d0a3cac5"
   },
   "source": [
    "Here is some code from the previous experiment.\n",
    "\n",
    "Load the dataset file with \"question\" and \"answer\" and put it into the prompt to process.\n",
    "\n",
    "Now you can see a data with \"question\" and \"answer\" here.\n",
    "\n",
    "Let's run this tokenizer on one of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b406b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95b406b6",
    "outputId": "a8f6683e-c41e-45e0-d1b9-51577aa7a4ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One datapoint in the finetuning dataset:\n",
      "{'answer': 'Lamini has documentation on Getting Started, Authentication, '\n",
      "           'Question Answer Model, Python Library, Batching, Error Handling, '\n",
      "           'Advanced topics, and class documentation on LLM Engine available '\n",
      "           'at https://lamini-ai.github.io/.',\n",
      " 'question': '### Question:\\n'\n",
      "             'What are the different types of documents available in the '\n",
      "             'repository (e.g., installation guide, API documentation, '\n",
      "             \"developer's guide)?\\n\"\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = \"lamini_docs.jsonl\"\n",
    "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
    "examples = instruction_dataset_df.to_dict()\n",
    "\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]\n",
    "\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer: \"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "  text_with_prompt_template = prompt_template.format(question=question)\n",
    "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"One datapoint in the finetuning dataset:\") #微调数据集中的一个数据点\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7e805",
   "metadata": {
    "id": "72e7e805"
   },
   "source": [
    "#### 2.1.5 Tokenizing a Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30GwKNRe-LlQ",
   "metadata": {
    "id": "30GwKNRe-LlQ"
   },
   "source": [
    "First the question is concatenated with the answer and then tokenized by a tokenizer.\n",
    "\n",
    "For simplicity, the tensor is simply returned as a NumPy array and padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5175d1b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5175d1b1",
    "outputId": "fc5ab43a-e332-4328-ad2a-66d8a33ca364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4118 19782    27   187  1276   403   253  1027  3510   273  7177  2130\n",
      "    275   253 18491   313    70    15    72   904 12692  7102    13  8990\n",
      "  10097    13 13722   434  7102  6177   187   187  4118 37741    27    45\n",
      "   4988    74   556 10097   327 27669 11075   264    13  5271 23058    13\n",
      "  19782 37741 10031    13 13814 11397    13   378 16464    13 11759 10535\n",
      "   1981    13 21798 12989    13   285   966 10097   327 21708    46 10797\n",
      "   2130   387  5987  1358    77  4988    74    14  2284    15  7280    15\n",
      "    900 14206]]\n"
     ]
    }
   ],
   "source": [
    "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    padding=True\n",
    ")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dVp2iO73-LaM",
   "metadata": {
    "id": "dVp2iO73-LaM"
   },
   "source": [
    "Because you are not sure about the actual length of these tokens.\n",
    "\n",
    "Therefore, set the configured maximum length to the minimum of the maximum length and the token length.\n",
    "\n",
    "Of course, you can always set the padding length to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3240b6b2",
   "metadata": {
    "id": "3240b6b2"
   },
   "outputs": [],
   "source": [
    "max_length = 2048\n",
    "max_length = min(\n",
    "    tokenized_inputs[\"input_ids\"].shape[1],\n",
    "    max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EHAB-Hqs_rir",
   "metadata": {
    "id": "EHAB-Hqs_rir"
   },
   "source": [
    "Then, it is tokenized again and truncated to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cac7a18a",
   "metadata": {
    "id": "cac7a18a"
   },
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3dbd287",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3dbd287",
    "outputId": "78385553-38aa-44d4-bcec-53981076f6a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4118 19782    27   187  1276   403   253  1027  3510   273  7177  2130\n",
      "    275   253 18491   313    70    15    72   904 12692  7102    13  8990\n",
      "  10097    13 13722   434  7102  6177   187   187  4118 37741    27    45\n",
      "   4988    74   556 10097   327 27669 11075   264    13  5271 23058    13\n",
      "  19782 37741 10031    13 13814 11397    13   378 16464    13 11759 10535\n",
      "   1981    13 21798 12989    13   285   966 10097   327 21708    46 10797\n",
      "   2130   387  5987  1358    77  4988    74    14  2284    15  7280    15\n",
      "    900 14206]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb8c75",
   "metadata": {
    "id": "ebcb8c75"
   },
   "source": [
    "#### 2.1.6 Tokenized instruction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1_iISc4mAQKa",
   "metadata": {
    "id": "1_iISc4mAQKa"
   },
   "source": [
    "Wrapping the above process into a function makes it easy to run it on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "908f0485",
   "metadata": {
    "id": "908f0485"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    对输入进行 token 化，并进行填充和截断处理，返回经过处理后的 token 作为结果\n",
    "\n",
    "    Args:\n",
    "        examples (dict): 待 token 化的数据，可以是包含\"question\"和\"answer\"键的字典，或包含\"input\"和\"output\"键的字典，或包含\"text\"键的字典\n",
    "\n",
    "    Returns:\n",
    "        dict: 经过处理后的输入数据的 token，包含经过 token 化并进行填充和截断处理后的输入数据\n",
    "    \"\"\"\n",
    "\n",
    "# Merge instruction pairs\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "# Tokenization\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W6OY3UYK9sfm",
   "metadata": {
    "id": "W6OY3UYK9sfm"
   },
   "source": [
    "Now we load the dataset.\n",
    "\n",
    "We map the tokenization function to the dataset using the map method.\n",
    "\n",
    "We set batch_size to 1 so that we can process in batches.\n",
    "\n",
    "Set drop_last_batch to True to handle mixed-size inputs, since the last batch will have a different size than batch_size when the data length is not a multiple of batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04ef80c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "cd8509a93c9f4c7bb46bfebf0cfd0e48",
      "59516bee277041b0b740c3888075fc34",
      "9b35fa6fe47d425baa8d0a8007cdc56a",
      "f06b46b61ebe4a12934a58436b71628a",
      "a84b7fb90c86409ea0fde265016cb088",
      "16be33567f7e4f748e1e65d399fe6f68",
      "fc2c70ebd25f4de9b3b0f198ebf144d8",
      "f370a17ceb104fe3b67206e4451f4479",
      "92db973050e04cc0bf186782c620db39",
      "57908b6719774ad39f7a8ed79575a0f1",
      "dd19399a96ce417e87ed6f09e8883323"
     ]
    },
    "id": "04ef80c4",
    "outputId": "7f6170e6-53b0-41e6-add4-525bf1713454"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8509a93c9f4c7bb46bfebf0cfd0e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Ju3KbXrDeHh",
   "metadata": {
    "id": "6Ju3KbXrDeHh"
   },
   "source": [
    "Add a label column so the model can learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e984c2a0",
   "metadata": {
    "id": "e984c2a0"
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fec6c",
   "metadata": {
    "id": "176fec6c"
   },
   "source": [
    "### 2.2 Splitting of test/training datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uBDrZU_NAbj7",
   "metadata": {
    "id": "uBDrZU_NAbj7"
   },
   "source": [
    "Running this train-test split function will specify a test size of 10% of the data.\n",
    "\n",
    "Of course, you can change this setting depending on the size of your dataset.\n",
    "\n",
    "`shuffle=True` is used to randomize the order of the dataset.\n",
    "\n",
    "Now you can see that the dataset has been split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2d0b28d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2d0b28d",
    "outputId": "d4d84cc1-b0f1-42f8-8d42-22182c0f7efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3d16e",
   "metadata": {
    "id": "b8b3d16e"
   },
   "source": [
    "#### 2.2.1 Some datasets for you to try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T_hSmMUfEI15",
   "metadata": {
    "id": "T_hSmMUfEI15"
   },
   "source": [
    "The data we use can be loaded directly through Hugging Face.\n",
    "\n",
    "This dataset is a professional dataset about a company, maybe it is similar to your company.\n",
    "\n",
    "You can adapt it to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "835c1848",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "835c1848",
    "outputId": "bb33571c-b60d-40a8-e2fe-1b848558bf80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JcK-fpjS99XJ",
   "metadata": {
    "id": "JcK-fpjS99XJ"
   },
   "source": [
    "If you think this dataset is a bit boring.\n",
    "\n",
    "We provide some more interesting datasets for you to choose from.\n",
    "1. `taylor_swift`'s dataset,\n",
    "2. The dataset of the popular band `BTS`.\n",
    "3. Actual open source large language model dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ad8af28",
   "metadata": {
    "id": "0ad8af28"
   },
   "outputs": [],
   "source": [
    "taylor_swift_dataset = \"lamini/taylor_swift\"\n",
    "bts_dataset = \"lamini/bts\"\n",
    "open_llms = \"lamini/open_llms\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ve7IZUogEdDs",
   "metadata": {
    "id": "Ve7IZUogEdDs"
   },
   "source": [
    "Now let's look at one piece of data from the `taylor_swift` dataset, okay.\n",
    "\n",
    "These datasets are also available through Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8ffb3a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8ffb3a7",
    "outputId": "d81db7b2-50eb-4c34-caa5-a2cf42624c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the most popular Taylor Swift song among millennials? How does this song relate to the millennial generation? What is the significance of this song in the millennial culture?', 'answer': 'Taylor Swift\\'s \"Shake It Off\" is the most popular song among millennials. This song relates to the millennial generation as it is an anthem of self-acceptance and embracing one\\'s individuality. The song\\'s message of not letting others bring you down and to just dance it off resonates with the millennial culture, which is often characterized by a strong sense of individuality and a rejection of societal norms. Additionally, the song\\'s upbeat and catchy melody makes it a perfect fit for the millennial generation, which is known for its love of pop music.', 'input_ids': [1276, 310, 253, 954, 4633, 11276, 24619, 4498, 2190, 24933, 8075, 32, 1359, 1057, 436, 4498, 14588, 281, 253, 24933, 451, 5978, 32, 1737, 310, 253, 8453, 273, 436, 4498, 275, 253, 24933, 451, 4466, 32, 37979, 24619, 434, 346, 2809, 640, 733, 5566, 3, 310, 253, 954, 4633, 4498, 2190, 24933, 8075, 15, 831, 4498, 7033, 281, 253, 24933, 451, 5978, 347, 352, 310, 271, 49689, 273, 1881, 14, 14764, 593, 285, 41859, 581, 434, 2060, 414, 15, 380, 4498, 434, 3935, 273, 417, 13872, 2571, 3324, 368, 1066, 285, 281, 816, 11012, 352, 745, 8146, 684, 342, 253, 24933, 451, 4466, 13, 534, 310, 2223, 7943, 407, 247, 2266, 3282, 273, 2060, 414, 285, 247, 18235, 273, 38058, 22429, 15, 9157, 13, 253, 4498, 434, 598, 19505, 285, 5834, 90, 40641, 2789, 352, 247, 3962, 4944, 323, 253, 24933, 451, 5978, 13, 534, 310, 1929, 323, 697, 2389, 273, 1684, 3440, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1276, 310, 253, 954, 4633, 11276, 24619, 4498, 2190, 24933, 8075, 32, 1359, 1057, 436, 4498, 14588, 281, 253, 24933, 451, 5978, 32, 1737, 310, 253, 8453, 273, 436, 4498, 275, 253, 24933, 451, 4466, 32, 37979, 24619, 434, 346, 2809, 640, 733, 5566, 3, 310, 253, 954, 4633, 4498, 2190, 24933, 8075, 15, 831, 4498, 7033, 281, 253, 24933, 451, 5978, 347, 352, 310, 271, 49689, 273, 1881, 14, 14764, 593, 285, 41859, 581, 434, 2060, 414, 15, 380, 4498, 434, 3935, 273, 417, 13872, 2571, 3324, 368, 1066, 285, 281, 816, 11012, 352, 745, 8146, 684, 342, 253, 24933, 451, 4466, 13, 534, 310, 2223, 7943, 407, 247, 2266, 3282, 273, 2060, 414, 285, 247, 18235, 273, 38058, 22429, 15, 9157, 13, 253, 4498, 434, 598, 19505, 285, 5834, 90, 40641, 2789, 352, 247, 3962, 4944, 323, 253, 24933, 451, 5978, 13, 534, 310, 1929, 323, 697, 2389, 273, 1684, 3440, 15]}\n"
     ]
    }
   ],
   "source": [
    "dataset_swiftie = datasets.load_dataset(taylor_swift_dataset)\n",
    "print(dataset_swiftie[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3789bbd",
   "metadata": {
    "id": "c3789bbd"
   },
   "outputs": [],
   "source": [
    "# Here is how to push your own dataset to Huggingface hub\n",
    "# !pip install huggingface_hub\n",
    "# !huggingface-cli login\n",
    "# split_dataset.push_to_hub(dataset_path_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kZ_Ao7pF9yBc",
   "metadata": {
    "id": "kZ_Ao7pF9yBc"
   },
   "source": [
    "We have prepared all the data and tokenized it. In the following experiments, we will use this data to train our model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16be33567f7e4f748e1e65d399fe6f68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57908b6719774ad39f7a8ed79575a0f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59516bee277041b0b740c3888075fc34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16be33567f7e4f748e1e65d399fe6f68",
      "placeholder": "​",
      "style": "IPY_MODEL_fc2c70ebd25f4de9b3b0f198ebf144d8",
      "value": "Map: 100%"
     }
    },
    "92db973050e04cc0bf186782c620db39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9b35fa6fe47d425baa8d0a8007cdc56a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f370a17ceb104fe3b67206e4451f4479",
      "max": 1400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_92db973050e04cc0bf186782c620db39",
      "value": 1400
     }
    },
    "a84b7fb90c86409ea0fde265016cb088": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd8509a93c9f4c7bb46bfebf0cfd0e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_59516bee277041b0b740c3888075fc34",
       "IPY_MODEL_9b35fa6fe47d425baa8d0a8007cdc56a",
       "IPY_MODEL_f06b46b61ebe4a12934a58436b71628a"
      ],
      "layout": "IPY_MODEL_a84b7fb90c86409ea0fde265016cb088"
     }
    },
    "dd19399a96ce417e87ed6f09e8883323": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f06b46b61ebe4a12934a58436b71628a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57908b6719774ad39f7a8ed79575a0f1",
      "placeholder": "​",
      "style": "IPY_MODEL_dd19399a96ce417e87ed6f09e8883323",
      "value": " 1400/1400 [00:03&lt;00:00, 482.96 examples/s]"
     }
    },
    "f370a17ceb104fe3b67206e4451f4479": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc2c70ebd25f4de9b3b0f198ebf144d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
