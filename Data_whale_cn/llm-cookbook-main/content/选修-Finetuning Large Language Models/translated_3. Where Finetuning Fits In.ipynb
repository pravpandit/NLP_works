{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Where Finetuning Fits In \n",
    "In this section, we will understand where finetuning really fits into the training process. Finetuning actually comes after the \"pre-training\" step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [3.1 - Some knowledge before the experiment](#3.1)\n",
    "- [3.2 - Experiment](#3.2)\n",
    "- [3.2.1 - Check out the pre-training dataset](#3.2.1)\n",
    "- [3.2.2 - Comparison of the company fine-tuning data we will use](#3.2.2)\n",
    "- [3.2.3 - Different ways to format data](#3.2.3)\n",
    "- [3.2.4 - Common ways to store data](#3.2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "# 3.1 Learn some knowledge before the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first step before fine-tuning happens. It starts with a completely random model. It knows nothing about the world. So all the weights, if we're familiar with weights, are completely random. It can't form English words at all. It doesn't have language skills yet. Its learning goal is next `token` prediction. Or, we know, in a simplified sense, it's just next word prediction. So we see the word `once`, so now we want it to predict the word `upon`. And then we see that the LLM is just `sd!!@`, which is far from the word `upon`. \n",
    "\n",
    "So that's where it starts. But it takes and reads data from a huge corpus of data that is usually scraped from all over the web. We usually call it unlabeled because it's not something we built together. We just got it from the web. There's a lot of data cleaning process that goes into it. So there's a lot of manual work that goes into making this dataset effective for model pre-training. This is often called self-supervised learning because the model is essentially supervising itself through next `token` prediction. All it has to do is predict the next word. There's no real labels beyond that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](../../figures/FLLM-3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after training, we can see that the model is now able to predict the word `upon`, or the tag `upon`. This is learned language. It has learned a lot from the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](../../figures/FLLM-3-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual understanding and knowledge behind this is often not very public. People don't know exactly what the datasets look like for closed source models from big companies. But there's an amazing open source project by Eleuther AI that created a dataset called The Pile, which we'll explore in the code. It's a set of 22 different datasets that were scraped from all over the internet. Here you can see this graph, you know, four score and seven years. This is Lincoln's Gettysburg Address. And Lincoln's carrot cake recipe. And of course, there's information about different medical literature that was scraped from PubMed. And then finally, there's code from GitHub. So it's a very clever set of datasets that were put together to infuse these models with knowledge. Now, this pre-training step is very expensive and time-consuming. It's actually quite expensive because to get this model to go through all this data, from absolutely random to understanding some of the text in it, you know, it has to learn about medicine and the Gettysburg Address while writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](../../figures/FLLM-3-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the picture below, when we input “What is the capital of Mexico?”, the output is obviously wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4](../../figures/FLLM-3-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it's not really useful in the sense of a chatbot interface. So how do we get information into a chatbot interface? Well, fine-tuning is one of the ways to get us there. It should be a tool in our toolbox. So pre-training is the first step to get a basic model. As we add more data, which is not a lot of data, we can use fine-tuning to get a fine-tuned model. And actually, even a fine-tuned model, we can continue to add fine-tuning steps afterwards. So fine-tuning is a step after that. We can use the same type of data. We can actually collect data from different sources and bring it together, and we'll see a little bit. That's unlabeled data. But we can also curate the data ourselves and make it more structured for the model to learn. We think one of the key things that distinguishes fine-tuning from pre-training is that it requires much less data. We're building on this basic model that has already learned a lot of knowledge and basic language skills, and we're just taking it to the next level. We don't need as much data. So it's really a tool in our toolbox. If you're coming from other machine learning fields, this could be fine-tuning on a discriminative task. Maybe we're working with images and we fine-tuned on ImageNet. We can see that the definition of fine-tuning here is a bit loose. For generation tasks, it is not well defined because we are actually updating the weights of the entire model, not just some of them.This is often the case with fine-tuning other types of models. So our fine-tuning training objective is the same as the pre-training objective here, which is to generate the next `token`. What we've done is to change the data so that it's more structured in some way. The model can be more consistent in outputting and emulating that structure. And there are more advanced ways to reduce the number of times you want to update this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5](../../figures/FLLM-3-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does fine-tuning do for us? We now have a sense of what it is, but what different tasks can we actually do with it? One big category that I like to think about is behavior change. We're changing the behavior of the model. We're telling it exactly that in this chat interface, we're now in a chat setting. That makes the model more consistent in its responses. That means the model can focus better. For example, this might be better for moderation. It's also generally just combing through its capabilities. So here, it's better at conversation. So it can now talk about a wide variety of things versus before where we had to do a lot of just-in-time engineering to comb through that information. Fine-tuning can also help the model acquire new knowledge. So that might be around specific topics that weren't in the base pre-trained model. It might mean correcting information that was previously incorrect. So maybe there's more up-to-date information that we want the model to actually inject. And of course, more commonly, both models are used. So generally, we're changing the behavior, we want it to acquire new knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6](../../figures/FLLM-3-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task to be completed by fine-tuning can be to extract text information or to expand the text, such as writing an email based on the basic information we provide. When we are more clear about what task we want to accomplish, the effect of fine-tuning will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![7](../../figures/FLLM-3-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time fine-tuning a model, follow these steps: \n",
    "1. First identify tasks that can be completed through the prompt project\n",
    "2. Find tasks that we think the large model does well\n",
    "3. Pick a task\n",
    "4. Get the corresponding task, which can be 1,000 pairs of input and output (better than the original large model)\n",
    "5. Use this data to fine-tune a small model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![8](../../figures/FLLM-3-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "## 3.2 Experiment\n",
    "Next, let's take a look at the difference between the original large model and the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the corresponding library\n",
    "import jsonlines\n",
    "import itertools\n",
    "import pandas as pd\n",
    "# The pprint() function is used to format the print output object to make the output more neat and beautiful\n",
    "from pprint import pprint\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2.1'></a>\n",
    "### 3.2.1 View the pre-training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/en/c4-train.00000-of-01024.json.gz\n",
    "pretrained_dataset = load_dataset(\"./c4-train.00000-of-01024.json/\", \"en\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained dataset:\n",
      "{'text': 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.', 'timestamp': datetime.datetime(2019, 4, 25, 12, 57, 54), 'url': 'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/'}\n",
      "{'text': 'Discussion in \\'Mac OS X Lion (10.7)\\' started by axboi87, Jan 20, 2012.\\nI\\'ve got a 500gb internal drive and a 240gb SSD.\\nWhen trying to restore using disk utility i\\'m given the error \"Not enough space on disk ____ to restore\"\\nBut I shouldn\\'t have to do that!!!\\nAny ideas or workarounds before resorting to the above?\\nUse Carbon Copy Cloner to copy one drive to the other. I\\'ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\\'t be bootable. CCC usually works in \"file mode\" and it can easily copy a larger drive (that\\'s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\\nI\\'ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\\'t fit is there was slightly more than 4 GB of data.', 'timestamp': datetime.datetime(2019, 4, 21, 10, 7, 13), 'url': 'https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/'}\n",
      "{'text': 'Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.', 'timestamp': datetime.datetime(2019, 4, 25, 10, 40, 23), 'url': 'https://awishcometrue.com/Catalogs/Clearance/Tweens/V1960-Find-A-Way'}\n",
      "{'text': \"How many backlinks per day for new site?\\nDiscussion in 'Black Hat SEO' started by Omoplata, Dec 3, 2010.\\n1) for a newly created site, what's the max # backlinks per day I should do to be safe?\\n2) how long do I have to let my site age before I can start making more blinks?\\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?\", 'timestamp': datetime.datetime(2019, 4, 21, 12, 46, 19), 'url': 'https://www.blackhatworld.com/seo/how-many-backlinks-per-day-for-new-site.258615/'}\n",
      "{'text': 'The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what’s included in the mill levy measure.', 'timestamp': datetime.datetime(2019, 4, 20, 14, 33, 21), 'url': 'http://bond.dpsk12.org/category/news/'}\n"
     ]
    }
   ],
   "source": [
    "# View the first n elements of the dataset (which is also an iterator)\n",
    "n = 5\n",
    "print(\"Pretrained dataset:\")\n",
    "# Syntax itertools.islice(iterable, start, stop, step) and returns an iterator object\n",
    "top_n = itertools.islice(pretrained_dataset, n)\n",
    "for i in top_n:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2.2'></a>\n",
    "### 3.2.2 Comparison with the company fine-tuning data we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the different types of documents avai...</td>\n",
       "      <td>Lamini has documentation on Getting Started, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the recommended way to set up and conf...</td>\n",
       "      <td>Lamini can be downloaded as a python package a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find the specific documentation I ne...</td>\n",
       "      <td>You can ask this model about documentation, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does the documentation include explanations of...</td>\n",
       "      <td>Our documentation provides both real-world and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does the documentation provide information abo...</td>\n",
       "      <td>External dependencies and libraries are all av...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>What is Lamini and what is its collaboration w...</td>\n",
       "      <td>Lamini is a library that simplifies the proces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>How does Lamini simplify the process of access...</td>\n",
       "      <td>Lamini simplifies data access in Databricks by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>What are some of the key features provided by ...</td>\n",
       "      <td>Lamini automatically manages the infrastructur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>How does Lamini ensure data privacy during the...</td>\n",
       "      <td>During the training process, Lamini ensures da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>Can you provide an example use case where Lami...</td>\n",
       "      <td>An example use case where Lamini outperforms C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     What are the different types of documents avai...   \n",
       "1     What is the recommended way to set up and conf...   \n",
       "2     How can I find the specific documentation I ne...   \n",
       "3     Does the documentation include explanations of...   \n",
       "4     Does the documentation provide information abo...   \n",
       "...                                                 ...   \n",
       "1395  What is Lamini and what is its collaboration w...   \n",
       "1396  How does Lamini simplify the process of access...   \n",
       "1397  What are some of the key features provided by ...   \n",
       "1398  How does Lamini ensure data privacy during the...   \n",
       "1399  Can you provide an example use case where Lami...   \n",
       "\n",
       "                                                 answer  \n",
       "0     Lamini has documentation on Getting Started, A...  \n",
       "1     Lamini can be downloaded as a python package a...  \n",
       "2     You can ask this model about documentation, wh...  \n",
       "3     Our documentation provides both real-world and...  \n",
       "4     External dependencies and libraries are all av...  \n",
       "...                                                 ...  \n",
       "1395  Lamini is a library that simplifies the proces...  \n",
       "1396  Lamini simplifies data access in Databricks by...  \n",
       "1397  Lamini automatically manages the infrastructur...  \n",
       "1398  During the training process, Lamini ensures da...  \n",
       "1399  An example use case where Lamini outperforms C...  \n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .jsonl is a common JOSN line format file extension, namely JOSN Lines. And .jsonal is an optimized JSON format, which is very suitable for storing and reading large amounts of structured data\n",
    "# Read the .jsonl file and view it\n",
    "filename = \"lamini_docs.jsonl\"\n",
    "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
    "instruction_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2.3'></a>\n",
    "### 3.2.3 Different ways to format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Form a dictionary between the \"question\" and \"answer\" columns of the above data, and check the first text after the connection\n",
    "examples = instruction_dataset_df.to_dict()\n",
    "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect different corresponding \"pairs\"\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a prompt template (including \"question\" and \"answer\")\n",
    "prompt_template_qa = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "{answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Question:\\nWhat are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\\n\\n### Answer:\\nLamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill the \"question\" and \"answer\" of the first text into the prompt template\n",
    "question = examples[\"question\"][0]\n",
    "answer = examples[\"answer\"][0]\n",
    "\n",
    "text_with_prompt_template = prompt_template_qa.format(question=question, answer=answer)\n",
    "text_with_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a prompt template (containing only \"questions\")\n",
    "prompt_template_q = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the length of the dictionary key-value pair, fill \"question\" and \"answer\" into the two templates in turn\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset_text_only = []\n",
    "finetuning_dataset_question_answer = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "\n",
    "  text_with_prompt_template_qa = prompt_template_qa.format(question=question, answer=answer)\n",
    "  finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
    "\n",
    "  text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
    "  finetuning_dataset_question_answer.append({\"question\": text_with_prompt_template_q, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '### Question:\\n'\n",
      "         'What are the different types of documents available in the '\n",
      "         \"repository (e.g., installation guide, API documentation, developer's \"\n",
      "         'guide)?\\n'\n",
      "         '\\n'\n",
      "         '### Answer:\\n'\n",
      "         'Lamini has documentation on Getting Started, Authentication, '\n",
      "         'Question Answer Model, Python Library, Batching, Error Handling, '\n",
      "         'Advanced topics, and class documentation on LLM Engine available at '\n",
      "         'https://lamini-ai.github.io/.'}\n"
     ]
    }
   ],
   "source": [
    "# Print the first element of the list under the view text template\n",
    "pprint(finetuning_dataset_text_only[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Lamini has documentation on Getting Started, Authentication, '\n",
      "           'Question Answer Model, Python Library, Batching, Error Handling, '\n",
      "           'Advanced topics, and class documentation on LLM Engine available '\n",
      "           'at https://lamini-ai.github.io/.',\n",
      " 'question': '### Question:\\n'\n",
      "             'What are the different types of documents available in the '\n",
      "             'repository (e.g., installation guide, API documentation, '\n",
      "             \"developer's guide)?\\n\"\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "# Print the first element of a list under another template\n",
    "pprint(finetuning_dataset_question_answer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2.4'></a>\n",
    "### 3.2.4 Common ways to store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, the content stored in the second template is in .jsonl format, and each line is in json format\n",
    "with jsonlines.open(f'lamini_docs_processed.jsonl', 'w') as writer:\n",
    "    writer.write_all(finetuning_dataset_question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# If we uploaded the data to Hugging Face before, we can pull the data as follows\n",
    "finetuning_dataset_name = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_name)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
