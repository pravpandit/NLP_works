{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb57584a",
   "metadata": {
    "id": "rKn-Y_Pk9WjC"
   },
   "source": [
    "# Chapter 4 Command Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd466c",
   "metadata": {},
   "source": [
    "In this lesson, you will learn about command fine-tuning, a variation of fine-tuning that turns GPT-3 into a chatty GPT and gives it the ability to chat. Okay, let's start giving all models chatty capabilities.\n",
    "\n",
    "Let's take a deeper look at what command fine-tuning is. Command fine-tuning is a type of fine-tuning. You can also do various other tasks like inference, routing, copilot (i.e. write code, chat, create different agents). But specifically, command fine-tuning (which may also be called command tuning or command-following LLMs) allows the model to follow commands and behave more like a chatbot.\n",
    "\n",
    "Just like we saw with the chatty GPT, this is a better user interface to interact with the model. It is this approach that turned GPT-3 into a chatty GPT, which greatly expanded the scope of AI from a few researchers like me to millions of people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb131f",
   "metadata": {},
   "source": [
    "![What is instruction finetuning](../../figures/What%20is%20instruction%20finetuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bdab69",
   "metadata": {},
   "source": [
    "For the dataset for fine-tuning instructions, you can use a lot of data that is already available online or company-specific. This data may be FAQs, customer support conversations, or Slack messages.\n",
    "\n",
    "So this is actually a conversation dataset or instruction response dataset. Of course, if you don't have data, there is no question. You can also convert the data into a question-answer pair format or an instruction response format by using a prompt template. Here you can see that a README file may be converted into a pair of question answers. You can also use other LLMs to do this. Stanford University has a technology called Alpaca that can do this using chat GPT. Of course, you can also use the workflow of different open source models to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d21fc4",
   "metadata": {},
   "source": [
    "![LLM data generation](../../figures/LLM%20data%20generation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d4007c",
   "metadata": {},
   "source": [
    "I think the cool thing about fine-tuning is that it can teach new behaviors to the model.\n",
    "You might have fine-tuning data about the capital of France being Paris. Because those are easy to get question-answer pairs. You can also generalize this idea of ​​question answering to data where you might not have given the model a fine-tuning dataset, but the model has learned this data in its pre-training steps, which might be code. This is actually what the ChatGPT paper found, where the model can now answer questions about code even though they didn't learn code before. Even though they didn't have question-answer pairs about code when they were fine-tuning. This is because it's really expensive for programmers to annotate datasets, ask questions about code, and write code for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790fd6d",
   "metadata": {},
   "source": [
    "![Instruction finetuning generalization](../../figures/Instruction%20finetuning%20generalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e535e",
   "metadata": {},
   "source": [
    "So the different steps of fine-tuning can be summarized as data preparation, training, and evaluation. And of course, after you evaluate the model, you need to prepare the data again to improve the model. It's a very iterative process of improving the model. When it comes to instruction fine-tuning and other different types of fine-tuning, data preparation is where it really makes a difference. This is where you change your data. You adjust your data based on the specific type of fine-tuning, the specific task you're fine-tuning. Training and evaluation are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e1f5a",
   "metadata": {},
   "source": [
    "![Different types of finetuning](../../figures/Different%20types%20of%20finetuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374338f",
   "metadata": {},
   "source": [
    "Now let's move on to the lab, where you can see the Alpaca dataset used for instruction tuning. You will also compare the instruction-tuned and non-instruction-tuned models again. You will also see models of different sizes.\n",
    "\n",
    "First, we need to import a few libraries. The most important one is the `load_dataset` function from the `datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b15ed29",
   "metadata": {
    "height": 166,
    "id": "YwB8OLqiflAl"
   },
   "outputs": [],
   "source": [
    "# Import the itertools library to create efficient iterators\n",
    "import itertools\n",
    "\n",
    "# Import the jsonlines library to process JSONL (JSON Lines) format files\n",
    "import jsonlines\n",
    "\n",
    "# Import the load_dataset function of the datasets library to load various natural language processing datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import the pprint library to print Python objects beautifully\n",
    "from pprint import pprint\n",
    "\n",
    "# Import the BasicModelRunner class of the llama library to simplify model running\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "# Import the AutoTokenizer class from the transformers library,\n",
    "# Used to automatically select and load pre-trained tokenizers suitable for a specific model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Import the AutoModelForCausalLM class from the transformers library,\n",
    "# Used to automatically select and load pre-trained models for causal language models\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Import the AutoModelForSeq2SeqLM class from the transformers library,\n",
    "# Used to automatically select and load pre-trained models suitable for sequence-to-sequence tasks\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b8ffe9",
   "metadata": {},
   "source": [
    "## 1. Loading instructions to fine-tune the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd9da7",
   "metadata": {},
   "source": [
    "Let's load this instruction tuning dataset, which is the Alpaca dataset. Again, we're using streaming because this is actually a large fine-tuning dataset, but it's not a heap of large. We're going to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9ff9e8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Use the `load_dataset` function to load the dataset named \"tatsu-lab/alpaca\" from the datasets library\n",
    "# Set split to \"train\" to load training data and enable streaming mode\n",
    "instruction_tuned_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04fa16a",
   "metadata": {},
   "source": [
    "Just like before, you will see several examples. Unlike the previous section, it is not unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "845a700a",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "指令微调的数据集是：\n",
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n",
      "{'instruction': 'What are the three primary colors?', 'input': '', 'output': 'The three primary colors are red, blue, and yellow.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.'}\n",
      "{'instruction': 'Describe the structure of an atom.', 'input': '', 'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Response:\\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}\n",
      "{'instruction': 'How can we reduce air pollution?', 'input': '', 'output': 'There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow can we reduce air pollution?\\n\\n### Response:\\nThere are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.'}\n",
      "{'instruction': 'Describe a time when you had to make a difficult decision.', 'input': '', 'output': 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe a time when you had to make a difficult decision.\\n\\n### Response:\\nI had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.'}\n"
     ]
    }
   ],
   "source": [
    "# Define m, which represents the number of records we want to view\n",
    "m = 5\n",
    "\n",
    "# Print a descriptive message\n",
    "print(\"指令微调的数据集是：\")\n",
    "\n",
    "# Use `itertools.islice` to slice the first m records from the dataset\n",
    "# and convert it into a list top_m\n",
    "top_m = list(itertools.islice(instruction_tuned_dataset, m))\n",
    "\n",
    "# Traverse the top_m list and print each record\n",
    "for j in top_m:\n",
    "  print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01377448",
   "metadata": {},
   "source": [
    "## 2. Two prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a470c",
   "metadata": {},
   "source": [
    "It's a little more organized here. But it's still not as clear cut as the question-answer pair. The cool thing is that the authors of the Alpaca paper actually have two prompt templates because they want the model to be able to handle two different types of prompts and two different types of tasks. One of them is the instruction, and the other is the additional input. For example, the instruction might be to add two numbers. The input might be that the first number is 3 and the second number is 4. There's also a prompt template with no input, which you can see in these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d460c8a",
   "metadata": {
    "height": 301
   },
   "outputs": [],
   "source": [
    "# Define two string templates: one for data points with input fields and another for data points without input fields\n",
    "prompt_template_with_input = \"\"\"下面是一条描述任务的指令，辅以一个提供进一步上下文的输入。请编写一个能合理完成请求的响应。\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response: \"\"\"\n",
    "\n",
    "prompt_template_without_input = \"\"\"下面是一条描述任务的指令。编写一个能合理完成请求的响应。\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Reponse: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47496fe8",
   "metadata": {},
   "source": [
    "## 3. Fusion prompts (adding data to prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a3800",
   "metadata": {},
   "source": [
    "Sometimes, the input is not important. So there is no input. These are the prompt templates that are being used. Again, very similar to the previous method, you just fuse these prompts together and then run it on the entire dataset. Let's print out a question and answer pair to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d405fc",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the processed data\n",
    "processed_data = []\n",
    "\n",
    "# Loop through each element j in the top_m list (you didn't give the definition of top_m, I assume it is a list containing multiple data points)\n",
    "for j in top_m:\n",
    "# Determine whether the \"input\" field of the current element j is empty or does not exist\n",
    "  if not j[\"input\"]:\n",
    "# If the \"input\" field is empty or does not exist, use the template without the input field and fill it with the \"instruction\" field in j\n",
    "    processed_prompt = prompt_template_without_input.format(instruction=j[\"instruction\"])\n",
    "  else:\n",
    "# If the \"input\" field exists and is not empty, use the template with the input field, filling it with the \"instruction\" and \"input\" fields in j\n",
    "    processed_prompt = prompt_template_with_input.format(instruction=j[\"instruction\"], input=j[\"input\"])\n",
    "\n",
    "# Create a new dictionary where the \"input\" field is the processed prompt and the \"output\" field is the \"output\" field in j and add it to the processed_data list\n",
    "  processed_data.append({\"input\": processed_prompt, \"output\": j[\"output\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0c6c25",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '下面是一条描述任务的指令。编写一个能合理完成请求的响应。\\n'\n",
      "          '\\n'\n",
      "          '### Instruction:\\n'\n",
      "          'Give three tips for staying healthy.\\n'\n",
      "          '\\n'\n",
      "          '### Reponse:',\n",
      " 'output': '1.Eat a balanced diet and make sure to include plenty of fruits '\n",
      "           'and vegetables. \\n'\n",
      "           '2. Exercise regularly to keep your body active and strong. \\n'\n",
      "           '3. Get enough sleep and maintain a consistent sleep schedule.'}\n"
     ]
    }
   ],
   "source": [
    "# Use the pprint function to print the first element of the processed_data list in a beautiful format\n",
    "pprint(processed_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30663d1b",
   "metadata": {},
   "source": [
    "So that's the input and output, and you know how it all fits into the prompt. It ends with `###Response`, and then it outputs the response here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8163deb",
   "metadata": {},
   "source": [
    "## 4. jsonl data storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b7a2d",
   "metadata": {},
   "source": [
    "Just like before, you can write this out to a JSON line file. You can upload it to Hugging Face Hub if you want. We've actually already got it loaded up on Lamini/Alpaca, so it's pretty stable. You can go there and take a look at it and use it, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ba3c30",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Open a jsonl file named 'alpaca_processed.jsonl' in write mode\n",
    "with jsonlines.open(f'alpaca_processed.jsonl', 'w') as writer:\n",
    "# Use the writer's write_all method to write all elements of the processed_data list to the jsonl file\n",
    "    writer.write_all(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76573514",
   "metadata": {},
   "source": [
    "## 5. Comparison of models without instruction fine-tuning and with instruction fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25daa0d",
   "metadata": {},
   "source": [
    "Great, now that you’ve seen what the command dataset looks like, I think the next thing to do is to compare how different models answer the question “tell me how to train my dog ​​to sit”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51b3bf8",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Import the load_dataset function from the HuggingFace datasets library\n",
    "dataset_path_hf = \"lamini/alpaca\"          # 设置数据集路径为\"lamini/alpaca\"\n",
    "dataset_hf = load_dataset(dataset_path_hf) # 加载数据集\n",
    "print(dataset_hf)                          # 打印加载的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b65be4",
   "metadata": {},
   "source": [
    "The first model is the llama2 model, which is also untrained. We're going to run it. Tell me how to train my dog ​​to sit. Okay, it starts with this period again, and then it continues to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b01cbd6",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not instruction-tuned output (Llama 2 Base): 来\n",
      "\n",
      "我的狗狗很难坐下来，它很难停下来，它停下来后很难又坐下来。\n",
      "\n",
      "我想要它坐下来，但是它很難停下来，很難又坐下去。\n",
      "\n",
      "我想要坐下来，且停下来，但是我很难很难吃下去。\n",
      "\n",
      "我想坐下来，依然停下来，依然难吃下来。\n",
      "\n",
      "我想坚持下来，但是依然很难却坐下来。\n"
     ]
    }
   ],
   "source": [
    "# Initialize a non-command-tuned model, the model path is \"meta-llama/Llama-2-7b-hf\"\n",
    "non_instruct_model = BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")\n",
    "# Use this model to generate responses on how to train a dog to sit\n",
    "non_instruct_output = non_instruct_model(\"告诉我如何训练狗狗坐下\")\n",
    "print(\"Not instruction-tuned output (Llama 2 Base):\", non_instruct_output) # 打印响应"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a914409",
   "metadata": {},
   "source": [
    "Keeping in mind the previous result, let's now compare it to the instruction-tuned model. It performs much better and actually produces different steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ca0317e",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction-tuned output (Llama 2):  ？\n",
      "\n",
      "很多人都想要训练犬坐下，但是它们不知道如何做。下面是一些简单的步骤，可以帮助你训练牧牛坐下：\n",
      "1. 选择合适的场合：选择一个安全的和舒适的场合，例如在家中或者在一个宽敞的地方。\n",
      "2. 预备好物品：您需要一些物品来训练炸犬坐下。例如，您可以使用一个够大的床垫，或者一个够舒适的毯子。\n",
      "3. 训练着犬坐下：您可以通过�����\n"
     ]
    }
   ],
   "source": [
    "# Use this model to generate responses on how to train a dog to sit\n",
    "instruct_output = instruct_model(\"告诉我如何训练狗狗坐下\")\n",
    "print(\"Instruction-tuned output (Llama 2): \", instruct_output) # 打印响应"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63307780",
   "metadata": {},
   "source": [
    "Finally, I want to share ChatGPT again so you can compare here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba69078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction-tuned output (ChatGPT):  训练狗狗坐下是一项基本的训练技巧，以下是一些步骤来帮助你训练狗狗坐下：\n",
      "\n",
      "1. 准备奖励：准备一些小零食或者狗狗喜欢的食物作为奖励，这将帮助你激励狗狗。\n",
      "\n",
      "2. 找一个安静的地方：选择一个安静的地方开始训练，这样可以减少干扰，让狗狗更容易集中注意力。\n",
      "\n",
      "3. 坐下手势：站在狗狗面前，拿起一小块食物，将手掌心朝上，然后慢慢将手从狗狗的鼻子上方移向后方，使狗狗的头部跟随你的\n"
     ]
    }
   ],
   "source": [
    "# Initialize a ChatGPT model named \"chat-gpt\"\n",
    "chatgpt = BasicModelRunner(\"chat-gpt\")\n",
    "# Use this model to generate responses on how to train a dog to sit\n",
    "instruct_output_chatgpt = chatgpt(\"告诉我如何训练狗狗坐下\")\n",
    "print(\"Instruction-tuned output (ChatGPT): \", instruct_output_chatgpt) # 打印响应"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767eec4e",
   "metadata": {},
   "source": [
    "Okay, this is a larger set of models. The ChatGPT is pretty large compared to the llama2 model. The llama2 model actually has 7 billion parameters, and the ChatGPT is said to be around 70 billion. So the models are pretty large. You're also going to explore some smaller models. One of them is a 70 million parameter model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6d8a4",
   "metadata": {},
   "source": [
    "## 6. Try a smaller model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9636c",
   "metadata": {},
   "source": [
    "I'm loading these models. This isn't really important yet. You'll explore this more later. But I'm going to load two different things to process the data and then run the model. You can see that our tag here is `EleutherAI/pythia-70m`. This is a 70 million parameter model that has not been tuned yet. I'm going to paste some code here. This is a function that runs inference or basically runs a model on text. In the next few experiments, we're going to go into detail on different parts of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f931feab",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f51d6a9",
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "# Define the inference function\n",
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "# Tokenize: Convert input text into Token IDs\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",  # 返回PyTorch张量\n",
    "          truncation=True,  # 如果文本太长，进行截断\n",
    "          max_length=max_input_tokens  # 输入文本的最大长度\n",
    "  )\n",
    "\n",
    "# Generate: Generate output using the model\n",
    "  device = model.device  # 获取模型所在的设备（CPU或GPU）\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),  # 将输入数据移到相同的设备\n",
    "    max_length=max_output_tokens  # 输出的最大长度\n",
    "  )\n",
    "\n",
    "# Decode: Decode the generated Token IDs back into text\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "# Strip the prompt: Remove the input text from the output to get pure response\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer  # 返回生成的文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99369ac",
   "metadata": {},
   "source": [
    "This model hasn't been fine-tuned yet. It doesn't know anything specific about the companies. But we can load our company dataset again. So, we're going to give this model a question from this dataset. For example, maybe just the first example in the test set. So we can run it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18c6d1bf",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset for fine-tuning\n",
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)  # 打印数据集信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1cd2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Can Lamini generate technical documentation or user manuals for software projects?', 'answer': 'Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.', 'input_ids': [5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15]}\n",
      "\n",
      "\n",
      "I have a question about the following:\n",
      "\n",
      "How do I get the correct documentation to work?\n",
      "\n",
      "A:\n",
      "\n",
      "I think you need to use the following code:\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following\n"
     ]
    }
   ],
   "source": [
    "# Get test samples\n",
    "test_sample = finetuning_dataset[\"test\"][0]\n",
    "print(test_sample)  # 打印测试样本\n",
    "\n",
    "# Perform inference using the base model\n",
    "print(inference(test_sample[\"question\"], model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc34d41",
   "metadata": {},
   "source": [
    "The question is, can Lamini generate technical documentation or user manuals for software projects? The actual answer is yes, Lamini can generate technical documentation and user manuals for software projects. It keeps running. But the model's answer is, I have a question about the following. How can the right documentation work? The answer is, I think you need to use the following code, etc. So it's far from the right answer.\n",
    "\n",
    "Of course, it learned English, and it also understands the word documentation. So, it may understand that it is answering the question because it uses A to represent the answer. But this is obviously wrong. So in terms of knowledge, it doesn't quite understand this dataset, nor does it understand the behavior we expect it to do. So it doesn't know that it should answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a086a12",
   "metadata": {},
   "source": [
    "## VII. Comparison with a smaller model after fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c288283",
   "metadata": {},
   "source": [
    "Now, compare this to the model that we fine-tuned for you, which you're actually going to fine-tune for the following instructions. Load this model, and then we can run the same problem through this model and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876d556",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "instruction_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cefb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Lamini can generate technical documentation or user manuals for software projects. This can be achieved by providing a prompt for a specific technical question or question to the LLM Engine, or by providing a prompt for a specific technical question or question. Additionally, Lamini can be trained on specific technical questions or questions to help users understand the process and provide feedback to the LLM Engine. Additionally, Lamini\n"
     ]
    }
   ],
   "source": [
    "# Use the fine-tuned model for inference\n",
    "print(inference(test_sample[\"question\"], instruction_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324380f1",
   "metadata": {},
   "source": [
    "The results show that, yes, Lamini can generate technical documentation or user manuals for software projects and so on. So, this model is much more accurate than the previous one. It follows the correct behavior as we expect.\n",
    "\n",
    "Now that you have seen how to fine-tune the instructions, the next step is to learn about the tokenizer, how to pre-process our data so that the model can be trained with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f2112",
   "metadata": {
    "height": 217,
    "id": "mSJMi8I4Sgrw"
   },
   "outputs": [],
   "source": [
    "# If you want to know how to upload your own dataset to Huggingface\n",
    "# This is how we implement it\n",
    "\n",
    "# !pip install huggingface_hub\n",
    "# !huggingface-cli login\n",
    "\n",
    "# import pandas as pd\n",
    "# import datasets\n",
    "# from datasets import Dataset\n",
    "\n",
    "# finetuning_dataset = Dataset.from_pandas(pd.DataFrame(data=finetuning_dataset))\n",
    "# finetuning_dataset.push_to_hub(dataset_path_hf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
