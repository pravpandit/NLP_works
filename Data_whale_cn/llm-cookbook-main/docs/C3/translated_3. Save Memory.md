# Chapter 3 Storage

When interacting with language models, you may have noticed a key problem: they do not remember your previous communication content, which brings great challenges when we build some applications (such as chatbots), making the conversation seem to lack real continuity. Therefore, in this section we will introduce the storage module in LangChain, that is, how to embed previous conversations into the language model so that it has the ability of continuous conversation.

When using the storage (Memory) module in LangChain, it aims to save, organize and track the history of the entire conversation, thereby providing a continuous context for the interaction between the user and the model.

LangChain provides multiple storage types. Among them, buffer storage allows the retention of recent chat messages, and summary storage provides a summary of the entire conversation. Entity storage allows information about specific entities to be retained in multiple rounds of conversations. These memory components are modular and can be used in combination with other components to enhance the robot's conversation management capabilities. The storage module can be accessed and updated through simple API calls, allowing developers to more easily manage and maintain conversation history.

This course mainly introduces four of the storage modules, and other modules can be viewed in the document to learn.
- Conversation Buffer Memory
- Conversation Buffer Window Memory (ConversationBufferWindowMemory）
- Conversation Token Buffer Memory
- Conversation Summary Buffer Memory

In LangChain, storage refers to the short-term memory of the large language model (LLM). Why is it short-term memory? That's because after the LLM is trained (some long-term memory is obtained), its parameters will not change due to user input. When the user has a conversation with the trained LLM, the LLM will temporarily remember the user's input and the output it has generated in order to predict the subsequent output. After the model outputs, it will "forget" the previous user's input and its output. Therefore, this previous information can only be called the short-term memory of the LLM. 

In order to extend the retention time of the LLM short-term memory, it is necessary to use some external storage methods to memorize it, so that in the conversation between the user and the LLM, the LLM can know as much as possible about the historical conversation information between the user and it. 

## 1. Conversation Cache Storage

### 1.1 Initialize the Conversation Model

Let's initialize the conversation model first.

```python
from langchain.chains import ConversationChain
fromlangchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory

# Here we set the parameter temperature to 0.0 to reduce the randomness of the generated answers.
# If you want to get different and novel answers every time, you can try to increase this parameter.
llm = ChatOpenAI(temperature=0.0) 
memory = ConversationBufferMemory()

# Create a new instance of ConversationChain Class
# When the verbose parameter is set to True, the program will output more detailed information to provide more debugging or runtime information.
# On the contrary, when the verbose parameter is set to False, the program will run in a more concise way and only output key information.
conversation = ConversationChain(llm=llm, memory = memory, verbose=True )
```

### 1.2 First round of conversation

When we run prediction (predict), some prompts are generated, as shown below, heSay "The following is a friendly conversation between a human and an AI, the AI ​​is talkative" and so on, this is actually a prompt generated by LangChain to make the system have a hopeful and friendly conversation, and the conversation must be saved, and the current completed model chain is prompted.

```python
conversation.predict(input="Hello, my name is Pipilu")
```

> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI ​​is talkative and provides lots of specific details from its context. If the AI ​​does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hello,My name is Pipilu
AI:

> Finished chain.

'Hello, Pipilu! Nice to meet you. I'm an AI assistant that can answer your questions and provide help. Is there anything I can help you with? '

### 1.3 Second round of conversation

When we have the second round of conversation, it will keep the above prompt

```python
conversation.predict(input="1+1 equals how much?")
```

> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI ​​is talkative and provides lots of specific details from its context. If the AI ​​does not know the answer to a question, it truthfully saysit does not know.

Current conversation:
Human: Hello, my name is Pipilu
AI: Hello, Pipilu! Nice to meet you. I am an AI assistant that can answer your questions and provide help. Is there anything I can help you with?
Human: What is 1+1?
AI:

> Finished chain.

'1+1 equals 2. '

### 1.4 The third round of conversation

In order to verify whether he has remembered the previous conversation, we asked him to answer what he had said before (my name). We can see that he did output the correct name, so this conversation chain will get longer and longer as it goes on.

```python
conversation.predict(input="What is my name?")
```

> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human andan AI. The AI ​​is talkative and provides lots of specific details from its context. If the AI ​​does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hello, my name is Pipilu

AI: Hello, Pipilu! Nice to meet you. I am an AI assistant that can answer your questions and provide help. Is there anything I can help you with?

Human: What is 1+1?

AI: 1+1 is 2.

Human: What is my name?

AI:

> Finished chain.

'Your name is Pipilu. '

### 1.5 View storage cache

Storage cache (buffer), that is, all conversation information so far is stored

```python
print(memory.buffer) 
```

Human: Hello, my name is Pipilu

AI: Hello, Pipilu! Nice to meet you. I am an AI assistant who can answer your questions and provide help. Is there anything I can help you with?
Human: What is 1+1?
AI: 1+1 is 2.
Human: What is my name?
AI: Your name is Pipilu.

You can also print the historical messages in the cache through `load_memory_variables({})`. Here `{}` is an empty dictionary, and there are some more advanced functions that allow users to use more complex inputs. For more advanced usage, please refer to the official documentation of LangChain.

```python
print(memory.load_memory_variables({}))
```

{'history': 'Human: Hello, my name is Pipilu\nAI: Hello, Pipilu! Nice to meet you. I am an AI assistant who can answer your questions and provide help. Is there anything I can help you with? \nHuman: What is 1+1? \nAI: 1+1 is 2. \nHuman: What is my name? \nAI: Your name is Pipilu. '}

### 1.6 Add content directly to the storage cache

We can use `save_context` to directly add content to `buffer` in.

```python
memory = ConversationBufferMemory()
memory.save_context({"input": "Hello, my name is Pipilu"}, {"output": "Hello, my name is Luxixi"})
memory.load_memory_variables({})
```

{'history': 'Human: Hello, my name is Pipilu\nAI: Hello, my name is Luxixi'}

Continue to add new content

```python
memory.save_context({"input": "Nice to be friends with you!"}, {"output": "Yes, let's go on an adventure together!"})
memory.load_memory_variables({})
```

{'history': 'Human: Hello, my name is Pipilu\nAI: Hello, my name is Luxixi\nHuman: Nice to be friends with you! \nAI: Yes, let's go on an adventure together! '}

You can see that the conversation history is saved!

When we use a large language model for a chat conversation, the large language model itself is actually stateless.The model itself does not remember the historical conversation so far**. Each call to the API node is independent. Storage (Memory) can store all the terms or conversations so far and input or attach context to the LLM to generate output. It looks like it remembers what was said before when it is in the next round of conversation.

## 2. Conversation Cache Window Storage

As the conversation becomes longer and longer, the amount of memory required becomes very long. The cost of sending a large number of tokens to the LLM will also become more expensive, which is why the API call fee is usually charged based on the number of tokens it needs to process.

In response to the above problems, LangChain also provides several convenient storage methods to save historical conversations. Among them, the conversation cache window storage only retains a window size of conversation. It only uses the most recent n interactions. This can be used to keep a sliding window of recent interactions so that the buffer is not too large.

### 2.1 Add two rounds of conversation to the window storage

Let's first try to use `ConversationBufferWindowMemory` to implement the sliding window of interaction, and set `k=1`, indicating that only one conversation memory is retained. Next, we manually add two rounds of conversations to the window storage, and then view the stored conversations.

```python
from langchain.memory importt ConversationBufferWindowMemory

# k=1 means only one conversation memory is kept
memory = ConversationBufferWindowMemory(k=1)
memory.save_context({"input": "Hello, my name is Pipilu"}, {"output": "Hello, my name is Luxixi"})
memory.save_context({"input": "Nice to be your friend!"}, {"output": "Yes, let's go on an adventure together!"})
memory.load_memory_variables({})
```

{'history': 'Human: Nice to be your friend! \nAI: Yes, let's go on an adventure together! '}

From the result, we can see that there is only the last round of chat records in the window storage.

### 2.2 Applying window storage in the conversation chain

Next, let's take a look at how to use `ConversationBufferWindowMemory` in `ConversationChain`!

```python
llm = ChatOpenAI(temperature=0.0)
memory = ConversationBufferWindowMemory(k=1)
conversation = ConversationChain(llm=llm, memory=memory, verbose=False )

print("First round of conversation:")
print(conversation.predict(input="Hello, my name is Pipilu"))

print("Second round of conversation:")
print(conversation.predict(input="1+1 equals how much?""))

print("Third round of conversation:")
print(conversation.predict(input="What is my name?""))
```

First round of conversation:
Hello, Pipilu! Nice to meet you. I am an AI assistant that can answer your questions and provide help. Is there anything I can do for you?
Second round of conversation:
1+1 equals 2.
Third round of conversation:
Sorry, I can't know your name.

Pay attention here! Since a window of memory is used here, only one round of historical messages can be saved. Therefore, the AI ​​cannot know the name you mentioned in the first round of conversation. It can onlyRemember the conversation information of the previous round (round 2)

## 3. Conversation character cache storage

Using the conversation character cache memory, the memory will limit the number of tokens saved. If the number of characters exceeds the specified number, it will cut off the early part of this conversation
To keep the number of characters corresponding to the most recent communication, but not exceeding the character limit.

Add conversation to Token cache storage, limit the number of tokens, and test

```python
from langchain.llms import OpenAI
from langchain.memory import ConversationTokenBufferMemory
memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)
memory.save_context({"input": "I left Baidi in the morning among the colorful clouds,"}, {"output": "I returned to Jiangling in a day after traveling a thousand miles."})
memory.save_context({"input": "The monkeys on both sides of the river kept crying,"}, {"output": "The boat has passed through thousands of mountains."})
memory.load_memory_variables({})
```

{'history': 'AI: The boat has passed through thousands of mountains. '}

ChatGPT uses a method based on Byte Pair Encoding (BPE) for tokenization (splitting the input text into tokens). BPE is a common tokenization technology that divides the input text into smaller subword units. OpenAI has released a new open source Python library [tiktoken](https://github.com/openai/tiktoken)(https://github.com/openai/tiktoken) on its official GitHub. This library is mainly used to calculate the number of tokens. Compared with HuggingFace's tokenizer, its speed has increased several times.
For specific token calculation methods, especially the difference between Chinese characters and English words, please refer to [Zhihu article](https://www.zhihu.com/question/594159910)(https://www.zhihu.com/question/594159910).

## 4. Conversation summary cache storage

Conversation summary cache storage, **useLLM automatically summarizes the historical conversation so far** and saves it.

### 4.1 Using the conversation summary cache

We create a long string containing someone's schedule.

```python
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory

# Create a long string
schedule = "At 8 o'clock you have a meeting with your product team. \
You need to make a PPT. \
From 9 am to 12 am you need to be busy with LangChain. \
Langchain is a useful tool, so your project progresses very quickly. \
At noon, have lunch with a customer who drove in at an Italian restaurant \
Walked for more than an hour to meet you, just to learn about the latest AI. \
Make sure you bring a laptop to show the latest LLM examples."

llm = ChatOpenAI(temperature=0.0)
memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)
memory.save_context({"input": "Hello, my name is Pipilu"}, {"output": "Hello, my name is Luxixi"})
memory.save_context({"input": "Nice to be friends with you!"}, {"output": "Yes, let's go on an adventure together!"})
memory.save_context({"input": "What's your schedule today?"}, {"output": f"{schedule}"})

print(memory.load_memory_variables({})['history'])
```

System: The human introduces themselves as Pipilu and the AI ​​introduces themselves as Luxixi. They express happiness at becoming friends and decide to go on an adventure together. The human asks about the schedule for the day. The AI ​​informs them that they have a meeting with their product team at 8 o'clock and need to prepare a PowerPoint presentation. From 9 am to 12 pm, they will be busy with LangChain, a useful tool that helps their project progress quickly. At noon, they will have lunch with a customer who has driven for over an hour just to learn about the latest AI. The AI ​​advises the human to bring their laptop to showcase the latest LLM samples.

### 4.2 Based on the conversation summaryConversation chain to be cached

Based on the above conversation summary cache storage, we create a new conversation chain.

```python
conversation = ConversationChain(llm=llm, memory=memory, verbose=True)
conversation.predict(input="What kind of example is best to show?")
```

> Entering new ConversationChain chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI ​​is talkative and provides lots of specific details from its context. If the AI ​​does not know the answer to a question, it truthfully says it does not know.

Current conversation:
System: The human introduces themselves as Pipilu and the AI ​​introduces themselves as Luxixi. They express happiness at becoming friends and decide to go on an adventure together. The human asks about the schedule for the day. The AI ​​informs them that they have a meeting with their product team at 8 o'clock and need to prepare a PowerPoint presentation. From 9 am to 12 pm, they will be busy with LangChain, a useful tool that helps their project progress quickly. At noon, the AI ​​willey will have lunch with a customer who has driven for over an hour just to learn about the latest AI. The AI ​​advises the human to bring their laptop to showcase the latest LLM samples.
Human: What kind of samples are best to show?
AI:

> Finished chain.

'Showing some diverse and innovative samples may be the best choice. You can show some applications in different fields, such as natural language processing, image recognition, speech synthesis, etc. In addition, you can also show some samples with practical application value, such as intelligent customer service, intelligent recommendation, etc. In short, choosing samples that can show the power and diversity of our AI technology will leave a deep impression on customers. '

```python
print(memory.load_memory_variables({})) # Summary record updated
```

{'history': "System: The human introduces themselves as Pipilu and the AI ​​introduces themselves as Luxixi. They express happiness at becoming friends and decide to go on an adventure together. The human asks about the schedule for the day. The AI ​​informs them that they have a meeting with their product team at 8 o'clock and need to prepare a PowerPoint presentation. From 9 am to 12 pm, they will be busy with LangChain, a useful tool that helps their project progress quickly. At noon, they will have lunch with a customer who hasdriven for over an hour just to learn about the latest AI. The AI ​​advises the human to bring their laptop to showcase the latest LLM samples. The human asks what kind of samples would be best to showcase. The AI ​​suggests that showcasing diverse and innovative samples would be the best choice. They recommend demonstrating applications in different fields such as natural language processing, image recognition, and speech synthesis. Additionally, they suggest showcasing practical examples like intelligent customer service and personalized recommendations to impress the customer with the power and versatility of their AI technology."}

By comparing the last output, it is found that the summary record has been updated, and the content summary of the latest conversation has been added.

## English version tips

**1. Conversation cache storage**

```python
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory

llm = ChatOpenAI(temperature=0.0) 
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory = memory, verbose=True )

print("First round of conversation:")
conversation.predict(input="Hi, my name is Andrew")

print("Second round of conversation:")
conversation.predict(input="What is 1+1?")

print("Third round of conversation:")
conversation.predict(input="What is my name?")
```

First round of conversation:

> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI ​​is talkative and provides lots of specific details from its context. If the AI ​​does not know the answer to a question, it truthfully saysit does not know.

Current conversation:

Human: Hi, my name is Andrew
AI:

> Finished chain.
Second round of conversation:

> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI ​​is talkative and provides lots of specific details from its context. If the AI ​​does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi, my name is AndrewAI: Hello Andrew! It's nice to meet you. How can I assist you today?
Human: What is 1+1?
AI:

> Finished chain.
Third round of conversation:

> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI ​​is talkative and provides lots of specific details from its context. If the AI ​​does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: Hi, my nameis Andrew
AI: Hello Andrew! It's nice to meet you. How can I assist you today?
Human: What is 1+1?
AI: 1+1 is equal to 2.
Human: What is my name?
AI:

> Finished chain.

'Your name is Andrew.'

```python
print("View storage cache method 1:")
print(memory.buffer)

print("View storage cache method 2:")
print(memory.load_memory_variables({}))

print("Add the input and output of the specified conversation to the buffer area, and view")
memory = ConversationBufferMemory() # Create a new empty conversation buffer memory
memory.save_context({"input": "Hi"}, {"output": "What's up"}) # Add to the buffer areaAdd input and output of the specified dialogue
print(memory.buffer) # View the results of the buffer area
print(memory.load_memory_variables({}))# Load the memory variables again
print("Continue to add the input and output of the specified dialogue to the buffer area, and view")
memory.save_context({"input": "Not much, just hanging"}, {"output": "Cool"})
print(memory.buffer) # View the results of the buffer area
print(memory.load_memory_variables({}))# Load the memory variables again

```

View storage cache method 1:
Human: Hi, my name is Andrew
AI: Hello Andrew! It's nice to meet you. How can I assist you today?
Human: What is 1+1?
AI: 1+1 is equal to 2.
Human: What is my name?
AI:Your name is Andrew.
View storage cache method 2:
{'history': "Human: Hi, my name is Andrew\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\nHuman: What is 1+1?\nAI: 1+1 is equal to 2.\nHuman: What is my name?\nAI: Your name is Andrew."}
Add the input and output of the specified dialogue to the cache, and view
Human: Hi
AI: What's up
{'history': "Human: Hi\nAI: What's up"}
Continue to add the input and output of the specified dialogue to the cache, and view
Human: Hi
AI: What's up
Human: Not much, just hanging
AI: Cool
{'history': "Human: Hi\nAI: What's up\nHuman: Not much, just hanging\nAI: Cool"}

**2. Conversation Cache Window Storage**

```python
from langchain.memory import ConversationBufferWindowMemory

# k is the window parameter, k=1 means only one conversation memory is kept
memory = ConversationBufferWindowMemory(k=1)

# Add two rounds of conversation to memory
memory.save_context({"input": "Hi"}, {"output": "What's up"})
memory.save_context({"input": "Not much, just hanging"}, {"output": "Cool"})

# And view the current record of the memory variable
memory.load_memory_variables({})

llm = ChatOpenAI(temperature=0.0)
memory = ConversationBufferWindowMemory(k=1)
conversation = ConversationChain(llm=llm, memory=memory, verbose=False )

print("First round of conversation:")

print(conversation.predict(input="Hi, my name is Andrew"))

print("Second round of conversation:")

print(conversation.predict(input="What is 1+1?"))

print("Third round of conversation:")

print(conversation.predict(input="What is my name?"))
```

First round of conversation:
Hello Andrew! It's nice to meet you. How can I assist you today?
Second round of conversation:
1+1 is equal to 2.
Third round of conversation:
I'm sorry, but I don't have access to personal information.

**3. Conversation character cache storage**

```python
from langchain.llms import OpenAI
from langchain.memory import ConversationTokenBufferMemory
memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)
memory.save_context({"input": "AI is what?!"}, {"output": "Amazing!"})
memory.save_context({"input": "Backpropagation is what?"}, {"output": "Beautiful!"})
memory.save_context({"input": "Chatbots are what?"}, {"output": "Charming!"})
print(memory.load_memory_variables({}))
```

{'history': 'AI: Beautiful!\nHuman: Chatbots are what?at?\nAI: Charming!'}

**4. Conversation summary cache storage**

```python
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory

# Create a long string
schedule = "There is a meeting at 8am with your product team. \
You will need your powerpoint presentation prepared. \
9am-12pm have time to work on your LangChain \
project which will go quickly because Langchain is such a powerful tool. \
At Noon, lunch at the italian resturant with a customer who is driving \
from over an hour away to meet you to understand the latest in AI. \
Be sure to bring your laptop to show the latest LLM demo."

# Use the conversation summary cache
llm = ChatOpenAI(temperature=0.0)
memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)
memory.save_context({"input": "Hello"}, {"output": "What's up"})
memory.save_context({"input": "Not much, just hanging"}, {"output": "Cool"})
memory.save_context({"input": "What is on the schedule today?"}, {"output": f"{schedule}"})

print("View the conversation summary cache")
print(memory.load_memory_variables({})['history'])

conversation = ConversationChain(llm=llm, memory=memory, verbose=True)

print("Conversation chain based on the conversation summary cache")
conversation.predict(input="What would be a good demo to show?")

print("View the conversation summary cache again")
print(memory.load_memory_variables({})['history'])
```

View the conversation summary cache
System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI ​​provides a detailed schedule, including a meeting with theproduct team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI ​​emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.
Dialogue chain based on dialogue summary cache storage

> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI ​​is talkative and provides lots of specific details from its context. If the AI ​​does not know the answer to a question, it truthfully says it does not know.

Current conversation:
System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI ​​provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI ​​emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting.
Human: What would be a good demo to show?
AI:> Finished chain.
View conversation summary cache storage again
System: The human and AI exchange greetings and discuss the schedule for the day. The AI ​​provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI ​​emphasizes the importance of bringing a laptop to showcase the latest LLM demo during the lunch meeting. The human asks what would be a good demo to show, and the AI ​​suggests showcasing the latest LLM (Language Model) demo. The LLM is a cutting-edge AI model that can generate human-like text based on a given prompt. It has been trained on a vast amount of data and can generate coherent and contextually relevant responses. By showcasing the LLM demo, the AI ​​can demonstrate the capabilities of their AI technology and how it can be applied to various industries and use cases.