{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Teacher network and student network\n",
    "from nets.resnet import resnet32x4, resnet8x4\n",
    "# Loss function of knowledge distillation KD\n",
    "from loss.dkd import dkd_loss\n",
    "\n",
    "#TensorBoard\n",
    "Train_Info = \"DKD : Res32x4 To Res8x4\"\n",
    "writer = SummaryWriter(comment=Train_Info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random number seed so that it can be reproduced\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU, replace the number in '0' with the gpu number on your device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 4               # temperature : 知识蒸馏中的温度\n",
    "ALPHA = 1.0         # alpha : TCKD 部分的loss weight\n",
    "BETA = 8.0          # beta : NCKD 部分的loss weight\n",
    "LOSS_CE = 1.0       # loss_ce : 交叉熵的loss weight\n",
    "N = 100             # num_classes : 类别数\n",
    "EPOCH = 20          # epoch : 训练轮数\n",
    "BATCH_SIZE = 128    # batch_size : 批处理大小 \n",
    "LR = 0.05           # learning_rate : 初试学习率\n",
    "\n",
    "# Other hyperparameters, such as momentum, weight_decay, milestones, gamma, etc. in the optimizer are generally rarely changed.\n",
    "# When EPOCH changes, milestones should also change accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the teacher model and define the student network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res32x4 = resnet32x4(num_classes=N)\n",
    "ckpt = torch.load(\"checkpoints/teacher/ckpt_epoch_240.pth\", map_location='cpu')\n",
    "res32x4.load_state_dict(ckpt[\"model\"])\n",
    "res32x4 = nn.DataParallel(res32x4).cuda()\n",
    "res32x4.eval()\n",
    "\n",
    "res8x4 = resnet8x4(num_classes=N)\n",
    "res8x4 = torch.nn.DataParallel(res8x4).cuda()\n",
    "\n",
    "teacher_net = res32x4\n",
    "student_net = res8x4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "The first time you use it, it will be downloaded first. If the download is slow, you can manually download the dataset and drag it into the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset and preprocess it\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # 先四周填充0，在把图像随机裁剪成32*32\n",
    "    transforms.RandomHorizontalFlip(),     # 图像一半的概率翻转，一半的概率不翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)), #R,G,B每层的归一化用到的均值和方差\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "# Training dataset\n",
    "# num_workers generally depends on the performance of the cpu\n",
    "trainset = torchvision.datasets.CIFAR100(root=DATA_PATH, train=True, download=True, transform=transform_train) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)   \n",
    "# Test dataset\n",
    "testset = torchvision.datasets.CIFAR100(root=DATA_PATH, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Generally, in various papers, the `epoch` on the cifar-100 dataset is set to `240`\n",
    "\n",
    "The corresponding `milestones` value is `[150, 180, 210]`\n",
    "\n",
    "Here, for demonstration only, all `epoch` are set to `40`, and the `milestones` value is `[15, 25, 35]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(student_net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 25, 35], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function and Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best Acc on the training set and test set respectively, use global to modify it as a global variable, and then update it during training\n",
    "best_train_acc = 0\n",
    "best_test_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(epoch):\n",
    "    global best_train_acc\n",
    "\n",
    "# Set the student model to training mode\n",
    "    student_net.train()\n",
    "\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "# Wrap trainloader with tqdm to display a progress bar\n",
    "    with tqdm(trainloader, desc=f\"Training Epoch {epoch}\", total=len(trainloader)) as pbar:\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits_student, _ = student_net(inputs)\n",
    "            with torch.no_grad():\n",
    "                logits_teacher, _ = teacher_net(inputs)\n",
    "\n",
    "# Hard loss\n",
    "            ce_loss = nn.CrossEntropyLoss()(logits_student, targets)\n",
    "# Soft loss\n",
    "            kd_loss = loss(logits_student, logits_teacher, temperature=T)\n",
    "            total_loss = ALPHA * ce_loss + BETA * kd_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += total_loss.item()\n",
    "            _, predicted = logits_student.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "# Update TensorBoard\n",
    "            writer.add_scalar('Train/Accuracy', 100. * correct / total, batch_idx + (epoch - 1) * 782)\n",
    "            writer.add_scalar('Train/Loss', total_loss.item(), batch_idx + (epoch - 1) * 782)\n",
    "\n",
    "# Use set_postfix to update the suffix of the progress bar\n",
    "            pbar.set_postfix(loss=train_loss / (batch_idx + 1), acc=f\"{100. * correct / total:.1f}%\")\n",
    "\n",
    "# If the accuracy on the current training set is higher than best_test_acc, update best_test_acc\n",
    "    acc = 100 * correct / total\n",
    "    if acc > best_train_acc:\n",
    "        best_train_acc = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, epoch):\n",
    "    global best_test_acc\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "# Use tqdm to wrap testloader to display a progress bar\n",
    "        with tqdm(testloader, desc=f\"Testing Epoch {epoch}\", total=len(testloader)) as pbar:\n",
    "            for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                logits_student, _ = net(inputs)\n",
    "\n",
    "                loss = nn.CrossEntropyLoss()(logits_student, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = logits_student.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "# Display current loss and accuracy in the suffix of tqdm progress bar\n",
    "                pbar.set_postfix(loss=test_loss / (batch_idx + 1), acc=f\"{100. * correct / total:.1f}%\")\n",
    "\n",
    "# Update TensorBoard\n",
    "                writer.add_scalar('Test/Accuracy', 100. * correct / total, batch_idx + (epoch - 1) * 157)\n",
    "                writer.add_scalar('Test/Loss', loss.item(), batch_idx + (epoch - 1) * 157)\n",
    "\n",
    "# Calculate the accuracy on the current test set\n",
    "        acc = 100. * correct / total\n",
    "\n",
    "# If the accuracy on the current test set is higher than best_test_acc, update best_test_acc\n",
    "# And save the student model\n",
    "        if acc > best_test_acc:\n",
    "            print('Saving..')\n",
    "            torch.save(student_net, 'checkpoints/student/kd_res8x4.pth')\n",
    "            best_test_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCH + 1) :\n",
    "    train(epoch)\n",
    "    test(student_net, epoch)\n",
    "\n",
    "# Update learning rate\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
